,Unnamed: 0,id,code,label
72,72,7ff203be36e439b535894764c37a8446351627ec,"lib/Shine/Commands/Base/Command.py/n/n# Command.py -- Base command class
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Support.Debug import Debug

from CommandRCDefs import *

import getopt


#
# Command exceptions are defined in Shine.Command.Exceptions
#

class Command(object):
    """"""
    The base class for command objects that can be added to the commands
    registry.
    """"""
    def __init__(self):
        self.options = {}
        self.getopt_string = """"
        self.params_desc = """"
        self.last_optional = 0
        self.arguments = None

        # All commands have debug support.
        self.debug_support = Debug(self)

    def is_hidden(self):
        """"""Return whether the command should not be displayed to user.""""""
        return False
    
    def get_name(self):
        raise NotImplementedError(""Derived classes must implement."")

    def get_desc(self):
        return ""Undocumented""

    def get_params_desc(self):
        pdesc = self.params_desc.strip()
        if self.has_subcommand():
            return ""%s %s"" % ('|'.join(self.get_subcommands()), pdesc)
        return pdesc

    def has_subcommand(self):
        """"""Return whether the command supports subcommand(s).""""""
        return False

    def get_subcommands(self):
        """"""Return the list of subcommand(s).""""""
        raise NotImplementedError(""Derived classes must implement."")
    
    def add_option(self, flag, arg, attr, cb=None):
        """"""
        Add an option for getopt with optional argument.
        """"""
        assert flag not in self.options

        optional = attr.get('optional', False)
        hidden = attr.get('hidden', False)

        if cb:
            self.options[flag] = cb

        object.__setattr__(self, ""opt_%s"" % flag, None)
            
        self.getopt_string += flag
        if optional:
            leftmark = '['
            rightmark = ']'
        else:
            leftmark = ''
            rightmark = ''

        if arg:
            self.getopt_string += "":""
            if not hidden:
                self.params_desc += ""%s-%s <%s>%s "" % (leftmark,
                    flag, arg, rightmark)
                self.last_optional = 0
        elif not hidden:
            if self.last_optional == 0:
                self.params_desc += ""%s-%s%s "" % (leftmark, flag, rightmark)
            else:
                self.params_desc = self.params_desc[:-2] + ""%s%s "" % (flag,
                    rightmark)
            
            if optional:
                self.last_optional = 1
            else:
                self.last_optional = 2

    def parse(self, args):
        """"""
        Parse command arguments.
        """"""
        options, arguments = getopt.gnu_getopt(args, self.getopt_string)
        self.arguments = arguments

        for opt, arg in options:
            trim_opt = opt[1:]
            callback = self.options.get(trim_opt)
            if callback:
                callback(trim_opt, arg)
            object.__setattr__(self, ""opt_%s"" % trim_opt, arg or True)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation.
        
        Return True when the user confirms the action, False otherwise.
        """"""
        i = raw_input(""%s (y)es/(N)o: "" % prompt)
        return i == 'y' or i == 'Y'


    def filter_rc(self, rc):
        """"""
        Allow derived classes to filter return codes.
        """"""
        # default is to not filter return code
        return rc

/n/n/nlib/Shine/Commands/Base/RemoteCommand.py/n/n# RemoteCommand.py -- Base command with remote capabilities
# Copyright (C) 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *
from Command import Command
from CommandRCDefs import *
from RemoteCallEventHandler import RemoteCallEventHandler
from Support.Nodes import Nodes
from Support.Yes import Yes

import socket


class RemoteCommand(Command):
    
    def __init__(self):
        Command.__init__(self)
        self.remote_call = False
        self.local_flag = False
        attr = { 'optional' : True, 'hidden' : True }
        self.add_option('L', None, attr, cb=self.parse_L)
        self.add_option('R', None, attr, cb=self.parse_R)
        self.nodes_support = Nodes(self)
        self.eventhandler = None

    def parse_L(self, opt, arg):
        self.local_flag = True

    def parse_R(self, opt, arg):
        self.remote_call = True

    def has_local_flag(self):
        return self.local_flag or self.remote_call

    def init_execute(self):
        """"""
        Initialize execution of remote command, if needed. Should be called
        first from derived classes before really executing the command.
        """"""
        # Limit the scope of the command if called with local flag (-L) or
        # called remotely (-R).
        if self.has_local_flag():
            self.opt_n = socket.gethostname().split('.', 1)[0]

    def install_eventhandler(self, local_eventhandler, global_eventhandler):
        """"""
        Select and install the appropriate event handler.
        """"""
        if self.remote_call:
            # When called remotely (-R), install a special event handler
            # that knows how to speak the Shine Proxy Protocol using pickle.
            self.eventhandler = RemoteCallEventHandler()
        elif self.local_flag:
            self.eventhandler = local_eventhandler
        else:
            self.eventhandler = global_eventhandler
        # return handler for convenience
        return self.eventhandler

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation. Overrides Command.ask_confirm to
        avoid confirmation when called remotely (-R).

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.remote_call or Command.ask_confirm(self, prompt)

    def filter_rc(self, rc):
        """"""
        When called remotely, return code are not used to handle shine action
        success or failure, nor for status info. To properly detect ssh or remote
        shine installation failures, we filter the return code here.
        """"""
        if self.remote_call:
            # Only errors of type RUNTIME ERROR are allowed to go up.
            rc &= RC_FLAG_RUNTIME_ERROR

        return Command.filter_rc(self, rc)


class RemoteCriticalCommand(RemoteCommand):

    def __init__(self):
        RemoteCommand.__init__(self)
        self.yes_support = Yes(self)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation if -y not specified.

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.yes_support.has_yes() or RemoteCommand.ask_confirm(self, prompt)

/n/n/nlib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        rc = command.execute()

        # Filter rc
        return command.filter_rc(rc)

/n/n/nlib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.CommandRCDefs import *
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes

from Exceptions import *

class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            raise CommandHelpException(""Lustre model file path (-m <model_file>) argument required."", self)
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return RC_OK

/n/n/nlib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Starting %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if not self.remote_call:
                if rc == RC_OK:
                    if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                        print ""Mount successful on %s"" % m_nodes
                elif rc == RC_RUNTIME_ERROR:
                    for nodes, msg in fs.proxy_errors:
                        print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.CommandRCDefs import *
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError %s"" % ex
            return RC_RUNTIME_ERROR

        return RC_OK
/n/n/nlib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

        return result
/n/n/nlib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)
            if not statusdict:
                continue

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if not self.remote_call and vlevel > 0:
                if view == ""fs"":
                    self.status_view_fs(fs)
                elif view.startswith(""target""):
                    self.status_view_targets(fs)
                elif view.startswith(""disk""):
                    self.status_view_disks(fs)
                else:
                    raise CommandBadParameterError(self.view_support.get_view(),
                            ""fs, targets, disks"")

        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/nlib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Stopping %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                    print ""Unmount successful on %s"" % m_nodes
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Configuration/FileSystem.py/n/n# FileSystem.py -- Lustre file system configuration
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$


from Globals import Globals
from Model import Model
from Exceptions import *
from TuningModel import TuningModel

from ClusterShell.NodeSet import NodeSet

from NidMap import NidMap
from TargetDevice import TargetDevice

import copy
import os
import sys


class FileSystem(Model):
    """"""
    Lustre File System Configuration class.
    """"""
    def __init__(self, fs_name=None, lmf=None, tuning_file=None):
        """""" Initialize File System config
        """"""
        self.backend = None

        globals = Globals()

        fs_conf_dir = os.path.expandvars(globals.get_conf_dir())
        fs_conf_dir = os.path.normpath(fs_conf_dir)

        # Load the file system from model or extended model
        if not fs_name and lmf:
            Model.__init__(self, lmf)

            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, self.get_one('fs_name'))

            self._setup_target_devices()

            # Reload
            self.set_filename(self.xmf_path)

        elif fs_name:
            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, fs_name)
            Model.__init__(self, self.xmf_path)

        self._setup_nid_map(self.get_one('nid_map'))

        self.fs_name = self.get_one('fs_name')
        
        # Initialize the tuning model to None if no special tuning configuration
        # is provided
        self.tuning_model = None
        
        if tuning_file:
            # It a tuning configuration file is provided load it
            self.tuning_model = TuningModel(tuning_file)
        else:
            self.tuning_model = TuningModel()

        #self._start_backend()

    def _start_backend(self):
        """"""
        Load and start backend subsystem once
        """"""
        if not self.backend:

            from Backend.BackendRegistry import BackendRegistry
            from Backend.Backend import Backend

            # Start the selected config backend system.
            self.backend = BackendRegistry().get_selected()
            if self.backend:
                self.backend.start()

        return self.backend

    def _setup_target_devices(self):
        """""" Generate the eXtended Model File XMF
        """"""
        self._start_backend()

        for target in [ 'mgt', 'mdt', 'ost' ]:

            if self.backend:

                # Returns a list of TargetDevices
                candidates = copy.copy(self.backend.get_target_devices(target))

                try:
                    # Save the model target selection
                    target_models = copy.copy(self.get(target))
                except KeyError, e:
                    raise ConfigException(""No %s target found"" %(target))

                # Delete it (to be replaced... see below)
                self.delete(target)
                 
                # Iterates on ModelDevices
                i = 0
                for target_model in target_models:
                    result = target_model.match_device(candidates)
                    if len(result) == 0 and not target == 'mgt' :
                        raise ConfigDeviceNotFoundError(target_model)
                    for matching in result:
                        candidates.remove(matching)
                        #
                        # target index is now mandatory in XMF files
                        if not matching.has_index():
                            matching.add_index(i)
                            i += 1

                        # `matching' is a TargetDevice, we want to add it to the
                        # underlying Model object. The current way to do this to
                        # create a configuration line string (performed by
                        # TargetDevice.getline()) and then call Model.add(). 
                        # TODO: add methods to Model/ModelDevice to avoid the use
                        #       of temporary configuration string line.
                        self.add(target, matching.getline())
            else:
                # no backend support

                devices = copy.copy(self.get_with_dict(target))

                self.delete(target)

                target_devices = []
                i = 0
                for dict in devices:
                    t = TargetDevice(target, dict)
                    if not t.has_index():
                        t.add_index(i)
                        i += 1
                    target_devices.append(TargetDevice(target, dict))
                    self.add(target, t.getline())

                if len(target_devices) == 0:
                    raise ConfigDeviceNotFoundError(self)




        # Save XMF
        self.save(self.xmf_path, ""Shine Lustre file system config file for %s"" % \
                self.get_one('fs_name'))
            
    def _setup_nid_map(self, maps):
        """"""
        Set self.nid_map using the NidMap helper class
        """"""
        #self.nid_map = NidMap().fromlist(maps)
        self.nid_map = NidMap(maps.get_one('nodes'), maps.get_one('nids'))

    def get_nid(self, node):
        try:
            return self.nid_map[node]
        except KeyError:
            raise ConfigException(""Cannot get NID for %s, aborting. Please verify `nid_map' configuration."" % node)

    def __str__(self):
        return "">> BACKEND:\n%s\n>> MODEL:\n%s"" % (self.backend, Model.__str__(self))

    def close(self):
        if self.backend:
            self.backend.stop()
            self.backend = None
    
    def register_client(self, node):
        """"""
        This function aims to register a new client that will be able to mount the
        file system.
        Parameters:
        @type node: string
        @param node : is the new client node name
        """"""
        if self._start_backend():
            self.backend.register_client(self.fs_name, node)
        
    def unregister_client(self, node):
        """"""
        This function aims to unregister a client of this  file system
        Parameters:
        @type node: string
        @param node : is name of the client node to unregister
        """"""
        if self._start_backend():
            self.backend.unregister_client(self.fs_name, node)
    
    def set_status_client_mount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                    self.backend.MOUNT_COMPLETE, options)

    def set_status_client_mount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_FAILED, options)

    def set_status_client_mount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_WARNING, options)

    def set_status_client_umount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_COMPLETE, options)

    def set_status_client_umount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_FAILED, options)

    def set_status_client_umount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_WARNING, options)

    def get_status_clients(self):
        if self._start_backend():
            return self.backend.get_status_clients(self.fs_name)

    def set_status_target_unknown(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNKNOWN
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, node, 
                self.backend.TARGET_UNKNOWN, options)

    def set_status_target_ko(self, target, options):
        """"""
        This function is used to set the specified target status
        to KO
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                backend.TARGET_KO, options)

    def set_status_target_available(self, target, options):
        """"""
        This function is used to set the specified target status
        to AVAILABLE
        """"""
        if self._start_backend():
            # Set the fs_name to Free since these targets are availble
            # which means not used by any file system.
            self.backend.set_status_target(None, target,
                self.backend.TARGET_AVAILABLE, options)

    def set_status_target_formating(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATING, options)

    def set_status_target_format_failed(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMAT_FAILED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMAT_FAILED, options)

    def set_status_target_formated(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATED, options)

    def set_status_target_offline(self, target, options):
        """"""
        This function is used to set the specified target status
        to OFFLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_OFFLINE, options)

    def set_status_target_starting(self, target, options):
        """"""
        This function is used to set the specified target status
        to STARTING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STARTING, options)

    def set_status_target_online(self, target, options):
        """"""
        This function is used to set the specified target status
        to ONLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_ONLINE, options)

    def set_status_target_critical(self, target, options):
        """"""
        This function is used to set the specified target status
        to CRITICAL
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_CRITICAL, options)

    def set_status_target_stopping(self, target, options):
        """"""
        This function is used to set the specified target status
        to STOPPING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STOPPING, options)

    def set_status_target_unreachable(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNREACHABLE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_UNREACHABLE, options)

    def get_status_targets(self):
        """"""
        This function returns the status of each targets
        involved in the current file system.
        """"""
        if self._start_backend():
            return self.backend.get_status_targets(self.fs_name)

    def register(self):
        """"""
        This function aims to register the file system configuration
        to the backend.
        """"""
        if self._start_backend():
            return self.backend.register_fs(self)

    def unregister(self):
        """"""
        This function aims to remove a file system configuration from
        the backend.        
        """"""
        result = 0
        if self._start_backend():
            result = self.backend.unregister_fs(self)

        if not result:
            os.unlink(self.xmf_path)

        return result
/n/n/nlib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
        except KeyError:
            raise
        
        return RC_RUNTIME_ERROR


/n/n/nlib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # some common remote errors:
            # rc 127 = command not found
            # rc 126 = found but not executable
            # rc 1 = python failure...
            if rc != 0:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/nlib/Shine/Lustre/FileSystem.py/n/n# FileSystem.py -- Lustre FS
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Lustre FileSystem class.

Represents a Lustre FS.
""""""

import copy
from sets import Set
import socket

from ClusterShell.NodeSet import NodeSet, RangeSet

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

# Action exceptions
from Actions.Action import ActionErrorException
from Actions.Proxies.ProxyAction import *

from Actions.Install import Install
from Actions.Proxies.Preinstall import Preinstall
from Actions.Proxies.FSProxyAction import FSProxyAction
from Actions.Proxies.FSClientProxyAction import FSClientProxyAction

from EventHandler import *
from Client import *
from Server import *
from Target import *


class FSException(Exception):
    def __init__(self, message):
        self.message = message
    def __str__(self):
        return self.message

class FSError(FSException):
    """"""
    Base FileSystem error exception.
    """"""

class FSSyntaxError(FSError):
    def __init__(self, message):
        self.message = ""Syntax error: \""%s\"""" % (message)
    def __str__(self):
        return self.message

class FSBadTargetError(FSSyntaxError):
    def __init__(self, target_name):
        self.message = ""Syntax error: unrecognized target \""%s\"""" % (target_name)

class FSStructureError(FSError):
    """"""
    Lustre file system structure error, raised after an invalid configuration
    is encountered. For example, you will get this error if you try to assign
    two targets `MGT' to a filesystem.
    """"""

class FSRemoteError(FSError):
    """"""
    Remote host(s) not available, or a remote operation failed.
    """"""
    def __init__(self, nodes, rc, message):
        FSError.__init__(self, message)
        self.nodes = nodes
        self.rc = int(rc)

    def __str__(self):
        return ""%s: %s [rc=%d]"" % (self.nodes, self.message, self.rc)


STATUS_SERVERS      = 0x01
STATUS_HASERVERS    = 0x02
STATUS_CLIENTS      = 0x10
STATUS_ANY          = 0xff


class FileSystem:
    """"""
    The Lustre FileSystem abstract class.
    """"""

    def __init__(self, fs_name, event_handler=None):
        self.fs_name = fs_name
        self.debug = False
        self.set_eventhandler(event_handler)
        self.proxy_errors = []

        self.local_hostname = socket.gethostname()
        self.local_hostname_short = self.local_hostname.split('.', 1)[0]

        # file system MGT
        self.mgt = None

        # All FS server targets (MGT, MDT, OST...)
        self.targets = []

        # All FS clients
        self.clients = []

        # filled after successful install
        self.mgt_servers = NodeSet()
        self.mgt_count = 0

        self.mdt_servers = NodeSet()
        self.mdt_count = 0

        self.ost_servers = NodeSet()
        self.ost_count = 0

        self.target_count = 0
        self.target_servers = NodeSet()

    def set_debug(self, debug):
        self.debug = debug

    #
    # file system event handling
    #

    def _invoke_event(self, event, **kwargs):
        if 'target' in kwargs or 'client' in kwargs:
            kwargs.setdefault('node', None)
        getattr(self.event_handler, event)(**kwargs)

    def _invoke_dummy(self, event, **kwargs):
        pass

    def set_eventhandler(self, event_handler):
        self.event_handler = event_handler
        if self.event_handler is None:
            self._invoke = self._invoke_dummy
        else:
            self._invoke = self._invoke_event

    def _handle_shine_event(self, event, node, **params):
        #print ""_handle_shine_event %s %s"" % (event, params)
        target = params.get('target')
        if target:
            found = False
            for t in self.targets:
                if t.match(target):
                    # perform sanity checks here
                    old_nids = t.get_nids()
                    if old_nids != target.get_nids():
                        print ""NIDs mismatch %s -> %s"" % \
                                (','.join(old.nids), ','.join(target.get_nids))
                    # update target from remote one
                    t.update(target)
                    # substitute target parameter by local one
                    params['target'] = t
                    found = True
            if not found:
                print ""Target Update FAILED (%s)"" % target
        
        client = params.get('client')
        if client:
            found = False
            for c in self.clients:
                if c.match(client):
                    # update client from remote one
                    c.update(client)
                    # substitute client parameter
                    params['client'] = c
                    found = True
            if not found:
                print ""Client Update FAILED (%s)"" % client

        self._invoke(event, node=node, **params)

    def _handle_shine_proxy_error(self, nodes, message):
        self.proxy_errors.append((NodeSet(nodes), message))

    #
    # file system construction
    #

    def _attach_target(self, target):
        self.targets.append(target)
        if target.type == 'mgt':
            self.mgt = target
        self._update_structure()

    def _attach_client(self, client):
        self.clients.append(client)
        self._update_structure()

    def new_target(self, server, type, index, dev, jdev=None, group=None,
            tag=None, enabled=True):
        """"""
        Create a new attached target.
        """"""
        #print ""new_target on %s type %s (enabled=%s)"" % (server, type, enabled)

        if type == 'mgt' and self.mgt and len(self.mgt.get_nids()) > 0:
            raise FSStructureError(""A Lustre FS has only one MGT."")

        # Instantiate matching target class (eg. 'ost' -> OST).
        target = getattr(sys.modules[self.__class__.__module__], type.upper())(fs=self,
                server=server, index=index, dev=dev, jdev=jdev, group=group, tag=tag,
                enabled=enabled)
        
        return target

    def new_client(self, server, mount_path, enabled=True):
        """"""
        Create a new attached client.
        """"""
        client = Client(self, server, mount_path, enabled)

        return client

    def get_mgs_nids(self):
        return self.mgt.get_nids()
    
    def get_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients])

    def get_enabled_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients if c.action_enabled])

    def get_enabled_target_servers(self):
        return NodeSet.fromlist([t.server for t in self.targets if t.action_enabled])

    def get_client_statecounters(self):
        """"""
        Get (ignored, offline, error, runtime_error, mounted) client state counters tuple.
        """"""
        ignored = 0
        states = {}
        for client in self.clients:
            if client.action_enabled:
                state = states.setdefault(client.state, 0)
                states[client.state] = state + 1
            else:
                ignored += 1
        
        return (ignored,
                states.get(OFFLINE, 0),
                states.get(CLIENT_ERROR, 0),
                states.get(RUNTIME_ERROR, 0),
                states.get(MOUNTED, 0))

    def targets_by_state(self, state):
        for target in self.targets:
            #print target, target.state
            if target.action_enabled and target.state == state:
                yield target

    def target_servers_by_state(self, state):
        servers = NodeSet()
        for target in self.targets_by_state(state):
            #print ""OK %s"" % target
            servers.add(target.servers[0])
        return servers

    def _distant_action_by_server(self, action_class, servers, **kwargs):

        task = task_self()

        # filter local server
        if self.local_hostname in servers:
            distant_servers = servers.difference(self.local_hostname)
        elif self.local_hostname_short in servers:
            distant_servers = servers.difference(self.local_hostname_short)
        else:
            distant_servers = servers

        # perform action on distant servers
        if len(distant_servers) > 0:
            action = action_class(nodes=distant_servers, fs=self, **kwargs)
            action.launch()
            task.resume()

    def install(self, fs_config_file, nodes=None):
        """"""
        Install FS config files.
        """"""
        servers = NodeSet()

        for target in self.targets:
            # install on failover partners too
            for s in target.servers:
                if not nodes or s in nodes:
                    servers.add(s)

        for client in self.clients:
            # install on failover partners too
            if not nodes or client.server in nodes:
                servers.add(client.server)

        assert len(servers) > 0, ""no servers?""

        try:
            self._distant_action_by_server(Preinstall, servers)
            self._distant_action_by_server(Install, servers, config_file=fs_config_file)
        except ProxyActionError, e:
            # switch to public exception
            raise FSRemoteError(e.nodes, e.rc, e.message)
        
    def remove(self):
        """"""
        Remove FS config files.
        """"""

        result = 0

        servers = NodeSet()

        self.action_refcnt = 0
        self.proxy_errors = []

        # iterate over lustre servers
        for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
            if not e_s_targets:
                continue

            if server.is_local():
                # remove local fs configuration file
                conf_dir_path = Globals().get_conf_dir()
                fs_file = os.path.join(Globals().get_conf_dir(), ""%s.xmf"" % self.fs_name)
                rc = os.unlink(fs_file)
                result = max(result, rc)
            else:
                servers.add(server)

        if len(servers) > 0:
            # Perform the remove operations on all targets for these nodes.
            action = FSProxyAction(self, 'remove', servers, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR
        
        return result

    def _update_structure(self):
        # convenience
        for type, targets, servers in self._iter_targets_servers_by_type():
            if type == 'ost':
                self.ost_count = len(targets)
                self.ost_servers = NodeSet(servers)
            elif type == 'mdt':
                self.mdt_count = len(targets)
                self.mdt_servers = NodeSet(servers)
            elif type == 'mgt':
                self.mgt_count = len(targets)
                self.mgt_servers = NodeSet(servers)

        self.target_count = self.mgt_count + self.mdt_count + self.ost_count
        self.target_servers = self.mgt_servers | self.mdt_servers | self.ost_servers

    def _iter_targets_servers_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns a tuple (list of targets,
        list of servers) per target type.
        """"""
        last_target_type = None
        servers = NodeSet()
        targets = Set()

        #self.targets.sort()

        if reverse:
            self.targets.reverse()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(targets) > 0:
                    yield last_target_type, targets, servers
                    servers.clear()     # ClusterShell 1.1+ needed (sorry)
                    targets.clear()

            if target.action_enabled:
                targets.add(target)
                # select server: change master_server for -F node
                servers.add(target.get_selected_server())
            last_target_type = target.type

        if len(targets) > 0:
            yield last_target_type, targets, servers

    def targets_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns the following tuple:
        (type, (list of all targets of this type, list of enabled targets))
        per target type.
        """"""
        last_target_type = None
        a_targets = Set()
        e_targets = Set()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(a_targets) > 0:
                    yield last_target_type, (a_targets, e_targets)
                    a_targets.clear()
                    e_targets.clear()

            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)
            last_target_type = target.type

        if len(a_targets) > 0:
            yield last_target_type, (a_targets, e_targets)

    def _iter_targets_by_server(self):
        """"""
        Per server of target iterator : returns the following tuple:
        (server, (list of all server targets, list of enabled targets))
        per target server.
        """"""
        servers = {}
        for target in self.targets:
            a_targets, e_targets = servers.setdefault(target.get_selected_server(), (Set(), Set()))
            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)

        return servers.iteritems()


    def _iter_type_idx_for_targets(self, targets):
        last_target_type = None

        indexes = RangeSet(autostep=3)

        #self.targets.sort()

        for target in targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(indexes) > 0:
                    yield last_target_type, indexes
                    indexes.clear()     # CS 1.1+
            indexes.add(int(target.index))
            last_target_type = target.type

        if len(indexes) > 0:
            yield last_target_type, indexes

    def format(self, **kwargs):

        # Remember format launched, so we can check their status once
        # all operations are done.
        format_launched = Set()

        servers_formatall = NodeSet()

        self.proxy_errors = []
        self.action_refcnt = 0

        for server, (a_targets, e_targets) in self._iter_targets_by_server():

            if server.is_local():
                # local server
                for target in e_targets:
                    target.format(**kwargs)
                    self.action_refcnt += 1

                format_launched.update(e_targets)

            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action if ""format all targets on this server""
                    # is detected
                    servers_formatall.add(server)
                else:
                    # otherwise, format per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'format',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

                format_launched.update(e_targets)

        if len(servers_formatall) > 0:
            action = FSProxyAction(self, 'format', servers_formatall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check.
        for target in format_launched:
            if target.state != OFFLINE:
                return target.state

        return OFFLINE

    def status(self, flags=STATUS_ANY):
        """"""
        Get status of filesystem.
        """"""

        status_target_launched = Set()
        status_client_launched = Set()
        servers_statusall = NodeSet()
        self.action_refcnt = 0
        self.proxy_errors = []

        # prepare servers status checks
        if flags & STATUS_SERVERS:
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                if len(e_s_targets) == 0:
                    continue

                if server.is_local():
                    for target in e_s_targets:
                        target.status()
                        self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)
                else:
                    # distant server: check if all server targets have been selected
                    if len(a_s_targets) == len(e_s_targets):
                        # ""status on all targets for this server"" detected
                        servers_statusall.add(server)
                    else:
                        # status per selected targets on this server
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(e_s_targets):
                            action = FSProxyAction(self, 'status',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)

        # prepare clients status checks
        if flags & STATUS_CLIENTS:
            for client in self.clients:
                if client.action_enabled:
                    server = client.server
                    if server.is_local():
                        client.status()
                        self.action_refcnt += 1
                    elif server not in servers_statusall:
                        servers_statusall.add(server)
                    status_client_launched.add(client)

        # launch distant actions
        if len(servers_statusall) > 0:
            action = FSProxyAction(self, 'status', servers_statusall, self.debug)
            action.launch()
            self.action_refcnt += 1

        # run loop
        task_self().resume()
        
        # return a dict of {state : target list}
        rdict = {}

        # all launched targets+clients
        launched = (status_target_launched | status_client_launched)
        if self.proxy_errors:
            # find targets/clients affected by the runtime error(s)
            for target in launched:
                for nodes, msg in self.proxy_errors:
                    if target.server in nodes:
                        target.state = RUNTIME_ERROR

        for target in launched:
            if target.state == None:
                print target, target.server
            assert target.state != None
            targets = rdict.setdefault(target.state, [])
            targets.append(target)
        return rdict

    def status_target(self, target):
        """"""
        Launch a status request for a specific local or remote target.
        """"""

        # Don't call me if the target itself is not enabled.
        assert target.action_enabled

        server = target.get_selected_server()

        if server.is_local():
            # Target is local
            target.status()
        else:
            action = FSProxyAction(self, 'status', NodeSet(server), self.debug,
                    target.type, RangeSet(str(target.index)))
            action.launch()

        self.action_refcnt = 1
        task_self().resume()

    def start(self, **kwargs):
        """"""
        Start Lustre file system servers.
        """"""
        self.proxy_errors = []

        # What starting order to use?
        for target in self.targets:
            if isinstance(target, MDT) and target.action_enabled:
                # Found enabled MDT: perform writeconf check.
                self.status_target(target)
                if target.has_first_time_flag() or target.has_writeconf_flag():
                    # first_time or writeconf flag found, start MDT before OSTs
                    MDT.target_order = 2 # change MDT class variable order

        self.targets.sort()

        # servers_startall is used for optimization, it contains nodes
        # where we have to perform the start operation on all targets
        # found for this FS. This will limit the number of FSProxyAction
        # to spawn.
        servers_startall = NodeSet()

        # Remember targets launched, so we can check their status once
        # all operations are done (here, status are checked after all
        # targets of the same type have completed the start operation -
        # with possible failure).
        targets_launched = Set()

        # Keep number of actions in order to abort task correctly in
        # action's ev_close.
        self.action_refcnt = 0

        result = 0

        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():
            
            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():

                # To summary, we keep targets that are:
                # 1. enabled
                # 2. of according type
                # 3. on this server
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Start targets if we are on the good server.
                    for target in type_e_targets:
                        # Note that target.start() should never block here:
                        # it will perform necessary non-blocking actions and
                        # (when needed) will start local ClusterShell workers.
                        target.start(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""start all FS targets on this server"" detected
                        servers_startall.add(server)
                    else:
                        # Start per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'start',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched targets of this server for late status check.
                targets_launched.update(type_e_targets)

            if len(servers_startall) > 0:
                # Perform the start operations on all targets for these nodes.
                action = FSProxyAction(self, 'start', servers_startall, self.debug)
                action.launch()
                self.action_refcnt += 1

            # Resume current task, ie. start runloop, process workers events
            # and also act as a target-type barrier.
            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_launched:
                if target.state > result:
                    result = target.state
                    if result > RECOVERING:
                        # Avoid broken cascading starts, so we break now if
                        # a target of the previous type failed to start.
                        return result

            # Some needed cleanup before next target type.
            servers_startall.clear()
            targets_launched.clear()

        return result


    def stop(self, **kwargs):
        """"""
        Stop file system.
        """"""
        rc = MOUNTED

        # Stop: reverse order
        self.targets.sort()
        self.targets.reverse()

        # servers_stopall is used for optimization, see the comment in
        # start() for servers_startall.
        servers_stopall = NodeSet()

        # Remember targets when stop was launched.
        targets_stopping = Set()

        self.action_refcnt = 0
        self.proxy_errors = []

        # We use a similar logic than start(): see start() for comments.
        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():

            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Stop targets if we are on the good server.
                    for target in type_e_targets:
                        target.stop(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""stop all FS targets on this server"" detected
                        servers_stopall.add(server)
                    else:
                        # Stop per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'stop',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched stopping targets of this server for late status check.
                targets_stopping.update(type_e_targets)

            if len(servers_stopall) > 0:
                # Perform the stop operations on all targets for these nodes.
                action = FSProxyAction(self, 'stop', servers_stopall, self.debug)
                action.launch()
                self.action_refcnt += 1

            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_stopping:
                if target.state > rc:
                    rc = target.state

            # Some needed cleanup before next target type.
            servers_stopall.clear()
            targets_stopping.clear()

        return rc

    def mount(self, **kwargs):
        """"""
        Mount FS clients.
        """"""
        servers_mountall = NodeSet()
        clients_mounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.start(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_mountall.add(client.server)

            clients_mounting.add(client)

        if len(servers_mountall) > 0:
            action = FSClientProxyAction(self, 'mount', servers_mountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_mounting:
            if client.state != MOUNTED:
                return client.state

        return MOUNTED

    def umount(self, **kwargs):
        """"""
        Unmount FS clients.
        """"""
        servers_umountall = NodeSet()
        clients_umounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.stop(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_umountall.add(client.server)

            clients_umounting.add(client)

        if len(servers_umountall) > 0:
            action = FSClientProxyAction(self, 'umount', servers_umountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_umounting:
            if client.state != OFFLINE:
                return client.state

        return OFFLINE

    def info(self):
        pass

    def tune(self, tuning_model):
        """"""
        Tune server.
        """"""
        task = task_self()
        tune_all = NodeSet()
        type_map = { 'mgt': 'mgs', 'mdt': 'mds', 'ost' : 'oss' }
        self.action_refcnt = 0
        self.proxy_errors = []
        result = 0

        # Install tuning.conf on enabled distant servers
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if e_targets and not server.is_local():
                tune_all.add(server)
        if len(tune_all) > 0:
            self._distant_action_by_server(Install, tune_all, config_file=Globals().get_tuning_file())
            self.action_refcnt += 1
            task.resume()
            tune_all.clear()

        # Apply tunings
        self.action_refcnt = 0
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if not e_targets:
                continue
            if server.is_local():
                types = Set()
                for t in e_targets:
                    types.add(type_map[t.type])

                rc = server.tune(tuning_model, types, self.fs_name)
                result = max(result, rc)
            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action
                    tune_all.add(server)
                else:
                    # otherwise, tune per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'tune',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

        if len(tune_all) > 0:
            action = FSProxyAction(self, 'tune', tune_all, self.debug)
            action.launch()
            self.action_refcnt += 1

        task.resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        return result

/n/n/n",0
73,73,7ff203be36e439b535894764c37a8446351627ec,"/lib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()

/n/n/n/lib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            print ""Bad argument""
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return 0

/n/n/n/lib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Mount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError""
            raise

/n/n/n/lib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc
/n/n/n/lib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == ""fs"":
                self.status_view_fs(fs)
            elif view.startswith(""target""):
                self.status_view_targets(fs)
            elif view.startswith(""disk""):
                self.status_view_disks(fs)
            else:
                raise CommandBadParameterError(self.view_support.get_view(),
                        ""fs, targets, disks"")
        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/n/lib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Unmount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
            return RC_USER_ERROR
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
            return RC_RUNTIME_ERROR
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except KeyError:
            print ""Error - Unrecognized action""
            print
            raise
        
        return 1


/n/n/n/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/n",1
24,24,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
25,25,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
26,26,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
27,27,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
30,30,329c0a8ae6fde575a7d9077f1013fa4a86112d0c,"flex/core.py/n/nfrom __future__ import unicode_literals

from six.moves import urllib_parse as urlparse
import os
import collections
import requests

import six
import json
import yaml

from flex.context_managers import ErrorDict
from flex.exceptions import ValidationError
from flex.loading.definitions import (
    definitions_validator,
)
from flex.loading.schema import (
    swagger_schema_validator,
)
from flex.loading.schema.paths.path_item.operation.responses.single.schema import (
    schema_validator,
)
from flex.http import (
    normalize_request,
    normalize_response,
)
from flex.validation.common import validate_object
from flex.validation.request import validate_request
from flex.validation.response import validate_response


def load_source(source):
    """"""
    Common entry point for loading some form of raw swagger schema.

    Supports:
        - python object (dictionary-like)
        - path to yaml file
        - path to json file
        - file object (json or yaml).
        - json string.
        - yaml string.
    """"""
    if isinstance(source, collections.Mapping):
        return source
    elif hasattr(source, 'read') and callable(source.read):
        raw_source = source.read()
    elif os.path.exists(os.path.expanduser(str(source))):
        with open(os.path.expanduser(str(source)), 'r') as source_file:
            raw_source = source_file.read()
    elif isinstance(source, six.string_types):
        parts = urlparse.urlparse(source)
        if parts.scheme and parts.netloc:
            response = requests.get(source)
            if isinstance(response.content, six.binary_type):
                raw_source = six.text_type(response.content, encoding='utf-8')
            else:
                raw_source = response.content
        else:
            raw_source = source

    try:
        try:
            return json.loads(raw_source)
        except ValueError:
            pass

        try:
            return yaml.safe_load(raw_source)
        except (yaml.scanner.ScannerError, yaml.parser.ParserError):
            pass
    except NameError:
        pass

    raise ValueError(
        ""Unable to parse `{0}`.  Tried yaml and json."".format(source),
    )


def parse(raw_schema):
    context = {
        'deferred_references': set(),
    }
    swagger_definitions = definitions_validator(raw_schema, context=context)

    swagger_schema = swagger_schema_validator(
        raw_schema,
        context=swagger_definitions,
    )
    return swagger_schema


def load(target):
    """"""
    Given one of the supported target formats, load a swagger schema into it's
    python representation.
    """"""
    raw_schema = load_source(target)
    return parse(raw_schema)


def validate(raw_schema, target=None, **kwargs):
    """"""
    Given the python representation of a JSONschema as defined in the swagger
    spec, validate that the schema complies to spec.  If `target` is provided,
    that target will be validated against the provided schema.
    """"""
    schema = schema_validator(raw_schema, **kwargs)
    if target is not None:
        validate_object(target, schema=schema, **kwargs)


def validate_api_request(schema, raw_request):
    request = normalize_request(raw_request)

    with ErrorDict():
        validate_request(request=request, schema=schema)


def validate_api_response(schema, raw_response, request_method='get', raw_request=None):
    """"""
    Validate the response of an api call against a swagger schema.
    """"""
    request = None
    if raw_request is not None:
        request = normalize_request(raw_request)

    response = None
    if raw_response is not None:
        response = normalize_response(raw_response, request=request)

    if response is not None:
        validate_response(
            response=response,
            request_method=request_method,
            schema=schema
        )


def validate_api_call(schema, raw_request, raw_response):
    """"""
    Validate the request/response cycle of an api call against a swagger
    schema.  Request/Response objects from the `requests` and `urllib` library
    are supported.
    """"""
    request = normalize_request(raw_request)

    with ErrorDict() as errors:
        try:
            validate_request(
                request=request,
                schema=schema,
            )
        except ValidationError as err:
            errors['request'].add_error(err.messages or getattr(err, 'detail'))
            return

        response = normalize_response(raw_response, raw_request)

        try:
            validate_response(
                response=response,
                request_method=request.method,
                schema=schema
            )
        except ValidationError as err:
            errors['response'].add_error(err.messages or getattr(err, 'detail'))
/n/n/ntests/core/test_load_source.py/n/nfrom __future__ import unicode_literals

import tempfile
import collections

import six

import json
import yaml

from flex.core import load_source


def test_native_mapping_is_passthrough():
    source = {'foo': 'bar'}
    result = load_source(source)

    assert result == source


def test_json_string():
    native = {'foo': 'bar'}
    source = json.dumps(native)
    result = load_source(source)

    assert result == native


def test_yaml_string():
    native = {b'foo': b'bar'}
    source = yaml.dump(native)
    result = load_source(source)

    assert result == native


def test_json_file_object():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.file.seek(0)

    with open(tmp_file.name) as json_file:
        result = load_source(json_file)

    assert result == native


def test_json_file_path():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_yaml_file_object():
    native = {b'foo': b'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.flush()

    with open(tmp_file.name) as yaml_file:
        result = load_source(yaml_file)

    assert result == native


def test_yaml_file_path():
    native = {b'foo': b'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_url(httpbin):
    native = {
        'origin': '127.0.0.1',
        #'headers': {
        #    'Content-Length': '',
        #    'Accept-Encoding': 'gzip, deflate',
        #    'Host': '127.0.0.1:54634',
        #    'Accept': '*/*',
        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',
        #    'Connection': 'keep-alive',
        #},
        'args': {},
        #'url': 'http://127.0.0.1:54634/get',
    }
    source = httpbin.url + '/get'
    result = load_source(source)
    assert isinstance(result, collections.Mapping)
    result.pop('headers')
    result.pop('url')
    assert result == native
/n/n/n",0
31,31,329c0a8ae6fde575a7d9077f1013fa4a86112d0c,"/flex/core.py/n/nfrom __future__ import unicode_literals

from six.moves import urllib_parse as urlparse
import os
import collections
import requests

import six
import json
import yaml

from flex.context_managers import ErrorDict
from flex.exceptions import ValidationError
from flex.loading.definitions import (
    definitions_validator,
)
from flex.loading.schema import (
    swagger_schema_validator,
)
from flex.loading.schema.paths.path_item.operation.responses.single.schema import (
    schema_validator,
)
from flex.http import (
    normalize_request,
    normalize_response,
)
from flex.validation.common import validate_object
from flex.validation.request import validate_request
from flex.validation.response import validate_response


def load_source(source):
    """"""
    Common entry point for loading some form of raw swagger schema.

    Supports:
        - python object (dictionary-like)
        - path to yaml file
        - path to json file
        - file object (json or yaml).
        - json string.
        - yaml string.
    """"""
    if isinstance(source, collections.Mapping):
        return source
    elif hasattr(source, 'read') and callable(source.read):
        raw_source = source.read()
    elif os.path.exists(os.path.expanduser(str(source))):
        with open(os.path.expanduser(str(source)), 'r') as source_file:
            raw_source = source_file.read()
    elif isinstance(source, six.string_types):
        parts = urlparse.urlparse(source)
        if parts.scheme and parts.netloc:
            response = requests.get(source)
            if isinstance(response.content, six.binary_type):
                raw_source = six.text_type(response.content, encoding='utf-8')
            else:
                raw_source = response.content
        else:
            raw_source = source

    try:
        try:
            return json.loads(raw_source)
        except ValueError:
            pass

        try:
            return yaml.load(raw_source)
        except (yaml.scanner.ScannerError, yaml.parser.ParserError):
            pass
    except NameError:
        pass

    raise ValueError(
        ""Unable to parse `{0}`.  Tried yaml and json."".format(source),
    )


def parse(raw_schema):
    context = {
        'deferred_references': set(),
    }
    swagger_definitions = definitions_validator(raw_schema, context=context)

    swagger_schema = swagger_schema_validator(
        raw_schema,
        context=swagger_definitions,
    )
    return swagger_schema


def load(target):
    """"""
    Given one of the supported target formats, load a swagger schema into it's
    python representation.
    """"""
    raw_schema = load_source(target)
    return parse(raw_schema)


def validate(raw_schema, target=None, **kwargs):
    """"""
    Given the python representation of a JSONschema as defined in the swagger
    spec, validate that the schema complies to spec.  If `target` is provided,
    that target will be validated against the provided schema.
    """"""
    schema = schema_validator(raw_schema, **kwargs)
    if target is not None:
        validate_object(target, schema=schema, **kwargs)


def validate_api_request(schema, raw_request):
    request = normalize_request(raw_request)

    with ErrorDict():
        validate_request(request=request, schema=schema)


def validate_api_response(schema, raw_response, request_method='get', raw_request=None):
    """"""
    Validate the response of an api call against a swagger schema.
    """"""
    request = None
    if raw_request is not None:
        request = normalize_request(raw_request)

    response = None
    if raw_response is not None:
        response = normalize_response(raw_response, request=request)

    if response is not None:
        validate_response(
            response=response,
            request_method=request_method,
            schema=schema
        )


def validate_api_call(schema, raw_request, raw_response):
    """"""
    Validate the request/response cycle of an api call against a swagger
    schema.  Request/Response objects from the `requests` and `urllib` library
    are supported.
    """"""
    request = normalize_request(raw_request)

    with ErrorDict() as errors:
        try:
            validate_request(
                request=request,
                schema=schema,
            )
        except ValidationError as err:
            errors['request'].add_error(err.messages or getattr(err, 'detail'))
            return

        response = normalize_response(raw_response, raw_request)

        try:
            validate_response(
                response=response,
                request_method=request.method,
                schema=schema
            )
        except ValidationError as err:
            errors['response'].add_error(err.messages or getattr(err, 'detail'))
/n/n/n/tests/core/test_load_source.py/n/nfrom __future__ import unicode_literals

import tempfile
import collections

import six

import json
import yaml

from flex.core import load_source


def test_native_mapping_is_passthrough():
    source = {'foo': 'bar'}
    result = load_source(source)

    assert result == source


def test_json_string():
    native = {'foo': 'bar'}
    source = json.dumps(native)
    result = load_source(source)

    assert result == native


def test_yaml_string():
    native = {'foo': 'bar'}
    source = yaml.dump(native)
    result = load_source(source)

    assert result == native


def test_json_file_object():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.file.seek(0)

    with open(tmp_file.name) as json_file:
        result = load_source(json_file)

    assert result == native


def test_json_file_path():
    native = {'foo': 'bar'}
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_yaml_file_object():
    native = {'foo': 'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.flush()

    with open(tmp_file.name) as yaml_file:
        result = load_source(yaml_file)

    assert result == native


def test_yaml_file_path():
    native = {'foo': 'bar'}
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_url(httpbin):
    native = {
        'origin': '127.0.0.1',
        #'headers': {
        #    'Content-Length': '',
        #    'Accept-Encoding': 'gzip, deflate',
        #    'Host': '127.0.0.1:54634',
        #    'Accept': '*/*',
        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',
        #    'Connection': 'keep-alive',
        #},
        'args': {},
        #'url': 'http://127.0.0.1:54634/get',
    }
    source = httpbin.url + '/get'
    result = load_source(source)
    assert isinstance(result, collections.Mapping)
    result.pop('headers')
    result.pop('url')
    assert result == native
/n/n/n",1
40,40,ee20e7e1058d24191320e54f444a5f7c22adb1e8,"azurelinuxagent/common/event.py/n/n# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import atexit
import datetime
import json
import os
import sys
import time
import traceback

from datetime import datetime

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger

from azurelinuxagent.common.exception import EventError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.protocol.restapi import TelemetryEventParam, \
    TelemetryEvent, \
    get_properties
from azurelinuxagent.common.utils import textutil
from azurelinuxagent.common.version import CURRENT_VERSION

_EVENT_MSG = ""Event: name={0}, op={1}, message={2}, duration={3}""


class WALAEventOperation:
    ActivateResourceDisk = ""ActivateResourceDisk""
    AgentBlacklisted = ""AgentBlacklisted""
    AgentEnabled = ""AgentEnabled""
    AutoUpdate = ""AutoUpdate""
    CustomData = ""CustomData""
    Deploy = ""Deploy""
    Disable = ""Disable""
    Downgrade = ""Downgrade""
    Download = ""Download""
    Enable = ""Enable""
    ExtensionProcessing = ""ExtensionProcessing""
    Firewall = ""Firewall""
    GetArtifactExtended = ""GetArtifactExtended""
    HealthCheck = ""HealthCheck""
    HeartBeat = ""HeartBeat""
    HostPlugin = ""HostPlugin""
    HostPluginHeartbeat = ""HostPluginHeartbeat""
    HttpErrors = ""HttpErrors""
    ImdsHeartbeat = ""ImdsHeartbeat""
    Install = ""Install""
    InitializeCGroups = ""InitializeCGroups""
    InitializeHostPlugin = ""InitializeHostPlugin""
    Log = ""Log""
    Partition = ""Partition""
    ProcessGoalState = ""ProcessGoalState""
    Provision = ""Provision""
    ProvisionGuestAgent = ""ProvisionGuestAgent""
    RemoteAccessHandling = ""RemoteAccessHandling""
    ReportStatus = ""ReportStatus""
    ReportStatusExtended = ""ReportStatusExtended""
    Restart = ""Restart""
    SkipUpdate = ""SkipUpdate""
    UnhandledError = ""UnhandledError""
    UnInstall = ""UnInstall""
    Unknown = ""Unknown""
    Upgrade = ""Upgrade""
    Update = ""Update""


SHOULD_ENCODE_MESSAGE_LEN = 80
SHOULD_ENCODE_MESSAGE_OP = [
    WALAEventOperation.Disable,
    WALAEventOperation.Enable,
    WALAEventOperation.Install,
    WALAEventOperation.UnInstall,
]

class EventStatus(object):
    EVENT_STATUS_FILE = ""event_status.json""

    def __init__(self):
        self._path = None
        self._status = {}

    def clear(self):
        self._status = {}
        self._save()

    def event_marked(self, name, version, op):
        return self._event_name(name, version, op) in self._status

    def event_succeeded(self, name, version, op):
        event = self._event_name(name, version, op)
        if event not in self._status:
            return True
        return self._status[event] is True

    def initialize(self, status_dir=conf.get_lib_dir()):
        self._path = os.path.join(status_dir, EventStatus.EVENT_STATUS_FILE)
        self._load()

    def mark_event_status(self, name, version, op, status):
        event = self._event_name(name, version, op)
        self._status[event] = (status is True)
        self._save()

    def _event_name(self, name, version, op):
        return ""{0}-{1}-{2}"".format(name, version, op)

    def _load(self):
        try:
            self._status = {}
            if os.path.isfile(self._path):
                with open(self._path, 'r') as f:
                    self._status = json.load(f)
        except Exception as e:
            logger.warn(""Exception occurred loading event status: {0}"".format(e))
            self._status = {}

    def _save(self):
        try:
            with open(self._path, 'w') as f:
                json.dump(self._status, f)
        except Exception as e:
            logger.warn(""Exception occurred saving event status: {0}"".format(e))


__event_status__ = EventStatus()
__event_status_operations__ = [
        WALAEventOperation.AutoUpdate,
        WALAEventOperation.ReportStatus
    ]


def _encode_message(op, message):
    """"""
    Gzip and base64 encode a message based on the operation.

    The intent of this message is to make the logs human readable and include the
    stdout/stderr from extension operations.  Extension operations tend to generate
    a lot of noise, which makes it difficult to parse the line-oriented waagent.log.
    The compromise is to encode the stdout/stderr so we preserve the data and do
    not destroy the line oriented nature.

    The data can be recovered using the following command:

      $ echo '<encoded data>' | base64 -d | pigz -zd

    You may need to install the pigz command.

    :param op: Operation, e.g. Enable or Install
    :param message: Message to encode
    :return: gzip'ed and base64 encoded message, or the original message
    """"""

    if len(message) == 0:
        return message

    if op not in SHOULD_ENCODE_MESSAGE_OP:
        return message

    try:
        return textutil.compress(message)
    except Exception:
        # If the message could not be encoded a dummy message ('<>') is returned.
        # The original message was still sent via telemetry, so all is not lost.
        return ""<>""


def _log_event(name, op, message, duration, is_success=True):
    global _EVENT_MSG

    message = _encode_message(op, message)
    if not is_success:
        logger.error(_EVENT_MSG, name, op, message, duration)
    else:
        logger.info(_EVENT_MSG, name, op, message, duration)


class EventLogger(object):
    def __init__(self):
        self.event_dir = None
        self.periodic_events = {}

    def save_event(self, data):
        if self.event_dir is None:
            logger.warn(""Cannot save event -- Event reporter is not initialized."")
            return

        if not os.path.exists(self.event_dir):
            os.mkdir(self.event_dir)
            os.chmod(self.event_dir, 0o700)

        existing_events = os.listdir(self.event_dir)
        if len(existing_events) >= 1000:
            existing_events.sort()
            oldest_files = existing_events[:-999]
            logger.warn(""Too many files under: {0}, removing oldest"".format(self.event_dir))
            try:
                for f in oldest_files:
                    os.remove(os.path.join(self.event_dir, f))
            except IOError as e:
                raise EventError(e)

        filename = os.path.join(self.event_dir,
                                ustr(int(time.time() * 1000000)))
        try:
            with open(filename + "".tmp"", 'wb+') as hfile:
                hfile.write(data.encode(""utf-8""))
            os.rename(filename + "".tmp"", filename + "".tld"")
        except IOError as e:
            raise EventError(""Failed to write events to file:{0}"", e)

    def reset_periodic(self):
        self.periodic_events = {}

    def is_period_elapsed(self, delta, h):
        return h not in self.periodic_events or \
            (self.periodic_events[h] + delta) <= datetime.now()

    def add_periodic(self,
        delta, name, op=WALAEventOperation.Unknown, is_success=True, duration=0,
        version=CURRENT_VERSION, message="""", evt_type="""",
        is_internal=False, log_event=True, force=False):

        h = hash(name+op+ustr(is_success)+message)
        
        if force or self.is_period_elapsed(delta, h):
            self.add_event(name,
                op=op, is_success=is_success, duration=duration,
                version=version, message=message, evt_type=evt_type,
                is_internal=is_internal, log_event=log_event)
            self.periodic_events[h] = datetime.now()

    def add_event(self,
                  name,
                  op=WALAEventOperation.Unknown,
                  is_success=True,
                  duration=0,
                  version=CURRENT_VERSION,
                  message="""",
                  evt_type="""",
                  is_internal=False,
                  log_event=True):

        if not is_success or log_event:
            _log_event(name, op, message, duration, is_success=is_success)

        self._add_event(duration, evt_type, is_internal, is_success, message, name, op, version, eventId=1)
        self._add_event(duration, evt_type, is_internal, is_success, message, name, op, version, eventId=6)

    def _add_event(self, duration, evt_type, is_internal, is_success, message, name, op, version, eventId):
        event = TelemetryEvent(eventId, ""69B669B9-4AF8-4C50-BDC4-6006FA76E975"")
        event.parameters.append(TelemetryEventParam('Name', name))
        event.parameters.append(TelemetryEventParam('Version', str(version)))
        event.parameters.append(TelemetryEventParam('IsInternal', is_internal))
        event.parameters.append(TelemetryEventParam('Operation', op))
        event.parameters.append(TelemetryEventParam('OperationSuccess',
                                                    is_success))
        event.parameters.append(TelemetryEventParam('Message', message))
        event.parameters.append(TelemetryEventParam('Duration', duration))
        event.parameters.append(TelemetryEventParam('ExtensionType', evt_type))

        data = get_properties(event)
        try:
            self.save_event(json.dumps(data))
        except EventError as e:
            logger.error(""{0}"", e)

    def add_log_event(self, level, message):
        # By the time the message has gotten to this point it is formatted as
        #
        #   YYYY/MM/DD HH:mm:ss.fffffff LEVEL <text>.
        #
        # The timestamp and the level are redundant, and should be stripped.
        # The logging library does not schematize this data, so I am forced
        # to parse the message.  The format is regular, so the burden is low.

        parts = message.split(' ', 3)
        msg = parts[3] if len(parts) == 4 \
            else message

        event = TelemetryEvent(7, ""FFF0196F-EE4C-4EAF-9AA5-776F622DEB4F"")
        event.parameters.append(TelemetryEventParam('EventName', WALAEventOperation.Log))
        event.parameters.append(TelemetryEventParam('CapabilityUsed', logger.LogLevel.STRINGS[level]))
        event.parameters.append(TelemetryEventParam('Context1', msg))
        event.parameters.append(TelemetryEventParam('Context2', ''))
        event.parameters.append(TelemetryEventParam('Context3', ''))

        data = get_properties(event)
        try:
            self.save_event(json.dumps(data))
        except EventError:
            pass

    def add_metric(self, category, counter, instance, value, log_event=False):
        """"""
        Create and save an event which contains a telemetry event.

        :param str category: The category of metric (e.g. ""cpu"", ""memory"")
        :param str counter: The specific metric within the category (e.g. ""%idle"")
        :param str instance: For instanced metrics, the instance identifier (filesystem name, cpu core#, etc.)
        :param value: Value of the metric
        :param bool log_event: If true, log the collected metric in the agent log
        """"""
        if log_event:
            from azurelinuxagent.common.version import AGENT_NAME
            message = ""Metric {0}/{1} [{2}] = {3}"".format(category, counter, instance, value)
            _log_event(AGENT_NAME, ""METRIC"", message, 0)

        event = TelemetryEvent(4, ""69B669B9-4AF8-4C50-BDC4-6006FA76E975"")
        event.parameters.append(TelemetryEventParam('Category', category))
        event.parameters.append(TelemetryEventParam('Counter', counter))
        event.parameters.append(TelemetryEventParam('Instance', instance))
        event.parameters.append(TelemetryEventParam('Value', value))

        data = get_properties(event)
        try:
            self.save_event(json.dumps(data))
        except EventError as e:
            logger.error(""{0}"", e)


__event_logger__ = EventLogger()


def elapsed_milliseconds(utc_start):
    now = datetime.utcnow()
    if now < utc_start:
        return 0

    d = now - utc_start
    return int(((d.days * 24 * 60 * 60 + d.seconds) * 1000) + \
                    (d.microseconds / 1000.0))


def report_event(op, is_success=True, message=''):
    from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION
    add_event(AGENT_NAME,
              version=CURRENT_VERSION,
              is_success=is_success,
              message=message,
              op=op)


def report_periodic(delta, op, is_success=True, message=''):
    from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION
    add_periodic(delta, AGENT_NAME,
              version=CURRENT_VERSION,
              is_success=is_success,
              message=message,
              op=op)


def report_metric(category, counter, instance, value, log_event=False, reporter=__event_logger__):
    """"""
    Send a telemetry event reporting a single instance of a performance counter.
    :param str category: The category of the metric (cpu, memory, etc)
    :param str counter: The name of the metric (""%idle"", etc)
    :param str instance: For instanced metrics, the identifier of the instance. E.g. a disk drive name, a cpu core#
    :param     value: The value of the metric
    :param bool log_event: If True, log the metric in the agent log as well
    :param EventLogger reporter: The EventLogger instance to which metric events should be sent
    """"""
    if reporter.event_dir is None:
        from azurelinuxagent.common.version import AGENT_NAME
        logger.warn(""Cannot report metric event -- Event reporter is not initialized."")
        message = ""Metric {0}/{1} [{2}] = {3}"".format(category, counter, instance, value)
        _log_event(AGENT_NAME, ""METRIC"", message, 0)
        return
    reporter.add_metric(category, counter, instance, value, log_event)


def add_event(name, op=WALAEventOperation.Unknown, is_success=True, duration=0,
              version=CURRENT_VERSION,
              message="""", evt_type="""", is_internal=False, log_event=True,
              reporter=__event_logger__):
    if reporter.event_dir is None:
        logger.warn(""Cannot add event -- Event reporter is not initialized."")
        _log_event(name, op, message, duration, is_success=is_success)
        return

    if should_emit_event(name, version, op, is_success):
        mark_event_status(name, version, op, is_success)
        reporter.add_event(
            name, op=op, is_success=is_success, duration=duration,
            version=str(version), message=message, evt_type=evt_type,
            is_internal=is_internal, log_event=log_event)


def add_log_event(level, message, reporter=__event_logger__):
    if reporter.event_dir is None:
        return

    reporter.add_log_event(level, message)


def add_periodic(
    delta, name, op=WALAEventOperation.Unknown, is_success=True, duration=0,
    version=CURRENT_VERSION,
    message="""", evt_type="""", is_internal=False, log_event=True, force=False,
    reporter=__event_logger__):
    if reporter.event_dir is None:
        logger.warn(""Cannot add periodic event -- Event reporter is not initialized."")
        _log_event(name, op, message, duration, is_success=is_success)
        return

    reporter.add_periodic(
        delta, name, op=op, is_success=is_success, duration=duration,
        version=str(version), message=message, evt_type=evt_type,
        is_internal=is_internal, log_event=log_event, force=force)


def mark_event_status(name, version, op, status):
    if op in __event_status_operations__:
        __event_status__.mark_event_status(name, version, op, status)


def should_emit_event(name, version, op, status):
    return \
        op not in __event_status_operations__ or \
        __event_status__ is None or \
        not __event_status__.event_marked(name, version, op) or \
        __event_status__.event_succeeded(name, version, op) != status


def init_event_logger(event_dir):
    __event_logger__.event_dir = event_dir


def init_event_status(status_dir):
    __event_status__.initialize(status_dir)


def dump_unhandled_err(name):
    if hasattr(sys, 'last_type') and hasattr(sys, 'last_value') and \
            hasattr(sys, 'last_traceback'):
        last_type = getattr(sys, 'last_type')
        last_value = getattr(sys, 'last_value')
        last_traceback = getattr(sys, 'last_traceback')
        error = traceback.format_exception(last_type, last_value,
                                           last_traceback)
        message = """".join(error)
        add_event(name, is_success=False, message=message,
                  op=WALAEventOperation.UnhandledError)


def enable_unhandled_err_dump(name):
    atexit.register(dump_unhandled_err, name)
/n/n/nazurelinuxagent/common/osutil/alpine.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class AlpineOSUtil(DefaultOSUtil):

    def __init__(self):
        super(AlpineOSUtil, self).__init__()
        self.agent_conf_file_path = '/etc/waagent.conf'
        self.jit_enabled = True

    def is_dhcp_enabled(self):
        return True

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output('pidof dhcpcd', chk_err=False)
        if ret[0] == 0:
            logger.info('dhcpcd is pid {}'.format(ret[1]))
            return ret[1].strip()
        return None

    def restart_if(self, ifname):
        logger.info('restarting {} (sort of, actually SIGHUPing dhcpcd)'.format(ifname))
        pid = self.get_dhcp_pid()
        if pid != None:
            ret = shellutil.run_get_output('kill -HUP {}'.format(pid))

    def set_ssh_client_alive_interval(self):
        # Alpine will handle this.
        pass

    def conf_sshd(self, disable_password):
        # Alpine will handle this.
        pass
/n/n/nazurelinuxagent/common/osutil/arch.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class ArchUtil(DefaultOSUtil):
    def __init__(self):
        super(ArchUtil, self).__init__()
        self.jit_enabled = True
    
    def is_dhcp_enabled(self):
        return True

    def start_network(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def restart_if(self, iface):
        shellutil.run(""systemctl restart systemd-networkd"")

    def restart_ssh_service(self):
        # SSH is socket activated on CoreOS.  No need to restart it.
        pass

    def stop_dhcp_service(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret= shellutil.run_get_output(""pidof systemd-networkd"")
        return ret[1] if ret[0] == 0 else None

    def conf_sshd(self, disable_password):
        # Don't whack the system default sshd conf
        pass/n/n/nazurelinuxagent/common/osutil/bigip.py/n/n# Copyright 2016 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import fcntl
import os
import platform
import re
import socket
import struct
import time

try:
    # WAAgent > 2.1.3
    import azurelinuxagent.common.logger as logger
    import azurelinuxagent.common.utils.shellutil as shellutil

    from azurelinuxagent.common.exception import OSUtilError
    from azurelinuxagent.common.osutil.default import DefaultOSUtil
except ImportError:
    # WAAgent <= 2.1.3
    import azurelinuxagent.logger as logger
    import azurelinuxagent.utils.shellutil as shellutil

    from azurelinuxagent.exception import OSUtilError
    from azurelinuxagent.distro.default.osutil import DefaultOSUtil


class BigIpOSUtil(DefaultOSUtil):

    def __init__(self):
        super(BigIpOSUtil, self).__init__()

    def _wait_until_mcpd_is_initialized(self):
        """"""Wait for mcpd to become available

        All configuration happens in mcpd so we need to wait that this is
        available before we go provisioning the system. I call this method
        at the first opportunity I have (during the DVD mounting call).
        This ensures that the rest of the provisioning does not need to wait
        for mcpd to be available unless it absolutely wants to.

        :return bool: Returns True upon success
        :raises OSUtilError: Raises exception if mcpd does not come up within
                             roughly 50 minutes (100 * 30 seconds)
        """"""
        for retries in range(1, 100):
            # Retry until mcpd completes startup:
            logger.info(""Checking to see if mcpd is up"")
            rc = shellutil.run(""/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running"", chk_err=False)
            if rc == 0:
                logger.info(""mcpd is up!"")
                break
            time.sleep(30)

        if rc is 0:
            return True

        raise OSUtilError(
            ""mcpd hasn't completed initialization! Cannot proceed!""
        )

    def _save_sys_config(self):
        cmd = ""/usr/bin/tmsh save sys config""
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""WARNING: Cannot save sys config on 1st boot."")
        return rc

    def restart_ssh_service(self):
        return shellutil.run(""/usr/bin/bigstart restart sshd"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --add waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --del waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""/sbin/pidof dhclient"")
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """"""Set the static hostname of the device

        Normally, tmsh is used to set the hostname for the system. For our
        purposes at this time though, I would hesitate to trust this function.

        Azure(Stack) uses the name that you provide in the Web UI or ARM (for
        example) as the value of the hostname argument to this method. The
        problem is that there is nowhere in the UI that specifies the
        restrictions and checks that tmsh has for the hostname.

        For example, if you set the name ""bigip1"" in the Web UI, Azure(Stack)
        considers that a perfectly valid name. When WAAgent gets around to
        running though, tmsh will reject that value because it is not a fully
        qualified domain name. The proper value should have been bigip.xxx.yyy

        WAAgent will not fail if this command fails, but the hostname will not
        be what the user set either. Currently we do not set the hostname when
        WAAgent starts up, so I am passing on setting it here too.

        :param hostname: The hostname to set on the device
        """"""
        return None

    def set_dhcp_hostname(self, hostname):
        """"""Sets the DHCP hostname

        See `set_hostname` for an explanation of why I pass here

        :param hostname: The hostname to set on the device
        """"""
        return None

    def useradd(self, username, expiration=None, comment=None):
        """"""Create user account using tmsh

        Our policy is to create two accounts when booting a BIG-IP instance.
        The first account is the one that the user specified when they did
        the instance creation. The second one is the admin account that is,
        or should be, built in to the system.

        :param username: The username that you want to add to the system
        :param expiration: The expiration date to use. We do not use this
                           value.
        :param comment: description of the account.  We do not use this value.
        """"""
        if self.get_userentry(username):
            logger.info(""User {0} already exists, skip useradd"", username)
            return None

        cmd = ""/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash"" % (username)
        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)
        if retcode != 0:
            raise OSUtilError(
                ""Failed to create user account:{0}, retcode:{1}, output:{2}"".format(username, retcode, out)
            )
        self._save_sys_config()
        return retcode

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        """"""Change a user's password with tmsh

        Since we are creating the user specified account and additionally
        changing the password of the built-in 'admin' account, both must
        be modified in this method.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        :param username: The username whose password to change
        :param password: The unencrypted password to set for the user
        :param crypt_id: If encrypting the password, the crypt_id that was used
        :param salt_len: If encrypting the password, the length of the salt
                         value used to do it.
        """"""

        # Start by setting the password of the user provided account
        cmd = ""/usr/bin/tmsh modify auth user {0} password '{1}'"".format(username, password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for {0}: {1}"".format(username, output)
            )

        # Next, set the password of the built-in 'admin' account to be have
        # the same password as the user provided account
        userentry = self.get_userentry('admin')
        if userentry is None:
            raise OSUtilError(""The 'admin' user account was not found!"")

        cmd = ""/usr/bin/tmsh modify auth user 'admin' password '{0}'"".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for 'admin': {0}"".format(output)
            )
        self._save_sys_config()
        return ret

    def del_account(self, username):
        """"""Deletes a user account.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        We also don't use sudo, so we remove that method call as well.

        :param username:
        :return:
        """"""
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""/usr/bin/tmsh delete auth user "" + username)

    def get_dvd_device(self, dev_dir='/dev'):
        """"""Find BIG-IP's CD/DVD device

        This device is almost certainly /dev/cdrom so I added the ? to this pattern.
        Note that this method will return upon the first device found, but in my
        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the
        correct CD/DVD device though.

        :todo: Consider just always returning ""/dev/cdrom"" here if that device device
               exists on all platforms that are supported on Azure(Stack)
        :param dev_dir: The root directory from which to look for devices
        """"""
        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'
        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        raise OSUtilError(""Failed to get dvd device"")

    def mount_dvd(self, **kwargs):
        """"""Mount the DVD containing the provisioningiso.iso file

        This is the _first_ hook that WAAgent provides for us, so this is the
        point where we should wait for mcpd to load. I am just overloading
        this method to add the mcpd wait. Then I proceed with the stock code.

        :param max_retry: Maximum number of retries waagent will make when
                          mounting the provisioningiso.iso DVD
        :param chk_err: Whether to check for errors or not in the mounting
                        commands
        """"""
        self._wait_until_mcpd_is_initialized()
        return super(BigIpOSUtil, self).mount_dvd(**kwargs)

    def eject_dvd(self, chk_err=True):
        """"""Runs the eject command to eject the provisioning DVD

        BIG-IP does not include an eject command. It is sufficient to just
        umount the DVD disk. But I will log that we do not support this for
        future reference.

        :param chk_err: Whether or not to check for errors raised by the eject
                        command
        """"""
        logger.warn(""Eject is not supported on this platform"")

    def get_first_if(self):
        """"""Return the interface name, and ip addr of the management interface.

        We need to add a struct_size check here because, curiously, our 64bit
        platform is identified by python in Azure(Stack) as 32 bit and without
        adjusting the struct_size, we can't get the information we need.

        I believe this may be caused by only python i686 being shipped with
        BIG-IP instead of python x86_64??
        """"""
        iface = ''
        expected = 16  # how many devices should I expect...

        python_arc = platform.architecture()[0]
        if python_arc == '64bit':
            struct_size = 40  # for 64bit the size is 40 bytes
        else:
            struct_size = 32  # for 32bit the size is 32 bytes
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        buff = array.array('B', b'\0' * (expected * struct_size))
        param = struct.pack('iL',
                            expected*struct_size,
                            buff.buffer_info()[0])
        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)
        retsize = (struct.unpack('iL', ret)[0])
        if retsize == (expected * struct_size):
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)
        sock = buff.tostring()
        for i in range(0, struct_size * expected, struct_size):
            iface = self._format_single_interface_name(sock, i)

            # Azure public was returning ""lo:1"" when deploying WAF
            if b'lo' in iface:
                continue
            else:
                break
        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])

    def _format_single_interface_name(self, sock, offset):
        return sock[offset:offset+16].split(b'\0', 1)[0]

    def route_add(self, net, mask, gateway):
        """"""Add specified route using tmsh.

        :param net:
        :param mask:
        :param gateway:
        :return:
        """"""
        cmd = (""/usr/bin/tmsh create net route ""
               ""{0}/{1} gw {2}"").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def device_for_ide_port(self, port_id):
        """"""Return device name attached to ide port 'n'.

        Include a wait in here because BIG-IP may not have yet initialized
        this list of devices.

        :param port_id:
        :return:
        """"""
        for retries in range(1, 100):
            # Retry until devices are ready
            if os.path.exists(""/sys/bus/vmbus/devices/""):
                break
            else:
                time.sleep(10)
        return super(BigIpOSUtil, self).device_for_ide_port(port_id)
/n/n/nazurelinuxagent/common/osutil/clearlinux.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import base64
import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class ClearLinuxUtil(DefaultOSUtil):

    def __init__(self):
        super(ClearLinuxUtil, self).__init__()
        self.agent_conf_file_path = '/usr/share/defaults/waagent/waagent.conf'
        self.jit_enabled = True

    def is_dhcp_enabled(self):
        return True

    def start_network(self) :
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def restart_if(self, iface):
        shellutil.run(""systemctl restart systemd-networkd"")

    def restart_ssh_service(self):
        # SSH is socket activated. No need to restart it.
        pass

    def stop_dhcp_service(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret= shellutil.run_get_output(""pidof systemd-networkd"")
        return ret[1] if ret[0] == 0 else None

    def conf_sshd(self, disable_password):
        # Don't whack the system default sshd conf
        pass

    def del_root_password(self):
        try:
            passwd_file_path = conf.get_passwd_file_path()
            try:
                passwd_content = fileutil.read_file(passwd_file_path)
                if not passwd_content:
                    # Empty file is no better than no file
                    raise FileNotFoundError
            except FileNotFoundError:
                new_passwd = [""root:*LOCK*:14600::::::""]
            else:
                passwd = passwd_content.split('\n')
                new_passwd = [x for x in passwd if not x.startswith(""root:"")]
                new_passwd.insert(0, ""root:*LOCK*:14600::::::"")
            fileutil.write_file(passwd_file_path, ""\n"".join(new_passwd))
        except IOError as e:
            raise OSUtilError(""Failed to delete root password:{0}"".format(e))
        pass
/n/n/nazurelinuxagent/common/osutil/coreos.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class CoreOSUtil(DefaultOSUtil):

    def __init__(self):
        super(CoreOSUtil, self).__init__()
        self.agent_conf_file_path = '/usr/share/oem/waagent.conf'
        self.waagent_path = '/usr/share/oem/bin/waagent'
        self.python_path = '/usr/share/oem/python/bin'
        self.jit_enabled = True
        if 'PATH' in os.environ:
            path = ""{0}:{1}"".format(os.environ['PATH'], self.python_path)
        else:
            path = self.python_path
        os.environ['PATH'] = path

        if 'PYTHONPATH' in os.environ:
            py_path = os.environ['PYTHONPATH']
            py_path = ""{0}:{1}"".format(py_path, self.waagent_path)
        else:
            py_path = self.waagent_path
        os.environ['PYTHONPATH'] = py_path

    def is_sys_user(self, username):
        # User 'core' is not a sysuser.
        if username == 'core':
            return False
        return super(CoreOSUtil, self).is_sys_user(username)

    def is_dhcp_enabled(self):
        return True

    def start_network(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def restart_if(self, *dummy, **_):
        shellutil.run(""systemctl restart systemd-networkd"")

    def restart_ssh_service(self):
        # SSH is socket activated on CoreOS.  No need to restart it.
        pass

    def stop_dhcp_service(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""systemctl show -p MainPID ""
                                       ""systemd-networkd"", chk_err=False)
        pid = ret[1].split('=', 1)[-1].strip() if ret[0] == 0 else None
        return pid if pid != '0' else None

    def conf_sshd(self, disable_password):
        # In CoreOS, /etc/sshd_config is mount readonly.  Skip the setting.
        pass
/n/n/nazurelinuxagent/common/osutil/debian.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import base64
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class DebianOSUtil(DefaultOSUtil):

    def __init__(self):
        super(DebianOSUtil, self).__init__()
        self.jit_enabled = True

    def restart_ssh_service(self):
        return shellutil.run(""systemctl --job-mode=ignore-dependencies try-reload-or-restart ssh"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""service azurelinuxagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""service azurelinuxagent start"", chk_err=False)

    def start_network(self):
        pass

    def remove_rules_files(self, rules_files=""""):
        pass

    def restore_rules_files(self, rules_files=""""):
        pass

    def get_dhcp_lease_endpoint(self):
        return self.get_endpoint_from_leases_path('/var/lib/dhcp/dhclient.*.leases')
/n/n/nazurelinuxagent/common/osutil/default.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import base64
import datetime
import errno
import fcntl
import glob
import multiprocessing
import os
import platform
import pwd
import re
import shutil
import socket
import struct
import sys
import time

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil

from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.utils.flexible_version import FlexibleVersion

from pwd import getpwall

__RULES_FILES__ = [ ""/lib/udev/rules.d/75-persistent-net-generator.rules"",
                    ""/etc/udev/rules.d/70-persistent-net.rules"" ]

""""""
Define distro specific behavior. OSUtil class defines default behavior
for all distros. Each concrete distro classes could overwrite default behavior
if needed.
""""""

IPTABLES_VERSION_PATTERN = re.compile(""^[^\d\.]*([\d\.]+).*$"")
IPTABLES_VERSION = ""iptables --version""
IPTABLES_LOCKING_VERSION = FlexibleVersion('1.4.21')

FIREWALL_ACCEPT = ""iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m owner --uid-owner {3} -j ACCEPT""
# Note:
# -- Initially ""flight"" the change to ACCEPT packets and develop a metric baseline
#    A subsequent release will convert the ACCEPT to DROP
# FIREWALL_DROP = ""iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m conntrack --ctstate INVALID,NEW -j ACCEPT""
FIREWALL_DROP = ""iptables {0} -t security -{1} OUTPUT -d {2} -p tcp -m conntrack --ctstate INVALID,NEW -j DROP""
FIREWALL_LIST = ""iptables {0} -t security -L -nxv""
FIREWALL_PACKETS = ""iptables {0} -t security -L OUTPUT --zero OUTPUT -nxv""
FIREWALL_FLUSH = ""iptables {0} -t security --flush""

# Precisely delete the rules created by the agent.
# this rule was used <= 2.2.25.  This rule helped to validate our change, and determine impact.
FIREWALL_DELETE_CONNTRACK_ACCEPT = ""iptables {0} -t security -D OUTPUT -d {1} -p tcp -m conntrack --ctstate INVALID,NEW -j ACCEPT""
FIREWALL_DELETE_OWNER_ACCEPT = ""iptables {0} -t security -D OUTPUT -d {1} -p tcp -m owner --uid-owner {2} -j ACCEPT""
FIREWALL_DELETE_CONNTRACK_DROP = ""iptables {0} -t security -D OUTPUT -d {1} -p tcp -m conntrack --ctstate INVALID,NEW -j DROP""

PACKET_PATTERN = ""^\s*(\d+)\s+(\d+)\s+DROP\s+.*{0}[^\d]*$""
ALL_CPUS_REGEX = re.compile('^cpu .*')


_enable_firewall = True

DMIDECODE_CMD = 'dmidecode --string system-uuid'
PRODUCT_ID_FILE = '/sys/class/dmi/id/product_uuid'
UUID_PATTERN = re.compile(
    r'^\s*[A-F0-9]{8}(?:\-[A-F0-9]{4}){3}\-[A-F0-9]{12}\s*$',
    re.IGNORECASE)

IOCTL_SIOCGIFCONF = 0x8912
IOCTL_SIOCGIFFLAGS = 0x8913
IOCTL_SIOCGIFHWADDR = 0x8927
IFNAMSIZ = 16


class DefaultOSUtil(object):
    def __init__(self):
        self.agent_conf_file_path = '/etc/waagent.conf'
        self.selinux = None
        self.disable_route_warning = False
        self.jit_enabled = False

    def get_firewall_dropped_packets(self, dst_ip=None):
        # If a previous attempt failed, do not retry
        global _enable_firewall
        if not _enable_firewall:
            return 0

        try:
            wait = self.get_firewall_will_wait()

            rc, output = shellutil.run_get_output(FIREWALL_PACKETS.format(wait), log_cmd=False)
            if rc == 3:
                # Transient error  that we ignore.  This code fires every loop
                # of the daemon (60m), so we will get the value eventually.
                return 0

            if rc != 0:
                return -1

            pattern = re.compile(PACKET_PATTERN.format(dst_ip))
            for line in output.split('\n'):
                m = pattern.match(line)
                if m is not None:
                    return int(m.group(1))
            
            return 0

        except Exception as e:
            _enable_firewall = False
            logger.warn(""Unable to retrieve firewall packets dropped""
                        ""{0}"".format(ustr(e)))
            return -1

    def get_firewall_will_wait(self):
        # Determine if iptables will serialize access
        rc, output = shellutil.run_get_output(IPTABLES_VERSION)
        if rc != 0:
            msg = ""Unable to determine version of iptables""
            logger.warn(msg)
            raise Exception(msg)

        m = IPTABLES_VERSION_PATTERN.match(output)
        if m is None:
            msg = ""iptables did not return version information""
            logger.warn(msg)
            raise Exception(msg)

        wait = ""-w"" \
                if FlexibleVersion(m.group(1)) >= IPTABLES_LOCKING_VERSION \
                else """"
        return wait

    def _delete_rule(self, rule):
        """"""
        Continually execute the delete operation until the return
        code is non-zero or the limit has been reached.
        """"""
        for i in range(1, 100):
            rc = shellutil.run(rule, chk_err=False)
            if rc == 1:
                return
            elif rc == 2:
                raise Exception(""invalid firewall deletion rule '{0}'"".format(rule))

    def remove_firewall(self, dst_ip=None, uid=None):
        # If a previous attempt failed, do not retry
        global _enable_firewall
        if not _enable_firewall:
            return False

        try:
            if dst_ip is None or uid is None:
                msg = ""Missing arguments to enable_firewall""
                logger.warn(msg)
                raise Exception(msg)

            wait = self.get_firewall_will_wait()

            # This rule was <= 2.2.25 only, and may still exist on some VMs.  Until 2.2.25
            # has aged out, keep this cleanup in place.
            self._delete_rule(FIREWALL_DELETE_CONNTRACK_ACCEPT.format(wait, dst_ip))

            self._delete_rule(FIREWALL_DELETE_OWNER_ACCEPT.format(wait, dst_ip, uid))
            self._delete_rule(FIREWALL_DELETE_CONNTRACK_DROP.format(wait, dst_ip))

            return True

        except Exception as e:
            _enable_firewall = False
            logger.info(""Unable to remove firewall -- ""
                        ""no further attempts will be made: ""
                        ""{0}"".format(ustr(e)))
            return False

    def enable_firewall(self, dst_ip=None, uid=None):
        # If a previous attempt failed, do not retry
        global _enable_firewall
        if not _enable_firewall:
            return False

        try:
            if dst_ip is None or uid is None:
                msg = ""Missing arguments to enable_firewall""
                logger.warn(msg)
                raise Exception(msg)

            wait = self.get_firewall_will_wait()

            # If the DROP rule exists, make no changes
            drop_rule = FIREWALL_DROP.format(wait, ""C"", dst_ip)
            rc = shellutil.run(drop_rule, chk_err=False)
            if rc == 0:
                logger.verbose(""Firewall appears established"")
                return True
            elif rc == 2:
                self.remove_firewall(dst_ip, uid)
                msg = ""please upgrade iptables to a version that supports the -C option""
                logger.warn(msg)
                raise Exception(msg)

            # Otherwise, append both rules
            accept_rule = FIREWALL_ACCEPT.format(wait, ""A"", dst_ip, uid)
            drop_rule = FIREWALL_DROP.format(wait, ""A"", dst_ip)

            if shellutil.run(accept_rule) != 0:
                msg = ""Unable to add ACCEPT firewall rule '{0}'"".format(
                    accept_rule)
                logger.warn(msg)
                raise Exception(msg)

            if shellutil.run(drop_rule) != 0:
                msg = ""Unable to add DROP firewall rule '{0}'"".format(
                    drop_rule)
                logger.warn(msg)
                raise Exception(msg)

            logger.info(""Successfully added Azure fabric firewall rules"")

            rc, output = shellutil.run_get_output(FIREWALL_LIST.format(wait))
            if rc == 0:
                logger.info(""Firewall rules:\n{0}"".format(output))
            else:
                logger.warn(""Listing firewall rules failed: {0}"".format(output))

            return True

        except Exception as e:
            _enable_firewall = False
            logger.info(""Unable to establish firewall -- ""
                        ""no further attempts will be made: ""
                        ""{0}"".format(ustr(e)))
            return False

    def _correct_instance_id(self, id):
        '''
        Azure stores the instance ID with an incorrect byte ordering for the
        first parts. For example, the ID returned by the metadata service:

            D0DF4C54-4ECB-4A4B-9954-5BDF3ED5C3B8

        will be found as:

            544CDFD0-CB4E-4B4A-9954-5BDF3ED5C3B8

        This code corrects the byte order such that it is consistent with
        that returned by the metadata service.
        '''

        if not UUID_PATTERN.match(id):
            return id

        parts = id.split('-')
        return '-'.join([
                textutil.swap_hexstring(parts[0], width=2),
                textutil.swap_hexstring(parts[1], width=2),
                textutil.swap_hexstring(parts[2], width=2),
                parts[3],
                parts[4]
            ])

    def is_current_instance_id(self, id_that):
        '''
        Compare two instance IDs for equality, but allow that some IDs
        may have been persisted using the incorrect byte ordering.
        '''
        id_this = self.get_instance_id()
        return id_that == id_this or \
            id_that == self._correct_instance_id(id_this)

    def is_cgroups_supported(self):
        return False

    def mount_cgroups(self):
        pass

    def get_agent_conf_file_path(self):
        return self.agent_conf_file_path

    def get_instance_id(self):
        '''
        Azure records a UUID as the instance ID
        First check /sys/class/dmi/id/product_uuid.
        If that is missing, then extracts from dmidecode
        If nothing works (for old VMs), return the empty string
        '''
        if os.path.isfile(PRODUCT_ID_FILE):
            s = fileutil.read_file(PRODUCT_ID_FILE).strip()

        else:
            rc, s = shellutil.run_get_output(DMIDECODE_CMD)
            if rc != 0 or UUID_PATTERN.match(s) is None:
                return """"

        return self._correct_instance_id(s.strip())

    def get_userentry(self, username):
        try:
            return pwd.getpwnam(username)
        except KeyError:
            return None

    def is_sys_user(self, username):
        """"""
        Check whether use is a system user. 
        If reset sys user is allowed in conf, return False
        Otherwise, check whether UID is less than UID_MIN
        """"""
        if conf.get_allow_reset_sys_user():
            return False

        userentry = self.get_userentry(username)
        uidmin = None
        try:
            uidmin_def = fileutil.get_line_startingwith(""UID_MIN"",
                                                        ""/etc/login.defs"")
            if uidmin_def is not None:
                uidmin = int(uidmin_def.split()[1])
        except IOError as e:
            pass
        if uidmin == None:
            uidmin = 100
        if userentry != None and userentry[2] < uidmin:
            return True
        else:
            return False

    def useradd(self, username, expiration=None, comment=None):
        """"""
        Create user account with 'username'
        """"""
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.info(""User {0} already exists, skip useradd"", username)
            return

        if expiration is not None:
            cmd = ""useradd -m {0} -e {1}"".format(username, expiration)
        else:
            cmd = ""useradd -m {0}"".format(username)
        
        if comment is not None:
            cmd += "" -c {0}"".format(comment)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError((""Failed to create user account:{0}, ""
                               ""retcode:{1}, ""
                               ""output:{2}"").format(username, retcode, out))

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user, ""
                               ""will not set password."").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = ""usermod -p '{0}' {1}"".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))
    
    def get_users(self):
        return getpwall()

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        sudoers_dir = conf.get_sudoers_dir()
        sudoers_wagent = os.path.join(sudoers_dir, 'waagent')

        if not remove:
            # for older distros create sudoers.d
            if not os.path.isdir(sudoers_dir):
                sudoers_file = os.path.join(sudoers_dir, '../sudoers')
                # create the sudoers.d directory
                os.mkdir(sudoers_dir)
                # add the include of sudoers.d to the /etc/sudoers
                sudoers = '\n#includedir ' + sudoers_dir + '\n'
                fileutil.append_file(sudoers_file, sudoers)
            sudoer = None
            if nopasswd:
                sudoer = ""{0} ALL=(ALL) NOPASSWD: ALL"".format(username)
            else:
                sudoer = ""{0} ALL=(ALL) ALL"".format(username)
            if not os.path.isfile(sudoers_wagent) or \
                    fileutil.findstr_in_file(sudoers_wagent, sudoer) is False:
                fileutil.append_file(sudoers_wagent, ""{0}\n"".format(sudoer))
            fileutil.chmod(sudoers_wagent, 0o440)
        else:
            # remove user from sudoers
            if os.path.isfile(sudoers_wagent):
                try:
                    content = fileutil.read_file(sudoers_wagent)
                    sudoers = content.split(""\n"")
                    sudoers = [x for x in sudoers if username not in x]
                    fileutil.write_file(sudoers_wagent, ""\n"".join(sudoers))
                except IOError as e:
                    raise OSUtilError(""Failed to remove sudoer: {0}"".format(e))

    def del_root_password(self):
        try:
            passwd_file_path = conf.get_passwd_file_path()
            passwd_content = fileutil.read_file(passwd_file_path)
            passwd = passwd_content.split('\n')
            new_passwd = [x for x in passwd if not x.startswith(""root:"")]
            new_passwd.insert(0, ""root:*LOCK*:14600::::::"")
            fileutil.write_file(passwd_file_path, ""\n"".join(new_passwd))
        except IOError as e:
            raise OSUtilError(""Failed to delete root password:{0}"".format(e))

    def _norm_path(self, filepath):
        home = conf.get_home_dir()
        # Expand HOME variable if present in path
        path = os.path.normpath(filepath.replace(""$HOME"", home))
        return path

    def deploy_ssh_keypair(self, username, keypair):
        """"""
        Deploy id_rsa and id_rsa.pub
        """"""
        path, thumbprint = keypair
        path = self._norm_path(path)
        dir_path = os.path.dirname(path)
        fileutil.mkdir(dir_path, mode=0o700, owner=username)
        lib_dir = conf.get_lib_dir()
        prv_path = os.path.join(lib_dir, thumbprint + '.prv')
        if not os.path.isfile(prv_path):
            raise OSUtilError(""Can't find {0}.prv"".format(thumbprint))
        shutil.copyfile(prv_path, path)
        pub_path = path + '.pub'
        crytputil = CryptUtil(conf.get_openssl_cmd())
        pub = crytputil.get_pubkey_from_prv(prv_path)
        fileutil.write_file(pub_path, pub)
        self.set_selinux_context(pub_path, 'unconfined_u:object_r:ssh_home_t:s0')
        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')
        os.chmod(path, 0o644)
        os.chmod(pub_path, 0o600)

    def openssl_to_openssh(self, input_file, output_file):
        cryptutil = CryptUtil(conf.get_openssl_cmd())
        cryptutil.crt_to_ssh(input_file, output_file)

    def deploy_ssh_pubkey(self, username, pubkey):
        """"""
        Deploy authorized_key
        """"""
        path, thumbprint, value = pubkey
        if path is None:
            raise OSUtilError(""Public key path is None"")

        crytputil = CryptUtil(conf.get_openssl_cmd())

        path = self._norm_path(path)
        dir_path = os.path.dirname(path)
        fileutil.mkdir(dir_path, mode=0o700, owner=username)
        if value is not None:
            if not value.startswith(""ssh-""):
                raise OSUtilError(""Bad public key: {0}"".format(value))
            fileutil.write_file(path, value)
        elif thumbprint is not None:
            lib_dir = conf.get_lib_dir()
            crt_path = os.path.join(lib_dir, thumbprint + '.crt')
            if not os.path.isfile(crt_path):
                raise OSUtilError(""Can't find {0}.crt"".format(thumbprint))
            pub_path = os.path.join(lib_dir, thumbprint + '.pub')
            pub = crytputil.get_pubkey_from_crt(crt_path)
            fileutil.write_file(pub_path, pub)
            self.set_selinux_context(pub_path,
                                     'unconfined_u:object_r:ssh_home_t:s0')
            self.openssl_to_openssh(pub_path, path)
            fileutil.chmod(pub_path, 0o600)
        else:
            raise OSUtilError(""SSH public key Fingerprint and Value are None"")

        self.set_selinux_context(path, 'unconfined_u:object_r:ssh_home_t:s0')
        fileutil.chowner(path, username)
        fileutil.chmod(path, 0o644)

    def is_selinux_system(self):
        """"""
        Checks and sets self.selinux = True if SELinux is available on system.
        """"""
        if self.selinux == None:
            if shellutil.run(""which getenforce"", chk_err=False) == 0:
                self.selinux = True
            else:
                self.selinux = False
        return self.selinux

    def is_selinux_enforcing(self):
        """"""
        Calls shell command 'getenforce' and returns True if 'Enforcing'.
        """"""
        if self.is_selinux_system():
            output = shellutil.run_get_output(""getenforce"")[1]
            return output.startswith(""Enforcing"")
        else:
            return False

    def set_selinux_context(self, path, con):
        """"""
        Calls shell 'chcon' with 'path' and 'con' context.
        Returns exit result.
        """"""
        if self.is_selinux_system():
            if not os.path.exists(path):
                logger.error(""Path does not exist: {0}"".format(path))
                return 1
            return shellutil.run('chcon ' + con + ' ' + path)

    def conf_sshd(self, disable_password):
        option = ""no"" if disable_password else ""yes""
        conf_file_path = conf.get_sshd_conf_file_path()
        conf_file = fileutil.read_file(conf_file_path).split(""\n"")
        textutil.set_ssh_config(conf_file, ""PasswordAuthentication"", option)
        textutil.set_ssh_config(conf_file, ""ChallengeResponseAuthentication"", option)
        textutil.set_ssh_config(conf_file, ""ClientAliveInterval"", str(conf.get_ssh_client_alive_interval()))
        fileutil.write_file(conf_file_path, ""\n"".join(conf_file))
        logger.info(""{0} SSH password-based authentication methods.""
                    .format(""Disabled"" if disable_password else ""Enabled""))
        logger.info(""Configured SSH client probing to keep connections alive."")

    def get_dvd_device(self, dev_dir='/dev'):
        pattern = r'(sr[0-9]|hd[c-z]|cdrom[0-9]|cd[0-9])'
        device_list = os.listdir(dev_dir)
        for dvd in [re.match(pattern, dev) for dev in device_list]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        inner_detail = ""The following devices were found, but none matched "" \
                       ""the pattern [{0}]: {1}\n"".format(pattern, device_list)
        raise OSUtilError(msg=""Failed to get dvd device from {0}"".format(dev_dir),
                          inner=inner_detail)

    def mount_dvd(self,
                  max_retry=6,
                  chk_err=True,
                  dvd_device=None,
                  mount_point=None,
                  sleep_time=5):
        if dvd_device is None:
            dvd_device = self.get_dvd_device()
        if mount_point is None:
            mount_point = conf.get_dvd_mount_point()
        mount_list = shellutil.run_get_output(""mount"")[1]
        existing = self.get_mount_point(mount_list, dvd_device)

        if existing is not None:
            # already mounted
            logger.info(""{0} is already mounted at {1}"", dvd_device, existing)
            return

        if not os.path.isdir(mount_point):
            os.makedirs(mount_point)

        err = ''
        for retry in range(1, max_retry):
            return_code, err = self.mount(dvd_device,
                                          mount_point,
                                          option=""-o ro -t udf,iso9660"",
                                          chk_err=False)
            if return_code == 0:
                logger.info(""Successfully mounted dvd"")
                return
            else:
                logger.warn(
                    ""Mounting dvd failed [retry {0}/{1}, sleeping {2} sec]"",
                    retry,
                    max_retry - 1,
                    sleep_time)
                if retry < max_retry:
                    time.sleep(sleep_time)
        if chk_err:
            raise OSUtilError(""Failed to mount dvd device"", inner=err)

    def umount_dvd(self, chk_err=True, mount_point=None):
        if mount_point is None:
            mount_point = conf.get_dvd_mount_point()
        return_code = self.umount(mount_point, chk_err=chk_err)
        if chk_err and return_code != 0:
            raise OSUtilError(""Failed to unmount dvd device at {0}"",
                              mount_point)

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""eject {0}"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject dvd: ret={0}"".format(retcode))

    def try_load_atapiix_mod(self):
        try:
            self.load_atapiix_mod()
        except Exception as e:
            logger.warn(""Could not load ATAPI driver: {0}"".format(e))

    def load_atapiix_mod(self):
        if self.is_atapiix_mod_loaded():
            return
        ret, kern_version = shellutil.run_get_output(""uname -r"")
        if ret != 0:
            raise Exception(""Failed to call uname -r"")
        mod_path = os.path.join('/lib/modules',
                                kern_version.strip('\n'),
                                'kernel/drivers/ata/ata_piix.ko')
        if not os.path.isfile(mod_path):
            raise Exception(""Can't find module file:{0}"".format(mod_path))

        ret, output = shellutil.run_get_output(""insmod "" + mod_path)
        if ret != 0:
            raise Exception(""Error calling insmod for ATAPI CD-ROM driver"")
        if not self.is_atapiix_mod_loaded(max_retry=3):
            raise Exception(""Failed to load ATAPI CD-ROM driver"")

    def is_atapiix_mod_loaded(self, max_retry=1):
        for retry in range(0, max_retry):
            ret = shellutil.run(""lsmod | grep ata_piix"", chk_err=False)
            if ret == 0:
                logger.info(""Module driver for ATAPI CD-ROM is already present."")
                return True
            if retry < max_retry - 1:
                time.sleep(1)
        return False

    def mount(self, device, mount_point, option="""", chk_err=True):
        cmd = ""mount {0} {1} {2}"".format(option, device, mount_point)
        retcode, err = shellutil.run_get_output(cmd, chk_err)
        if retcode != 0:
            detail = ""[{0}] returned {1}: {2}"".format(cmd, retcode, err)
            err = detail
        return retcode, err

    def umount(self, mount_point, chk_err=True):
        return shellutil.run(""umount {0}"".format(mount_point), chk_err=chk_err)

    def allow_dhcp_broadcast(self):
        # Open DHCP port if iptables is enabled.
        # We supress error logging on error.
        shellutil.run(""iptables -D INPUT -p udp --dport 68 -j ACCEPT"",
                      chk_err=False)
        shellutil.run(""iptables -I INPUT -p udp --dport 68 -j ACCEPT"",
                      chk_err=False)


    def remove_rules_files(self, rules_files=__RULES_FILES__):
        lib_dir = conf.get_lib_dir()
        for src in rules_files:
            file_name = fileutil.base_name(src)
            dest = os.path.join(lib_dir, file_name)
            if os.path.isfile(dest):
                os.remove(dest)
            if os.path.isfile(src):
                logger.warn(""Move rules file {0} to {1}"", file_name, dest)
                shutil.move(src, dest)

    def restore_rules_files(self, rules_files=__RULES_FILES__):
        lib_dir = conf.get_lib_dir()
        for dest in rules_files:
            filename = fileutil.base_name(dest)
            src = os.path.join(lib_dir, filename)
            if os.path.isfile(dest):
                continue
            if os.path.isfile(src):
                logger.warn(""Move rules file {0} to {1}"", filename, dest)
                shutil.move(src, dest)

    def get_mac_addr(self):
        """"""
        Convenience function, returns mac addr bound to
        first non-loopback interface.
        """"""
        ifname = self.get_if_name()
        addr = self.get_if_mac(ifname)
        return textutil.hexstr_to_bytearray(addr)

    def get_if_mac(self, ifname):
        """"""
        Return the mac-address bound to the socket.
        """"""
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        param = struct.pack('256s', (ifname[:15]+('\0'*241)).encode('latin-1'))
        info = fcntl.ioctl(sock.fileno(), IOCTL_SIOCGIFHWADDR, param)
        sock.close()
        return ''.join(['%02X' % textutil.str_to_ord(char) for char in info[18:24]])

    @staticmethod
    def _get_struct_ifconf_size():
        """"""
        Return the sizeof struct ifinfo. On 64-bit platforms the size is 40 bytes;
        on 32-bit platforms the size is 32 bytes.
        """"""
        python_arc = platform.architecture()[0]
        struct_size = 32 if python_arc == '32bit' else 40
        return struct_size

    def _get_all_interfaces(self):
        """"""
        Return a dictionary mapping from interface name to IPv4 address.
        Interfaces without a name are ignored.
        """"""
        expected=16 # how many devices should I expect...
        struct_size = DefaultOSUtil._get_struct_ifconf_size()
        array_size = expected * struct_size

        buff = array.array('B', b'\0' * array_size)
        param = struct.pack('iL', array_size, buff.buffer_info()[0])

        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        ret = fcntl.ioctl(sock.fileno(), IOCTL_SIOCGIFCONF, param)
        retsize = (struct.unpack('iL', ret)[0])
        sock.close()

        if retsize == array_size:
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)

        ifconf_buff = buff.tostring()

        ifaces = {}
        for i in range(0, array_size, struct_size):
            iface = ifconf_buff[i:i+IFNAMSIZ].split(b'\0', 1)[0]
            if len(iface) > 0:
                iface_name = iface.decode('latin-1')
                if iface_name not in ifaces:
                    ifaces[iface_name] = socket.inet_ntoa(ifconf_buff[i+20:i+24])
        return ifaces


    def get_first_if(self):
        """"""
        Return the interface name, and IPv4 addr of the ""primary"" interface or,
        failing that, any active non-loopback interface.
        """"""
        primary = self.get_primary_interface()
        ifaces = self._get_all_interfaces()

        if primary in ifaces:
            return primary, ifaces[primary]

        for iface_name in ifaces.keys():
            if not self.is_loopback(iface_name):
                logger.info(""Choosing non-primary [{0}]"".format(iface_name))
                return iface_name, ifaces[iface_name]

        return '', ''


    def get_primary_interface(self):
        """"""
        Get the name of the primary interface, which is the one with the
        default route attached to it; if there are multiple default routes,
        the primary has the lowest Metric.
        :return: the interface which has the default route
        """"""
        # from linux/route.h
        RTF_GATEWAY = 0x02
        DEFAULT_DEST = ""00000000""

        hdr_iface = ""Iface""
        hdr_dest = ""Destination""
        hdr_flags = ""Flags""
        hdr_metric = ""Metric""

        idx_iface = -1
        idx_dest = -1
        idx_flags = -1
        idx_metric = -1
        primary = None
        primary_metric = None

        if not self.disable_route_warning:
            logger.info(""Examine /proc/net/route for primary interface"")
        with open('/proc/net/route') as routing_table:
            idx = 0
            for header in filter(lambda h: len(h) > 0, routing_table.readline().strip("" \n"").split(""\t"")):
                if header == hdr_iface:
                    idx_iface = idx
                elif header == hdr_dest:
                    idx_dest = idx
                elif header == hdr_flags:
                    idx_flags = idx
                elif header == hdr_metric:
                    idx_metric = idx
                idx = idx + 1
            for entry in routing_table.readlines():
                route = entry.strip("" \n"").split(""\t"")
                if route[idx_dest] == DEFAULT_DEST and int(route[idx_flags]) & RTF_GATEWAY == RTF_GATEWAY:
                    metric = int(route[idx_metric])
                    iface = route[idx_iface]
                    if primary is None or metric < primary_metric:
                        primary = iface
                        primary_metric = metric

        if primary is None:
            primary = ''
            if not self.disable_route_warning:
                with open('/proc/net/route') as routing_table_fh:
                    routing_table_text = routing_table_fh.read()
                    logger.warn('Could not determine primary interface, '
                                'please ensure /proc/net/route is correct')
                    logger.warn('Contents of /proc/net/route:\n{0}'.format(routing_table_text))
                    logger.warn('Primary interface examination will retry silently')
                    self.disable_route_warning = True
        else:
            logger.info('Primary interface is [{0}]'.format(primary))
            self.disable_route_warning = False
        return primary

    def is_primary_interface(self, ifname):
        """"""
        Indicate whether the specified interface is the primary.
        :param ifname: the name of the interface - eth0, lo, etc.
        :return: True if this interface binds the default route
        """"""
        return self.get_primary_interface() == ifname

    def is_loopback(self, ifname):
        """"""
        Determine if a named interface is loopback.
        """"""
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        ifname_buff = ifname + ('\0'*256)
        result = fcntl.ioctl(s.fileno(), IOCTL_SIOCGIFFLAGS, ifname_buff)
        flags, = struct.unpack('H', result[16:18])
        isloopback = flags & 8 == 8
        if not self.disable_route_warning:
            logger.info('interface [{0}] has flags [{1}], '
                        'is loopback [{2}]'.format(ifname, flags, isloopback))
        s.close()
        return isloopback

    def get_dhcp_lease_endpoint(self):
        """"""
        OS specific, this should return the decoded endpoint of
        the wireserver from option 245 in the dhcp leases file
        if it exists on disk.
        :return: The endpoint if available, or None
        """"""
        return None

    @staticmethod
    def get_endpoint_from_leases_path(pathglob):
        """"""
        Try to discover and decode the wireserver endpoint in the
        specified dhcp leases path.
        :param pathglob: The path containing dhcp lease files
        :return: The endpoint if available, otherwise None
        """"""
        endpoint = None

        HEADER_LEASE = ""lease""
        HEADER_OPTION = ""option unknown-245""
        HEADER_DNS = ""option domain-name-servers""
        HEADER_EXPIRE = ""expire""
        FOOTER_LEASE = ""}""
        FORMAT_DATETIME = ""%Y/%m/%d %H:%M:%S""

        logger.info(""looking for leases in path [{0}]"".format(pathglob))
        for lease_file in glob.glob(pathglob):
            leases = open(lease_file).read()
            if HEADER_OPTION in leases:
                cached_endpoint = None
                has_option_245 = False
                expired = True  # assume expired
                for line in leases.splitlines():
                    if line.startswith(HEADER_LEASE):
                        cached_endpoint = None
                        has_option_245 = False
                        expired = True
                    elif HEADER_DNS in line:
                        cached_endpoint = line.replace(HEADER_DNS, '').strip("" ;"")
                    elif HEADER_OPTION in line:
                        has_option_245 = True
                    elif HEADER_EXPIRE in line:
                        if ""never"" in line:
                            expired = False
                        else:
                            try:
                                expire_string = line.split("" "", 4)[-1].strip("";"")
                                expire_date = datetime.datetime.strptime(expire_string, FORMAT_DATETIME)
                                if expire_date > datetime.datetime.utcnow():
                                    expired = False
                            except:
                                logger.error(""could not parse expiry token '{0}'"".format(line))
                    elif FOOTER_LEASE in line:
                        logger.info(""dhcp entry:{0}, 245:{1}, expired:{2}"".format(
                            cached_endpoint, has_option_245, expired))
                        if not expired and cached_endpoint is not None and has_option_245:
                            endpoint = cached_endpoint
                            logger.info(""found endpoint [{0}]"".format(endpoint))
                            # we want to return the last valid entry, so
                            # keep searching
        if endpoint is not None:
            logger.info(""cached endpoint found [{0}]"".format(endpoint))
        else:
            logger.info(""cached endpoint not found"")
        return endpoint

    def is_missing_default_route(self):
        routes = shellutil.run_get_output(""route -n"")[1]
        for route in routes.split(""\n""):
            if route.startswith(""0.0.0.0 "") or route.startswith(""default ""):
               return False
        return True

    def get_if_name(self):
        if_name = ''
        if_found = False
        while not if_found:
            if_name = self.get_first_if()[0]
            if_found = len(if_name) >= 2
            if not if_found:
                time.sleep(2)
        return if_name

    def get_ip4_addr(self):
        return self.get_first_if()[1]

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 dev {0}"".format(ifname),
                             chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route del 255.255.255.255 dev {0}"".format(ifname),
                      chk_err=False)

    def is_dhcp_enabled(self):
        return False

    def stop_dhcp_service(self):
        pass

    def start_dhcp_service(self):
        pass

    def start_network(self):
        pass

    def start_agent_service(self):
        pass

    def stop_agent_service(self):
        pass

    def register_agent_service(self):
        pass

    def unregister_agent_service(self):
        pass

    def restart_ssh_service(self):
        pass

    def route_add(self, net, mask, gateway):
        """"""
        Add specified route using /sbin/route add -net.
        """"""
        cmd = (""/sbin/route add -net ""
               ""{0} netmask {1} gw {2}"").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        fileutil.write_file('/etc/hostname', hostname)
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def set_dhcp_hostname(self, hostname):
        autosend = r'^[^#]*?send\s*host-name.*?(<hostname>|gethostname[(,)])'
        dhclient_files = ['/etc/dhcp/dhclient.conf', '/etc/dhcp3/dhclient.conf', '/etc/dhclient.conf']
        for conf_file in dhclient_files:
            if not os.path.isfile(conf_file):
                continue
            if fileutil.findre_in_file(conf_file, autosend):
                #Return if auto send host-name is configured
                return
            fileutil.update_conf_file(conf_file,
                                      'send host-name',
                                      'send host-name ""{0}"";'.format(hostname))

    def restart_if(self, ifname, retries=3, wait=5):
        retry_limit=retries+1
        for attempt in range(1, retry_limit):
            return_code=shellutil.run(""ifdown {0} && ifup {0}"".format(ifname))
            if return_code == 0:
                return
            logger.warn(""failed to restart {0}: return code {1}"".format(ifname, return_code))
            if attempt < retry_limit:
                logger.info(""retrying in {0} seconds"".format(wait))
                time.sleep(wait)
            else:
                logger.warn(""exceeded restart retries"")

    def publish_hostname(self, hostname):
        self.set_dhcp_hostname(hostname)
        self.set_hostname_record(hostname)
        ifname = self.get_if_name()
        self.restart_if(ifname)

    def set_scsi_disks_timeout(self, timeout):
        for dev in os.listdir(""/sys/block""):
            if dev.startswith('sd'):
                self.set_block_device_timeout(dev, timeout)

    def set_block_device_timeout(self, dev, timeout):
        if dev is not None and timeout is not None:
            file_path = ""/sys/block/{0}/device/timeout"".format(dev)
            content = fileutil.read_file(file_path)
            original = content.splitlines()[0].rstrip()
            if original != timeout:
                fileutil.write_file(file_path, timeout)
                logger.info(""Set block dev timeout: {0} with timeout: {1}"",
                            dev, timeout)

    def get_mount_point(self, mountlist, device):
        """"""
        Example of mountlist:
            /dev/sda1 on / type ext4 (rw)
            proc on /proc type proc (rw)
            sysfs on /sys type sysfs (rw)
            devpts on /dev/pts type devpts (rw,gid=5,mode=620)
            tmpfs on /dev/shm type tmpfs
            (rw,rootcontext=""system_u:object_r:tmpfs_t:s0"")
            none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
            /dev/sdb1 on /mnt/resource type ext4 (rw)
        """"""
        if (mountlist and device):
            for entry in mountlist.split('\n'):
                if(re.search(device, entry)):
                    tokens = entry.split()
                    #Return the 3rd column of this line
                    return tokens[2] if len(tokens) > 2 else None
        return None

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        if port_id > 3:
            return None
        g0 = ""00000000""
        if port_id > 1:
            g0 = ""00000001""
            port_id = port_id - 2
        device = None
        path = ""/sys/bus/vmbus/devices/""
        if os.path.exists(path):
            try:
                for vmbus in os.listdir(path):
                    deviceid = fileutil.read_file(os.path.join(path, vmbus, ""device_id""))
                    guid = deviceid.lstrip('{').split('-')
                    if guid[0] == g0 and guid[1] == ""000"" + ustr(port_id):
                        for root, dirs, files in os.walk(path + vmbus):
                            if root.endswith(""/block""):
                                device = dirs[0]
                                break
                            else:
                                # older distros
                                for d in dirs:
                                    if ':' in d and ""block"" == d.split(':')[0]:
                                        device = d.split(':')[1]
                                        break
                        break
            except OSError as oe:
                logger.warn('Could not obtain device for IDE port {0}: {1}', port_id, ustr(oe))
        return device

    def set_hostname_record(self, hostname):
        fileutil.write_file(conf.get_published_hostname(), contents=hostname)

    def get_hostname_record(self):
        hostname_record = conf.get_published_hostname()
        if not os.path.exists(hostname_record):
            # this file is created at provisioning time with agents >= 2.2.3
            hostname = socket.gethostname()
            logger.info('Hostname record does not exist, '
                        'creating [{0}] with hostname [{1}]',
                        hostname_record,
                        hostname)
            self.set_hostname_record(hostname)
        record = fileutil.read_file(hostname_record)
        return record

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."", username)
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""userdel -f -r "" + username)
        self.conf_sudoer(username, remove=True)

    def decode_customdata(self, data):
        return base64.b64decode(data).decode('utf-8')

    def get_total_mem(self):
        # Get total memory in bytes and divide by 1024**2 to get the value in MB.
        return os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024**2)

    def get_processor_cores(self):
        return multiprocessing.cpu_count()

    def check_pid_alive(self, pid):
        try:
            pid = int(pid)
            os.kill(pid, 0)
        except (ValueError, TypeError):
            return False
        except OSError as e:
            if e.errno == errno.EPERM:
                return True
            return False
        return True

    @property
    def is_64bit(self):
        return sys.maxsize > 2**32

    @staticmethod
    def _get_proc_stat():
        """"""
        Get the contents of /proc/stat.
        # cpu  813599 3940 909253 154538746 874851 0 6589 0 0 0
        # cpu0 401094 1516 453006 77276738 452939 0 3312 0 0 0
        # cpu1 412505 2423 456246 77262007 421912 0 3276 0 0 0

        :return: A single string with the contents of /proc/stat
        :rtype: str
        """"""
        results = None
        try:
            results = fileutil.read_file('/proc/stat')
        except (OSError, IOError) as ex:
            logger.warn(""Couldn't read /proc/stat: {0}"".format(ex.strerror))

        return results

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        """"""
        Compute the number of USER_HZ units of time that have elapsed in all categories, across all cores, since boot.

        :return: int
        """"""
        system_cpu = 0
        proc_stat = DefaultOSUtil._get_proc_stat()
        if proc_stat is not None:
            for line in proc_stat.splitlines():
                if ALL_CPUS_REGEX.match(line):
                    system_cpu = sum(int(i) for i in line.split()[1:7])
                    break
        return system_cpu
/n/n/nazurelinuxagent/common/osutil/freebsd.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.future import ustr

class FreeBSDOSUtil(DefaultOSUtil):

    def __init__(self):
        super(FreeBSDOSUtil, self).__init__()
        self._scsi_disks_timeout_set = False
        self.jit_enabled = True

    def set_hostname(self, hostname):
        rc_file_path = '/etc/rc.conf'
        conf_file = fileutil.read_file(rc_file_path).split(""\n"")
        textutil.set_ini_config(conf_file, ""hostname"", hostname)
        fileutil.write_file(rc_file_path, ""\n"".join(conf_file))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('service sshd restart', chk_err=False)

    def useradd(self, username, expiration=None, comment=None):
        """"""
        Create user account with 'username'
        """"""
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.warn(""User {0} already exists, skip useradd"", username)
            return
        if expiration is not None:
            cmd = ""pw useradd {0} -e {1} -m"".format(username, expiration)
        else:
            cmd = ""pw useradd {0} -m"".format(username)
        if comment is not None:
            cmd += "" -c {0}"".format(comment)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError((""Failed to create user account:{0}, ""
                               ""retcode:{1}, ""
                               ""output:{2}"").format(username, retcode, out))

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."", username)
        shellutil.run('> /var/run/utx.active')
        shellutil.run('rmuser -y ' + username)
        self.conf_sudoer(username, remove=True)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user, ""
                               ""will not set password."").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = ""echo '{0}'|pw usermod {1} -H 0 "".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))

    def del_root_password(self):
        err = shellutil.run('pw usermod root -h -')
        if err:
            raise OSUtilError(""Failed to delete root password: Failed to update password database."")

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        """"""
        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to
        specify the route manually to get it work in a VNET environment.
        SEE ALSO: man ip(4) IP_ONESBCAST,
        """"""
        return True

    def is_dhcp_enabled(self):
        return True

    def start_dhcp_service(self):
        shellutil.run(""/etc/rc.d/dhclient start {0}"".format(self.get_if_name()), chk_err=False)

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route delete 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pgrep -n dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""cdcontrol -f {0} eject"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject dvd: ret={0}"".format(retcode))

    def restart_if(self, ifname):
        # Restart dhclient only to publish hostname
        shellutil.run(""/etc/rc.d/dhclient restart {0}"".format(ifname), chk_err=False)

    def get_total_mem(self):
        cmd = ""sysctl hw.physmem |awk '{print $2}'""
        ret, output = shellutil.run_get_output(cmd)
        if ret:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output(""sysctl hw.ncpu |awk '{print $2}'"")
        if ret:
            raise OSUtilError(""Failed to get processor cores."")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def set_scsi_disks_timeout(self, timeout):
        if self._scsi_disks_timeout_set:
            return

        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))
        if ret:
            raise OSUtilError(""Failed set SCSI disks timeout: {0}"".format(output))
        self._scsi_disks_timeout_set = True

    def check_pid_alive(self, pid):
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """"""
        There is no SIOCGIFCONF
        on freeBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """"""
        iface = ''
        inet = ''
        mac = ''

        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)
        if err:
            raise OSUtilError(""Can't find ether interface:{0}"".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError(""Can't find ether interface."")
        iface = ifaces[0]

        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)
        if err:
            raise OSUtilError(""Can't get info for interface:{0}"".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('ether ') != -1:
                mac = line.split()[1]
        logger.verbose(""Interface info: ({0},{1},{2})"", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        if port_id > 3:
            return None
        g0 = ""00000000""
        if port_id > 1:
            g0 = ""00000001""
            port_id = port_id - 2
        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')
        if err:
            return None
        g1 = ""000"" + ustr(port_id)
        g0g1 = ""{0}-{1}"".format(g0, g1)
        """"""
        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'
        """"""
        cmd_search_ide = ""sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}"".format(g0g1)
        err, output = shellutil.run_get_output(cmd_search_ide)
        if err:
            return None
        cmd_extract_id = cmd_search_ide + ""|awk -F . '{print $3}'""
        err, output = shellutil.run_get_output(cmd_extract_id)
        """"""
        try to search 'blkvscX' and 'storvscX' to find device name
        """"""
        output = output.rstrip()
        cmd_search_blkvsc = ""camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_blkvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible

        cmd_search_storvsc = ""camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_storvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible
        return None

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0
/n/n/nazurelinuxagent/common/osutil/gaia.py/n/n#
# Copyright 2017 Check Point Software Technologies
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import base64
import socket
import struct
import time

import azurelinuxagent.common.conf as conf
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.future import ustr, bytebuffer
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.utils.cryptutil import CryptUtil
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil


class GaiaOSUtil(DefaultOSUtil):

    def __init__(self):
        super(GaiaOSUtil, self).__init__()

    def _run_clish(self, cmd, log_cmd=True):
        for i in xrange(10):
            ret, out = shellutil.run_get_output(
                ""/bin/clish -s -c '"" + cmd + ""'"", log_cmd=log_cmd)
            if not ret:
                break
            if 'NMSHST0025' in out:  # Entry for [hostname] already present
                ret = 0
                break
            time.sleep(2)
        return ret, out

    def useradd(self, username, expiration=None):
        logger.warn('useradd is not supported on GAiA')

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        logger.info('chpasswd')
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        ret, out = self._run_clish(
            'set user admin password-hash ' + passwd_hash, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format('admin', out))

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        logger.info('conf_sudoer is not supported on GAiA')

    def del_root_password(self):
        logger.info('del_root_password')
        ret, out = self._run_clish('set user admin password-hash *LOCK*')
        if ret != 0:
            raise OSUtilError(""Failed to delete root password"")

    def _replace_user(self, path, username):
        if path.startswith('$HOME'):
            path = '/home' + path[5:]
        parts = path.split('/')
        parts[2] = username
        return '/'.join(parts)

    def deploy_ssh_keypair(self, username, keypair):
        logger.info('deploy_ssh_keypair')
        username = 'admin'
        path, thumbprint = keypair
        path = self._replace_user(path, username)
        super(GaiaOSUtil, self).deploy_ssh_keypair(
            username, (path, thumbprint))

    def openssl_to_openssh(self, input_file, output_file):
        cryptutil = CryptUtil(conf.get_openssl_cmd())
        ret, out = shellutil.run_get_output(
            conf.get_openssl_cmd() +
            "" rsa -pubin -noout -text -in '"" + input_file + ""'"")
        if ret != 0:
            raise OSUtilError('openssl failed with {0}'.format(ret))

        modulus = []
        exponent = []
        buf = None
        for line in out.split('\n'):
            if line.startswith('Modulus:'):
                buf = modulus
                buf.append(line)
                continue
            if line.startswith('Exponent:'):
                buf = exponent
                buf.append(line)
                continue
            if buf and line:
                buf.append(line.strip().replace(':', ''))

        def text_to_num(buf):
            if len(buf) == 1:
                return int(buf[0].split()[1])
            return long(''.join(buf[1:]), 16)

        n = text_to_num(modulus)
        e = text_to_num(exponent)

        keydata = bytearray()
        keydata.extend(struct.pack('>I', len('ssh-rsa')))
        keydata.extend(b'ssh-rsa')
        keydata.extend(struct.pack('>I', len(cryptutil.num_to_bytes(e))))
        keydata.extend(cryptutil.num_to_bytes(e))
        keydata.extend(struct.pack('>I', len(cryptutil.num_to_bytes(n)) + 1))
        keydata.extend(b'\0')
        keydata.extend(cryptutil.num_to_bytes(n))
        keydata_base64 = base64.b64encode(bytebuffer(keydata))
        fileutil.write_file(output_file,
                            ustr(b'ssh-rsa ' + keydata_base64 + b'\n',
                                 encoding='utf-8'))

    def deploy_ssh_pubkey(self, username, pubkey):
        logger.info('deploy_ssh_pubkey')
        username = 'admin'
        path, thumbprint, value = pubkey
        path = self._replace_user(path, username)
        super(GaiaOSUtil, self).deploy_ssh_pubkey(
            username, (path, thumbprint, value))

    def eject_dvd(self, chk_err=True):
        logger.warn('eject is not supported on GAiA')

    def mount(self, device, mount_point, option="""", chk_err=True):
        logger.info('mount {0} {1} {2}', device, mount_point, option)
        if 'udf,iso9660' in option:
            ret, out = super(GaiaOSUtil, self).mount(
                device, mount_point, option=option.replace('udf,iso9660', 'udf'),
                chk_err=chk_err)
            if not ret:
                return ret, out
        return super(GaiaOSUtil, self).mount(
            device, mount_point, option=option, chk_err=chk_err)

    def allow_dhcp_broadcast(self):
        logger.info('allow_dhcp_broadcast is ignored on GAiA')

    def remove_rules_files(self, rules_files=''):
        pass

    def restore_rules_files(self, rules_files=''):
        logger.info('restore_rules_files is ignored on GAiA')

    def restart_ssh_service(self):
        return shellutil.run('/sbin/service sshd condrestart', chk_err=False)

    def _address_to_string(self, addr):
        return socket.inet_ntoa(struct.pack(""!I"", addr))

    def _get_prefix(self, mask):
        return str(sum([bin(int(x)).count('1') for x in mask.split('.')]))

    def route_add(self, net, mask, gateway):
        logger.info('route_add {0} {1} {2}', net, mask, gateway)

        if net == 0 and mask == 0:
            cidr = 'default'
        else:
            cidr = self._address_to_string(net) + '/' + self._get_prefix(
                self._address_to_string(mask))

        ret, out = self._run_clish(
            'set static-route ' + cidr +
            ' nexthop gateway address ' +
            self._address_to_string(gateway) + ' on')
        return ret

    def set_hostname(self, hostname):
        logger.warn('set_hostname is ignored on GAiA')

    def set_dhcp_hostname(self, hostname):
        logger.warn('set_dhcp_hostname is ignored on GAiA')

    def publish_hostname(self, hostname):
        logger.warn('publish_hostname is ignored on GAiA')

    def del_account(self, username):
        logger.warn('del_account is ignored on GAiA')
/n/n/nazurelinuxagent/common/osutil/openbsd.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
# Copyright 2017 Reyk Floeter <reyk@openbsd.org>
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and OpenSSL 1.0+

import os
import re
import time
import glob
import datetime

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.conf as conf

from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil

UUID_PATTERN = re.compile(
    r'^\s*[A-F0-9]{8}(?:\-[A-F0-9]{4}){3}\-[A-F0-9]{12}\s*$',
    re.IGNORECASE)

class OpenBSDOSUtil(DefaultOSUtil):

    def __init__(self):
        super(OpenBSDOSUtil, self).__init__()
        self.jit_enabled = True
        self._scsi_disks_timeout_set = False

    def get_instance_id(self):
        ret, output = shellutil.run_get_output(""sysctl -n hw.uuid"")
        if ret != 0 or UUID_PATTERN.match(output) is None:
            return """"
        return output.strip()

    def set_hostname(self, hostname):
        fileutil.write_file(""/etc/myname"", ""{}\n"".format(hostname))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('rcctl restart sshd', chk_err=False)

    def start_agent_service(self):
        return shellutil.run('rcctl start waagent', chk_err=False)

    def stop_agent_service(self):
        return shellutil.run('rcctl stop waagent', chk_err=False)

    def register_agent_service(self):
        shellutil.run('chmod 0555 /etc/rc.d/waagent', chk_err=False)
        return shellutil.run('rcctl enable waagent', chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run('rcctl disable waagent', chk_err=False)

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."",
                         username)
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""userdel -r "" + username)
        self.conf_sudoer(username, remove=True)

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        doas_conf = ""/etc/doas.conf""
        doas = None
        if not remove:
            if not os.path.isfile(doas_conf):
                # always allow root to become root
                doas = ""permit keepenv nopass root\n""
                fileutil.append_file(doas_conf, doas)
            if nopasswd:
                doas = ""permit keepenv nopass {0}\n"".format(username)
            else:
                doas = ""permit keepenv persist {0}\n"".format(username)
            fileutil.append_file(doas_conf, doas)
            fileutil.chmod(doas_conf, 0o644)
        else:
            # Remove user from doas.conf
            if os.path.isfile(doas_conf):
                try:
                    content = fileutil.read_file(doas_conf)
                    doas = content.split(""\n"")
                    doas = [x for x in doas if username not in x]
                    fileutil.write_file(doas_conf, ""\n"".join(doas))
                except IOError as err:
                    raise OSUtilError(""Failed to remove sudoer: ""
                                      ""{0}"".format(err))

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user. ""
                               ""Will not set passwd."").format(username))
        cmd = ""echo -n {0}|encrypt"".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to encrypt password for {0}: {1}""
                               """").format(username, output))
        passwd_hash = output.strip()
        cmd = ""usermod -p '{0}' {1}"".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))

    def del_root_password(self):
        ret, output = shellutil.run_get_output('usermod -p ""*"" root')
        if ret:
            raise OSUtilError(""Failed to delete root password: ""
                              ""{0}"".format(output))

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        ret = shellutil.run(""route -n get default"", chk_err=False)
        if ret == 0:
            return False
        return True

    def is_dhcp_enabled(self):
        pass

    def start_dhcp_service(self):
        pass

    def stop_dhcp_service(self):
        pass

    def get_dhcp_lease_endpoint(self):
        """"""
        OpenBSD has a sligthly different lease file format.
        """"""
        endpoint = None
        pathglob = '/var/db/dhclient.leases.{}'.format(self.get_first_if()[0])

        HEADER_LEASE = ""lease""
        HEADER_OPTION = ""option option-245""
        HEADER_EXPIRE = ""expire""
        FOOTER_LEASE = ""}""
        FORMAT_DATETIME = ""%Y/%m/%d %H:%M:%S %Z""

        logger.info(""looking for leases in path [{0}]"".format(pathglob))
        for lease_file in glob.glob(pathglob):
            leases = open(lease_file).read()
            if HEADER_OPTION in leases:
                cached_endpoint = None
                has_option_245 = False
                expired = True  # assume expired
                for line in leases.splitlines():
                    if line.startswith(HEADER_LEASE):
                        cached_endpoint = None
                        has_option_245 = False
                        expired = True
                    elif HEADER_OPTION in line:
                        try:
                            ipaddr = line.split("" "")[-1].strip("";"").split("":"")
                            cached_endpoint = \
                               ""."".join(str(int(d, 16)) for d in ipaddr)
                            has_option_245 = True
                        except ValueError:
                            logger.error(""could not parse '{0}'"".format(line))
                    elif HEADER_EXPIRE in line:
                        if ""never"" in line:
                            expired = False
                        else:
                            try:
                                expire_string = line.split(
                                    "" "", 4)[-1].strip("";"")
                                expire_date = datetime.datetime.strptime(
                                    expire_string, FORMAT_DATETIME)
                                if expire_date > datetime.datetime.utcnow():
                                    expired = False
                            except ValueError:
                                logger.error(""could not parse expiry token ""
                                             ""'{0}'"".format(line))
                    elif FOOTER_LEASE in line:
                        logger.info(""dhcp entry:{0}, 245:{1}, expired: {2}""
                                    .format(cached_endpoint, has_option_245, expired))
                        if not expired and cached_endpoint is not None and has_option_245:
                            endpoint = cached_endpoint
                            logger.info(""found endpoint [{0}]"".format(endpoint))
                            # we want to return the last valid entry, so
                            # keep searching
        if endpoint is not None:
            logger.info(""cached endpoint found [{0}]"".format(endpoint))
        else:
            logger.info(""cached endpoint not found"")
        return endpoint

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 -iface ""
                             ""{0}"".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route delete 255.255.255.255 -iface ""
                      ""{0}"".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret, output = shellutil.run_get_output(""pgrep -n dhclient"",
                                               chk_err=False)
        return output if ret == 0 else None

    def get_dvd_device(self, dev_dir='/dev'):
        pattern = r'cd[0-9]c'
        for dvd in [re.match(pattern, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        raise OSUtilError(""Failed to get DVD device"")

    def mount_dvd(self,
                  max_retry=6,
                  chk_err=True,
                  dvd_device=None,
                  mount_point=None,
                  sleep_time=5):
        if dvd_device is None:
            dvd_device = self.get_dvd_device()
        if mount_point is None:
            mount_point = conf.get_dvd_mount_point()
        if not os.path.isdir(mount_point):
            os.makedirs(mount_point)

        for retry in range(0, max_retry):
            retcode = self.mount(dvd_device,
                                mount_point,
                                option=""-o ro -t udf"",
                                chk_err=False)
            if retcode == 0:
                logger.info(""Successfully mounted DVD"")
                return
            if retry < max_retry - 1:
                mountlist = shellutil.run_get_output(""/sbin/mount"")[1]
                existing = self.get_mount_point(mountlist, dvd_device)
                if existing is not None:
                    logger.info(""{0} is mounted at {1}"", dvd_device, existing)
                    return
                logger.warn(""Mount DVD failed: retry={0}, ret={1}"", retry,
                            retcode)
                time.sleep(sleep_time)
        if chk_err:
            raise OSUtilError(""Failed to mount DVD."")

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""cdio eject {0}"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject DVD: ret={0}"".format(retcode))

    def restart_if(self, ifname, retries=3, wait=5):
        # Restart dhclient only to publish hostname
        shellutil.run(""/sbin/dhclient {0}"".format(ifname), chk_err=False)

    def get_total_mem(self):
        ret, output = shellutil.run_get_output(""sysctl -n hw.physmem"")
        if ret:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output(""sysctl -n hw.ncpu"")
        if ret:
            raise OSUtilError(""Failed to get processor cores."")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def set_scsi_disks_timeout(self, timeout):
        pass

    def check_pid_alive(self, pid):
        if not pid:
            return
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """"""
        There is no SIOCGIFCONF
        on OpenBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """"""
        iface = ''
        inet = ''
        mac = ''

        ret, output = shellutil.run_get_output(
            'ifconfig hvn | grep -E ""^hvn.:"" | sed ""s/:.*//g""', chk_err=False)
        if ret:
            raise OSUtilError(""Can't find ether interface:{0}"".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError(""Can't find ether interface."")
        iface = ifaces[0]

        ret, output = shellutil.run_get_output(
            'ifconfig ' + iface, chk_err=False)
        if ret:
            raise OSUtilError(""Can't get info for interface:{0}"".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('lladdr ') != -1:
                mac = line.split()[1]
        logger.verbose(""Interface info: ({0},{1},{2})"", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        return ""wd{0}"".format(port_id)

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0
/n/n/nazurelinuxagent/common/osutil/redhat.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import base64
import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.future import ustr, bytebuffer
from azurelinuxagent.common.exception import OSUtilError, CryptError
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.osutil.default import DefaultOSUtil


class Redhat6xOSUtil(DefaultOSUtil):

    def __init__(self):
        super(Redhat6xOSUtil, self).__init__()
        self.jit_enabled = True

    def start_network(self):
        return shellutil.run(""/sbin/service networking start"", chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run(""/sbin/service sshd condrestart"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""chkconfig --add waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""chkconfig --del waagent"", chk_err=False)

    def openssl_to_openssh(self, input_file, output_file):
        pubkey = fileutil.read_file(input_file)
        try:
            cryptutil = CryptUtil(conf.get_openssl_cmd())
            ssh_rsa_pubkey = cryptutil.asn1_to_ssh(pubkey)
        except CryptError as e:
            raise OSUtilError(ustr(e))
        fileutil.write_file(output_file, ssh_rsa_pubkey)

    # Override
    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """"""
        Set /etc/sysconfig/network
        """"""
        fileutil.update_conf_file('/etc/sysconfig/network',
                                  'HOSTNAME',
                                  'HOSTNAME={0}'.format(hostname))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def set_dhcp_hostname(self, hostname):
        ifname = self.get_if_name()
        filepath = ""/etc/sysconfig/network-scripts/ifcfg-{0}"".format(ifname)
        fileutil.update_conf_file(filepath,
                                  'DHCP_HOSTNAME',
                                  'DHCP_HOSTNAME={0}'.format(hostname))

    def get_dhcp_lease_endpoint(self):
        return self.get_endpoint_from_leases_path('/var/lib/dhclient/dhclient-*.leases')


class RedhatOSUtil(Redhat6xOSUtil):
    def __init__(self):
        super(RedhatOSUtil, self).__init__()

    def set_hostname(self, hostname):
        """"""
        Unlike redhat 6.x, redhat 7.x will set hostname via hostnamectl
        Due to a bug in systemd in Centos-7.0, if this call fails, fallback
        to hostname.
        """"""
        hostnamectl_cmd = ""hostnamectl set-hostname {0} --static"".format(hostname)
        if shellutil.run(hostnamectl_cmd, chk_err=False) != 0:
            logger.warn(""[{0}] failed, attempting fallback"".format(hostnamectl_cmd))
            DefaultOSUtil.set_hostname(self, hostname)

    def publish_hostname(self, hostname):
        """"""
        Restart NetworkManager first before publishing hostname
        """"""
        shellutil.run(""service NetworkManager restart"")
        super(RedhatOSUtil, self).publish_hostname(hostname)

    def register_agent_service(self):
        return shellutil.run(""systemctl enable waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""systemctl disable waagent"", chk_err=False)

    def openssl_to_openssh(self, input_file, output_file):
        DefaultOSUtil.openssl_to_openssh(self, input_file, output_file)

    def get_dhcp_lease_endpoint(self):
        # dhclient
        endpoint = self.get_endpoint_from_leases_path('/var/lib/dhclient/dhclient-*.lease')

        if endpoint is None:
            # NetworkManager
            endpoint = self.get_endpoint_from_leases_path('/var/lib/NetworkManager/dhclient-*.lease')

        return endpoint
/n/n/nazurelinuxagent/common/osutil/suse.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import re
import pwd
import shutil
import socket
import array
import struct
import fcntl
import time
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
from azurelinuxagent.common.version import DISTRO_NAME, DISTRO_VERSION, DISTRO_FULL_NAME
from azurelinuxagent.common.osutil.default import DefaultOSUtil

class SUSE11OSUtil(DefaultOSUtil):

    def __init__(self):
        super(SUSE11OSUtil, self).__init__()
        self.jit_enabled = True
        self.dhclient_name='dhcpcd'

    def set_hostname(self, hostname):
        fileutil.write_file('/etc/HOSTNAME', hostname)
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof {0}"".format(self.dhclient_name),
                                       chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def is_dhcp_enabled(self):
        return True

    def stop_dhcp_service(self):
        cmd = ""/sbin/service {0} stop"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_dhcp_service(self):
        cmd = ""/sbin/service {0} start"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_network(self) :
        return shellutil.run(""/sbin/service start network"", chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run(""/sbin/service sshd restart"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""/sbin/insserv waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""/sbin/insserv -r waagent"", chk_err=False)

class SUSEOSUtil(SUSE11OSUtil):
    def __init__(self):
        super(SUSEOSUtil, self).__init__()
        self.dhclient_name = 'wickedd-dhcp4'

    def stop_dhcp_service(self):
        cmd = ""systemctl stop {0}"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_dhcp_service(self):
        cmd = ""systemctl start {0}"".format(self.dhclient_name)
        return shellutil.run(cmd, chk_err=False)

    def start_network(self) :
        return shellutil.run(""systemctl start network"", chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run(""systemctl restart sshd"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop waagent"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""systemctl start waagent"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""systemctl enable waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""systemctl disable waagent"", chk_err=False)


/n/n/nazurelinuxagent/common/osutil/ubuntu.py/n/n#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import os
import time

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil

from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.osutil.default import DefaultOSUtil


def _cgroup_path(tail=""""):
    return os.path.join('/sys/fs/cgroup/', tail).rstrip(os.path.sep)


class Ubuntu14OSUtil(DefaultOSUtil):

    def __init__(self):
        super(Ubuntu14OSUtil, self).__init__()
        self.jit_enabled = True

    def start_network(self):
        return shellutil.run(""service networking start"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""service walinuxagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""service walinuxagent start"", chk_err=False)

    def remove_rules_files(self, rules_files=""""):
        pass

    def restore_rules_files(self, rules_files=""""):
        pass

    def get_dhcp_lease_endpoint(self):
        return self.get_endpoint_from_leases_path('/var/lib/dhcp/dhclient.*.leases')

    def is_cgroups_supported(self):
        if 'TRAVIS' in os.environ and os.environ['TRAVIS'] == 'true':
            return False
        return True

    def mount_cgroups(self):
        try:
            if not os.path.exists(_cgroup_path()):
                fileutil.mkdir(_cgroup_path())
                self.mount(device='cgroup_root',
                           mount_point=_cgroup_path(),
                           option=""-t tmpfs"",
                           chk_err=False)
            elif not os.path.isdir(_cgroup_path()):
                logger.error(""Could not mount cgroups: ordinary file at {0}"".format(_cgroup_path()))
                return

            for metric_hierarchy in ['cpu,cpuacct', 'memory']:
                target_path = _cgroup_path(metric_hierarchy)
                if not os.path.exists(target_path):
                    fileutil.mkdir(target_path)
                self.mount(device=metric_hierarchy,
                           mount_point=target_path,
                           option=""-t cgroup -o {0}"".format(metric_hierarchy),
                           chk_err=False)

            for metric_hierarchy in ['cpu', 'cpuacct']:
                target_path = _cgroup_path(metric_hierarchy)
                if not os.path.exists(target_path):
                    os.symlink(_cgroup_path('cpu,cpuacct'), target_path)

        except Exception as e:
            logger.error(""Could not mount cgroups: {0}"", ustr(e))


class Ubuntu12OSUtil(Ubuntu14OSUtil):
    def __init__(self):
        super(Ubuntu12OSUtil, self).__init__()

    # Override
    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof dhclient3"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def mount_cgroups(self):
        pass

class Ubuntu16OSUtil(Ubuntu14OSUtil):
    """"""
    Ubuntu 16.04, 16.10, and 17.04.
    """"""
    def __init__(self):
        super(Ubuntu16OSUtil, self).__init__()

    def register_agent_service(self):
        return shellutil.run(""systemctl unmask walinuxagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""systemctl mask walinuxagent"", chk_err=False)

    def mount_cgroups(self):
        """"""
        Mounted by default in Ubuntu 16.04
        """"""
        pass


class Ubuntu18OSUtil(Ubuntu16OSUtil):
    """"""
    Ubuntu 18.04
    """"""
    def __init__(self):
        super(Ubuntu18OSUtil, self).__init__()

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pidof systemd-networkd"")
        return ret[1] if ret[0] == 0 else None

    def start_network(self):
        return shellutil.run(""systemctl start systemd-networkd"", chk_err=False)

    def stop_network(self):
        return shellutil.run(""systemctl stop systemd-networkd"", chk_err=False)

    def start_dhcp_service(self):
        return self.start_network()

    def stop_dhcp_service(self):
        return self.stop_network()

    def start_agent_service(self):
        return shellutil.run(""systemctl start walinuxagent"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""systemctl stop walinuxagent"", chk_err=False)


class UbuntuOSUtil(Ubuntu16OSUtil):
    def __init__(self):
        super(UbuntuOSUtil, self).__init__()

    def restart_if(self, ifname, retries=3, wait=5):
        """"""
        Restart an interface by bouncing the link. systemd-networkd observes
        this event, and forces a renew of DHCP.
        """"""
        retry_limit=retries+1
        for attempt in range(1, retry_limit):
            return_code=shellutil.run(""ip link set {0} down && ip link set {0} up"".format(ifname))
            if return_code == 0:
                return
            logger.warn(""failed to restart {0}: return code {1}"".format(ifname, return_code))
            if attempt < retry_limit:
                logger.info(""retrying in {0} seconds"".format(wait))
                time.sleep(wait)
            else:
                logger.warn(""exceeded restart retries"")


class UbuntuSnappyOSUtil(Ubuntu14OSUtil):
    def __init__(self):
        super(UbuntuSnappyOSUtil, self).__init__()
        self.conf_file_path = '/apps/walinuxagent/current/waagent.conf'

    def mount_cgroups(self):
        """"""
        Already mounted in Snappy
        """"""
        pass
/n/n/nazurelinuxagent/common/protocol/restapi.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#
import socket
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.restutil as restutil
from azurelinuxagent.common.exception import ProtocolError, HttpError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.version import DISTRO_VERSION, DISTRO_NAME, CURRENT_VERSION


def validate_param(name, val, expected_type):
    if val is None:
        raise ProtocolError(""{0} is None"".format(name))
    if not isinstance(val, expected_type):
        raise ProtocolError((""{0} type should be {1} not {2}""
                             """").format(name, expected_type, type(val)))


def set_properties(name, obj, data):
    if isinstance(obj, DataContract):
        validate_param(""Property '{0}'"".format(name), data, dict)
        for prob_name, prob_val in data.items():
            prob_full_name = ""{0}.{1}"".format(name, prob_name)
            try:
                prob = getattr(obj, prob_name)
            except AttributeError:
                logger.warn(""Unknown property: {0}"", prob_full_name)
                continue
            prob = set_properties(prob_full_name, prob, prob_val)
            setattr(obj, prob_name, prob)
        return obj
    elif isinstance(obj, DataContractList):
        validate_param(""List '{0}'"".format(name), data, list)
        for item_data in data:
            item = obj.item_cls()
            item = set_properties(name, item, item_data)
            obj.append(item)
        return obj
    else:
        return data


def get_properties(obj):
    if isinstance(obj, DataContract):
        data = {}
        props = vars(obj)
        for prob_name, prob in list(props.items()):
            data[prob_name] = get_properties(prob)
        return data
    elif isinstance(obj, DataContractList):
        data = []
        for item in obj:
            item_data = get_properties(item)
            data.append(item_data)
        return data
    else:
        return obj


class DataContract(object):
    pass


class DataContractList(list):
    def __init__(self, item_cls):
        self.item_cls = item_cls


""""""
Data contract between guest and host
""""""


class VMInfo(DataContract):
    def __init__(self,
                 subscriptionId=None,
                 vmName=None,
                 containerId=None,
                 roleName=None,
                 roleInstanceName=None,
                 tenantName=None):
        self.subscriptionId = subscriptionId
        self.vmName = vmName
        self.containerId = containerId
        self.roleName = roleName
        self.roleInstanceName = roleInstanceName
        self.tenantName = tenantName


class CertificateData(DataContract):
    def __init__(self, certificateData=None):
        self.certificateData = certificateData


class Cert(DataContract):
    def __init__(self,
                 name=None,
                 thumbprint=None,
                 certificateDataUri=None,
                 storeName=None,
                 storeLocation=None):
        self.name = name
        self.thumbprint = thumbprint
        self.certificateDataUri = certificateDataUri
        self.storeLocation = storeLocation
        self.storeName = storeName


class CertList(DataContract):
    def __init__(self):
        self.certificates = DataContractList(Cert)


# TODO: confirm vmagent manifest schema
class VMAgentManifestUri(DataContract):
    def __init__(self, uri=None):
        self.uri = uri


class VMAgentManifest(DataContract):
    def __init__(self, family=None):
        self.family = family
        self.versionsManifestUris = DataContractList(VMAgentManifestUri)


class VMAgentManifestList(DataContract):
    def __init__(self):
        self.vmAgentManifests = DataContractList(VMAgentManifest)


class Extension(DataContract):
    def __init__(self,
                 name=None,
                 sequenceNumber=None,
                 publicSettings=None,
                 protectedSettings=None,
                 certificateThumbprint=None):
        self.name = name
        self.sequenceNumber = sequenceNumber
        self.publicSettings = publicSettings
        self.protectedSettings = protectedSettings
        self.certificateThumbprint = certificateThumbprint


class ExtHandlerProperties(DataContract):
    def __init__(self):
        self.version = None
        self.upgradePolicy = None
        self.upgradeGuid = None
        self.dependencyLevel = None
        self.state = None
        self.extensions = DataContractList(Extension)


class ExtHandlerVersionUri(DataContract):
    def __init__(self):
        self.uri = None


class ExtHandler(DataContract):
    def __init__(self, name=None):
        self.name = name
        self.properties = ExtHandlerProperties()
        self.versionUris = DataContractList(ExtHandlerVersionUri)

    def sort_key(self):
        level = self.properties.dependencyLevel
        if level is None:
            level = 0
        # Process uninstall or disabled before enabled, in reverse order
        # remap 0 to -1, 1 to -2, 2 to -3, etc
        if self.properties.state != u""enabled"":
            level = (0 - level) - 1
        return level


class ExtHandlerList(DataContract):
    def __init__(self):
        self.extHandlers = DataContractList(ExtHandler)


class ExtHandlerPackageUri(DataContract):
    def __init__(self, uri=None):
        self.uri = uri


class ExtHandlerPackage(DataContract):
    def __init__(self, version=None):
        self.version = version
        self.uris = DataContractList(ExtHandlerPackageUri)
        # TODO update the naming to align with metadata protocol
        self.isinternal = False
        self.disallow_major_upgrade = False


class ExtHandlerPackageList(DataContract):
    def __init__(self):
        self.versions = DataContractList(ExtHandlerPackage)


class VMProperties(DataContract):
    def __init__(self, certificateThumbprint=None):
        # TODO need to confirm the property name
        self.certificateThumbprint = certificateThumbprint


class ProvisionStatus(DataContract):
    def __init__(self, status=None, subStatus=None, description=None):
        self.status = status
        self.subStatus = subStatus
        self.description = description
        self.properties = VMProperties()


class ExtensionSubStatus(DataContract):
    def __init__(self, name=None, status=None, code=None, message=None):
        self.name = name
        self.status = status
        self.code = code
        self.message = message


class ExtensionStatus(DataContract):
    def __init__(self,
                 configurationAppliedTime=None,
                 operation=None,
                 status=None,
                 seq_no=None,
                 code=None,
                 message=None):
        self.configurationAppliedTime = configurationAppliedTime
        self.operation = operation
        self.status = status
        self.sequenceNumber = seq_no
        self.code = code
        self.message = message
        self.substatusList = DataContractList(ExtensionSubStatus)


class ExtHandlerStatus(DataContract):
    def __init__(self,
                 name=None,
                 version=None,
                 upgradeGuid=None,
                 status=None,
                 code=0,
                 message=None):
        self.name = name
        self.version = version
        self.upgradeGuid = upgradeGuid
        self.status = status
        self.code = code
        self.message = message
        self.extensions = DataContractList(ustr)


class VMAgentStatus(DataContract):
    def __init__(self, status=None, message=None):
        self.status = status
        self.message = message
        self.hostname = socket.gethostname()
        self.version = str(CURRENT_VERSION)
        self.osname = DISTRO_NAME
        self.osversion = DISTRO_VERSION
        self.extensionHandlers = DataContractList(ExtHandlerStatus)


class VMStatus(DataContract):
    def __init__(self, status, message):
        self.vmAgent = VMAgentStatus(status=status, message=message)


class TelemetryEventParam(DataContract):
    def __init__(self, name=None, value=None):
        self.name = name
        self.value = value


class TelemetryEvent(DataContract):
    def __init__(self, eventId=None, providerId=None):
        self.eventId = eventId
        self.providerId = providerId
        self.parameters = DataContractList(TelemetryEventParam)


class TelemetryEventList(DataContract):
    def __init__(self):
        self.events = DataContractList(TelemetryEvent)

class RemoteAccessUser(DataContract):
    def __init__(self, name, encrypted_password, expiration):
        self.name = name
        self.encrypted_password = encrypted_password
        self.expiration = expiration

class RemoteAccessUsersList(DataContract):
    def __init__(self):
        self.users = DataContractList(RemoteAccessUser)


class Protocol(DataContract):
    def detect(self):
        raise NotImplementedError()

    def get_vminfo(self):
        raise NotImplementedError()

    def get_certs(self):
        raise NotImplementedError()

    def get_incarnation(self):
        raise NotImplementedError()

    def get_vmagent_manifests(self):
        raise NotImplementedError()

    def get_vmagent_pkgs(self, manifest):
        raise NotImplementedError()

    def get_ext_handlers(self):
        raise NotImplementedError()

    def get_ext_handler_pkgs(self, extension):
        raise NotImplementedError()

    def get_artifacts_profile(self):
        raise NotImplementedError()

    def download_ext_handler_pkg(self, uri, headers=None, use_proxy=True):
        pkg = None
        try:
            resp = restutil.http_get(uri, headers=headers, use_proxy=use_proxy)
            if restutil.request_succeeded(resp):
                pkg = resp.read()
        except Exception as e:
            logger.warn(""Failed to download from: {0}"".format(uri), e)
        return pkg

    def report_provision_status(self, provision_status):
        raise NotImplementedError()

    def report_vm_status(self, vm_status):
        raise NotImplementedError()

    def report_ext_status(self, ext_handler_name, ext_name, ext_status):
        raise NotImplementedError()

    def report_event(self, event):
        raise NotImplementedError()
/n/n/nazurelinuxagent/common/protocol/wire.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

from datetime import datetime

import json
import os
import random
import re
import sys
import time
import xml.sax.saxutils as saxutils

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.textutil as textutil

from azurelinuxagent.common.exception import ProtocolNotFoundError, \
                                            ResourceGoneError
from azurelinuxagent.common.future import httpclient, bytebuffer
from azurelinuxagent.common.protocol.hostplugin import HostPluginProtocol, URI_FORMAT_GET_EXTENSION_ARTIFACT, \
    HOST_PLUGIN_PORT
from azurelinuxagent.common.protocol.restapi import *
from azurelinuxagent.common.utils.archive import StateFlusher
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.utils.textutil import parse_doc, findall, find, \
    findtext, getattrib, gettext, remove_bom, get_bytes_from_pem, parse_json
from azurelinuxagent.common.version import AGENT_NAME
from azurelinuxagent.common.osutil import get_osutil

VERSION_INFO_URI = ""http://{0}/?comp=versions""
GOAL_STATE_URI = ""http://{0}/machine/?comp=goalstate""
HEALTH_REPORT_URI = ""http://{0}/machine?comp=health""
ROLE_PROP_URI = ""http://{0}/machine?comp=roleProperties""
TELEMETRY_URI = ""http://{0}/machine?comp=telemetrydata""

WIRE_SERVER_ADDR_FILE_NAME = ""WireServer""
INCARNATION_FILE_NAME = ""Incarnation""
GOAL_STATE_FILE_NAME = ""GoalState.{0}.xml""
HOSTING_ENV_FILE_NAME = ""HostingEnvironmentConfig.xml""
SHARED_CONF_FILE_NAME = ""SharedConfig.xml""
CERTS_FILE_NAME = ""Certificates.xml""
REMOTE_ACCESS_FILE_NAME = ""RemoteAccess.{0}.xml""
P7M_FILE_NAME = ""Certificates.p7m""
PEM_FILE_NAME = ""Certificates.pem""
EXT_CONF_FILE_NAME = ""ExtensionsConfig.{0}.xml""
MANIFEST_FILE_NAME = ""{0}.{1}.manifest.xml""
AGENTS_MANIFEST_FILE_NAME = ""{0}.{1}.agentsManifest""
TRANSPORT_CERT_FILE_NAME = ""TransportCert.pem""
TRANSPORT_PRV_FILE_NAME = ""TransportPrivate.pem""

PROTOCOL_VERSION = ""2012-11-30""
ENDPOINT_FINE_NAME = ""WireServer""

SHORT_WAITING_INTERVAL = 1  # 1 second


class UploadError(HttpError):
    pass


class WireProtocol(Protocol):
    """"""Slim layer to adapt wire protocol data to metadata protocol interface""""""

    # TODO: Clean-up goal state processing
    #  At present, some methods magically update GoalState (e.g.,
    #  get_vmagent_manifests), others (e.g., get_vmagent_pkgs)
    #  assume its presence. A better approach would make an explicit update
    #  call that returns the incarnation number and
    #  establishes that number the ""context"" for all other calls (either by
    #  updating the internal state of the protocol or
    #  by having callers pass the incarnation number to the method).

    def __init__(self, endpoint):
        if endpoint is None:
            raise ProtocolError(""WireProtocol endpoint is None"")
        self.endpoint = endpoint
        self.client = WireClient(self.endpoint)

    def detect(self):
        self.client.check_wire_protocol_version()

        trans_prv_file = os.path.join(conf.get_lib_dir(),
                                      TRANSPORT_PRV_FILE_NAME)
        trans_cert_file = os.path.join(conf.get_lib_dir(),
                                       TRANSPORT_CERT_FILE_NAME)
        cryptutil = CryptUtil(conf.get_openssl_cmd())
        cryptutil.gen_transport_cert(trans_prv_file, trans_cert_file)

        self.update_goal_state(forced=True)

    def update_goal_state(self, forced=False, max_retry=3):
        self.client.update_goal_state(forced=forced, max_retry=max_retry)

    def get_vminfo(self):
        goal_state = self.client.get_goal_state()
        hosting_env = self.client.get_hosting_env()

        vminfo = VMInfo()
        vminfo.subscriptionId = None
        vminfo.vmName = hosting_env.vm_name
        vminfo.tenantName = hosting_env.deployment_name
        vminfo.roleName = hosting_env.role_name
        vminfo.roleInstanceName = goal_state.role_instance_id
        vminfo.containerId = goal_state.container_id
        return vminfo

    def get_certs(self):
        certificates = self.client.get_certs()
        return certificates.cert_list

    def get_incarnation(self):
        path = os.path.join(conf.get_lib_dir(), INCARNATION_FILE_NAME)
        if os.path.exists(path):
            return fileutil.read_file(path)
        else:
            return 0

    def get_vmagent_manifests(self):
        # Update goal state to get latest extensions config
        self.update_goal_state()
        goal_state = self.client.get_goal_state()
        ext_conf = self.client.get_ext_conf()
        return ext_conf.vmagent_manifests, goal_state.incarnation

    def get_vmagent_pkgs(self, vmagent_manifest):
        goal_state = self.client.get_goal_state()
        ga_manifest = self.client.get_gafamily_manifest(vmagent_manifest, goal_state)
        valid_pkg_list = self.client.filter_package_list(vmagent_manifest.family, ga_manifest, goal_state)
        return valid_pkg_list

    def get_ext_handlers(self):
        logger.verbose(""Get extension handler config"")
        # Update goal state to get latest extensions config
        self.update_goal_state()
        goal_state = self.client.get_goal_state()
        ext_conf = self.client.get_ext_conf()
        # In wire protocol, incarnation is equivalent to ETag
        return ext_conf.ext_handlers, goal_state.incarnation

    def get_ext_handler_pkgs(self, ext_handler):
        logger.verbose(""Get extension handler package"")
        goal_state = self.client.get_goal_state()
        man = self.client.get_ext_manifest(ext_handler, goal_state)
        return man.pkg_list

    def get_artifacts_profile(self):
        logger.verbose(""Get In-VM Artifacts Profile"")
        return self.client.get_artifacts_profile()

    def download_ext_handler_pkg(self, uri, headers=None, use_proxy=True):
        package = self.client.fetch(uri, headers=headers, use_proxy=use_proxy, decode=False)

        if package is None:
            logger.verbose(""Download did not succeed, falling back to host plugin"")
            host = self.client.get_host_plugin()
            uri, headers = host.get_artifact_request(uri, host.manifest_uri)
            package = self.client.fetch(uri, headers=headers, use_proxy=False, decode=False)

        return package

    def report_provision_status(self, provision_status):
        validate_param(""provision_status"", provision_status, ProvisionStatus)

        if provision_status.status is not None:
            self.client.report_health(provision_status.status,
                                      provision_status.subStatus,
                                      provision_status.description)
        if provision_status.properties.certificateThumbprint is not None:
            thumbprint = provision_status.properties.certificateThumbprint
            self.client.report_role_prop(thumbprint)

    def report_vm_status(self, vm_status):
        validate_param(""vm_status"", vm_status, VMStatus)
        self.client.status_blob.set_vm_status(vm_status)
        self.client.upload_status_blob()

    def report_ext_status(self, ext_handler_name, ext_name, ext_status):
        validate_param(""ext_status"", ext_status, ExtensionStatus)
        self.client.status_blob.set_ext_status(ext_handler_name, ext_status)

    def report_event(self, events):
        validate_param(""events"", events, TelemetryEventList)
        self.client.report_event(events)


def _build_role_properties(container_id, role_instance_id, thumbprint):
    xml = (u""<?xml version=\""1.0\"" encoding=\""utf-8\""?>""
           u""<RoleProperties>""
           u""<Container>""
           u""<ContainerId>{0}</ContainerId>""
           u""<RoleInstances>""
           u""<RoleInstance>""
           u""<Id>{1}</Id>""
           u""<Properties>""
           u""<Property name=\""CertificateThumbprint\"" value=\""{2}\"" />""
           u""</Properties>""
           u""</RoleInstance>""
           u""</RoleInstances>""
           u""</Container>""
           u""</RoleProperties>""
           u"""").format(container_id, role_instance_id, thumbprint)
    return xml


def _build_health_report(incarnation, container_id, role_instance_id,
                         status, substatus, description):
    # Escape '&', '<' and '>'
    description = saxutils.escape(ustr(description))
    detail = u''
    if substatus is not None:
        substatus = saxutils.escape(ustr(substatus))
        detail = (u""<Details>""
                  u""<SubStatus>{0}</SubStatus>""
                  u""<Description>{1}</Description>""
                  u""</Details>"").format(substatus, description)
    xml = (u""<?xml version=\""1.0\"" encoding=\""utf-8\""?>""
           u""<Health ""
           u""xmlns:xsi=\""http://www.w3.org/2001/XMLSchema-instance\""""
           u"" xmlns:xsd=\""http://www.w3.org/2001/XMLSchema\"">""
           u""<GoalStateIncarnation>{0}</GoalStateIncarnation>""
           u""<Container>""
           u""<ContainerId>{1}</ContainerId>""
           u""<RoleInstanceList>""
           u""<Role>""
           u""<InstanceId>{2}</InstanceId>""
           u""<Health>""
           u""<State>{3}</State>""
           u""{4}""
           u""</Health>""
           u""</Role>""
           u""</RoleInstanceList>""
           u""</Container>""
           u""</Health>""
           u"""").format(incarnation,
                       container_id,
                       role_instance_id,
                       status,
                       detail)
    return xml


def ga_status_to_guest_info(ga_status):
    """"""
    Convert VMStatus object to status blob format
    """"""
    v1_ga_guest_info = {
        ""computerName"" : ga_status.hostname,
        ""osName"" : ga_status.osname,
        ""osVersion"" : ga_status.osversion,
        ""version"" : ga_status.version,
    }
    return v1_ga_guest_info


def ga_status_to_v1(ga_status):
    formatted_msg = {
        'lang': 'en-US',
        'message': ga_status.message
    }
    v1_ga_status = {
        ""version"" : ga_status.version,
        ""status"" : ga_status.status,
        ""formattedMessage"" : formatted_msg
    }
    return v1_ga_status


def ext_substatus_to_v1(sub_status_list):
    status_list = []
    for substatus in sub_status_list:
        status = {
            ""name"": substatus.name,
            ""status"": substatus.status,
            ""code"": substatus.code,
            ""formattedMessage"": {
                ""lang"": ""en-US"",
                ""message"": substatus.message
            }
        }
        status_list.append(status)
    return status_list


def ext_status_to_v1(ext_name, ext_status):
    if ext_status is None:
        return None
    timestamp = time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime())
    v1_sub_status = ext_substatus_to_v1(ext_status.substatusList)
    v1_ext_status = {
        ""status"": {
            ""name"": ext_name,
            ""configurationAppliedTime"": ext_status.configurationAppliedTime,
            ""operation"": ext_status.operation,
            ""status"": ext_status.status,
            ""code"": ext_status.code,
            ""formattedMessage"": {
                ""lang"": ""en-US"",
                ""message"": ext_status.message
            }
        },
        ""version"": 1.0,
        ""timestampUTC"": timestamp
    }
    if len(v1_sub_status) != 0:
        v1_ext_status['status']['substatus'] = v1_sub_status
    return v1_ext_status


def ext_handler_status_to_v1(handler_status, ext_statuses, timestamp):
    v1_handler_status = {
        'handlerVersion': handler_status.version,
        'handlerName': handler_status.name,
        'status': handler_status.status,
        'code': handler_status.code
    }
    if handler_status.message is not None:
        v1_handler_status[""formattedMessage""] = {
            ""lang"": ""en-US"",
            ""message"": handler_status.message
        }

    if handler_status.upgradeGuid is not None:
        v1_handler_status[""upgradeGuid""] = handler_status.upgradeGuid

    if len(handler_status.extensions) > 0:
        # Currently, no more than one extension per handler
        ext_name = handler_status.extensions[0]
        ext_status = ext_statuses.get(ext_name)
        v1_ext_status = ext_status_to_v1(ext_name, ext_status)
        if ext_status is not None and v1_ext_status is not None:
            v1_handler_status[""runtimeSettingsStatus""] = {
                'settingsStatus': v1_ext_status,
                'sequenceNumber': ext_status.sequenceNumber
            }
    return v1_handler_status


def vm_status_to_v1(vm_status, ext_statuses):
    timestamp = time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime())

    v1_ga_guest_info = ga_status_to_guest_info(vm_status.vmAgent)
    v1_ga_status = ga_status_to_v1(vm_status.vmAgent)
    v1_handler_status_list = []
    for handler_status in vm_status.vmAgent.extensionHandlers:
        v1_handler_status = ext_handler_status_to_v1(handler_status,
                                                     ext_statuses, timestamp)
        if v1_handler_status is not None:
            v1_handler_status_list.append(v1_handler_status)

    v1_agg_status = {
        'guestAgentStatus': v1_ga_status,
        'handlerAggregateStatus': v1_handler_status_list
    }
    v1_vm_status = {
        'version': '1.1',
        'timestampUTC': timestamp,
        'aggregateStatus': v1_agg_status,
        'guestOSInfo' : v1_ga_guest_info
    }
    return v1_vm_status


class StatusBlob(object):
    def __init__(self, client):
        self.vm_status = None
        self.ext_statuses = {}
        self.client = client
        self.type = None
        self.data = None

    def set_vm_status(self, vm_status):
        validate_param(""vmAgent"", vm_status, VMStatus)
        self.vm_status = vm_status

    def set_ext_status(self, ext_handler_name, ext_status):
        validate_param(""extensionStatus"", ext_status, ExtensionStatus)
        self.ext_statuses[ext_handler_name] = ext_status

    def to_json(self):
        report = vm_status_to_v1(self.vm_status, self.ext_statuses)
        return json.dumps(report)

    __storage_version__ = ""2014-02-14""

    def prepare(self, blob_type):
        logger.verbose(""Prepare status blob"")
        self.data = self.to_json()
        self.type = blob_type

    def upload(self, url):
        try:
            if not self.type in [""BlockBlob"", ""PageBlob""]:
                raise ProtocolError(""Illegal blob type: {0}"".format(self.type))

            if self.type == ""BlockBlob"":
                self.put_block_blob(url, self.data)
            else:
                self.put_page_blob(url, self.data)
            return True

        except Exception as e:
            logger.verbose(""Initial status upload failed: {0}"", e)

        return False

    def get_block_blob_headers(self, blob_size):
        return {
            ""Content-Length"": ustr(blob_size),
            ""x-ms-blob-type"": ""BlockBlob"",
            ""x-ms-date"": time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime()),
            ""x-ms-version"": self.__class__.__storage_version__
        }

    def put_block_blob(self, url, data):
        logger.verbose(""Put block blob"")
        headers = self.get_block_blob_headers(len(data))
        resp = self.client.call_storage_service(restutil.http_put, url, data, headers)
        if resp.status != httpclient.CREATED:
            raise UploadError(
                ""Failed to upload block blob: {0}"".format(resp.status))

    def get_page_blob_create_headers(self, blob_size):
        return {
            ""Content-Length"": ""0"",
            ""x-ms-blob-content-length"": ustr(blob_size),
            ""x-ms-blob-type"": ""PageBlob"",
            ""x-ms-date"": time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime()),
            ""x-ms-version"": self.__class__.__storage_version__
        }

    def get_page_blob_page_headers(self, start, end):
        return {
            ""Content-Length"": ustr(end - start),
            ""x-ms-date"": time.strftime(""%Y-%m-%dT%H:%M:%SZ"", time.gmtime()),
            ""x-ms-range"": ""bytes={0}-{1}"".format(start, end - 1),
            ""x-ms-page-write"": ""update"",
            ""x-ms-version"": self.__class__.__storage_version__
        }

    def put_page_blob(self, url, data):
        logger.verbose(""Put page blob"")

        # Convert string into bytes and align to 512 bytes
        data = bytearray(data, encoding='utf-8')
        page_blob_size = int((len(data) + 511) / 512) * 512

        headers = self.get_page_blob_create_headers(page_blob_size)
        resp = self.client.call_storage_service(restutil.http_put, url, """", headers)
        if resp.status != httpclient.CREATED:
            raise UploadError(
                ""Failed to clean up page blob: {0}"".format(resp.status))

        if url.count(""?"") <= 0:
            url = ""{0}?comp=page"".format(url)
        else:
            url = ""{0}&comp=page"".format(url)

        logger.verbose(""Upload page blob"")
        page_max = 4 * 1024 * 1024  # Max page size: 4MB
        start = 0
        end = 0
        while end < len(data):
            end = min(len(data), start + page_max)
            content_size = end - start
            # Align to 512 bytes
            page_end = int((end + 511) / 512) * 512
            buf_size = page_end - start
            buf = bytearray(buf_size)
            buf[0: content_size] = data[start: end]
            headers = self.get_page_blob_page_headers(start, page_end)
            resp = self.client.call_storage_service(
                restutil.http_put,
                url,
                bytebuffer(buf),
                headers)
            if resp is None or resp.status != httpclient.CREATED:
                raise UploadError(
                    ""Failed to upload page blob: {0}"".format(resp.status))
            start = end


def event_param_to_v1(param):
    param_format = '<Param Name=""{0}"" Value={1} T=""{2}"" />'
    param_type = type(param.value)
    attr_type = """"
    if param_type is int:
        attr_type = 'mt:uint64'
    elif param_type is str:
        attr_type = 'mt:wstr'
    elif ustr(param_type).count(""'unicode'"") > 0:
        attr_type = 'mt:wstr'
    elif param_type is bool:
        attr_type = 'mt:bool'
    elif param_type is float:
        attr_type = 'mt:float64'
    return param_format.format(param.name,
                               saxutils.quoteattr(ustr(param.value)),
                               attr_type)


def event_to_v1(event):
    params = """"
    for param in event.parameters:
        params += event_param_to_v1(param)
    event_str = ('<Event id=""{0}"">'
                 '<![CDATA[{1}]]>'
                 '</Event>').format(event.eventId, params)
    return event_str


class WireClient(object):
    def __init__(self, endpoint):
        logger.info(""Wire server endpoint:{0}"", endpoint)
        self.endpoint = endpoint
        self.goal_state = None
        self.updated = None
        self.hosting_env = None
        self.shared_conf = None
        self.remote_access = None
        self.certs = None
        self.ext_conf = None
        self.host_plugin = None
        self.status_blob = StatusBlob(self)
        self.goal_state_flusher = StateFlusher(conf.get_lib_dir())

    def call_wireserver(self, http_req, *args, **kwargs):
        try:
            # Never use the HTTP proxy for wireserver
            kwargs['use_proxy'] = False
            resp = http_req(*args, **kwargs)

            if restutil.request_failed(resp):
                msg = ""[Wireserver Failed] URI {0} "".format(args[0])
                if resp is not None:
                    msg += "" [HTTP Failed] Status Code {0}"".format(resp.status)
                raise ProtocolError(msg)

        # If the GoalState is stale, pass along the exception to the caller
        except ResourceGoneError:
            raise

        except Exception as e:
            raise ProtocolError(""[Wireserver Exception] {0}"".format(
                ustr(e)))

        return resp

    def decode_config(self, data):
        if data is None:
            return None
        data = remove_bom(data)
        xml_text = ustr(data, encoding='utf-8')
        return xml_text

    def fetch_config(self, uri, headers):
        resp = self.call_wireserver(restutil.http_get,
                                    uri,
                                    headers=headers)
        return self.decode_config(resp.read())

    def fetch_cache(self, local_file):
        if not os.path.isfile(local_file):
            raise ProtocolError(""{0} is missing."".format(local_file))
        try:
            return fileutil.read_file(local_file)
        except IOError as e:
            raise ProtocolError(""Failed to read cache: {0}"".format(e))

    def save_cache(self, local_file, data):
        try:
            fileutil.write_file(local_file, data)
        except IOError as e:
            fileutil.clean_ioerror(e,
                paths=[local_file])
            raise ProtocolError(""Failed to write cache: {0}"".format(e))

    @staticmethod
    def call_storage_service(http_req, *args, **kwargs):
        # Default to use the configured HTTP proxy
        if not 'use_proxy' in kwargs or kwargs['use_proxy'] is None:
            kwargs['use_proxy'] = True

        return http_req(*args, **kwargs)

    def fetch_manifest(self, version_uris):
        logger.verbose(""Fetch manifest"")
        version_uris_shuffled = version_uris
        random.shuffle(version_uris_shuffled)

        for version in version_uris_shuffled:
            # GA expects a location and failoverLocation in ExtensionsConfig, but
            # this is not always the case. See #1147.
            if version.uri is None:
                logger.verbose('The specified manifest URL is empty, ignored.')
                continue

            response = None
            if not HostPluginProtocol.is_default_channel():
                response = self.fetch(version.uri)

            if not response:
                if HostPluginProtocol.is_default_channel():
                    logger.verbose(""Using host plugin as default channel"")
                else:
                    logger.verbose(""Failed to download manifest, ""
                                   ""switching to host plugin"")

                try:
                    host = self.get_host_plugin()
                    uri, headers = host.get_artifact_request(version.uri)
                    response = self.fetch(uri, headers, use_proxy=False)

                # If the HostPlugin rejects the request,
                # let the error continue, but set to use the HostPlugin
                except ResourceGoneError:
                    HostPluginProtocol.set_default_channel(True)
                    raise

                host.manifest_uri = version.uri
                logger.verbose(""Manifest downloaded successfully from host plugin"")
                if not HostPluginProtocol.is_default_channel():
                    logger.info(""Setting host plugin as default channel"")
                    HostPluginProtocol.set_default_channel(True)

            if response:
                return response

        raise ProtocolError(""Failed to fetch manifest from all sources"")

    def fetch(self, uri, headers=None, use_proxy=None, decode=True):
        content = None
        logger.verbose(""Fetch [{0}] with headers [{1}]"", uri, headers)
        try:
            resp = self.call_storage_service(
                        restutil.http_get,
                        uri,
                        headers=headers,
                        use_proxy=use_proxy)

            if restutil.request_failed(resp):
                error_response = restutil.read_response_error(resp)
                msg = ""Fetch failed from [{0}]: {1}"".format(uri, error_response)
                logger.warn(msg)
                if self.host_plugin is not None:
                    self.host_plugin.report_fetch_health(uri,
                                                         is_healthy=not restutil.request_failed_at_hostplugin(resp),
                                                         source='WireClient',
                                                         response=error_response)
                raise ProtocolError(msg)
            else:
                response_content = resp.read()
                content = self.decode_config(response_content) if decode else response_content
                if self.host_plugin is not None:
                    self.host_plugin.report_fetch_health(uri, source='WireClient')

        except (HttpError, ProtocolError, IOError) as e:
            logger.verbose(""Fetch failed from [{0}]: {1}"", uri, e)
            if isinstance(e, ResourceGoneError):
                raise

        return content

    def update_hosting_env(self, goal_state):
        if goal_state.hosting_env_uri is None:
            raise ProtocolError(""HostingEnvironmentConfig uri is empty"")
        local_file = os.path.join(conf.get_lib_dir(), HOSTING_ENV_FILE_NAME)
        xml_text = self.fetch_config(goal_state.hosting_env_uri,
                                     self.get_header())
        self.save_cache(local_file, xml_text)
        self.hosting_env = HostingEnv(xml_text)

    def update_shared_conf(self, goal_state):
        if goal_state.shared_conf_uri is None:
            raise ProtocolError(""SharedConfig uri is empty"")
        local_file = os.path.join(conf.get_lib_dir(), SHARED_CONF_FILE_NAME)
        xml_text = self.fetch_config(goal_state.shared_conf_uri,
                                     self.get_header())
        self.save_cache(local_file, xml_text)
        self.shared_conf = SharedConfig(xml_text)

    def update_certs(self, goal_state):
        if goal_state.certs_uri is None:
            return
        local_file = os.path.join(conf.get_lib_dir(), CERTS_FILE_NAME)
        xml_text = self.fetch_config(goal_state.certs_uri,
                                     self.get_header_for_cert())
        self.save_cache(local_file, xml_text)
        self.certs = Certificates(self, xml_text)

    def update_remote_access_conf(self, goal_state):
        if goal_state.remote_access_uri is None:
            # Nothing in accounts data.  Just return, nothing to do.
            return
        xml_text = self.fetch_config(goal_state.remote_access_uri, 
                                     self.get_header_for_cert())
        self.remote_access = RemoteAccess(xml_text)
        local_file = os.path.join(conf.get_lib_dir(), REMOTE_ACCESS_FILE_NAME.format(self.remote_access.incarnation))
        self.save_cache(local_file, xml_text)

    def get_remote_access(self):
        incarnation_file = os.path.join(conf.get_lib_dir(),
                                        INCARNATION_FILE_NAME)
        incarnation = self.fetch_cache(incarnation_file)
        file_name = REMOTE_ACCESS_FILE_NAME.format(incarnation)
        remote_access_file = os.path.join(conf.get_lib_dir(), file_name)
        if not os.path.isfile(remote_access_file):
            # no remote access data.
            return None
        xml_text = self.fetch_cache(remote_access_file)
        remote_access = RemoteAccess(xml_text)
        return remote_access
        
    def update_ext_conf(self, goal_state):
        if goal_state.ext_uri is None:
            logger.info(""ExtensionsConfig.xml uri is empty"")
            self.ext_conf = ExtensionsConfig(None)
            return
        incarnation = goal_state.incarnation
        local_file = os.path.join(conf.get_lib_dir(),
                                  EXT_CONF_FILE_NAME.format(incarnation))
        xml_text = self.fetch_config(goal_state.ext_uri, self.get_header())
        self.save_cache(local_file, xml_text)
        self.ext_conf = ExtensionsConfig(xml_text)

    def update_goal_state(self, forced=False, max_retry=3):
        incarnation_file = os.path.join(conf.get_lib_dir(),
                                        INCARNATION_FILE_NAME)
        uri = GOAL_STATE_URI.format(self.endpoint)

        goal_state = None
        for retry in range(0, max_retry):
            try:
                if goal_state is None:
                    xml_text = self.fetch_config(uri, self.get_header())
                    goal_state = GoalState(xml_text)

                    if not forced:
                        last_incarnation = None
                        if os.path.isfile(incarnation_file):
                            last_incarnation = fileutil.read_file(
                                                    incarnation_file)
                        new_incarnation = goal_state.incarnation
                        if last_incarnation is not None and \
                                        last_incarnation == new_incarnation:
                            # Goalstate is not updated.
                            return                
                self.goal_state_flusher.flush(datetime.utcnow())

                self.goal_state = goal_state
                file_name = GOAL_STATE_FILE_NAME.format(goal_state.incarnation)
                goal_state_file = os.path.join(conf.get_lib_dir(), file_name)
                self.save_cache(goal_state_file, xml_text)
                self.update_hosting_env(goal_state)
                self.update_shared_conf(goal_state)
                self.update_certs(goal_state)
                self.update_ext_conf(goal_state)
                self.update_remote_access_conf(goal_state)
                self.save_cache(incarnation_file, goal_state.incarnation)

                if self.host_plugin is not None:
                    self.host_plugin.container_id = goal_state.container_id
                    self.host_plugin.role_config_name = goal_state.role_config_name

                return

            except ResourceGoneError:
                logger.info(""GoalState is stale -- re-fetching"")
                goal_state = None

            except Exception as e:
                log_method = logger.info \
                                if type(e) is ProtocolError \
                                else logger.warn
                log_method(
                    ""Exception processing GoalState-related files: {0}"".format(
                        ustr(e)))

                if retry < max_retry-1:
                    continue
                raise

        raise ProtocolError(""Exceeded max retry updating goal state"")

    def get_goal_state(self):
        if self.goal_state is None:
            incarnation_file = os.path.join(conf.get_lib_dir(),
                                            INCARNATION_FILE_NAME)
            incarnation = self.fetch_cache(incarnation_file)

            file_name = GOAL_STATE_FILE_NAME.format(incarnation)
            goal_state_file = os.path.join(conf.get_lib_dir(), file_name)
            xml_text = self.fetch_cache(goal_state_file)
            self.goal_state = GoalState(xml_text)
        return self.goal_state

    def get_hosting_env(self):
        if self.hosting_env is None:
            local_file = os.path.join(conf.get_lib_dir(),
                                      HOSTING_ENV_FILE_NAME)
            xml_text = self.fetch_cache(local_file)
            self.hosting_env = HostingEnv(xml_text)
        return self.hosting_env

    def get_shared_conf(self):
        if self.shared_conf is None:
            local_file = os.path.join(conf.get_lib_dir(),
                                      SHARED_CONF_FILE_NAME)
            xml_text = self.fetch_cache(local_file)
            self.shared_conf = SharedConfig(xml_text)
        return self.shared_conf

    def get_certs(self):
        if self.certs is None:
            local_file = os.path.join(conf.get_lib_dir(), CERTS_FILE_NAME)
            xml_text = self.fetch_cache(local_file)
            self.certs = Certificates(self, xml_text)
        if self.certs is None:
            return None
        return self.certs

    def get_ext_conf(self):
        if self.ext_conf is None:
            goal_state = self.get_goal_state()
            if goal_state.ext_uri is None:
                self.ext_conf = ExtensionsConfig(None)
            else:
                local_file = EXT_CONF_FILE_NAME.format(goal_state.incarnation)
                local_file = os.path.join(conf.get_lib_dir(), local_file)
                xml_text = self.fetch_cache(local_file)
                self.ext_conf = ExtensionsConfig(xml_text)
        return self.ext_conf      

    def get_ext_manifest(self, ext_handler, goal_state):
        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    self.update_goal_state(forced=True)
                    goal_state = self.get_goal_state()

                local_file = MANIFEST_FILE_NAME.format(
                                ext_handler.name,
                                goal_state.incarnation)
                local_file = os.path.join(conf.get_lib_dir(), local_file)
                xml_text = self.fetch_manifest(ext_handler.versionUris)
                self.save_cache(local_file, xml_text)
                return ExtensionManifest(xml_text)

            except ResourceGoneError:
                continue

        raise ProtocolError(""Failed to retrieve extension manifest"")

    def filter_package_list(self, family, ga_manifest, goal_state):
        complete_list = ga_manifest.pkg_list
        agent_manifest = os.path.join(conf.get_lib_dir(),
                                      AGENTS_MANIFEST_FILE_NAME.format(
                                          family,
                                          goal_state.incarnation))

        if not os.path.exists(agent_manifest):
            # clear memory cache
            ga_manifest.allowed_versions = None

            # create disk cache
            with open(agent_manifest, mode='w') as manifest_fh:
                for version in complete_list.versions:
                    manifest_fh.write('{0}\n'.format(version.version))
            fileutil.chmod(agent_manifest, 0o644)

            return complete_list

        else:
            # use allowed versions from cache, otherwise from disk
            if ga_manifest.allowed_versions is None:
                with open(agent_manifest, mode='r') as manifest_fh:
                    ga_manifest.allowed_versions = [v.strip('\n') for v
                                                    in manifest_fh.readlines()]

            # use the updated manifest urls for allowed versions
            allowed_list = ExtHandlerPackageList()
            allowed_list.versions = [version for version
                                     in complete_list.versions
                                     if version.version
                                     in ga_manifest.allowed_versions]

            return allowed_list

    def get_gafamily_manifest(self, vmagent_manifest, goal_state):
        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    self.update_goal_state(forced=True)
                    goal_state = self.get_goal_state()

                self._remove_stale_agent_manifest(
                    vmagent_manifest.family,
                    goal_state.incarnation)

                local_file = MANIFEST_FILE_NAME.format(
                                vmagent_manifest.family,
                                goal_state.incarnation)
                local_file = os.path.join(conf.get_lib_dir(), local_file)
                xml_text = self.fetch_manifest(
                            vmagent_manifest.versionsManifestUris)
                fileutil.write_file(local_file, xml_text)
                return ExtensionManifest(xml_text)

            except ResourceGoneError:
                continue

        raise ProtocolError(""Failed to retrieve GAFamily manifest"")

    def _remove_stale_agent_manifest(self, family, incarnation):
        """"""
        The incarnation number can reset at any time, which means there
        could be a stale agentsManifest on disk.  Stale files are cleaned
        on demand as new goal states arrive from WireServer. If the stale
        file is not removed agent upgrade may be delayed.

        :param family: GA family, e.g. Prod or Test
        :param incarnation: incarnation of the current goal state
        """"""
        fn = AGENTS_MANIFEST_FILE_NAME.format(
            family,
            incarnation)

        agent_manifest = os.path.join(conf.get_lib_dir(), fn)

        if os.path.exists(agent_manifest):
            os.unlink(agent_manifest)

    def check_wire_protocol_version(self):
        uri = VERSION_INFO_URI.format(self.endpoint)
        version_info_xml = self.fetch_config(uri, None)
        version_info = VersionInfo(version_info_xml)

        preferred = version_info.get_preferred()
        if PROTOCOL_VERSION == preferred:
            logger.info(""Wire protocol version:{0}"", PROTOCOL_VERSION)
        elif PROTOCOL_VERSION in version_info.get_supported():
            logger.info(""Wire protocol version:{0}"", PROTOCOL_VERSION)
            logger.info(""Server preferred version:{0}"", preferred)
        else:
            error = (""Agent supported wire protocol version: {0} was not ""
                     ""advised by Fabric."").format(PROTOCOL_VERSION)
            raise ProtocolNotFoundError(error)

    def upload_status_blob(self):
        try:
            self.update_goal_state()
            ext_conf = self.get_ext_conf()

            blob_uri = ext_conf.status_upload_blob
            blob_type = ext_conf.status_upload_blob_type

            if blob_uri is None:
                raise ProtocolError(""No blob uri found"")

            if blob_type not in [""BlockBlob"", ""PageBlob""]:
                blob_type = ""BlockBlob""
                logger.verbose(""Status Blob type is unspecified, assuming BlockBlob"")

            try:
                self.status_blob.prepare(blob_type)
            except Exception as e:
                raise ProtocolError(""Exception creating status blob: {0}"", ustr(e))

            # Swap the order of use for the HostPlugin vs. the ""direct"" route.
            # Prefer the use of HostPlugin. If HostPlugin fails fall back to the
            # direct route.
            #
            # The code previously preferred the ""direct"" route always, and only fell back
            # to the HostPlugin *if* there was an error.  We would like to move to
            # the HostPlugin for all traffic, but this is a big change.  We would like
            # to see how this behaves at scale, and have a fallback should things go
            # wrong.  This is why we try HostPlugin then direct.
            try:
                host = self.get_host_plugin()
                host.put_vm_status(self.status_blob,
                                   ext_conf.status_upload_blob,
                                   ext_conf.status_upload_blob_type)
                return
            except ResourceGoneError:
                # do not attempt direct, force goal state update and wait to try again
                self.update_goal_state(forced=True)
                return
            except Exception as e:
                # for all other errors, fall back to direct
                pass

            self.report_status_event(""direct"")
            if self.status_blob.upload(blob_uri):
                return

        except Exception as e:
            self.report_status_event(""Exception uploading status blob: {0}"", ustr(e))

        raise ProtocolError(""Failed to upload status blob via either channel"")

    def report_role_prop(self, thumbprint):
        goal_state = self.get_goal_state()
        role_prop = _build_role_properties(goal_state.container_id,
                                           goal_state.role_instance_id,
                                           thumbprint)
        role_prop = role_prop.encode(""utf-8"")
        role_prop_uri = ROLE_PROP_URI.format(self.endpoint)
        headers = self.get_header_for_xml_content()
        try:
            resp = self.call_wireserver(restutil.http_post,
                                        role_prop_uri,
                                        role_prop,
                                        headers=headers)
        except HttpError as e:
            raise ProtocolError((u""Failed to send role properties: ""
                                 u""{0}"").format(e))
        if resp.status != httpclient.ACCEPTED:
            raise ProtocolError((u""Failed to send role properties: ""
                                 u"",{0}: {1}"").format(resp.status,
                                                      resp.read()))

    def report_health(self, status, substatus, description):
        goal_state = self.get_goal_state()
        health_report = _build_health_report(goal_state.incarnation,
                                             goal_state.container_id,
                                             goal_state.role_instance_id,
                                             status,
                                             substatus,
                                             description)
        health_report = health_report.encode(""utf-8"")
        health_report_uri = HEALTH_REPORT_URI.format(self.endpoint)
        headers = self.get_header_for_xml_content()
        try:
            # 30 retries with 10s sleep gives ~5min for wireserver updates;
            # this is retried 3 times with 15s sleep before throwing a
            # ProtocolError, for a total of ~15min.
            resp = self.call_wireserver(restutil.http_post,
                                        health_report_uri,
                                        health_report,
                                        headers=headers,
                                        max_retry=30,
                                        retry_delay=15)
        except HttpError as e:
            raise ProtocolError((u""Failed to send provision status: ""
                                 u""{0}"").format(e))
        if restutil.request_failed(resp):
            raise ProtocolError((u""Failed to send provision status: ""
                                 u"",{0}: {1}"").format(resp.status,
                                                      resp.read()))

    def send_event(self, provider_id, event_str):
        uri = TELEMETRY_URI.format(self.endpoint)
        data_format = ('<?xml version=""1.0""?>'
                       '<TelemetryData version=""1.0"">'
                       '<Provider id=""{0}"">{1}'
                       '</Provider>'
                       '</TelemetryData>')
        data = data_format.format(provider_id, event_str)
        try:
            header = self.get_header_for_xml_content()
            resp = self.call_wireserver(restutil.http_post, uri, data, header)
        except HttpError as e:
            raise ProtocolError(""Failed to send events:{0}"".format(e))

        if restutil.request_failed(resp):
            logger.verbose(resp.read())
            raise ProtocolError(
                ""Failed to send events:{0}"".format(resp.status))

    def report_event(self, event_list):
        buf = {}
        # Group events by providerId
        for event in event_list.events:
            if event.providerId not in buf:
                buf[event.providerId] = """"
            event_str = event_to_v1(event)
            if len(event_str) >= 63 * 1024:
                logger.warn(""Single event too large: {0}"", event_str[300:])
                continue
            if len(buf[event.providerId] + event_str) >= 63 * 1024:
                self.send_event(event.providerId, buf[event.providerId])
                buf[event.providerId] = """"
            buf[event.providerId] = buf[event.providerId] + event_str

        # Send out all events left in buffer.
        for provider_id in list(buf.keys()):
            if len(buf[provider_id]) > 0:
                self.send_event(provider_id, buf[provider_id])

    def report_status_event(self, message, *args):
        from azurelinuxagent.common.event import report_event, \
                WALAEventOperation

        message = message.format(*args)
        logger.warn(message)
        report_event(op=WALAEventOperation.ReportStatus,
                    is_success=False,
                    message=message)

    def get_header(self):
        return {
            ""x-ms-agent-name"": ""WALinuxAgent"",
            ""x-ms-version"": PROTOCOL_VERSION
        }

    def get_header_for_xml_content(self):
        return {
            ""x-ms-agent-name"": ""WALinuxAgent"",
            ""x-ms-version"": PROTOCOL_VERSION,
            ""Content-Type"": ""text/xml;charset=utf-8""
        }

    def get_header_for_cert(self):
        trans_cert_file = os.path.join(conf.get_lib_dir(),
                                       TRANSPORT_CERT_FILE_NAME)
        content = self.fetch_cache(trans_cert_file)
        cert = get_bytes_from_pem(content)
        return {
            ""x-ms-agent-name"": ""WALinuxAgent"",
            ""x-ms-version"": PROTOCOL_VERSION,
            ""x-ms-cipher-name"": ""DES_EDE3_CBC"",
            ""x-ms-guest-agent-public-x509-cert"": cert
        }

    def get_host_plugin(self):
        if self.host_plugin is None:
            goal_state = self.get_goal_state()
            self.host_plugin = HostPluginProtocol(self.endpoint,
                                                  goal_state.container_id,
                                                  goal_state.role_config_name)
        return self.host_plugin

    def has_artifacts_profile_blob(self):
        return self.ext_conf and not \
               textutil.is_str_none_or_whitespace(self.ext_conf.artifacts_profile_blob)

    def get_artifacts_profile(self):
        artifacts_profile = None
        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    self.update_goal_state(forced=True)

                if self.has_artifacts_profile_blob():
                    blob = self.ext_conf.artifacts_profile_blob

                    profile = None
                    if not HostPluginProtocol.is_default_channel():
                        logger.verbose(""Retrieving the artifacts profile"")
                        profile = self.fetch(blob)

                    if profile is None:
                        if HostPluginProtocol.is_default_channel():
                            logger.verbose(""Using host plugin as default channel"")
                        else:
                            logger.verbose(""Failed to download artifacts profile, ""
                                           ""switching to host plugin"")

                        host = self.get_host_plugin()
                        uri, headers = host.get_artifact_request(blob)
                        profile = self.fetch(uri, headers, use_proxy=False)

                    if not textutil.is_str_none_or_whitespace(profile):
                        logger.verbose(""Artifacts profile downloaded"")
                        artifacts_profile = InVMArtifactsProfile(profile)

                return artifacts_profile

            except ResourceGoneError:
                HostPluginProtocol.set_default_channel(True)
                continue

            except Exception as e:
                logger.warn(
                    ""Exception retrieving artifacts profile: {0}"".format(
                        ustr(e)))

        return None


class VersionInfo(object):
    def __init__(self, xml_text):
        """"""
        Query endpoint server for wire protocol version.
        Fail if our desired protocol version is not seen.
        """"""
        logger.verbose(""Load Version.xml"")
        self.parse(xml_text)

    def parse(self, xml_text):
        xml_doc = parse_doc(xml_text)
        preferred = find(xml_doc, ""Preferred"")
        self.preferred = findtext(preferred, ""Version"")
        logger.info(""Fabric preferred wire protocol version:{0}"",
                    self.preferred)

        self.supported = []
        supported = find(xml_doc, ""Supported"")
        supported_version = findall(supported, ""Version"")
        for node in supported_version:
            version = gettext(node)
            logger.verbose(""Fabric supported wire protocol version:{0}"",
                           version)
            self.supported.append(version)

    def get_preferred(self):
        return self.preferred

    def get_supported(self):
        return self.supported


class GoalState(object):
    def __init__(self, xml_text):
        if xml_text is None:
            raise ValueError(""GoalState.xml is None"")
        logger.verbose(""Load GoalState.xml"")
        self.incarnation = None
        self.expected_state = None
        self.hosting_env_uri = None
        self.shared_conf_uri = None
        self.remote_access_uri = None
        self.certs_uri = None
        self.ext_uri = None
        self.role_instance_id = None
        self.role_config_name = None
        self.container_id = None
        self.load_balancer_probe_port = None
        self.xml_text = None
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Request configuration data from endpoint server.
        """"""
        self.xml_text = xml_text
        xml_doc = parse_doc(xml_text)
        self.incarnation = findtext(xml_doc, ""Incarnation"")
        self.expected_state = findtext(xml_doc, ""ExpectedState"")
        self.hosting_env_uri = findtext(xml_doc, ""HostingEnvironmentConfig"")
        self.shared_conf_uri = findtext(xml_doc, ""SharedConfig"")
        self.certs_uri = findtext(xml_doc, ""Certificates"")
        self.ext_uri = findtext(xml_doc, ""ExtensionsConfig"")
        role_instance = find(xml_doc, ""RoleInstance"")
        self.role_instance_id = findtext(role_instance, ""InstanceId"")
        role_config = find(role_instance, ""Configuration"")
        self.role_config_name = findtext(role_config, ""ConfigName"")
        container = find(xml_doc, ""Container"")
        self.container_id = findtext(container, ""ContainerId"")
        self.remote_access_uri = findtext(container, ""RemoteAccessInfo"")
        lbprobe_ports = find(xml_doc, ""LBProbePorts"")
        self.load_balancer_probe_port = findtext(lbprobe_ports, ""Port"")
        return self


class HostingEnv(object):
    """"""
    parse Hosting enviromnet config and store in
    HostingEnvironmentConfig.xml
    """"""

    def __init__(self, xml_text):
        if xml_text is None:
            raise ValueError(""HostingEnvironmentConfig.xml is None"")
        logger.verbose(""Load HostingEnvironmentConfig.xml"")
        self.vm_name = None
        self.role_name = None
        self.deployment_name = None
        self.xml_text = None
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        parse and create HostingEnvironmentConfig.xml.
        """"""
        self.xml_text = xml_text
        xml_doc = parse_doc(xml_text)
        incarnation = find(xml_doc, ""Incarnation"")
        self.vm_name = getattrib(incarnation, ""instance"")
        role = find(xml_doc, ""Role"")
        self.role_name = getattrib(role, ""name"")
        deployment = find(xml_doc, ""Deployment"")
        self.deployment_name = getattrib(deployment, ""name"")
        return self


class SharedConfig(object):
    """"""
    parse role endpoint server and goal state config.
    """"""

    def __init__(self, xml_text):
        logger.verbose(""Load SharedConfig.xml"")
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        parse and write configuration to file SharedConfig.xml.
        """"""
        # Not used currently
        return self


class RemoteAccess(object):
    """"""
    Object containing information about user accounts
    """"""
    #
    # <RemoteAccess>
    #   <Version/>
    #   <Incarnation/>
    #    <Users>
    #       <User>
    #         <Name/>
    #         <Password/>
    #         <Expiration/>
    #       </User>
    #     </Users>
    #   </RemoteAccess>
    #

    def __init__(self, xml_text):
        logger.verbose(""Load RemoteAccess.xml"")
        self.version = None
        self.incarnation = None
        self.user_list = RemoteAccessUsersList()

        self.xml_text = None
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Parse xml document containing user account information
        """"""
        if xml_text is None or len(xml_text) == 0:
            return None
        self.xml_text = xml_text
        xml_doc = parse_doc(xml_text)
        self.incarnation = findtext(xml_doc, ""Incarnation"")
        self.version = findtext(xml_doc, ""Version"")
        user_collection = find(xml_doc, ""Users"")
        users = findall(user_collection, ""User"")

        for user in users:
            remote_access_user = self.parse_user(user)
            self.user_list.users.append(remote_access_user)
        return self

    def parse_user(self, user):
        name = findtext(user, ""Name"")
        encrypted_password = findtext(user, ""Password"")
        expiration = findtext(user, ""Expiration"")
        remote_access_user = RemoteAccessUser(name, encrypted_password, expiration)
        return remote_access_user

class UserAccount(object):
    """"""
    Stores information about single user account
    """"""
    def __init__(self):
        self.Name = None
        self.EncryptedPassword = None
        self.Password = None
        self.Expiration = None
        self.Groups = []


class Certificates(object):
    """"""
    Object containing certificates of host and provisioned user.
    """"""

    def __init__(self, client, xml_text):
        logger.verbose(""Load Certificates.xml"")
        self.client = client
        self.cert_list = CertList()
        self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Parse multiple certificates into seperate files.
        """"""
        xml_doc = parse_doc(xml_text)
        data = findtext(xml_doc, ""Data"")
        if data is None:
            return

        cryptutil = CryptUtil(conf.get_openssl_cmd())
        p7m_file = os.path.join(conf.get_lib_dir(), P7M_FILE_NAME)
        p7m = (""MIME-Version:1.0\n""
               ""Content-Disposition: attachment; filename=\""{0}\""\n""
               ""Content-Type: application/x-pkcs7-mime; name=\""{1}\""\n""
               ""Content-Transfer-Encoding: base64\n""
               ""\n""
               ""{2}"").format(p7m_file, p7m_file, data)

        self.client.save_cache(p7m_file, p7m)

        trans_prv_file = os.path.join(conf.get_lib_dir(),
                                      TRANSPORT_PRV_FILE_NAME)
        trans_cert_file = os.path.join(conf.get_lib_dir(),
                                       TRANSPORT_CERT_FILE_NAME)
        pem_file = os.path.join(conf.get_lib_dir(), PEM_FILE_NAME)
        # decrypt certificates
        cryptutil.decrypt_p7m(p7m_file, trans_prv_file, trans_cert_file,
                              pem_file)

        # The parsing process use public key to match prv and crt.
        buf = []
        begin_crt = False
        begin_prv = False
        prvs = {}
        thumbprints = {}
        index = 0
        v1_cert_list = []
        with open(pem_file) as pem:
            for line in pem.readlines():
                buf.append(line)
                if re.match(r'[-]+BEGIN.*KEY[-]+', line):
                    begin_prv = True
                elif re.match(r'[-]+BEGIN.*CERTIFICATE[-]+', line):
                    begin_crt = True
                elif re.match(r'[-]+END.*KEY[-]+', line):
                    tmp_file = self.write_to_tmp_file(index, 'prv', buf)
                    pub = cryptutil.get_pubkey_from_prv(tmp_file)
                    prvs[pub] = tmp_file
                    buf = []
                    index += 1
                    begin_prv = False
                elif re.match(r'[-]+END.*CERTIFICATE[-]+', line):
                    tmp_file = self.write_to_tmp_file(index, 'crt', buf)
                    pub = cryptutil.get_pubkey_from_crt(tmp_file)
                    thumbprint = cryptutil.get_thumbprint_from_crt(tmp_file)
                    thumbprints[pub] = thumbprint
                    # Rename crt with thumbprint as the file name
                    crt = ""{0}.crt"".format(thumbprint)
                    v1_cert_list.append({
                        ""name"": None,
                        ""thumbprint"": thumbprint
                    })
                    os.rename(tmp_file, os.path.join(conf.get_lib_dir(), crt))
                    buf = []
                    index += 1
                    begin_crt = False

        # Rename prv key with thumbprint as the file name
        for pubkey in prvs:
            thumbprint = thumbprints[pubkey]
            if thumbprint:
                tmp_file = prvs[pubkey]
                prv = ""{0}.prv"".format(thumbprint)
                os.rename(tmp_file, os.path.join(conf.get_lib_dir(), prv))

        for v1_cert in v1_cert_list:
            cert = Cert()
            set_properties(""certs"", cert, v1_cert)
            self.cert_list.certificates.append(cert)

    def write_to_tmp_file(self, index, suffix, buf):
        file_name = os.path.join(conf.get_lib_dir(),
                                 ""{0}.{1}"".format(index, suffix))
        self.client.save_cache(file_name, """".join(buf))
        return file_name


class ExtensionsConfig(object):
    """"""
    parse ExtensionsConfig, downloading and unpacking them to /var/lib/waagent.
    Install if <enabled>true</enabled>, remove if it is set to false.
    """"""

    def __init__(self, xml_text):
        logger.verbose(""Load ExtensionsConfig.xml"")
        self.ext_handlers = ExtHandlerList()
        self.vmagent_manifests = VMAgentManifestList()
        self.status_upload_blob = None
        self.status_upload_blob_type = None
        self.artifacts_profile_blob = None
        if xml_text is not None:
            self.parse(xml_text)

    def parse(self, xml_text):
        """"""
        Write configuration to file ExtensionsConfig.xml.
        """"""
        xml_doc = parse_doc(xml_text)

        ga_families_list = find(xml_doc, ""GAFamilies"")
        ga_families = findall(ga_families_list, ""GAFamily"")

        for ga_family in ga_families:
            family = findtext(ga_family, ""Name"")
            uris_list = find(ga_family, ""Uris"")
            uris = findall(uris_list, ""Uri"")
            manifest = VMAgentManifest()
            manifest.family = family
            for uri in uris:
                manifestUri = VMAgentManifestUri(uri=gettext(uri))
                manifest.versionsManifestUris.append(manifestUri)
            self.vmagent_manifests.vmAgentManifests.append(manifest)

        plugins_list = find(xml_doc, ""Plugins"")
        plugins = findall(plugins_list, ""Plugin"")
        plugin_settings_list = find(xml_doc, ""PluginSettings"")
        plugin_settings = findall(plugin_settings_list, ""Plugin"")

        for plugin in plugins:
            ext_handler = self.parse_plugin(plugin)
            self.ext_handlers.extHandlers.append(ext_handler)
            self.parse_plugin_settings(ext_handler, plugin_settings)

        self.status_upload_blob = findtext(xml_doc, ""StatusUploadBlob"")
        self.artifacts_profile_blob = findtext(xml_doc, ""InVMArtifactsProfileBlob"")

        status_upload_node = find(xml_doc, ""StatusUploadBlob"")
        self.status_upload_blob_type = getattrib(status_upload_node,
                                                 ""statusBlobType"")
        logger.verbose(""Extension config shows status blob type as [{0}]"",
                       self.status_upload_blob_type)

    def parse_plugin(self, plugin):
        ext_handler = ExtHandler()
        ext_handler.name = getattrib(plugin, ""name"")
        ext_handler.properties.version = getattrib(plugin, ""version"")
        ext_handler.properties.state = getattrib(plugin, ""state"")

        ext_handler.properties.upgradeGuid = getattrib(plugin, ""upgradeGuid"")
        if not ext_handler.properties.upgradeGuid:
            ext_handler.properties.upgradeGuid = None

        try:
            ext_handler.properties.dependencyLevel = int(getattrib(plugin, ""dependencyLevel""))
        except ValueError:
            ext_handler.properties.dependencyLevel = 0

        auto_upgrade = getattrib(plugin, ""autoUpgrade"")
        if auto_upgrade is not None and auto_upgrade.lower() == ""true"":
            ext_handler.properties.upgradePolicy = ""auto""
        else:
            ext_handler.properties.upgradePolicy = ""manual""

        location = getattrib(plugin, ""location"")
        failover_location = getattrib(plugin, ""failoverlocation"")
        for uri in [location, failover_location]:
            version_uri = ExtHandlerVersionUri()
            version_uri.uri = uri
            ext_handler.versionUris.append(version_uri)
        return ext_handler

    def parse_plugin_settings(self, ext_handler, plugin_settings):
        if plugin_settings is None:
            return

        name = ext_handler.name
        version = ext_handler.properties.version
        settings = [x for x in plugin_settings \
                    if getattrib(x, ""name"") == name and \
                    getattrib(x, ""version"") == version]

        if settings is None or len(settings) == 0:
            return

        runtime_settings = None
        runtime_settings_node = find(settings[0], ""RuntimeSettings"")
        seqNo = getattrib(runtime_settings_node, ""seqNo"")
        runtime_settings_str = gettext(runtime_settings_node)
        try:
            runtime_settings = json.loads(runtime_settings_str)
        except ValueError as e:
            logger.error(""Invalid extension settings"")
            return

        for plugin_settings_list in runtime_settings[""runtimeSettings""]:
            handler_settings = plugin_settings_list[""handlerSettings""]
            ext = Extension()
            # There is no ""extension name"" in wire protocol.
            # Put
            ext.name = ext_handler.name
            ext.sequenceNumber = seqNo
            ext.publicSettings = handler_settings.get(""publicSettings"")
            ext.protectedSettings = handler_settings.get(""protectedSettings"")
            thumbprint = handler_settings.get(
                ""protectedSettingsCertThumbprint"")
            ext.certificateThumbprint = thumbprint
            ext_handler.properties.extensions.append(ext)


class ExtensionManifest(object):
    def __init__(self, xml_text):
        if xml_text is None:
            raise ValueError(""ExtensionManifest is None"")
        logger.verbose(""Load ExtensionManifest.xml"")
        self.pkg_list = ExtHandlerPackageList()
        self.allowed_versions = None
        self.parse(xml_text)

    def parse(self, xml_text):
        xml_doc = parse_doc(xml_text)
        self._handle_packages(findall(find(xml_doc,
                                           ""Plugins""),
                                      ""Plugin""),
                              False)
        self._handle_packages(findall(find(xml_doc,
                                           ""InternalPlugins""),
                                      ""Plugin""),
                              True)

    def _handle_packages(self, packages, isinternal):
        for package in packages:
            version = findtext(package, ""Version"")

            disallow_major_upgrade = findtext(package,
                                              ""DisallowMajorVersionUpgrade"")
            if disallow_major_upgrade is None:
                disallow_major_upgrade = ''
            disallow_major_upgrade = disallow_major_upgrade.lower() == ""true""

            uris = find(package, ""Uris"")
            uri_list = findall(uris, ""Uri"")
            uri_list = [gettext(x) for x in uri_list]
            pkg = ExtHandlerPackage()
            pkg.version = version
            pkg.disallow_major_upgrade = disallow_major_upgrade
            for uri in uri_list:
                pkg_uri = ExtHandlerVersionUri()
                pkg_uri.uri = uri
                pkg.uris.append(pkg_uri)

            pkg.isinternal = isinternal
            self.pkg_list.versions.append(pkg)


# Do not extend this class
class InVMArtifactsProfile(object):
    """"""
    deserialized json string of InVMArtifactsProfile.
    It is expected to contain the following fields:
    * inVMArtifactsProfileBlobSeqNo
    * profileId (optional)
    * onHold (optional)
    * certificateThumbprint (optional)
    * encryptedHealthChecks (optional)
    * encryptedApplicationProfile (optional)
    """"""
    def __init__(self, artifacts_profile):
        if not textutil.is_str_none_or_whitespace(artifacts_profile):
            self.__dict__.update(parse_json(artifacts_profile))

    def is_on_hold(self):
        # hasattr() is not available in Python 2.6
        if 'onHold' in self.__dict__:
            return self.onHold.lower() == 'true'
        return False
/n/n/nazurelinuxagent/common/utils/cryptutil.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import base64
import struct
import sys
import os.path
import subprocess

from azurelinuxagent.common.future import ustr, bytebuffer
from azurelinuxagent.common.exception import CryptError

import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil

DECRYPT_SECRET_CMD = ""{0} cms -decrypt -inform DER -inkey {1} -in /dev/stdin""

class CryptUtil(object):
    def __init__(self, openssl_cmd):
        self.openssl_cmd = openssl_cmd

    def gen_transport_cert(self, prv_file, crt_file):
        """"""
        Create ssl certificate for https communication with endpoint server.
        """"""
        cmd = (""{0} req -x509 -nodes -subj /CN=LinuxTransport -days 730 ""
               ""-newkey rsa:2048 -keyout {1} ""
               ""-out {2}"").format(self.openssl_cmd, prv_file, crt_file)
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""Failed to create {0} and {1} certificates"".format(
                prv_file, crt_file))

    def get_pubkey_from_prv(self, file_name):
        cmd = ""{0} rsa -in {1} -pubout 2>/dev/null"".format(self.openssl_cmd, 
                                                           file_name)
        pub = shellutil.run_get_output(cmd)[1]
        return pub

    def get_pubkey_from_crt(self, file_name):
        cmd = ""{0} x509 -in {1} -pubkey -noout"".format(self.openssl_cmd, 
                                                       file_name)
        pub = shellutil.run_get_output(cmd)[1]
        return pub

    def get_thumbprint_from_crt(self, file_name):
        cmd=""{0} x509 -in {1} -fingerprint -noout"".format(self.openssl_cmd, 
                                                          file_name)
        thumbprint = shellutil.run_get_output(cmd)[1]
        thumbprint = thumbprint.rstrip().split('=')[1].replace(':', '').upper()
        return thumbprint

    def decrypt_p7m(self, p7m_file, trans_prv_file, trans_cert_file, pem_file):
        cmd = (""{0} cms -decrypt -in {1} -inkey {2} -recip {3} ""
               ""| {4} pkcs12 -nodes -password pass: -out {5}""
               """").format(self.openssl_cmd, p7m_file, trans_prv_file, 
                          trans_cert_file, self.openssl_cmd, pem_file)
        shellutil.run(cmd)
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""Failed to decrypt {0}"".format(p7m_file))

    def crt_to_ssh(self, input_file, output_file):
        shellutil.run(""ssh-keygen -i -m PKCS8 -f {0} >> {1}"".format(input_file,
                                                                    output_file))

    def asn1_to_ssh(self, pubkey):
        lines = pubkey.split(""\n"")
        lines = [x for x in lines if not x.startswith(""----"")]
        base64_encoded = """".join(lines)
        try:
            #TODO remove pyasn1 dependency
            from pyasn1.codec.der import decoder as der_decoder
            der_encoded = base64.b64decode(base64_encoded)
            der_encoded = der_decoder.decode(der_encoded)[0][1]
            key = der_decoder.decode(self.bits_to_bytes(der_encoded))[0]
            n=key[0]
            e=key[1]
            keydata = bytearray()
            keydata.extend(struct.pack('>I', len(""ssh-rsa"")))
            keydata.extend(b""ssh-rsa"")
            keydata.extend(struct.pack('>I', len(self.num_to_bytes(e))))
            keydata.extend(self.num_to_bytes(e))
            keydata.extend(struct.pack('>I', len(self.num_to_bytes(n)) + 1))
            keydata.extend(b""\0"")
            keydata.extend(self.num_to_bytes(n))
            keydata_base64 = base64.b64encode(bytebuffer(keydata))
            return ustr(b""ssh-rsa "" +  keydata_base64 + b""\n"", 
                        encoding='utf-8')
        except ImportError as e:
            raise CryptError(""Failed to load pyasn1.codec.der"")

    def num_to_bytes(self, num):
        """"""
        Pack number into bytes.  Retun as string.
        """"""
        result = bytearray()
        while num:
            result.append(num & 0xFF)
            num >>= 8
        result.reverse()
        return result

    def bits_to_bytes(self, bits):
        """"""
        Convert an array contains bits, [0,1] to a byte array
        """"""
        index = 7
        byte_array = bytearray()
        curr = 0
        for bit in bits:
            curr = curr | (bit << index)
            index = index - 1
            if index == -1:
                byte_array.append(curr)
                curr = 0
                index = 7
        return bytes(byte_array)

    def decrypt_secret(self, encrypted_password, private_key):
        try:
            decoded = base64.b64decode(encrypted_password)
        except Exception as e:
            raise CryptError(""Error decoding secret"", e)
        args = DECRYPT_SECRET_CMD.format(self.openssl_cmd, private_key).split(' ')
        p = subprocess.Popen(args, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.STDOUT)
        p.stdin.write(decoded)
        output = p.communicate()[0]
        retcode = p.poll()
        if retcode:
            raise subprocess.CalledProcessError(retcode, ""openssl cms -decrypt"", output=output)
        return output.decode('utf-16')
/n/n/nazurelinuxagent/ga/remoteaccess.py/n/n# Microsoft Azure Linux Agent
#
# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import datetime
import glob
import json
import operator
import os
import os.path
import pwd
import random
import re
import shutil
import stat
import subprocess
import textwrap
import time
import traceback
import zipfile

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.version as version
import azurelinuxagent.common.protocol.wire
import azurelinuxagent.common.protocol.metadata as metadata

from datetime import datetime, timedelta
from pwd import getpwall
from azurelinuxagent.common.errorstate import ErrorState

from azurelinuxagent.common.event import add_event, WALAEventOperation, elapsed_milliseconds
from azurelinuxagent.common.exception import ExtensionError, ProtocolError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.protocol.restapi import ExtHandlerStatus, \
                                                    ExtensionStatus, \
                                                    ExtensionSubStatus, \
                                                    VMStatus, ExtHandler, \
                                                    get_properties, \
                                                    set_properties
from azurelinuxagent.common.protocol.metadata import MetadataProtocol
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.utils.flexible_version import FlexibleVersion
from azurelinuxagent.common.utils.processutil import capture_from_process
from azurelinuxagent.common.protocol import get_protocol_util
from azurelinuxagent.common.version import AGENT_NAME, CURRENT_VERSION
from azurelinuxagent.common.osutil import get_osutil

REMOTE_USR_EXPIRATION_FORMAT = ""%a, %d %b %Y %H:%M:%S %Z""
DATE_FORMAT = ""%Y-%m-%d""
TRANSPORT_PRIVATE_CERT = ""TransportPrivate.pem""
REMOTE_ACCESS_ACCOUNT_COMMENT = ""JIT_Account""
MAX_TRY_ATTEMPT = 5
FAILED_ATTEMPT_THROTTLE = 1

def get_remote_access_handler():
    return RemoteAccessHandler()

class RemoteAccessHandler(object):
    def __init__(self):
        self.os_util = get_osutil()
        self.protocol_util = get_protocol_util()
        self.protocol = None
        self.cryptUtil = CryptUtil(conf.get_openssl_cmd())
        self.remote_access = None
        self.incarnation = 0

    def run(self):
        try:
            if self.os_util.jit_enabled:
                self.protocol = self.protocol_util.get_protocol()
                current_incarnation = self.protocol.get_incarnation()
                if self.incarnation != current_incarnation:
                    # something changed. Handle remote access if any.
                    self.incarnation = current_incarnation
                    self.remote_access = self.protocol.client.get_remote_access()
                    if self.remote_access is not None:
                        self.handle_remote_access()
        except Exception as e:
            msg = u""Exception processing remote access handler: {0} {1}"".format(ustr(e), traceback.format_exc())
            logger.error(msg)
            add_event(AGENT_NAME,
                      version=CURRENT_VERSION,
                      op=WALAEventOperation.RemoteAccessHandling,
                      is_success=False,
                      message=msg)

    def handle_remote_access(self):
        if self.remote_access is not None:
            # Get JIT user accounts.
            all_users = self.os_util.get_users()
            jit_users = set()
            for usr in all_users:
                if self.validate_jit_user(usr[4]):
                    jit_users.add(usr[0])
            for acc in self.remote_access.user_list.users:
                raw_expiration = acc.expiration
                account_expiration = datetime.strptime(raw_expiration, REMOTE_USR_EXPIRATION_FORMAT)
                now = datetime.utcnow()
                if acc.name not in jit_users and now < account_expiration:
                    self.add_user(acc.name, acc.encrypted_password, account_expiration)

    def validate_jit_user(self, comment):
        return comment == REMOTE_ACCESS_ACCOUNT_COMMENT

    def add_user(self, username, encrypted_password, account_expiration):
        try:
            expiration_date = (account_expiration + timedelta(days=1)).strftime(DATE_FORMAT)
            logger.verbose(""Adding user {0} with expiration date {1}""
                           .format(username, expiration_date))
            self.os_util.useradd(username, expiration_date, REMOTE_ACCESS_ACCOUNT_COMMENT)
        except OSError as oe:
            logger.error(""Error adding user {0}. {1}""
                         .format(username, oe.strerror))
            return
        except Exception as e:
            logger.error(""Error adding user {0}. {1}"".format(username, ustr(e)))
            return
        try:
            prv_key = os.path.join(conf.get_lib_dir(), TRANSPORT_PRIVATE_CERT)
            pwd = self.cryptUtil.decrypt_secret(encrypted_password, prv_key)
            self.os_util.chpasswd(username, pwd, conf.get_password_cryptid(), conf.get_password_crypt_salt_len())
            self.os_util.conf_sudoer(username)
            logger.info(""User '{0}' added successfully with expiration in {1}""
                        .format(username, expiration_date))
            return
        except OSError as oe:
            self.handle_failed_create(username, oe.strerror)
        except Exception as e:
            self.handle_failed_create(username, ustr(e))

    def handle_failed_create(self, username, error_message):
        logger.error(""Error creating user {0}. {1}""
                     .format(username, error_message))
        try:
            self.delete_user(username)
        except OSError as oe:
            logger.error(""Failed to clean up after account creation for {0}. {1}""
                         .format(username, oe.strerror()))
        except Exception as e:
            logger.error(""Failed to clean up after account creation for {0}. {1}""
                         .format(username, str(e)))

    def delete_user(self, username):
        self.os_util.del_account(username)
        logger.info(""User deleted {0}"".format(username))
/n/n/nazurelinuxagent/ga/update.py/n/n# Windows Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import glob
import json
import os
import platform
import random
import re
import shutil
import signal
import stat
import subprocess
import sys
import time
import traceback
import zipfile

from datetime import datetime, timedelta

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.logger as logger
import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.restutil as restutil
import azurelinuxagent.common.utils.textutil as textutil

from azurelinuxagent.common.event import add_event, add_periodic, \
                                    elapsed_milliseconds, \
                                    WALAEventOperation
from azurelinuxagent.common.exception import ProtocolError, \
                                            ResourceGoneError, \
                                            UpdateError
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.osutil import get_osutil
from azurelinuxagent.common.protocol import get_protocol_util
from azurelinuxagent.common.protocol.hostplugin import HostPluginProtocol
from azurelinuxagent.common.protocol.wire import WireProtocol
from azurelinuxagent.common.utils.flexible_version import FlexibleVersion
from azurelinuxagent.common.version import AGENT_NAME, AGENT_VERSION, AGENT_LONG_VERSION, \
                                            AGENT_DIR_GLOB, AGENT_PKG_GLOB, \
                                            AGENT_PATTERN, AGENT_NAME_PATTERN, AGENT_DIR_PATTERN, \
                                            CURRENT_AGENT, CURRENT_VERSION, \
                                            is_current_agent_installed

from azurelinuxagent.ga.exthandlers import HandlerManifest

AGENT_ERROR_FILE = ""error.json"" # File name for agent error record
AGENT_MANIFEST_FILE = ""HandlerManifest.json""
AGENT_PARTITION_FILE = ""partition""

CHILD_HEALTH_INTERVAL = 15 * 60
CHILD_LAUNCH_INTERVAL = 5 * 60
CHILD_LAUNCH_RESTART_MAX = 3
CHILD_POLL_INTERVAL = 60

MAX_FAILURE = 3 # Max failure allowed for agent before blacklisted

GOAL_STATE_INTERVAL = 3

ORPHAN_WAIT_INTERVAL = 15 * 60

AGENT_SENTINEL_FILE = ""current_version""

READONLY_FILE_GLOBS = [
    ""*.crt"",
    ""*.p7m"",
    ""*.pem"",
    ""*.prv"",
    ""ovf-env.xml""
]


def get_update_handler():
    return UpdateHandler()


def get_python_cmd():
    major_version = platform.python_version_tuple()[0]
    return ""python"" if int(major_version) <= 2 else ""python{0}"".format(major_version)


class UpdateHandler(object):

    def __init__(self):
        self.osutil = get_osutil()
        self.protocol_util = get_protocol_util()

        self.running = True
        self.last_attempt_time = None

        self.agents = []

        self.child_agent = None
        self.child_launch_time = None
        self.child_launch_attempts = 0
        self.child_process = None

        self.signal_handler = None

    def run_latest(self, child_args=None):
        """"""
        This method is called from the daemon to find and launch the most
        current, downloaded agent.

        Note:
        - Most events should be tagged to the launched agent (agent_version)
        """"""

        if self.child_process is not None:
            raise Exception(""Illegal attempt to launch multiple goal state Agent processes"")

        if self.signal_handler is None:
            self.signal_handler = signal.signal(signal.SIGTERM, self.forward_signal)

        latest_agent = self.get_latest_agent()
        if latest_agent is None:
            logger.info(u""Installed Agent {0} is the most current agent"", CURRENT_AGENT)
            agent_cmd = ""python -u {0} -run-exthandlers"".format(sys.argv[0])
            agent_dir = os.getcwd()
            agent_name = CURRENT_AGENT
            agent_version = CURRENT_VERSION
        else:
            logger.info(u""Determined Agent {0} to be the latest agent"", latest_agent.name)
            agent_cmd = latest_agent.get_agent_cmd()
            agent_dir = latest_agent.get_agent_dir()
            agent_name = latest_agent.name
            agent_version = latest_agent.version

        if child_args is not None:
            agent_cmd = ""{0} {1}"".format(agent_cmd, child_args)

        try:

            # Launch the correct Python version for python-based agents
            cmds = textutil.safe_shlex_split(agent_cmd)
            if cmds[0].lower() == ""python"":
                cmds[0] = get_python_cmd()
                agent_cmd = "" "".join(cmds)

            self._evaluate_agent_health(latest_agent)

            self.child_process = subprocess.Popen(
                cmds,
                cwd=agent_dir,
                stdout=sys.stdout,
                stderr=sys.stderr,
                env=os.environ)

            logger.verbose(u""Agent {0} launched with command '{1}'"", agent_name, agent_cmd)

            # If the most current agent is the installed agent and update is enabled,
            # assume updates are likely available and poll every second.
            # This reduces the start-up impact of finding / launching agent updates on
            # fresh VMs.
            if latest_agent is None and conf.get_autoupdate_enabled():
                poll_interval = 1
            else:
                poll_interval = CHILD_POLL_INTERVAL

            ret = None
            start_time = time.time()
            while (time.time() - start_time) < CHILD_HEALTH_INTERVAL:
                time.sleep(poll_interval)
                ret = self.child_process.poll()
                if ret is not None:
                    break

            if ret is None or ret <= 0:
                msg = u""Agent {0} launched with command '{1}' is successfully running"".format(
                    agent_name,
                    agent_cmd)
                logger.info(msg)
                add_event(
                    AGENT_NAME,
                    version=agent_version,
                    op=WALAEventOperation.Enable,
                    is_success=True,
                    message=msg,
                    log_event=False)

                if ret is None:
                    ret = self.child_process.wait()

            else:
                msg = u""Agent {0} launched with command '{1}' failed with return code: {2}"".format(
                    agent_name,
                    agent_cmd,
                    ret)
                logger.warn(msg)
                add_event(
                    AGENT_NAME,
                    version=agent_version,
                    op=WALAEventOperation.Enable,
                    is_success=False,
                    message=msg)

            if ret is not None and ret > 0:
                msg = u""Agent {0} launched with command '{1}' returned code: {2}"".format(
                    agent_name,
                    agent_cmd,
                    ret)
                logger.warn(msg)
                if latest_agent is not None:
                    latest_agent.mark_failure(is_fatal=True)

        except Exception as e:
            # Ignore child errors during termination
            if self.running:
                msg = u""Agent {0} launched with command '{1}' failed with exception: {2}"".format(
                    agent_name,
                    agent_cmd,
                    ustr(e))
                logger.warn(msg)
                add_event(
                    AGENT_NAME,
                    version=agent_version,
                    op=WALAEventOperation.Enable,
                    is_success=False,
                    message=msg)
                if latest_agent is not None:
                    latest_agent.mark_failure(is_fatal=True)

        self.child_process = None
        return

    def run(self):
        """"""
        This is the main loop which watches for agent and extension updates.
        """"""

        try:
            logger.info(u""Agent {0} is running as the goal state agent"",
                        CURRENT_AGENT)

            # Launch monitoring threads
            from azurelinuxagent.ga.monitor import get_monitor_handler
            monitor_thread = get_monitor_handler()
            monitor_thread.run()

            from azurelinuxagent.ga.env import get_env_handler
            env_thread = get_env_handler()
            env_thread.run()

            from azurelinuxagent.ga.exthandlers import get_exthandlers_handler, migrate_handler_state
            exthandlers_handler = get_exthandlers_handler()
            migrate_handler_state()

            from azurelinuxagent.ga.remoteaccess import get_remote_access_handler
            remote_access_handler = get_remote_access_handler()

            self._ensure_no_orphans()
            self._emit_restart_event()
            self._ensure_partition_assigned()
            self._ensure_readonly_files()

            while self.running:
                if self._is_orphaned:
                    logger.info(""Agent {0} is an orphan -- exiting"",
                                CURRENT_AGENT)
                    break

                if not monitor_thread.is_alive():
                    logger.warn(u""Monitor thread died, restarting"")
                    monitor_thread.start()

                if not env_thread.is_alive():
                    logger.warn(u""Environment thread died, restarting"")
                    env_thread.start()

                if self._upgrade_available():
                    available_agent = self.get_latest_agent()
                    if available_agent is None:
                        logger.info(
                            ""Agent {0} is reverting to the installed agent -- exiting"",
                            CURRENT_AGENT)
                    else:
                        logger.info(
                            u""Agent {0} discovered update {1} -- exiting"",
                            CURRENT_AGENT,
                            available_agent.name)
                    break

                utc_start = datetime.utcnow()

                last_etag = exthandlers_handler.last_etag
                exthandlers_handler.run()

                remote_access_handler.run()

                if last_etag != exthandlers_handler.last_etag:
                    self._ensure_readonly_files()
                    add_event(
                        AGENT_NAME,
                        version=CURRENT_VERSION,
                        op=WALAEventOperation.ProcessGoalState,
                        is_success=True,
                        duration=elapsed_milliseconds(utc_start),
                        message=""Incarnation {0}"".format(
                                            exthandlers_handler.last_etag),
                        log_event=True)

                time.sleep(GOAL_STATE_INTERVAL)

        except Exception as e:
            msg = u""Agent {0} failed with exception: {1}"".format(
                CURRENT_AGENT, ustr(e))
            self._set_sentinel(msg=msg)
            logger.warn(msg)
            logger.warn(traceback.format_exc())
            sys.exit(1)
            # additional return here because sys.exit is mocked in unit tests
            return

        self._shutdown()
        sys.exit(0)

    def forward_signal(self, signum, frame):
        if signum == signal.SIGTERM:
            self._shutdown()

        if self.child_process is None:
            return
        
        logger.info(
            u""Agent {0} forwarding signal {1} to {2}"",
            CURRENT_AGENT,
            signum,
            self.child_agent.name if self.child_agent is not None else CURRENT_AGENT)

        self.child_process.send_signal(signum)

        if self.signal_handler not in (None, signal.SIG_IGN, signal.SIG_DFL):
            self.signal_handler(signum, frame)
        elif self.signal_handler is signal.SIG_DFL:
            if signum == signal.SIGTERM:
                self._shutdown()
                sys.exit(0)
        return

    def get_latest_agent(self):
        """"""
        If autoupdate is enabled, return the most current, downloaded,
        non-blacklisted agent which is not the current version (if any).
        Otherwise, return None (implying to use the installed agent).
        """"""

        if not conf.get_autoupdate_enabled():
            return None
        
        self._find_agents()
        available_agents = [agent for agent in self.agents
                            if agent.is_available
                            and agent.version > FlexibleVersion(AGENT_VERSION)]

        return available_agents[0] if len(available_agents) >= 1 else None

    def _emit_restart_event(self):
        try:
            if not self._is_clean_start:
                msg = u""Agent did not terminate cleanly: {0}"".format(
                            fileutil.read_file(self._sentinel_file_path()))
                logger.info(msg)
                add_event(
                    AGENT_NAME,
                    version=CURRENT_VERSION,
                    op=WALAEventOperation.Restart,
                    is_success=False,
                    message=msg)
        except Exception:
            pass

        return

    def _ensure_no_orphans(self, orphan_wait_interval=ORPHAN_WAIT_INTERVAL):
        pid_files, ignored = self._write_pid_file()
        for pid_file in pid_files:
            try:
                pid = fileutil.read_file(pid_file)
                wait_interval = orphan_wait_interval

                while self.osutil.check_pid_alive(pid):
                    wait_interval -= GOAL_STATE_INTERVAL
                    if wait_interval <= 0:
                        logger.warn(
                            u""{0} forcibly terminated orphan process {1}"",
                            CURRENT_AGENT,
                            pid)
                        os.kill(pid, signal.SIGKILL)
                        break
                    
                    logger.info(
                        u""{0} waiting for orphan process {1} to terminate"",
                        CURRENT_AGENT,
                        pid)
                    time.sleep(GOAL_STATE_INTERVAL)

                os.remove(pid_file)

            except Exception as e:
                logger.warn(
                    u""Exception occurred waiting for orphan agent to terminate: {0}"",
                    ustr(e))
        return

    def _ensure_partition_assigned(self):
        """"""
        Assign the VM to a partition (0 - 99). Downloaded updates may be configured
        to run on only some VMs; the assigned partition determines eligibility.
        """"""
        if not os.path.exists(self._partition_file):
            partition = ustr(int(datetime.utcnow().microsecond / 10000))
            fileutil.write_file(self._partition_file, partition)
            add_event(
                AGENT_NAME,
                version=CURRENT_VERSION,
                op=WALAEventOperation.Partition,
                is_success=True,
                message=partition)

    def _ensure_readonly_files(self):
        for g in READONLY_FILE_GLOBS:
            for path in glob.iglob(os.path.join(conf.get_lib_dir(), g)):
                os.chmod(path, stat.S_IRUSR)

    def _evaluate_agent_health(self, latest_agent):
        """"""
        Evaluate the health of the selected agent: If it is restarting
        too frequently, raise an Exception to force blacklisting.
        """"""
        if latest_agent is None:
            self.child_agent = None
            return

        if self.child_agent is None or latest_agent.version != self.child_agent.version:
            self.child_agent = latest_agent
            self.child_launch_time = None
            self.child_launch_attempts = 0

        if self.child_launch_time is None:
            self.child_launch_time = time.time()

        self.child_launch_attempts += 1

        if (time.time() - self.child_launch_time) <= CHILD_LAUNCH_INTERVAL \
            and self.child_launch_attempts >= CHILD_LAUNCH_RESTART_MAX:
                msg = u""Agent {0} restarted more than {1} times in {2} seconds"".format(
                    self.child_agent.name,
                    CHILD_LAUNCH_RESTART_MAX,
                    CHILD_LAUNCH_INTERVAL)
                raise Exception(msg)
        return

    def _filter_blacklisted_agents(self):
        self.agents = [agent for agent in self.agents if not agent.is_blacklisted]

    def _find_agents(self):
        """"""
        Load all non-blacklisted agents currently on disk.
        """"""
        try:
            self._set_agents(self._load_agents())
            self._filter_blacklisted_agents()
        except Exception as e:
            logger.warn(u""Exception occurred loading available agents: {0}"", ustr(e))
        return

    def _get_host_plugin(self, protocol=None):
        return protocol.client.get_host_plugin() \
                                if protocol and \
                                    type(protocol) is WireProtocol and \
                                    protocol.client \
                                else None

    def _get_pid_parts(self):
        pid_file = conf.get_agent_pid_file_path()
        pid_dir = os.path.dirname(pid_file)
        pid_name = os.path.basename(pid_file)
        pid_re = re.compile(""(\d+)_{0}"".format(re.escape(pid_name)))
        return pid_dir, pid_name, pid_re

    def _get_pid_files(self):
        pid_dir, pid_name, pid_re = self._get_pid_parts()
        pid_files = [os.path.join(pid_dir, f) for f in os.listdir(pid_dir) if pid_re.match(f)]
        pid_files.sort(key=lambda f: int(pid_re.match(os.path.basename(f)).group(1)))
        return pid_files

    @property
    def _is_clean_start(self):
        return not os.path.isfile(self._sentinel_file_path())

    @property
    def _is_orphaned(self):
        parent_pid = os.getppid()
        if parent_pid in (1, None):
            return True

        if not os.path.isfile(conf.get_agent_pid_file_path()):
            return True

        return fileutil.read_file(conf.get_agent_pid_file_path()) != ustr(parent_pid)

    def _is_version_eligible(self, version):
        # Ensure the installed version is always eligible
        if version == CURRENT_VERSION and is_current_agent_installed():
            return True

        for agent in self.agents:
            if agent.version == version:
                return agent.is_available

        return False

    def _load_agents(self):
        path = os.path.join(conf.get_lib_dir(), ""{0}-*"".format(AGENT_NAME))
        return [GuestAgent(path=agent_dir)
                        for agent_dir in glob.iglob(path) if os.path.isdir(agent_dir)]

    def _partition(self):
        return int(fileutil.read_file(self._partition_file))

    @property
    def _partition_file(self):
        return os.path.join(conf.get_lib_dir(), AGENT_PARTITION_FILE)

    def _purge_agents(self):
        """"""
        Remove from disk all directories and .zip files of unknown agents
        (without removing the current, running agent).
        """"""
        path = os.path.join(conf.get_lib_dir(), ""{0}-*"".format(AGENT_NAME))

        known_versions = [agent.version for agent in self.agents]
        if CURRENT_VERSION not in known_versions:
            logger.verbose(
                u""Running Agent {0} was not found in the agent manifest - adding to list"",
                CURRENT_VERSION)
            known_versions.append(CURRENT_VERSION)

        for agent_path in glob.iglob(path):
            try:
                name = fileutil.trim_ext(agent_path, ""zip"")
                m = AGENT_DIR_PATTERN.match(name)
                if m is not None and FlexibleVersion(m.group(1)) not in known_versions:
                    if os.path.isfile(agent_path):
                        logger.info(u""Purging outdated Agent file {0}"", agent_path)
                        os.remove(agent_path)
                    else:
                        logger.info(u""Purging outdated Agent directory {0}"", agent_path)
                        shutil.rmtree(agent_path)
            except Exception as e:
                logger.warn(u""Purging {0} raised exception: {1}"", agent_path, ustr(e))
        return

    def _set_agents(self, agents=[]):
        self.agents = agents
        self.agents.sort(key=lambda agent: agent.version, reverse=True)
        return

    def _set_sentinel(self, agent=CURRENT_AGENT, msg=""Unknown cause""):
        try:
            fileutil.write_file(
                self._sentinel_file_path(),
                ""[{0}] [{1}]"".format(agent, msg))
        except Exception as e:
            logger.warn(
                u""Exception writing sentinel file {0}: {1}"",
                self._sentinel_file_path(),
                str(e))
        return

    def _sentinel_file_path(self):
        return os.path.join(conf.get_lib_dir(), AGENT_SENTINEL_FILE)

    def _shutdown(self):
        self.running = False

        if not os.path.isfile(self._sentinel_file_path()):
            return

        try:
            os.remove(self._sentinel_file_path())
        except Exception as e:
            logger.warn(
                u""Exception removing sentinel file {0}: {1}"",
                self._sentinel_file_path(),
                str(e))
        return

    def _upgrade_available(self, base_version=CURRENT_VERSION):
        # Emit an event expressing the state of AutoUpdate
        # Note:
        # - Duplicate events get suppressed; state transitions always emit
        add_event(
            AGENT_NAME,
            version=CURRENT_VERSION,
            op=WALAEventOperation.AutoUpdate,
            is_success=conf.get_autoupdate_enabled())

        # Ignore new agents if updating is disabled
        if not conf.get_autoupdate_enabled():
            return False

        now = time.time()
        if self.last_attempt_time is not None:
            next_attempt_time = self.last_attempt_time + \
                                    conf.get_autoupdate_frequency()
        else:
            next_attempt_time = now
        if next_attempt_time > now:
            return False

        family = conf.get_autoupdate_gafamily()
        logger.verbose(""Checking for agent family {0} updates"", family)

        self.last_attempt_time = now
        protocol = self.protocol_util.get_protocol()

        for update_goal_state in [False, True]:
            try:
                if update_goal_state:
                    protocol.update_goal_state(forced=True)

                manifest_list, etag = protocol.get_vmagent_manifests()

                manifests = [m for m in manifest_list.vmAgentManifests \
                                if m.family == family and \
                                    len(m.versionsManifestUris) > 0]
                if len(manifests) == 0:
                    logger.verbose(u""Incarnation {0} has no {1} agent updates"",
                                    etag, family)
                    return False

                pkg_list = protocol.get_vmagent_pkgs(manifests[0])

                # Set the agents to those available for download at least as
                # current as the existing agent and remove from disk any agent
                # no longer reported to the VM.
                # Note:
                #  The code leaves on disk available, but blacklisted, agents
                #  so as to preserve the state. Otherwise, those agents could be
                #  again downloaded and inappropriately retried.
                host = self._get_host_plugin(protocol=protocol)
                self._set_agents([GuestAgent(pkg=pkg, host=host) \
                                     for pkg in pkg_list.versions])

                self._purge_agents()
                self._filter_blacklisted_agents()

                # Return True if current agent is no longer available or an
                # agent with a higher version number is available
                return not self._is_version_eligible(base_version) \
                    or (len(self.agents) > 0 \
                        and self.agents[0].version > base_version)

            except Exception as e:
                if isinstance(e, ResourceGoneError):
                    continue

                msg = u""Exception retrieving agent manifests: {0}"".format(
                            ustr(traceback.format_exc()))
                logger.warn(msg)
                add_event(
                    AGENT_NAME,
                    op=WALAEventOperation.Download,
                    version=CURRENT_VERSION,
                    is_success=False,
                    message=msg)
                return False

    def _write_pid_file(self):
        pid_files = self._get_pid_files()

        pid_dir, pid_name, pid_re = self._get_pid_parts()

        previous_pid_file = None \
                        if len(pid_files) <= 0 \
                        else pid_files[-1]
        pid_index = -1 \
                    if previous_pid_file is None \
                    else int(pid_re.match(os.path.basename(previous_pid_file)).group(1))
        pid_file = os.path.join(pid_dir, ""{0}_{1}"".format(pid_index+1, pid_name))

        try:
            fileutil.write_file(pid_file, ustr(os.getpid()))
            logger.info(u""{0} running as process {1}"", CURRENT_AGENT, ustr(os.getpid()))
        except Exception as e:
            pid_file = None
            logger.warn(
                u""Expection writing goal state agent {0} pid to {1}: {2}"",
                CURRENT_AGENT,
                pid_file,
                ustr(e))

        return pid_files, pid_file


class GuestAgent(object):
    def __init__(self, path=None, pkg=None, host=None):
        self.pkg = pkg
        self.host = host
        version = None
        if path is not None:
            m = AGENT_DIR_PATTERN.match(path)
            if m == None:
                raise UpdateError(u""Illegal agent directory: {0}"".format(path))
            version = m.group(1)
        elif self.pkg is not None:
            version = pkg.version

        if version == None:
            raise UpdateError(u""Illegal agent version: {0}"".format(version))
        self.version = FlexibleVersion(version)

        location = u""disk"" if path is not None else u""package""
        logger.verbose(u""Loading Agent {0} from {1}"", self.name, location)

        self.error = GuestAgentError(self.get_agent_error_file())
        self.error.load()

        try:
            self._ensure_downloaded()
            self._ensure_loaded()
        except Exception as e:
            if isinstance(e, ResourceGoneError):
                raise

            # The agent was improperly blacklisting versions due to a timeout
            # encountered while downloading a later version. Errors of type
            # socket.error are IOError, so this should provide sufficient
            # protection against a large class of I/O operation failures.
            if isinstance(e, IOError):
                raise

            # Note the failure, blacklist the agent if the package downloaded
            # - An exception with a downloaded package indicates the package
            #   is corrupt (e.g., missing the HandlerManifest.json file)
            self.mark_failure(is_fatal=os.path.isfile(self.get_agent_pkg_path()))

            msg = u""Agent {0} install failed with exception: {1}"".format(
                        self.name, ustr(e))
            logger.warn(msg)
            add_event(
                AGENT_NAME,
                version=self.version,
                op=WALAEventOperation.Install,
                is_success=False,
                message=msg)

    @property
    def name(self):
        return ""{0}-{1}"".format(AGENT_NAME, self.version)

    def get_agent_cmd(self):
        return self.manifest.get_enable_command()

    def get_agent_dir(self):
        return os.path.join(conf.get_lib_dir(), self.name)

    def get_agent_error_file(self):
        return os.path.join(conf.get_lib_dir(), self.name, AGENT_ERROR_FILE)

    def get_agent_manifest_path(self):
        return os.path.join(self.get_agent_dir(), AGENT_MANIFEST_FILE)

    def get_agent_pkg_path(self):
        return ""."".join((os.path.join(conf.get_lib_dir(), self.name), ""zip""))

    def clear_error(self):
        self.error.clear()
        self.error.save()

    @property
    def is_available(self):
        return self.is_downloaded and not self.is_blacklisted

    @property
    def is_blacklisted(self):
        return self.error is not None and self.error.is_blacklisted

    @property
    def is_downloaded(self):
        return self.is_blacklisted or \
                os.path.isfile(self.get_agent_manifest_path())

    def mark_failure(self, is_fatal=False):
        try:
            if not os.path.isdir(self.get_agent_dir()):
                os.makedirs(self.get_agent_dir())
            self.error.mark_failure(is_fatal=is_fatal)
            self.error.save()
            if self.error.is_blacklisted:
                logger.warn(u""Agent {0} is permanently blacklisted"", self.name)
        except Exception as e:
            logger.warn(u""Agent {0} failed recording error state: {1}"", self.name, ustr(e))

    def _ensure_downloaded(self):
        logger.verbose(u""Ensuring Agent {0} is downloaded"", self.name)

        if self.is_downloaded:
            logger.verbose(u""Agent {0} was previously downloaded - skipping download"", self.name)
            return

        if self.pkg is None:
            raise UpdateError(u""Agent {0} is missing package and download URIs"".format(
                self.name))
        
        self._download()
        self._unpack()

        msg = u""Agent {0} downloaded successfully"".format(self.name)
        logger.verbose(msg)
        add_event(
            AGENT_NAME,
            version=self.version,
            op=WALAEventOperation.Install,
            is_success=True,
            message=msg)

    def _ensure_loaded(self):
        self._load_manifest()
        self._load_error()

    def _download(self):
        uris_shuffled = self.pkg.uris
        random.shuffle(uris_shuffled)
        for uri in uris_shuffled:
            if not HostPluginProtocol.is_default_channel() and self._fetch(uri.uri):
                break

            elif self.host is not None and self.host.ensure_initialized():
                if not HostPluginProtocol.is_default_channel():
                    logger.warn(""Download failed, switching to host plugin"")
                else:
                    logger.verbose(""Using host plugin as default channel"")

                uri, headers = self.host.get_artifact_request(uri.uri, self.host.manifest_uri)
                try:
                    if self._fetch(uri, headers=headers, use_proxy=False):
                        if not HostPluginProtocol.is_default_channel():
                            logger.verbose(""Setting host plugin as default channel"")
                            HostPluginProtocol.set_default_channel(True)
                        break
                    else:
                        logger.warn(""Host plugin download failed"")

                # If the HostPlugin rejects the request,
                # let the error continue, but set to use the HostPlugin
                except ResourceGoneError:
                    HostPluginProtocol.set_default_channel(True)
                    raise

            else:
                logger.error(""No download channels available"")

        if not os.path.isfile(self.get_agent_pkg_path()):
            msg = u""Unable to download Agent {0} from any URI"".format(self.name)
            add_event(
                AGENT_NAME,
                op=WALAEventOperation.Download,
                version=CURRENT_VERSION,
                is_success=False,
                message=msg)
            raise UpdateError(msg)

    def _fetch(self, uri, headers=None, use_proxy=True):
        package = None
        try:
            is_healthy = True
            error_response = ''
            resp = restutil.http_get(uri, use_proxy=use_proxy, headers=headers)
            if restutil.request_succeeded(resp):
                package = resp.read()
                fileutil.write_file(self.get_agent_pkg_path(),
                                    bytearray(package),
                                    asbin=True)
                logger.verbose(u""Agent {0} downloaded from {1}"", self.name, uri)
            else:
                error_response = restutil.read_response_error(resp)
                logger.verbose(""Fetch was unsuccessful [{0}]"", error_response)
                is_healthy = not restutil.request_failed_at_hostplugin(resp)

            if self.host is not None:
                self.host.report_fetch_health(uri, is_healthy, source='GuestAgent', response=error_response)

        except restutil.HttpError as http_error:
            if isinstance(http_error, ResourceGoneError):
                raise

            logger.verbose(u""Agent {0} download from {1} failed [{2}]"",
                           self.name,
                           uri,
                           http_error)

        return package is not None

    def _load_error(self):
        try:
            self.error = GuestAgentError(self.get_agent_error_file())
            self.error.load()
            logger.verbose(u""Agent {0} error state: {1}"", self.name, ustr(self.error))
        except Exception as e:
            logger.warn(u""Agent {0} failed loading error state: {1}"", self.name, ustr(e))

    def _load_manifest(self):
        path = self.get_agent_manifest_path()
        if not os.path.isfile(path):
            msg = u""Agent {0} is missing the {1} file"".format(self.name, AGENT_MANIFEST_FILE)
            raise UpdateError(msg)

        with open(path, ""r"") as manifest_file:
            try:
                manifests = json.load(manifest_file)
            except Exception as e:
                msg = u""Agent {0} has a malformed {1}"".format(self.name, AGENT_MANIFEST_FILE)
                raise UpdateError(msg)
            if type(manifests) is list:
                if len(manifests) <= 0:
                    msg = u""Agent {0} has an empty {1}"".format(self.name, AGENT_MANIFEST_FILE)
                    raise UpdateError(msg)
                manifest = manifests[0]
            else:
                manifest = manifests

        try:
            self.manifest = HandlerManifest(manifest)
            if len(self.manifest.get_enable_command()) <= 0:
                raise Exception(u""Manifest is missing the enable command"")
        except Exception as e:
            msg = u""Agent {0} has an illegal {1}: {2}"".format(
                self.name,
                AGENT_MANIFEST_FILE,
                ustr(e))
            raise UpdateError(msg)

        logger.verbose(
            u""Agent {0} loaded manifest from {1}"",
            self.name,
            self.get_agent_manifest_path())
        logger.verbose(u""Successfully loaded Agent {0} {1}: {2}"",
            self.name,
            AGENT_MANIFEST_FILE,
            ustr(self.manifest.data))
        return

    def _unpack(self):
        try:
            if os.path.isdir(self.get_agent_dir()):
                shutil.rmtree(self.get_agent_dir())

            zipfile.ZipFile(self.get_agent_pkg_path()).extractall(self.get_agent_dir())

        except Exception as e:
            fileutil.clean_ioerror(e,
                paths=[self.get_agent_dir(), self.get_agent_pkg_path()])

            msg = u""Exception unpacking Agent {0} from {1}: {2}"".format(
                self.name,
                self.get_agent_pkg_path(),
                ustr(e))
            raise UpdateError(msg)

        if not os.path.isdir(self.get_agent_dir()):
            msg = u""Unpacking Agent {0} failed to create directory {1}"".format(
                self.name,
                self.get_agent_dir())
            raise UpdateError(msg)

        logger.verbose(
            u""Agent {0} unpacked successfully to {1}"",
            self.name,
            self.get_agent_dir())
        return


class GuestAgentError(object):
    def __init__(self, path):
        if path is None:
            raise UpdateError(u""GuestAgentError requires a path"")
        self.path = path

        self.clear()
        return
   
    def mark_failure(self, is_fatal=False):
        self.last_failure = time.time()
        self.failure_count += 1
        self.was_fatal = is_fatal
        return

    def clear(self):
        self.last_failure = 0.0
        self.failure_count = 0
        self.was_fatal = False
        return

    @property
    def is_blacklisted(self):
        return self.was_fatal or self.failure_count >= MAX_FAILURE

    def load(self):
        if self.path is not None and os.path.isfile(self.path):
            with open(self.path, 'r') as f:
                self.from_json(json.load(f))
        return

    def save(self):
        if os.path.isdir(os.path.dirname(self.path)):
            with open(self.path, 'w') as f:
                json.dump(self.to_json(), f)
        return
    
    def from_json(self, data):
        self.last_failure = max(
            self.last_failure,
            data.get(u""last_failure"", 0.0))
        self.failure_count = max(
            self.failure_count,
            data.get(u""failure_count"", 0))
        self.was_fatal = self.was_fatal or data.get(u""was_fatal"", False)
        return

    def to_json(self):
        data = {
            u""last_failure"": self.last_failure,
            u""failure_count"": self.failure_count,
            u""was_fatal"" : self.was_fatal
        }  
        return data

    def __str__(self):
        return ""Last Failure: {0}, Total Failures: {1}, Fatal: {2}"".format(
            self.last_failure,
            self.failure_count,
            self.was_fatal)
/n/n/ntests/common/osutil/mock_osutil.py/n/n# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

from azurelinuxagent.common.osutil.default import DefaultOSUtil

class MockOSUtil(DefaultOSUtil):
    def __init__(self):
        self.all_users = {}
        self.sudo_users = set()
        self.jit_enabled = True

    def useradd(self, username, expiration=None, comment=None):
        if username == """":
            raise Exception(""test exception for bad username"")
        if username in self.all_users:
            raise Exception(""test exception, user already exists"")
        self.all_users[username] = (username, None, None, None, comment, None, None, expiration)

    def conf_sudoer(self, username, nopasswd=False, remove=False):
        if not remove:
            self.sudo_users.add(username)
        else:
            self.sudo_users.remove(username)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if password == """":
            raise Exception(""test exception for bad password"")
        user = self.all_users[username]
        self.all_users[username] = (user[0], password, user[2], user[3], user[4], user[5], user[6], user[7])

    def del_account(self, username):
        if username == """":
            raise Exception(""test exception, bad data"")
        if username not in self.all_users:
            raise Exception(""test exception, user does not exist to delete"")
        self.all_users.pop(username)

    def get_users(self):
        return self.all_users.values()/n/n/ntests/ga/test_remoteaccess.py/n/n# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#
import xml

from tests.tools import *
from azurelinuxagent.common.protocol.wire import *
from azurelinuxagent.common.osutil import get_osutil

class TestRemoteAccess(AgentTestCase):
    def test_parse_remote_access(self):
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(""1"", remote_access.incarnation)
        self.assertEquals(1, len(remote_access.user_list.users), ""User count does not match."")
        self.assertEquals(""testAccount"", remote_access.user_list.users[0].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[0].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[0].expiration, ""Expiration does not match."")

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',
    return_value=GoalState(load_data('wire/goal_state.xml')))
    def test_update_remote_access_conf_no_remote_access(self, _):
        protocol = WireProtocol('12.34.56.78')
        goal_state = protocol.client.get_goal_state()
        protocol.client.update_remote_access_conf(goal_state)

    def test_parse_two_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_two_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(""1"", remote_access.incarnation)
        self.assertEquals(2, len(remote_access.user_list.users), ""User count does not match."")
        self.assertEquals(""testAccount1"", remote_access.user_list.users[0].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[0].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[0].expiration, ""Expiration does not match."")
        self.assertEquals(""testAccount2"", remote_access.user_list.users[1].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[1].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[1].expiration, ""Expiration does not match."")

    def test_parse_ten_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(10, len(remote_access.user_list.users), ""User count does not match."")
    
    def test_parse_duplicate_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_duplicate_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(2, len(remote_access.user_list.users), ""User count does not match."")
        self.assertEquals(""testAccount"", remote_access.user_list.users[0].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[0].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[0].expiration, ""Expiration does not match."")
        self.assertEquals(""testAccount"", remote_access.user_list.users[1].name, ""Account name does not match"")
        self.assertEquals(""encryptedPasswordString"", remote_access.user_list.users[1].encrypted_password, ""Encrypted password does not match."")
        self.assertEquals(""2019-01-01"", remote_access.user_list.users[1].expiration, ""Expiration does not match."")

    def test_parse_zero_remote_access_accounts(self):
        data_str = load_data('wire/remote_access_no_accounts.xml')
        remote_access = RemoteAccess(data_str)
        self.assertNotEquals(None, remote_access)
        self.assertEquals(0, len(remote_access.user_list.users), ""User count does not match."")

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',
    return_value=GoalState(load_data('wire/goal_state_remote_access.xml')))
    @patch('azurelinuxagent.common.protocol.wire.WireClient.fetch_config',
    return_value=load_data('wire/remote_access_single_account.xml'))
    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_header_for_cert')
    def test_update_remote_access_conf_remote_access(self, _1, _2, _3):
        protocol = WireProtocol('12.34.56.78')
        goal_state = protocol.client.get_goal_state()
        protocol.client.update_remote_access_conf(goal_state)
        self.assertNotEquals(None, protocol.client.remote_access)
        self.assertEquals(1, len(protocol.client.remote_access.user_list.users))
        self.assertEquals('testAccount', protocol.client.remote_access.user_list.users[0].name)
        self.assertEquals('encryptedPasswordString', protocol.client.remote_access.user_list.users[0].encrypted_password)

    def test_parse_bad_remote_access_data(self):
        data = ""foobar""
        self.assertRaises(xml.parsers.expat.ExpatError, RemoteAccess, data)/n/n/ntests/ga/test_remoteaccess_handler.py/n/n# Copyright Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

from datetime import timedelta
from azurelinuxagent.common.protocol.wire import *
from azurelinuxagent.ga.remoteaccess import RemoteAccessHandler
from tests.common.osutil.mock_osutil import MockOSUtil
from tests.tools import *


info_messages = []
error_messages = []


def get_user_dictionary(users):
    user_dictionary = {}
    for user in users:
        user_dictionary[user[0]] = user
    return user_dictionary


def log_info(msg_format, *args):
    info_messages.append(msg_format.format(args))


def log_error(msg_format, *args):
    error_messages.append(msg_format.format(args))


class TestRemoteAccessHandler(AgentTestCase):

    def setUp(self):
        super(TestRemoteAccessHandler, self).setUp()
        del info_messages[:]
        del error_messages[:]

    # add_user tests
    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=""]aPPEv}uNg1FPnl?"")
    def test_add_user(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        actual_user = users[tstuser]
        expected_expiration = (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d"")
        self.assertEqual(actual_user[7], expected_expiration)
        self.assertEqual(actual_user[4], ""JIT_Account"")
        self.assertEqual(0, len(error_messages))
        self.assertEqual(1, len(info_messages))
        self.assertEqual(info_messages[0], ""User '{0}' added successfully with expiration in {1}""
                         .format(tstuser, expected_expiration))

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=""]aPPEv}uNg1FPnl?"")
    def test_add_user_bad_creation_data(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = """"
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        self.assertEqual(0, len(rah.os_util.get_users()))
        self.assertEqual(1, len(error_messages))
        self.assertEqual(0, len(info_messages))
        error = ""Error adding user {0}. test exception for bad username"".format(tstuser)
        self.assertEqual(error, error_messages[0])

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value="""")
    def test_add_user_bad_password_data(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = """"
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        self.assertEqual(0, len(rah.os_util.get_users()))
        self.assertEqual(1, len(error_messages))
        self.assertEqual(1, len(info_messages))
        error = ""Error creating user {0}. test exception for bad password"".format(tstuser)
        self.assertEqual(error, error_messages[0])
        self.assertEqual(""User deleted {0}"".format(tstuser), info_messages[0])

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_add_user_already_existing(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        self.assertEqual(1, len(users.keys()))
        actual_user = users[tstuser]
        self.assertEqual(actual_user[7], (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d""))
        # add the new duplicate user, ensure it's not created and does not overwrite the existing user.
        # this does not test the user add function as that's mocked, it tests processing skips the remaining
        # calls after the initial failure
        new_user_expiration = datetime.utcnow() + timedelta(days=5)
        rah.add_user(tstuser, pwd, new_user_expiration)
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users after dup user attempted"".format(tstuser))
        self.assertEqual(1, len(users.keys()))
        actual_user = users[tstuser]
        self.assertEqual(actual_user[7], (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d""))

    # delete_user tests
    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret', return_value=""]aPPEv}uNg1FPnl?"")
    def test_delete_user(self, _1, _2, _3):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        expected_expiration = (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d"")
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        rah.delete_user(tstuser)
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertFalse(tstuser in users)
        self.assertEqual(0, len(error_messages))
        self.assertEqual(2, len(info_messages))
        self.assertEqual(""User '{0}' added successfully with expiration in {1}"".format(tstuser, expected_expiration),
                         info_messages[0])
        self.assertEqual(""User deleted {0}"".format(tstuser), info_messages[1])

    def test_handle_failed_create_with_bad_data(self):
        mock_os_util = MockOSUtil()
        testusr = ""foobar""
        mock_os_util.all_users[testusr] = (testusr, None, None, None, None, None, None, None)
        rah = RemoteAccessHandler()
        rah.os_util = mock_os_util
        rah.handle_failed_create("""", ""test message"")
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(1, len(users.keys()))
        self.assertTrue(testusr in users, ""Expected user {0} missing"".format(testusr))

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    def test_delete_user_does_not_exist(self, _1, _2):
        mock_os_util = MockOSUtil()
        testusr = ""foobar""
        mock_os_util.all_users[testusr] = (testusr, None, None, None, None, None, None, None)
        rah = RemoteAccessHandler()
        rah.os_util = mock_os_util
        testuser = ""Carl""
        test_message = ""test message""
        rah.handle_failed_create(testuser, test_message)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(1, len(users.keys()))
        self.assertTrue(testusr in users, ""Expected user {0} missing"".format(testusr))
        self.assertEqual(2, len(error_messages))
        self.assertEqual(0, len(info_messages))
        self.assertEqual(""Error creating user {0}. {1}"".format(testuser, test_message), error_messages[0])
        msg = ""Failed to clean up after account creation for {0}. test exception, user does not exist to delete""\
            .format(testuser)
        self.assertEqual(msg, error_messages[1])

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_new_user(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        tstuser = remote_access.user_list.users[0].name
        expiration_date = datetime.utcnow() + timedelta(days=1)
        expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        remote_access.user_list.users[0].expiration = expiration
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        actual_user = users[tstuser]
        expected_expiration = (expiration_date + timedelta(days=1)).strftime(""%Y-%m-%d"")
        self.assertEqual(actual_user[7], expected_expiration)
        self.assertEqual(actual_user[4], ""JIT_Account"")

    def test_do_not_add_expired_user(self):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()      
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        expiration = (datetime.utcnow() - timedelta(days=2)).strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        remote_access.user_list.users[0].expiration = expiration
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertFalse(""testAccount"" in users)

    @patch('azurelinuxagent.common.logger.Logger.info', side_effect=log_info)
    @patch('azurelinuxagent.common.logger.Logger.error', side_effect=log_error)
    def test_error_add_user(self, _1, _2):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstuser = ""foobar""
        expiration = datetime.utcnow() + timedelta(days=1)
        pwd = ""bad password""
        rah.add_user(tstuser, pwd, expiration)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(0, len(users))
        self.assertEqual(1, len(error_messages))
        self.assertEqual(1, len(info_messages))
        error = ""Error creating user {0}. [CryptError] Error decoding secret\nInner error: Incorrect padding"".\
            format(tstuser)
        self.assertEqual(error, error_messages[0])
        self.assertEqual(""User deleted {0}"".format(tstuser), info_messages[0])

    def test_handle_remote_access_no_users(self):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_no_accounts.xml')
        remote_access = RemoteAccess(data_str)
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(0, len(users.keys()))

    def test_handle_remote_access_validate_jit_user_valid(self):
        rah = RemoteAccessHandler()
        comment = ""JIT_Account""
        result = rah.validate_jit_user(comment)
        self.assertTrue(result, ""Did not identify '{0}' as a JIT_Account"".format(comment))

    def test_handle_remote_access_validate_jit_user_invalid(self):
        rah = RemoteAccessHandler()
        test_users = [""John Doe"", None, """", "" ""]
        failed_results = """"
        for user in test_users:
            if rah.validate_jit_user(user):
                failed_results += ""incorrectly identified '{0} as a JIT_Account'.  "".format(user)
        if len(failed_results) > 0:
            self.fail(failed_results)

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_multiple_users(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_two_accounts.xml')
        remote_access = RemoteAccess(data_str)
        testusers = []
        count = 0
        while count < 2:
            user = remote_access.user_list.users[count].name
            expiration_date = datetime.utcnow() + timedelta(days=count + 1)
            expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
            remote_access.user_list.users[count].expiration = expiration
            testusers.append(user)
            count += 1
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(testusers[0] in users, ""{0} missing from users"".format(testusers[0]))
        self.assertTrue(testusers[1] in users, ""{0} missing from users"".format(testusers[1]))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    # max fabric supports in the Goal State
    def test_handle_remote_access_ten_users(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        count = 0
        for user in remote_access.user_list.users:
            count += 1
            user.name = ""tstuser{0}"".format(count)
            expiration_date = datetime.utcnow() + timedelta(days=count)
            user.expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(10, len(users.keys()))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_user_removed(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        count = 0
        for user in remote_access.user_list.users:
            count += 1
            user.name = ""tstuser{0}"".format(count)
            expiration_date = datetime.utcnow() + timedelta(days=count)
            user.expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(10, len(users.keys()))
        del rah.remote_access.user_list.users[:]
        self.assertEqual(10, len(users.keys()))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_bad_data_and_good_data(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_10_accounts.xml')
        remote_access = RemoteAccess(data_str)
        count = 0
        for user in remote_access.user_list.users:
            count += 1
            user.name = ""tstuser{0}"".format(count)
            if count is 2:
                user.name = """"
            expiration_date = datetime.utcnow() + timedelta(days=count)
            user.expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertEqual(9, len(users.keys()))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    def test_handle_remote_access_deleted_user_readded(self, _):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        data_str = load_data('wire/remote_access_single_account.xml')
        remote_access = RemoteAccess(data_str)
        tstuser = remote_access.user_list.users[0].name
        expiration_date = datetime.utcnow() + timedelta(days=1)
        expiration = expiration_date.strftime(""%a, %d %b %Y %H:%M:%S "") + ""UTC""
        remote_access.user_list.users[0].expiration = expiration
        rah.remote_access = remote_access
        rah.handle_remote_access()
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        os_util = rah.os_util
        os_util.__class__ = MockOSUtil
        os_util.all_users.clear()
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser not in users)
        rah.handle_remote_access()
        # refresh users
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))

    @patch('azurelinuxagent.common.utils.cryptutil.CryptUtil.decrypt_secret',
           return_value=""]aPPEv}uNg1FPnl?"")
    @patch('azurelinuxagent.common.osutil.get_osutil',
           return_value=MockOSUtil())
    @patch('azurelinuxagent.common.protocol.util.ProtocolUtil.get_protocol',
           return_value=WireProtocol(""12.34.56.78""))
    @patch('azurelinuxagent.common.protocol.wire.WireProtocol.get_incarnation',
           return_value=""1"")
    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_remote_access',
           return_value=""asdf"")
    def test_remote_access_handler_run_bad_data(self, _1, _2, _3, _4, _5):
        rah = RemoteAccessHandler()
        rah.os_util = MockOSUtil()
        tstpassword = ""]aPPEv}uNg1FPnl?""
        tstuser = ""foobar""
        expiration_date = datetime.utcnow() + timedelta(days=1)
        pwd = tstpassword
        rah.add_user(tstuser, pwd, expiration_date)
        users = get_user_dictionary(rah.os_util.get_users())
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
        rah.run()
        self.assertTrue(tstuser in users, ""{0} missing from users"".format(tstuser))
/n/n/ntests/ga/test_update.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the Apache License.

from __future__ import print_function

from azurelinuxagent.common.event import *
from azurelinuxagent.common.protocol.hostplugin import *
from azurelinuxagent.common.protocol.metadata import *
from azurelinuxagent.common.protocol.wire import *
from azurelinuxagent.common.utils.fileutil import *
from azurelinuxagent.ga.update import *

from tests.tools import *

NO_ERROR = {
    ""last_failure"" : 0.0,
    ""failure_count"" : 0,
    ""was_fatal"" : False
}

FATAL_ERROR = {
    ""last_failure"" : 42.42,
    ""failure_count"" : 2,
    ""was_fatal"" : True
}

WITH_ERROR = {
    ""last_failure"" : 42.42,
    ""failure_count"" : 2,
    ""was_fatal"" : False
}

EMPTY_MANIFEST = {
    ""name"": ""WALinuxAgent"",
    ""version"": 1.0,
    ""handlerManifest"": {
        ""installCommand"": """",
        ""uninstallCommand"": """",
        ""updateCommand"": """",
        ""enableCommand"": """",
        ""disableCommand"": """",
        ""rebootAfterInstall"": False,
        ""reportHeartbeat"": False
    }
}


def get_agent_pkgs(in_dir=os.path.join(data_dir, ""ga"")):
    path = os.path.join(in_dir, AGENT_PKG_GLOB)
    return glob.glob(path)


def get_agents(in_dir=os.path.join(data_dir, ""ga"")):
    path = os.path.join(in_dir, AGENT_DIR_GLOB)
    return [a for a in glob.glob(path) if os.path.isdir(a)]


def get_agent_file_path():
    return get_agent_pkgs()[0]


def get_agent_file_name():
    return os.path.basename(get_agent_file_path())


def get_agent_path():
    return fileutil.trim_ext(get_agent_file_path(), ""zip"")


def get_agent_name():
    return os.path.basename(get_agent_path())


def get_agent_version():
    return FlexibleVersion(get_agent_name().split(""-"")[1])


def faux_logger():
    print(""STDOUT message"")
    print(""STDERR message"", file=sys.stderr)
    return DEFAULT


class UpdateTestCase(AgentTestCase):

    def agent_bin(self, version, suffix):
        return ""bin/{0}-{1}{2}.egg"".format(AGENT_NAME, version, suffix)

    def rename_agent_bin(self, path, src_v, dst_v):
        src_bin = glob.glob(os.path.join(path, self.agent_bin(src_v, '*')))[0]
        dst_bin = os.path.join(path, self.agent_bin(dst_v, ''))
        shutil.move(src_bin, dst_bin)
    
    def agents(self):
        return [GuestAgent(path=path) for path in self.agent_dirs()]

    def agent_count(self):
        return len(self.agent_dirs())

    def agent_dirs(self):
        return get_agents(in_dir=self.tmp_dir)

    def agent_dir(self, version):
        return os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, version))

    def agent_paths(self):
        paths = glob.glob(os.path.join(self.tmp_dir, ""*""))
        paths.sort()
        return paths

    def agent_pkgs(self):
        return get_agent_pkgs(in_dir=self.tmp_dir)

    def agent_versions(self):
        v = [FlexibleVersion(AGENT_DIR_PATTERN.match(a).group(1)) for a in self.agent_dirs()]
        v.sort(reverse=True)
        return v

    def get_error_file(self, error_data=NO_ERROR):
        fp = tempfile.NamedTemporaryFile(mode=""w"")
        json.dump(error_data if error_data is not None else NO_ERROR, fp)
        fp.seek(0)
        return fp

    def create_error(self, error_data=NO_ERROR):
        with self.get_error_file(error_data) as path:
            err = GuestAgentError(path.name)
            err.load()
            return err

    def copy_agents(self, *agents):
        if len(agents) <= 0:
            agents = get_agent_pkgs()
        for agent in agents:
            shutil.copy(agent, self.tmp_dir)
        return

    def expand_agents(self):
        for agent in self.agent_pkgs():
            path = os.path.join(self.tmp_dir, fileutil.trim_ext(agent, ""zip""))
            zipfile.ZipFile(agent).extractall(path)

    def prepare_agent(self, version):
        """"""
        Create a download for the current agent version, copied from test data
        """"""
        self.copy_agents(get_agent_pkgs()[0])
        self.expand_agents()

        versions = self.agent_versions()
        src_v = FlexibleVersion(str(versions[0]))

        from_path = self.agent_dir(src_v)
        dst_v = FlexibleVersion(str(version))
        to_path = self.agent_dir(dst_v)

        if from_path != to_path:
            shutil.move(from_path + "".zip"", to_path + "".zip"")
            shutil.move(from_path, to_path)
            self.rename_agent_bin(to_path, src_v, dst_v)
        return

    def prepare_agents(self,
                       count=20,
                       is_available=True):

        # Ensure the test data is copied over
        agent_count = self.agent_count()
        if agent_count <= 0:
            self.copy_agents(get_agent_pkgs()[0])
            self.expand_agents()
            count -= 1

        # Determine the most recent agent version
        versions = self.agent_versions()
        src_v = FlexibleVersion(str(versions[0]))

        # Create agent packages and directories
        return self.replicate_agents(
            src_v=src_v,
            count=count-agent_count,
            is_available=is_available)

    def remove_agents(self):
        for agent in self.agent_paths():
            try:
                if os.path.isfile(agent):
                    os.remove(agent)
                else:
                    shutil.rmtree(agent)
            except:
                pass
        return

    def replicate_agents(self,
                         count=5,
                         src_v=AGENT_VERSION,
                         is_available=True,
                         increment=1):
        from_path = self.agent_dir(src_v)
        dst_v = FlexibleVersion(str(src_v))
        for i in range(0, count):
            dst_v += increment
            to_path = self.agent_dir(dst_v)
            shutil.copyfile(from_path + "".zip"", to_path + "".zip"")
            shutil.copytree(from_path, to_path)
            self.rename_agent_bin(to_path, src_v, dst_v)
            if not is_available:
                GuestAgent(to_path).mark_failure(is_fatal=True)
        return dst_v


class TestGuestAgentError(UpdateTestCase):
    def test_creation(self):
        self.assertRaises(TypeError, GuestAgentError)
        self.assertRaises(UpdateError, GuestAgentError, None)

        with self.get_error_file(error_data=WITH_ERROR) as path:
            err = GuestAgentError(path.name)
            err.load()
            self.assertEqual(path.name, err.path)
        self.assertNotEqual(None, err)

        self.assertEqual(WITH_ERROR[""last_failure""], err.last_failure)
        self.assertEqual(WITH_ERROR[""failure_count""], err.failure_count)
        self.assertEqual(WITH_ERROR[""was_fatal""], err.was_fatal)
        return

    def test_clear(self):
        with self.get_error_file(error_data=WITH_ERROR) as path:
            err = GuestAgentError(path.name)
            err.load()
            self.assertEqual(path.name, err.path)
        self.assertNotEqual(None, err)

        err.clear()
        self.assertEqual(NO_ERROR[""last_failure""], err.last_failure)
        self.assertEqual(NO_ERROR[""failure_count""], err.failure_count)
        self.assertEqual(NO_ERROR[""was_fatal""], err.was_fatal)
        return

    def test_save(self):
        err1 = self.create_error()
        err1.mark_failure()
        err1.mark_failure(is_fatal=True)

        err2 = self.create_error(err1.to_json())
        self.assertEqual(err1.last_failure, err2.last_failure)
        self.assertEqual(err1.failure_count, err2.failure_count)
        self.assertEqual(err1.was_fatal, err2.was_fatal)

    def test_mark_failure(self):
        err = self.create_error()
        self.assertFalse(err.is_blacklisted)

        for i in range(0, MAX_FAILURE):
            err.mark_failure()

        # Agent failed >= MAX_FAILURE, it should be blacklisted
        self.assertTrue(err.is_blacklisted)
        self.assertEqual(MAX_FAILURE, err.failure_count)
        return

    def test_mark_failure_permanent(self):
        err = self.create_error()

        self.assertFalse(err.is_blacklisted)

        # Fatal errors immediately blacklist
        err.mark_failure(is_fatal=True)
        self.assertTrue(err.is_blacklisted)
        self.assertTrue(err.failure_count < MAX_FAILURE)
        return

    def test_str(self):
        err = self.create_error(error_data=NO_ERROR)
        s = ""Last Failure: {0}, Total Failures: {1}, Fatal: {2}"".format(
            NO_ERROR[""last_failure""],
            NO_ERROR[""failure_count""],
            NO_ERROR[""was_fatal""])
        self.assertEqual(s, str(err))

        err = self.create_error(error_data=WITH_ERROR)
        s = ""Last Failure: {0}, Total Failures: {1}, Fatal: {2}"".format(
            WITH_ERROR[""last_failure""],
            WITH_ERROR[""failure_count""],
            WITH_ERROR[""was_fatal""])
        self.assertEqual(s, str(err))
        return


class TestGuestAgent(UpdateTestCase):
    def setUp(self):
        UpdateTestCase.setUp(self)
        self.copy_agents(get_agent_file_path())
        self.agent_path = os.path.join(self.tmp_dir, get_agent_name())

    def test_creation(self):
        self.assertRaises(UpdateError, GuestAgent, ""A very bad file name"")
        n = ""{0}-a.bad.version"".format(AGENT_NAME)
        self.assertRaises(UpdateError, GuestAgent, n)

        self.expand_agents()

        agent = GuestAgent(path=self.agent_path)
        self.assertNotEqual(None, agent)
        self.assertEqual(get_agent_name(), agent.name)
        self.assertEqual(get_agent_version(), agent.version)

        self.assertEqual(self.agent_path, agent.get_agent_dir())

        path = os.path.join(self.agent_path, AGENT_MANIFEST_FILE)
        self.assertEqual(path, agent.get_agent_manifest_path())

        self.assertEqual(
            os.path.join(self.agent_path, AGENT_ERROR_FILE),
            agent.get_agent_error_file())

        path = ""."".join((os.path.join(conf.get_lib_dir(), get_agent_name()), ""zip""))
        self.assertEqual(path, agent.get_agent_pkg_path())

        self.assertTrue(agent.is_downloaded)
        self.assertFalse(agent.is_blacklisted)
        self.assertTrue(agent.is_available)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    def test_clear_error(self, mock_downloaded):
        self.expand_agents()

        agent = GuestAgent(path=self.agent_path)
        agent.mark_failure(is_fatal=True)

        self.assertTrue(agent.error.last_failure > 0.0)
        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

        agent.clear_error()
        self.assertEqual(0.0, agent.error.last_failure)
        self.assertEqual(0, agent.error.failure_count)
        self.assertFalse(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_is_available(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)

        self.assertFalse(agent.is_available)
        agent._unpack()
        self.assertTrue(agent.is_available)

        agent.mark_failure(is_fatal=True)
        self.assertFalse(agent.is_available)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_is_blacklisted(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(agent.is_blacklisted)

        agent._unpack()
        self.assertFalse(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

        agent.mark_failure(is_fatal=True)
        self.assertTrue(agent.is_blacklisted)
        self.assertEqual(agent.is_blacklisted, agent.error.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_resource_gone_error_not_blacklisted(self, mock_loaded, mock_downloaded):
        try:
            mock_downloaded.side_effect = ResourceGoneError()
            agent = GuestAgent(path=self.agent_path)
            self.assertFalse(agent.is_blacklisted)
        except ResourceGoneError:
            pass
        except:
            self.fail(""Exception was not expected!"")

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_ioerror_not_blacklisted(self, mock_loaded, mock_downloaded):
        try:
            mock_downloaded.side_effect = IOError()
            agent = GuestAgent(path=self.agent_path)
            self.assertFalse(agent.is_blacklisted)
        except IOError:
            pass
        except:
            self.fail(""Exception was not expected!"")

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_is_downloaded(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(agent.is_downloaded)
        agent._unpack()
        self.assertTrue(agent.is_downloaded)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_mark_failure(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)

        agent.mark_failure()
        self.assertEqual(1, agent.error.failure_count)

        agent.mark_failure(is_fatal=True)
        self.assertEqual(2, agent.error.failure_count)
        self.assertTrue(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_unpack(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        self.assertTrue(os.path.isdir(agent.get_agent_dir()))
        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_unpack_fail(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        os.remove(agent.get_agent_pkg_path())
        self.assertRaises(UpdateError, agent._unpack)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        agent._unpack()
        agent._load_manifest()
        self.assertEqual(agent.manifest.get_enable_command(),
                         agent.get_agent_cmd())

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest_missing(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        os.remove(agent.get_agent_manifest_path())
        self.assertRaises(UpdateError, agent._load_manifest)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest_is_empty(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))

        with open(agent.get_agent_manifest_path(), ""w"") as file:
            json.dump(EMPTY_MANIFEST, file)
        self.assertRaises(UpdateError, agent._load_manifest)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    def test_load_manifest_is_malformed(self, mock_loaded, mock_downloaded):
        agent = GuestAgent(path=self.agent_path)
        self.assertFalse(os.path.isdir(agent.get_agent_dir()))
        agent._unpack()
        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))

        with open(agent.get_agent_manifest_path(), ""w"") as file:
            file.write(""This is not JSON data"")
        self.assertRaises(UpdateError, agent._load_manifest)

    def test_load_error(self):
        agent = GuestAgent(path=self.agent_path)
        agent.error = None

        agent._load_error()
        self.assertTrue(agent.error is not None)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    def test_download(self, mock_http_get, mock_loaded, mock_downloaded):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        agent_pkg = load_bin_data(os.path.join(""ga"", get_agent_file_name()))
        mock_http_get.return_value= ResponseMock(response=agent_pkg)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)
        agent._download()

        self.assertTrue(os.path.isfile(agent.get_agent_pkg_path()))

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    def test_download_fail(self, mock_http_get, mock_loaded, mock_downloaded):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        mock_http_get.return_value= ResponseMock(status=restutil.httpclient.SERVICE_UNAVAILABLE)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertRaises(UpdateError, agent._download)
        self.assertFalse(os.path.isfile(agent.get_agent_pkg_path()))
        self.assertFalse(agent.is_downloaded)

    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_downloaded"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._ensure_loaded"")
    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    @patch(""azurelinuxagent.ga.update.restutil.http_post"")
    def test_download_fallback(self, mock_http_post, mock_http_get, mock_loaded, mock_downloaded):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        mock_http_get.return_value = ResponseMock(
            status=restutil.httpclient.SERVICE_UNAVAILABLE,
            response="""")

        ext_uri = 'ext_uri'
        host_uri = 'host_uri'
        api_uri = URI_FORMAT_GET_API_VERSIONS.format(host_uri, HOST_PLUGIN_PORT)
        art_uri = URI_FORMAT_GET_EXTENSION_ARTIFACT.format(host_uri, HOST_PLUGIN_PORT)
        mock_host = HostPluginProtocol(host_uri,
                                       'container_id',
                                       'role_config')

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri(uri=ext_uri))
        agent = GuestAgent(pkg=pkg)
        agent.host = mock_host

        # ensure fallback fails gracefully, no http
        self.assertRaises(UpdateError, agent._download)
        self.assertEqual(mock_http_get.call_count, 2)
        self.assertEqual(mock_http_get.call_args_list[0][0][0], ext_uri)
        self.assertEqual(mock_http_get.call_args_list[1][0][0], api_uri)

        # ensure fallback fails gracefully, artifact api failure
        with patch.object(HostPluginProtocol,
                          ""ensure_initialized"",
                          return_value=True):
            self.assertRaises(UpdateError, agent._download)
            self.assertEqual(mock_http_get.call_count, 4)

            self.assertEqual(mock_http_get.call_args_list[2][0][0], ext_uri)

            self.assertEqual(mock_http_get.call_args_list[3][0][0], art_uri)
            a, k = mock_http_get.call_args_list[3]
            self.assertEqual(False, k['use_proxy'])

            # ensure fallback works as expected
            with patch.object(HostPluginProtocol,
                              ""get_artifact_request"",
                              return_value=[art_uri, {}]):
                self.assertRaises(UpdateError, agent._download)
                self.assertEqual(mock_http_get.call_count, 6)

                a, k = mock_http_get.call_args_list[3]
                self.assertEqual(False, k['use_proxy'])

                self.assertEqual(mock_http_get.call_args_list[4][0][0], ext_uri)
                a, k = mock_http_get.call_args_list[4]

                self.assertEqual(mock_http_get.call_args_list[5][0][0], art_uri)
                a, k = mock_http_get.call_args_list[5]
                self.assertEqual(False, k['use_proxy'])

    @patch(""azurelinuxagent.ga.update.restutil.http_get"")
    def test_ensure_downloaded(self, mock_http_get):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        agent_pkg = load_bin_data(os.path.join(""ga"", get_agent_file_name()))
        mock_http_get.return_value= ResponseMock(response=agent_pkg)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertTrue(os.path.isfile(agent.get_agent_manifest_path()))
        self.assertTrue(agent.is_downloaded)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"", side_effect=UpdateError)
    def test_ensure_downloaded_download_fails(self, mock_download):
        self.remove_agents()
        self.assertFalse(os.path.isdir(self.agent_path))

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertFalse(agent.error.was_fatal)
        self.assertFalse(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._unpack"", side_effect=UpdateError)
    def test_ensure_downloaded_unpack_fails(self, mock_unpack, mock_download):
        self.assertFalse(os.path.isdir(self.agent_path))

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.error.was_fatal)
        self.assertTrue(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._unpack"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._load_manifest"", side_effect=UpdateError)
    def test_ensure_downloaded_load_manifest_fails(self, mock_manifest, mock_unpack, mock_download):
        self.assertFalse(os.path.isdir(self.agent_path))

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.error.was_fatal)
        self.assertTrue(agent.is_blacklisted)

    @patch(""azurelinuxagent.ga.update.GuestAgent._download"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._unpack"")
    @patch(""azurelinuxagent.ga.update.GuestAgent._load_manifest"")
    def test_ensure_download_skips_blacklisted(self, mock_manifest, mock_unpack, mock_download):
        agent = GuestAgent(path=self.agent_path)
        self.assertEqual(0, mock_download.call_count)

        agent.clear_error()
        agent.mark_failure(is_fatal=True)
        self.assertTrue(agent.is_blacklisted)

        pkg = ExtHandlerPackage(version=str(get_agent_version()))
        pkg.uris.append(ExtHandlerPackageUri())
        agent = GuestAgent(pkg=pkg)

        self.assertEqual(1, agent.error.failure_count)
        self.assertTrue(agent.error.was_fatal)
        self.assertTrue(agent.is_blacklisted)
        self.assertEqual(0, mock_download.call_count)
        self.assertEqual(0, mock_unpack.call_count)


class TestUpdate(UpdateTestCase):
    def setUp(self):
        UpdateTestCase.setUp(self)
        self.event_patch = patch('azurelinuxagent.common.event.add_event')
        self.update_handler = get_update_handler()
        self.update_handler.protocol_util = Mock()

    def test_creation(self):
        self.assertTrue(self.update_handler.running)

        self.assertEqual(None, self.update_handler.last_attempt_time)

        self.assertEqual(0, len(self.update_handler.agents))

        self.assertEqual(None, self.update_handler.child_agent)
        self.assertEqual(None, self.update_handler.child_launch_time)
        self.assertEqual(0, self.update_handler.child_launch_attempts)
        self.assertEqual(None, self.update_handler.child_process)

        self.assertEqual(None, self.update_handler.signal_handler)

    def test_emit_restart_event_emits_event_if_not_clean_start(self):
        try:
            mock_event = self.event_patch.start()
            self.update_handler._set_sentinel()
            self.update_handler._emit_restart_event()
            self.assertEqual(1, mock_event.call_count)
        except Exception as e:
            pass
        self.event_patch.stop()

    def _create_protocol(self, count=20, versions=None):
        latest_version = self.prepare_agents(count=count)
        if versions is None or len(versions) <= 0:
            versions = [latest_version]
        return ProtocolMock(versions=versions)

    def _test_ensure_no_orphans(self, invocations=3, interval=ORPHAN_WAIT_INTERVAL, pid_count=0):
        with patch.object(self.update_handler, 'osutil') as mock_util:
            # Note:
            # - Python only allows mutations of objects to which a function has
            #   a reference. Incrementing an integer directly changes the
            #   reference. Incrementing an item of a list changes an item to
            #   which the code has a reference.
            #   See http://stackoverflow.com/questions/26408941/python-nested-functions-and-variable-scope
            iterations = [0]
            def iterator(*args, **kwargs):
                iterations[0] += 1
                return iterations[0] < invocations

            mock_util.check_pid_alive = Mock(side_effect=iterator)

            pid_files = self.update_handler._get_pid_files()
            self.assertEqual(pid_count, len(pid_files))

            with patch('os.getpid', return_value=42):
                with patch('time.sleep', return_value=None) as mock_sleep:
                    self.update_handler._ensure_no_orphans(orphan_wait_interval=interval)
                    for pid_file in pid_files:
                        self.assertFalse(os.path.exists(pid_file))
                    return mock_util.check_pid_alive.call_count, mock_sleep.call_count

    def test_ensure_no_orphans(self):
        fileutil.write_file(os.path.join(self.tmp_dir, ""0_waagent.pid""), ustr(41))
        calls, sleeps = self._test_ensure_no_orphans(invocations=3, pid_count=1)
        self.assertEqual(3, calls)
        self.assertEqual(2, sleeps)

    def test_ensure_no_orphans_skips_if_no_orphans(self):
        calls, sleeps = self._test_ensure_no_orphans(invocations=3)
        self.assertEqual(0, calls)
        self.assertEqual(0, sleeps)

    def test_ensure_no_orphans_ignores_exceptions(self):
        with patch('azurelinuxagent.common.utils.fileutil.read_file', side_effect=Exception):
            calls, sleeps = self._test_ensure_no_orphans(invocations=3)
            self.assertEqual(0, calls)
            self.assertEqual(0, sleeps)

    def test_ensure_no_orphans_kills_after_interval(self):
        fileutil.write_file(os.path.join(self.tmp_dir, ""0_waagent.pid""), ustr(41))
        with patch('os.kill') as mock_kill:
            calls, sleeps = self._test_ensure_no_orphans(
                                        invocations=4,
                                        interval=3*GOAL_STATE_INTERVAL,
                                        pid_count=1)
            self.assertEqual(3, calls)
            self.assertEqual(2, sleeps)
            self.assertEqual(1, mock_kill.call_count)

    @patch('azurelinuxagent.ga.update.datetime')
    def test_ensure_partition_assigned(self, mock_time):
        path = os.path.join(conf.get_lib_dir(), AGENT_PARTITION_FILE)
        mock_time.utcnow = Mock()

        self.assertFalse(os.path.exists(path))

        for n in range(0,99):
            mock_time.utcnow.return_value = Mock(microsecond=n* 10000)

            self.update_handler._ensure_partition_assigned()

            self.assertTrue(os.path.exists(path))
            s = fileutil.read_file(path)
            self.assertEqual(n, int(s))
            os.remove(path)

    def test_ensure_readonly_sets_readonly(self):
        test_files = [
            os.path.join(conf.get_lib_dir(), ""faux_certificate.crt""),
            os.path.join(conf.get_lib_dir(), ""faux_certificate.p7m""),
            os.path.join(conf.get_lib_dir(), ""faux_certificate.pem""),
            os.path.join(conf.get_lib_dir(), ""faux_certificate.prv""),
            os.path.join(conf.get_lib_dir(), ""ovf-env.xml"")
        ]
        for path in test_files:
            fileutil.write_file(path, ""Faux content"")
            os.chmod(path,
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)

        self.update_handler._ensure_readonly_files()

        for path in test_files:
            mode = os.stat(path).st_mode
            mode &= (stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            self.assertEqual(0, mode ^ stat.S_IRUSR)

    def test_ensure_readonly_leaves_unmodified(self):
        test_files = [
            os.path.join(conf.get_lib_dir(), ""faux.xml""),
            os.path.join(conf.get_lib_dir(), ""faux.json""),
            os.path.join(conf.get_lib_dir(), ""faux.txt""),
            os.path.join(conf.get_lib_dir(), ""faux"")
        ]
        for path in test_files:
            fileutil.write_file(path, ""Faux content"")
            os.chmod(path,
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)

        self.update_handler._ensure_readonly_files()

        for path in test_files:
            mode = os.stat(path).st_mode
            mode &= (stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
            self.assertEqual(
                stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH,
                mode)

    def _test_evaluate_agent_health(self, child_agent_index=0):
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertFalse(latest_agent.is_blacklisted)
        self.assertTrue(len(self.update_handler.agents) > 1)

        child_agent = self.update_handler.agents[child_agent_index]
        self.assertTrue(child_agent.is_available)
        self.assertFalse(child_agent.is_blacklisted)
        self.update_handler.child_agent = child_agent

        self.update_handler._evaluate_agent_health(latest_agent)

    def test_evaluate_agent_health_ignores_installed_agent(self):
        self.update_handler._evaluate_agent_health(None)

    def test_evaluate_agent_health_raises_exception_for_restarting_agent(self):
        self.update_handler.child_launch_time = time.time() - (4 * 60)
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX - 1
        self.assertRaises(Exception, self._test_evaluate_agent_health)

    def test_evaluate_agent_health_will_not_raise_exception_for_long_restarts(self):
        self.update_handler.child_launch_time = time.time() - 24 * 60
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX
        self._test_evaluate_agent_health()

    def test_evaluate_agent_health_will_not_raise_exception_too_few_restarts(self):
        self.update_handler.child_launch_time = time.time()
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX - 2
        self._test_evaluate_agent_health()

    def test_evaluate_agent_health_resets_with_new_agent(self):
        self.update_handler.child_launch_time = time.time() - (4 * 60)
        self.update_handler.child_launch_attempts = CHILD_LAUNCH_RESTART_MAX - 1
        self._test_evaluate_agent_health(child_agent_index=1)
        self.assertEqual(1, self.update_handler.child_launch_attempts)

    def test_filter_blacklisted_agents(self):
        self.prepare_agents()

        self.update_handler._set_agents([GuestAgent(path=path) for path in self.agent_dirs()])
        self.assertEqual(len(self.agent_dirs()), len(self.update_handler.agents))

        kept_agents = self.update_handler.agents[::2]
        blacklisted_agents = self.update_handler.agents[1::2]
        for agent in blacklisted_agents:
            agent.mark_failure(is_fatal=True)
        self.update_handler._filter_blacklisted_agents()
        self.assertEqual(kept_agents, self.update_handler.agents)

    def test_find_agents(self):
        self.prepare_agents()

        self.assertTrue(0 <= len(self.update_handler.agents))
        self.update_handler._find_agents()
        self.assertEqual(len(get_agents(self.tmp_dir)), len(self.update_handler.agents))

    def test_find_agents_does_reload(self):
        self.prepare_agents()

        self.update_handler._find_agents()
        agents = self.update_handler.agents

        self.update_handler._find_agents()
        self.assertNotEqual(agents, self.update_handler.agents)

    def test_find_agents_sorts(self):
        self.prepare_agents()
        self.update_handler._find_agents()

        v = FlexibleVersion(""100000"")
        for a in self.update_handler.agents:
            self.assertTrue(v > a.version)
            v = a.version

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_host_plugin')
    def test_get_host_plugin_returns_host_for_wireserver(self, mock_get_host):
        protocol = WireProtocol('12.34.56.78')
        mock_get_host.return_value = ""faux host""
        host = self.update_handler._get_host_plugin(protocol=protocol)
        print(""mock_get_host call cound={0}"".format(mock_get_host.call_count))
        self.assertEqual(1, mock_get_host.call_count)
        self.assertEqual(""faux host"", host)

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_host_plugin')
    def test_get_host_plugin_returns_none_otherwise(self, mock_get_host):
        protocol = MetadataProtocol()
        host = self.update_handler._get_host_plugin(protocol=protocol)
        mock_get_host.assert_not_called()
        self.assertEqual(None, host)

    def test_get_latest_agent(self):
        latest_version = self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertEqual(len(get_agents(self.tmp_dir)), len(self.update_handler.agents))
        self.assertEqual(latest_version, latest_agent.version)

    def test_get_latest_agent_excluded(self):
        self.prepare_agent(AGENT_VERSION)
        self.assertFalse(self._test_upgrade_available(
                                versions=self.agent_versions(),
                                count=1))
        self.assertEqual(None, self.update_handler.get_latest_agent())

    def test_get_latest_agent_no_updates(self):
        self.assertEqual(None, self.update_handler.get_latest_agent())

    def test_get_latest_agent_skip_updates(self):
        conf.get_autoupdate_enabled = Mock(return_value=False)
        self.assertEqual(None, self.update_handler.get_latest_agent())

    def test_get_latest_agent_skips_unavailable(self):
        self.prepare_agents()
        prior_agent = self.update_handler.get_latest_agent()

        latest_version = self.prepare_agents(count=self.agent_count()+1, is_available=False)
        latest_path = os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, latest_version))
        self.assertFalse(GuestAgent(latest_path).is_available)

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.version < latest_version)
        self.assertEqual(latest_agent.version, prior_agent.version)

    def test_get_pid_files(self):
        pid_files = self.update_handler._get_pid_files()
        self.assertEqual(0, len(pid_files))

    def test_get_pid_files_returns_previous(self):
        for n in range(1250):
            fileutil.write_file(os.path.join(self.tmp_dir, str(n)+""_waagent.pid""), ustr(n+1))
        pid_files = self.update_handler._get_pid_files()
        self.assertEqual(1250, len(pid_files))

        pid_dir, pid_name, pid_re = self.update_handler._get_pid_parts()
        for p in pid_files:
            self.assertTrue(pid_re.match(os.path.basename(p)))

    def test_is_clean_start_returns_true_when_no_sentinel(self):
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))
        self.assertTrue(self.update_handler._is_clean_start)

    def test_is_clean_start_returns_false_when_sentinel_exists(self):
        self.update_handler._set_sentinel(agent=CURRENT_AGENT)
        self.assertFalse(self.update_handler._is_clean_start)

    def test_is_clean_start_returns_false_for_exceptions(self):
        self.update_handler._set_sentinel()
        with patch(""azurelinuxagent.common.utils.fileutil.read_file"", side_effect=Exception):
            self.assertFalse(self.update_handler._is_clean_start)

    def test_is_orphaned_returns_false_if_parent_exists(self):
        fileutil.write_file(conf.get_agent_pid_file_path(), ustr(42))
        with patch('os.getppid', return_value=42):
            self.assertFalse(self.update_handler._is_orphaned)

    def test_is_orphaned_returns_true_if_parent_is_init(self):
        with patch('os.getppid', return_value=1):
            self.assertTrue(self.update_handler._is_orphaned)

    def test_is_orphaned_returns_true_if_parent_does_not_exist(self):
        fileutil.write_file(conf.get_agent_pid_file_path(), ustr(24))
        with patch('os.getppid', return_value=42):
            self.assertTrue(self.update_handler._is_orphaned)

    def test_is_version_available(self):
        self.prepare_agents(is_available=True)
        self.update_handler.agents = self.agents()

        for agent in self.agents():
            self.assertTrue(self.update_handler._is_version_eligible(agent.version))

    @patch(""azurelinuxagent.ga.update.is_current_agent_installed"", return_value=False)
    def test_is_version_available_rejects(self, mock_current):
        self.prepare_agents(is_available=True)
        self.update_handler.agents = self.agents()

        self.update_handler.agents[0].mark_failure(is_fatal=True)
        self.assertFalse(self.update_handler._is_version_eligible(self.agents()[0].version))

    @patch(""azurelinuxagent.ga.update.is_current_agent_installed"", return_value=True)
    def test_is_version_available_accepts_current(self, mock_current):
        self.update_handler.agents = []
        self.assertTrue(self.update_handler._is_version_eligible(CURRENT_VERSION))

    @patch(""azurelinuxagent.ga.update.is_current_agent_installed"", return_value=False)
    def test_is_version_available_rejects_by_default(self, mock_current):
        self.prepare_agents()
        self.update_handler.agents = []

        v = self.agents()[0].version
        self.assertFalse(self.update_handler._is_version_eligible(v))

    def test_purge_agents(self):
        self.prepare_agents()
        self.update_handler._find_agents()

        # Ensure at least three agents initially exist
        self.assertTrue(2 < len(self.update_handler.agents))

        # Purge every other agent
        kept_agents = self.update_handler.agents[1::2]
        purged_agents = self.update_handler.agents[::2]

        # Reload and assert only the kept agents remain on disk
        self.update_handler.agents = kept_agents
        self.update_handler._purge_agents()
        self.update_handler._find_agents()
        self.assertEqual(
            [agent.version for agent in kept_agents],
            [agent.version for agent in self.update_handler.agents])

        # Ensure both directories and packages are removed
        for agent in purged_agents:
            agent_path = os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, agent.version))
            self.assertFalse(os.path.exists(agent_path))
            self.assertFalse(os.path.exists(agent_path + "".zip""))

        # Ensure kept agent directories and packages remain
        for agent in kept_agents:
            agent_path = os.path.join(self.tmp_dir, ""{0}-{1}"".format(AGENT_NAME, agent.version))
            self.assertTrue(os.path.exists(agent_path))
            self.assertTrue(os.path.exists(agent_path + "".zip""))

    def _test_run_latest(self, mock_child=None, mock_time=None, child_args=None):
        if mock_child is None:
            mock_child = ChildMock()
        if mock_time is None:
            mock_time = TimeMock()

        with patch('subprocess.Popen', return_value=mock_child) as mock_popen:
            with patch('time.time', side_effect=mock_time.time):
                with patch('time.sleep', side_effect=mock_time.sleep):
                    self.update_handler.run_latest(child_args=child_args)
                    self.assertEqual(1, mock_popen.call_count)

                    return mock_popen.call_args

    def test_run_latest(self):
        self.prepare_agents()

        agent = self.update_handler.get_latest_agent()
        args, kwargs = self._test_run_latest()
        args = args[0]
        cmds = textutil.safe_shlex_split(agent.get_agent_cmd())
        if cmds[0].lower() == ""python"":
            cmds[0] = get_python_cmd()

        self.assertEqual(args, cmds)
        self.assertTrue(len(args) > 1)
        self.assertTrue(args[0].startswith(""python""))
        self.assertEqual(""-run-exthandlers"", args[len(args)-1])
        self.assertEqual(True, 'cwd' in kwargs)
        self.assertEqual(agent.get_agent_dir(), kwargs['cwd'])
        self.assertEqual(False, '\x00' in cmds[0])

    def test_run_latest_passes_child_args(self):
        self.prepare_agents()

        agent = self.update_handler.get_latest_agent()
        args, kwargs = self._test_run_latest(child_args=""AnArgument"")
        args = args[0]

        self.assertTrue(len(args) > 1)
        self.assertTrue(args[0].startswith(""python""))
        self.assertEqual(""AnArgument"", args[len(args)-1])

    def test_run_latest_polls_and_waits_for_success(self):
        mock_child = ChildMock(return_value=None)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/3)
        self._test_run_latest(mock_child=mock_child, mock_time=mock_time)
        self.assertEqual(2, mock_child.poll.call_count)
        self.assertEqual(1, mock_child.wait.call_count)

    def test_run_latest_polling_stops_at_success(self):
        mock_child = ChildMock(return_value=0)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/3)
        self._test_run_latest(mock_child=mock_child, mock_time=mock_time)
        self.assertEqual(1, mock_child.poll.call_count)
        self.assertEqual(0, mock_child.wait.call_count)

    def test_run_latest_polling_stops_at_failure(self):
        mock_child = ChildMock(return_value=42)
        mock_time = TimeMock()
        self._test_run_latest(mock_child=mock_child, mock_time=mock_time)
        self.assertEqual(1, mock_child.poll.call_count)
        self.assertEqual(0, mock_child.wait.call_count)

    def test_run_latest_polls_frequently_if_installed_is_latest(self):
        mock_child = ChildMock(return_value=0)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/2)
        self._test_run_latest(mock_time=mock_time)
        self.assertEqual(1, mock_time.sleep_interval)

    def test_run_latest_polls_moderately_if_installed_not_latest(self):
        self.prepare_agents()

        mock_child = ChildMock(return_value=0)
        mock_time = TimeMock(time_increment=CHILD_HEALTH_INTERVAL/2)
        self._test_run_latest(mock_time=mock_time)
        self.assertNotEqual(1, mock_time.sleep_interval)

    def test_run_latest_defaults_to_current(self):
        self.assertEqual(None, self.update_handler.get_latest_agent())

        args, kwargs = self._test_run_latest()

        self.assertEqual(args[0], [get_python_cmd(), ""-u"", sys.argv[0], ""-run-exthandlers""])
        self.assertEqual(True, 'cwd' in kwargs)
        self.assertEqual(os.getcwd(), kwargs['cwd'])

    def test_run_latest_forwards_output(self):
        try:
            tempdir = tempfile.mkdtemp()
            stdout_path = os.path.join(tempdir, ""stdout"")
            stderr_path = os.path.join(tempdir, ""stderr"")

            with open(stdout_path, ""w"") as stdout:
                with open(stderr_path, ""w"") as stderr:
                    saved_stdout, sys.stdout = sys.stdout, stdout
                    saved_stderr, sys.stderr = sys.stderr, stderr
                    try:
                        self._test_run_latest(mock_child=ChildMock(side_effect=faux_logger))
                    finally:
                        sys.stdout = saved_stdout
                        sys.stderr = saved_stderr

            with open(stdout_path, ""r"") as stdout:
                self.assertEqual(1, len(stdout.readlines()))
            with open(stderr_path, ""r"") as stderr:
                self.assertEqual(1, len(stderr.readlines()))
        finally:
            shutil.rmtree(tempdir, True)

    def test_run_latest_nonzero_code_marks_failures(self):
        # logger.add_logger_appender(logger.AppenderType.STDOUT)
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

        with patch('azurelinuxagent.ga.update.UpdateHandler.get_latest_agent', return_value=latest_agent):
            self._test_run_latest(mock_child=ChildMock(return_value=1))

        self.assertTrue(latest_agent.is_blacklisted)
        self.assertFalse(latest_agent.is_available)
        self.assertNotEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(1, latest_agent.error.failure_count)

    def test_run_latest_exception_blacklists(self):
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

        with patch('azurelinuxagent.ga.update.UpdateHandler.get_latest_agent', return_value=latest_agent):
            self._test_run_latest(mock_child=ChildMock(side_effect=Exception(""Force blacklisting"")))

        self.assertFalse(latest_agent.is_available)
        self.assertTrue(latest_agent.error.is_blacklisted)
        self.assertNotEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(1, latest_agent.error.failure_count)

    def test_run_latest_exception_does_not_blacklist_if_terminating(self):
        self.prepare_agents()

        latest_agent = self.update_handler.get_latest_agent()
        self.assertTrue(latest_agent.is_available)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

        with patch('azurelinuxagent.ga.update.UpdateHandler.get_latest_agent', return_value=latest_agent):
            self.update_handler.running = False
            self._test_run_latest(mock_child=ChildMock(side_effect=Exception(""Attempt blacklisting"")))

        self.assertTrue(latest_agent.is_available)
        self.assertFalse(latest_agent.error.is_blacklisted)
        self.assertEqual(0.0, latest_agent.error.last_failure)
        self.assertEqual(0, latest_agent.error.failure_count)

    @patch('signal.signal')
    def test_run_latest_captures_signals(self, mock_signal):
        self._test_run_latest()
        self.assertEqual(1, mock_signal.call_count)

    @patch('signal.signal')
    def test_run_latest_creates_only_one_signal_handler(self, mock_signal):
        self.update_handler.signal_handler = ""Not None""
        self._test_run_latest()
        self.assertEqual(0, mock_signal.call_count)

    def _test_run(self, invocations=1, calls=[call.run()], enable_updates=False):
        conf.get_autoupdate_enabled = Mock(return_value=enable_updates)

        # Note:
        # - Python only allows mutations of objects to which a function has
        #   a reference. Incrementing an integer directly changes the
        #   reference. Incrementing an item of a list changes an item to
        #   which the code has a reference.
        #   See http://stackoverflow.com/questions/26408941/python-nested-functions-and-variable-scope
        iterations = [0]
        def iterator(*args, **kwargs):
            iterations[0] += 1
            if iterations[0] >= invocations:
                self.update_handler.running = False
            return

        fileutil.write_file(conf.get_agent_pid_file_path(), ustr(42))

        with patch('azurelinuxagent.ga.exthandlers.get_exthandlers_handler') as mock_handler:
            with patch('azurelinuxagent.ga.remoteaccess.get_remote_access_handler') as mock_ra_handler:
                with patch('azurelinuxagent.ga.monitor.get_monitor_handler') as mock_monitor:
                    with patch('azurelinuxagent.ga.env.get_env_handler') as mock_env:
                        with patch('time.sleep', side_effect=iterator) as mock_sleep:
                            with patch('sys.exit') as mock_exit:
                                if isinstance(os.getppid, MagicMock):
                                    self.update_handler.run()
                                else:
                                    with patch('os.getppid', return_value=42):
                                        self.update_handler.run()

                                self.assertEqual(1, mock_handler.call_count)
                                self.assertEqual(mock_handler.return_value.method_calls, calls)
                                self.assertEqual(1, mock_ra_handler.call_count)
                                self.assertEqual(mock_ra_handler.return_value.method_calls, calls)
                                self.assertEqual(invocations, mock_sleep.call_count)
                                self.assertEqual(1, mock_monitor.call_count)
                                self.assertEqual(1, mock_env.call_count)
                                self.assertEqual(1, mock_exit.call_count)

    def test_run(self):
        self._test_run()

    def test_run_keeps_running(self):
        self._test_run(invocations=15, calls=[call.run()]*15)

    def test_run_stops_if_update_available(self):
        self.update_handler._upgrade_available = Mock(return_value=True)
        self._test_run(invocations=0, calls=[], enable_updates=True)

    def test_run_stops_if_orphaned(self):
        with patch('os.getppid', return_value=1):
            self._test_run(invocations=0, calls=[], enable_updates=True)

    def test_run_clears_sentinel_on_successful_exit(self):
        self._test_run()
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_run_leaves_sentinel_on_unsuccessful_exit(self):
        self.update_handler._upgrade_available = Mock(side_effect=Exception)
        self._test_run(invocations=0, calls=[], enable_updates=True)
        self.assertTrue(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_run_emits_restart_event(self):
        self.update_handler._emit_restart_event = Mock()
        self._test_run()
        self.assertEqual(1, self.update_handler._emit_restart_event.call_count)

    def test_set_agents_sets_agents(self):
        self.prepare_agents()

        self.update_handler._set_agents([GuestAgent(path=path) for path in self.agent_dirs()])
        self.assertTrue(len(self.update_handler.agents) > 0)
        self.assertEqual(len(self.agent_dirs()), len(self.update_handler.agents))

    def test_set_agents_sorts_agents(self):
        self.prepare_agents()

        self.update_handler._set_agents([GuestAgent(path=path) for path in self.agent_dirs()])

        v = FlexibleVersion(""100000"")
        for a in self.update_handler.agents:
            self.assertTrue(v > a.version)
            v = a.version

    def test_set_sentinel(self):
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))
        self.update_handler._set_sentinel()
        self.assertTrue(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_set_sentinel_writes_current_agent(self):
        self.update_handler._set_sentinel()
        self.assertTrue(
            fileutil.read_file(self.update_handler._sentinel_file_path()),
            CURRENT_AGENT)

    def test_shutdown(self):
        self.update_handler._set_sentinel()
        self.update_handler._shutdown()
        self.assertFalse(self.update_handler.running)
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_shutdown_ignores_missing_sentinel_file(self):
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))
        self.update_handler._shutdown()
        self.assertFalse(self.update_handler.running)
        self.assertFalse(os.path.isfile(self.update_handler._sentinel_file_path()))

    def test_shutdown_ignores_exceptions(self):
        self.update_handler._set_sentinel()

        try:
            with patch(""os.remove"", side_effect=Exception):
                self.update_handler._shutdown()
        except Exception as e:
            self.assertTrue(False, ""Unexpected exception"")

    def _test_upgrade_available(
            self,
            base_version=FlexibleVersion(AGENT_VERSION),
            protocol=None,
            versions=None,
            count=20):

        if protocol is None:
            protocol = self._create_protocol(count=count, versions=versions)

        self.update_handler.protocol_util = protocol
        conf.get_autoupdate_gafamily = Mock(return_value=protocol.family)

        return self.update_handler._upgrade_available(base_version=base_version)

    def test_upgrade_available_returns_true_on_first_use(self):
        self.assertTrue(self._test_upgrade_available())

    def test_upgrade_available_will_refresh_goal_state(self):
        protocol = self._create_protocol()
        protocol.emulate_stale_goal_state()
        self.assertTrue(self._test_upgrade_available(protocol=protocol))
        self.assertEqual(2, protocol.call_counts[""get_vmagent_manifests""])
        self.assertEqual(1, protocol.call_counts[""get_vmagent_pkgs""])
        self.assertEqual(1, protocol.call_counts[""update_goal_state""])
        self.assertTrue(protocol.goal_state_forced)

    def test_upgrade_available_handles_missing_family(self):
        extensions_config = ExtensionsConfig(load_data(""wire/ext_conf_missing_family.xml""))
        protocol = ProtocolMock()
        protocol.family = ""Prod""
        protocol.agent_manifests = extensions_config.vmagent_manifests
        self.update_handler.protocol_util = protocol
        with patch('azurelinuxagent.common.logger.warn') as mock_logger:
            with patch('tests.ga.test_update.ProtocolMock.get_vmagent_pkgs', side_effect=ProtocolError):
                self.assertFalse(self.update_handler._upgrade_available(base_version=CURRENT_VERSION))
                self.assertEqual(0, mock_logger.call_count)

    def test_upgrade_available_includes_old_agents(self):
        self.prepare_agents()

        old_version = self.agent_versions()[-1]
        old_count = old_version.version[-1]

        self.replicate_agents(src_v=old_version, count=old_count, increment=-1)
        all_count = len(self.agent_versions())

        self.assertTrue(self._test_upgrade_available(versions=self.agent_versions()))
        self.assertEqual(all_count, len(self.update_handler.agents))

    def test_upgrade_available_purges_old_agents(self):
        self.prepare_agents()
        agent_count = self.agent_count()
        self.assertEqual(20, agent_count)

        agent_versions = self.agent_versions()[:3]
        self.assertTrue(self._test_upgrade_available(versions=agent_versions))
        self.assertEqual(len(agent_versions), len(self.update_handler.agents))

        # Purging always keeps the running agent
        if CURRENT_VERSION not in agent_versions:
            agent_versions.append(CURRENT_VERSION)
        self.assertEqual(agent_versions, self.agent_versions())

    def test_update_available_returns_true_if_current_gets_blacklisted(self):
        self.update_handler._is_version_eligible = Mock(return_value=False)
        self.assertTrue(self._test_upgrade_available())

    def test_upgrade_available_skips_if_too_frequent(self):
        conf.get_autoupdate_frequency = Mock(return_value=10000)
        self.update_handler.last_attempt_time = time.time()
        self.assertFalse(self._test_upgrade_available())

    def test_upgrade_available_skips_if_when_no_new_versions(self):
        self.prepare_agents()
        base_version = self.agent_versions()[0] + 1
        self.update_handler._is_version_eligible = lambda x: x == base_version
        self.assertFalse(self._test_upgrade_available(base_version=base_version))

    def test_upgrade_available_skips_when_no_versions(self):
        self.assertFalse(self._test_upgrade_available(protocol=ProtocolMock()))

    def test_upgrade_available_skips_when_updates_are_disabled(self):
        conf.get_autoupdate_enabled = Mock(return_value=False)
        self.assertFalse(self._test_upgrade_available())

    def test_upgrade_available_sorts(self):
        self.prepare_agents()
        self._test_upgrade_available()

        v = FlexibleVersion(""100000"")
        for a in self.update_handler.agents:
            self.assertTrue(v > a.version)
            v = a.version

    def test_write_pid_file(self):
        for n in range(1112):
            fileutil.write_file(os.path.join(self.tmp_dir, str(n)+""_waagent.pid""), ustr(n+1))
        with patch('os.getpid', return_value=1112):
            pid_files, pid_file = self.update_handler._write_pid_file()
            self.assertEqual(1112, len(pid_files))
            self.assertEqual(""1111_waagent.pid"", os.path.basename(pid_files[-1]))
            self.assertEqual(""1112_waagent.pid"", os.path.basename(pid_file))
            self.assertEqual(fileutil.read_file(pid_file), ustr(1112))

    def test_write_pid_file_ignores_exceptions(self):
        with patch('azurelinuxagent.common.utils.fileutil.write_file', side_effect=Exception):
            with patch('os.getpid', return_value=42):
                pid_files, pid_file = self.update_handler._write_pid_file()
                self.assertEqual(0, len(pid_files))
                self.assertEqual(None, pid_file)

    @patch('azurelinuxagent.common.protocol.wire.WireClient.get_goal_state',
           return_value=GoalState(load_data('wire/goal_state.xml')))
    def test_package_filter_for_agent_manifest(self, _):

        protocol = WireProtocol('12.34.56.78')
        extension_config = ExtensionsConfig(load_data('wire/ext_conf.xml'))
        agent_manifest = extension_config.vmagent_manifests.vmAgentManifests[0]

        # has agent versions 13, 14
        ga_manifest_1 = ExtensionManifest(load_data('wire/ga_manifest_1.xml'))

        # has agent versions 13, 14, 15
        ga_manifest_2 = ExtensionManifest(load_data('wire/ga_manifest_2.xml'))

        goal_state = protocol.client.get_goal_state()
        disk_cache = os.path.join(conf.get_lib_dir(),
                                  AGENTS_MANIFEST_FILE_NAME.format(
                                      agent_manifest.family,
                                      goal_state.incarnation))

        self.assertFalse(os.path.exists(disk_cache))
        self.assertTrue(ga_manifest_1.allowed_versions is None)

        with patch(
                'azurelinuxagent.common.protocol.wire.WireClient'
                '.get_gafamily_manifest',
                return_value=ga_manifest_1):

            pkg_list_1 = protocol.get_vmagent_pkgs(agent_manifest)
            self.assertTrue(pkg_list_1 is not None)
            self.assertTrue(len(pkg_list_1.versions) == 2)
            self.assertTrue(pkg_list_1.versions[0].version == '2.2.13')
            self.assertTrue(pkg_list_1.versions[0].uris[0].uri == 'url1_13')
            self.assertTrue(pkg_list_1.versions[1].version == '2.2.14')
            self.assertTrue(pkg_list_1.versions[1].uris[0].uri == 'url1_14')

        self.assertTrue(os.path.exists(disk_cache))

        with patch(
                'azurelinuxagent.common.protocol.wire.WireClient'
                '.get_gafamily_manifest',
                return_value=ga_manifest_2):

            pkg_list_2 = protocol.get_vmagent_pkgs(agent_manifest)
            self.assertTrue(pkg_list_2 is not None)
            self.assertTrue(len(pkg_list_2.versions) == 2)
            self.assertTrue(pkg_list_2.versions[0].version == '2.2.13')
            self.assertTrue(pkg_list_2.versions[0].uris[0].uri == 'url2_13')
            self.assertTrue(pkg_list_2.versions[1].version == '2.2.14')
            self.assertTrue(pkg_list_2.versions[1].uris[0].uri == 'url2_14')
            # does not contain 2.2.15

        self.assertTrue(os.path.exists(disk_cache))
        self.assertTrue(ga_manifest_2.allowed_versions is not None)
        self.assertTrue(len(ga_manifest_2.allowed_versions) == 2)
        self.assertTrue(ga_manifest_2.allowed_versions[0] == '2.2.13')
        self.assertTrue(ga_manifest_2.allowed_versions[1] == '2.2.14')


class MonitorThreadTest(AgentTestCase):
    def setUp(self):
        AgentTestCase.setUp(self)
        self.event_patch = patch('azurelinuxagent.common.event.add_event')
        self.update_handler = get_update_handler()
        self.update_handler.protocol_util = Mock()

    def _test_run(self, invocations=1):
        iterations = [0]
        def iterator(*args, **kwargs):
            iterations[0] += 1
            if iterations[0] >= invocations:
                self.update_handler.running = False
            return

        with patch('os.getpid', return_value=42):
            with patch.object(UpdateHandler, '_is_orphaned') as mock_is_orphaned:
                mock_is_orphaned.__get__ = Mock(return_value=False)
                with patch('azurelinuxagent.ga.exthandlers.get_exthandlers_handler') as mock_handler:
                    with patch('azurelinuxagent.ga.remoteaccess.get_remote_access_handler') as mock_ra_handler:
                        with patch('time.sleep', side_effect=iterator) as mock_sleep:
                            with patch('sys.exit') as mock_exit:
                                self.update_handler.run()

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_start_threads(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=0)
        self.assertEqual(1, mock_monitor.call_count)
        self.assertEqual(1, mock_monitor_thread.run.call_count)
        self.assertEqual(1, mock_env.call_count)
        self.assertEqual(1, mock_env_thread.run.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_check_if_monitor_thread_is_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor_thread.is_alive = MagicMock(return_value=True)
        mock_monitor_thread.start = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        self._test_run(invocations=0)
        self.assertEqual(1, mock_monitor.call_count)
        self.assertEqual(1, mock_monitor_thread.run.call_count)
        self.assertEqual(1, mock_monitor_thread.is_alive.call_count)
        self.assertEqual(0, mock_monitor_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_check_if_env_thread_is_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env_thread.is_alive = MagicMock(return_value=True)
        mock_env_thread.start = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=1)
        self.assertEqual(1, mock_env.call_count)
        self.assertEqual(1, mock_env_thread.run.call_count)
        self.assertEqual(1, mock_env_thread.is_alive.call_count)
        self.assertEqual(0, mock_env_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_monitor_thread_if_not_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor_thread.is_alive = MagicMock(return_value=False)
        mock_monitor_thread.start = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        self._test_run(invocations=1)
        self.assertEqual(1, mock_monitor.call_count)
        self.assertEqual(1, mock_monitor_thread.run.call_count)
        self.assertEqual(1, mock_monitor_thread.is_alive.call_count)
        self.assertEqual(1, mock_monitor_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_env_thread_if_not_alive(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env_thread.is_alive = MagicMock(return_value=False)
        mock_env_thread.start = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=1)
        self.assertEqual(1, mock_env.call_count)
        self.assertEqual(1, mock_env_thread.run.call_count)
        self.assertEqual(1, mock_env_thread.is_alive.call_count)
        self.assertEqual(1, mock_env_thread.start.call_count)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_monitor_thread(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_monitor_thread = MagicMock()
        mock_monitor_thread.run = MagicMock()
        mock_monitor_thread.is_alive = MagicMock(return_value=False)
        mock_monitor_thread.start = MagicMock()
        mock_monitor.return_value = mock_monitor_thread

        self._test_run(invocations=0)
        self.assertEqual(True, mock_monitor.called)
        self.assertEqual(True, mock_monitor_thread.run.called)
        self.assertEqual(True, mock_monitor_thread.is_alive.called)
        self.assertEqual(True, mock_monitor_thread.start.called)

    @patch('azurelinuxagent.ga.monitor.get_monitor_handler')
    @patch('azurelinuxagent.ga.env.get_env_handler')
    def test_restart_env_thread(self, mock_env, mock_monitor):
        self.assertTrue(self.update_handler.running)

        mock_env_thread = MagicMock()
        mock_env_thread.run = MagicMock()
        mock_env_thread.is_alive = MagicMock(return_value=False)
        mock_env_thread.start = MagicMock()
        mock_env.return_value = mock_env_thread

        self._test_run(invocations=0)
        self.assertEqual(True, mock_env.called)
        self.assertEqual(True, mock_env_thread.run.called)
        self.assertEqual(True, mock_env_thread.is_alive.called)
        self.assertEqual(True, mock_env_thread.start.called)


class ChildMock(Mock):
    def __init__(self, return_value=0, side_effect=None):
        Mock.__init__(self, return_value=return_value, side_effect=side_effect)

        self.poll = Mock(return_value=return_value, side_effect=side_effect)
        self.wait = Mock(return_value=return_value, side_effect=side_effect)


class ProtocolMock(object):
    def __init__(self, family=""TestAgent"", etag=42, versions=None, client=None):
        self.family = family
        self.client = client
        self.call_counts = {
            ""get_vmagent_manifests"" : 0,
            ""get_vmagent_pkgs"" : 0,
            ""update_goal_state"" : 0
        }
        self.goal_state_is_stale = False
        self.goal_state_forced = False
        self.etag = etag
        self.versions = versions if versions is not None else []
        self.create_manifests()
        self.create_packages()

    def emulate_stale_goal_state(self):
        self.goal_state_is_stale = True

    def create_manifests(self):
        self.agent_manifests = VMAgentManifestList()
        if len(self.versions) <= 0:
            return

        if self.family is not None:
            manifest = VMAgentManifest(family=self.family)
            for i in range(0,10):
                manifest_uri = ""https://nowhere.msft/agent/{0}"".format(i)
                manifest.versionsManifestUris.append(VMAgentManifestUri(uri=manifest_uri))
            self.agent_manifests.vmAgentManifests.append(manifest)

    def create_packages(self):
        self.agent_packages = ExtHandlerPackageList()
        if len(self.versions) <= 0:
            return

        for version in self.versions:
            package = ExtHandlerPackage(str(version))
            for i in range(0,5):
                package_uri = ""https://nowhere.msft/agent_pkg/{0}"".format(i)
                package.uris.append(ExtHandlerPackageUri(uri=package_uri))
            self.agent_packages.versions.append(package)

    def get_protocol(self):
        return self

    def get_vmagent_manifests(self):
        self.call_counts[""get_vmagent_manifests""] += 1
        if self.goal_state_is_stale:
            self.goal_state_is_stale = False
            raise ResourceGoneError()
        return self.agent_manifests, self.etag

    def get_vmagent_pkgs(self, manifest):
        self.call_counts[""get_vmagent_pkgs""] += 1
        if self.goal_state_is_stale:
            self.goal_state_is_stale = False
            raise ResourceGoneError()
        return self.agent_packages

    def update_goal_state(self, forced=False, max_retry=3):
        self.call_counts[""update_goal_state""] += 1
        self.goal_state_forced = self.goal_state_forced or forced


class ResponseMock(Mock):
    def __init__(self, status=restutil.httpclient.OK, response=None, reason=None):
        Mock.__init__(self)
        self.status = status
        self.reason = reason
        self.response = response

    def read(self):
        return self.response


class TimeMock(Mock):
    def __init__(self, time_increment=1):
        Mock.__init__(self)
        self.next_time = time.time()
        self.time_call_count = 0
        self.time_increment = time_increment

        self.sleep_interval = None

    def sleep(self, n):
        self.sleep_interval = n

    def time(self):
        self.time_call_count += 1
        current_time = self.next_time
        self.next_time += self.time_increment
        return current_time


if __name__ == '__main__':
    unittest.main()
/n/n/ntests/test_import.py/n/nfrom tests.tools import *
import azurelinuxagent.common.osutil as osutil
import azurelinuxagent.common.dhcp as dhcp
import azurelinuxagent.common.protocol as protocol
import azurelinuxagent.pa.provision as provision
import azurelinuxagent.pa.deprovision as deprovision
import azurelinuxagent.daemon as daemon
import azurelinuxagent.daemon.resourcedisk as resourcedisk
import azurelinuxagent.daemon.scvmm as scvmm
import azurelinuxagent.ga.exthandlers as exthandlers
import azurelinuxagent.ga.monitor as monitor
import azurelinuxagent.ga.remoteaccess as remoteaccess
import azurelinuxagent.ga.update as update


class TestImportHandler(AgentTestCase):
    def test_get_handler(self):
        osutil.get_osutil()
        protocol.get_protocol_util()
        dhcp.get_dhcp_handler()
        provision.get_provision_handler()
        deprovision.get_deprovision_handler()
        daemon.get_daemon_handler()
        resourcedisk.get_resourcedisk_handler()
        scvmm.get_scvmm_handler()
        monitor.get_monitor_handler()
        update.get_update_handler()
        exthandlers.get_exthandlers_handler()
        remoteaccess.get_remote_access_handler()
/n/n/ntests/utils/test_crypt_util.py/n/n# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import base64
import binascii
import errno as errno
import glob
import random
import string
import subprocess
import tempfile
import uuid

import azurelinuxagent.common.conf as conf
import azurelinuxagent.common.utils.shellutil as shellutil
from azurelinuxagent.common.future import ustr
from azurelinuxagent.common.utils.cryptutil import CryptUtil
from azurelinuxagent.common.exception import CryptError
from azurelinuxagent.common.version import PY_VERSION_MAJOR
from tests.tools import *
from subprocess import CalledProcessError

class TestCryptoUtilOperations(AgentTestCase):
    def test_decrypt_encrypted_text(self):
        encrypted_string = load_data(""wire/encrypted.enc"")
        prv_key = os.path.join(self.tmp_dir, ""TransportPrivate.pem"") 
        with open(prv_key, 'w+') as c:
            c.write(load_data(""wire/sample.pem""))
        secret = ']aPPEv}uNg1FPnl?'
        crypto = CryptUtil(conf.get_openssl_cmd())
        decrypted_string = crypto.decrypt_secret(encrypted_string, prv_key)
        self.assertEquals(secret, decrypted_string, ""decrypted string does not match expected"")

    def test_decrypt_encrypted_text_missing_private_key(self):
        encrypted_string = load_data(""wire/encrypted.enc"")
        prv_key = os.path.join(self.tmp_dir, ""TransportPrivate.pem"")
        crypto = CryptUtil(conf.get_openssl_cmd())
        self.assertRaises(CalledProcessError, crypto.decrypt_secret, encrypted_string, ""abc"" + prv_key)
    
    def test_decrypt_encrypted_text_wrong_private_key(self):
        encrypted_string = load_data(""wire/encrypted.enc"")
        prv_key = os.path.join(self.tmp_dir, ""wrong.pem"")
        with open(prv_key, 'w+') as c:
            c.write(load_data(""wire/trans_prv""))
        crypto = CryptUtil(conf.get_openssl_cmd())
        self.assertRaises(CalledProcessError, crypto.decrypt_secret, encrypted_string, prv_key)

    def test_decrypt_encrypted_text_text_not_encrypted(self):
        encrypted_string = ""abc@123""
        prv_key = os.path.join(self.tmp_dir, ""TransportPrivate.pem"") 
        with open(prv_key, 'w+') as c:
            c.write(load_data(""wire/sample.pem""))
        crypto = CryptUtil(conf.get_openssl_cmd())
        self.assertRaises(CryptError, crypto.decrypt_secret, encrypted_string, prv_key)

if __name__ == '__main__':
    unittest.main()
/n/n/n",0
41,41,ee20e7e1058d24191320e54f444a5f7c22adb1e8,"/azurelinuxagent/common/osutil/bigip.py/n/n# Copyright 2016 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import fcntl
import os
import platform
import re
import socket
import struct
import time

try:
    # WAAgent > 2.1.3
    import azurelinuxagent.common.logger as logger
    import azurelinuxagent.common.utils.shellutil as shellutil

    from azurelinuxagent.common.exception import OSUtilError
    from azurelinuxagent.common.osutil.default import DefaultOSUtil
except ImportError:
    # WAAgent <= 2.1.3
    import azurelinuxagent.logger as logger
    import azurelinuxagent.utils.shellutil as shellutil

    from azurelinuxagent.exception import OSUtilError
    from azurelinuxagent.distro.default.osutil import DefaultOSUtil


class BigIpOSUtil(DefaultOSUtil):
    def __init__(self):
        super(BigIpOSUtil, self).__init__()

    def _wait_until_mcpd_is_initialized(self):
        """"""Wait for mcpd to become available

        All configuration happens in mcpd so we need to wait that this is
        available before we go provisioning the system. I call this method
        at the first opportunity I have (during the DVD mounting call).
        This ensures that the rest of the provisioning does not need to wait
        for mcpd to be available unless it absolutely wants to.

        :return bool: Returns True upon success
        :raises OSUtilError: Raises exception if mcpd does not come up within
                             roughly 50 minutes (100 * 30 seconds)
        """"""
        for retries in range(1, 100):
            # Retry until mcpd completes startup:
            logger.info(""Checking to see if mcpd is up"")
            rc = shellutil.run(""/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running"", chk_err=False)
            if rc == 0:
                logger.info(""mcpd is up!"")
                break
            time.sleep(30)

        if rc is 0:
            return True

        raise OSUtilError(
            ""mcpd hasn't completed initialization! Cannot proceed!""
        )

    def _save_sys_config(self):
        cmd = ""/usr/bin/tmsh save sys config""
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error(""WARNING: Cannot save sys config on 1st boot."")
        return rc

    def restart_ssh_service(self):
        return shellutil.run(""/usr/bin/bigstart restart sshd"", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run(""/sbin/service waagent stop"", chk_err=False)

    def start_agent_service(self):
        return shellutil.run(""/sbin/service waagent start"", chk_err=False)

    def register_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --add waagent"", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run(""/sbin/chkconfig --del waagent"", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""/sbin/pidof dhclient"")
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """"""Set the static hostname of the device

        Normally, tmsh is used to set the hostname for the system. For our
        purposes at this time though, I would hesitate to trust this function.

        Azure(Stack) uses the name that you provide in the Web UI or ARM (for
        example) as the value of the hostname argument to this method. The
        problem is that there is nowhere in the UI that specifies the
        restrictions and checks that tmsh has for the hostname.

        For example, if you set the name ""bigip1"" in the Web UI, Azure(Stack)
        considers that a perfectly valid name. When WAAgent gets around to
        running though, tmsh will reject that value because it is not a fully
        qualified domain name. The proper value should have been bigip.xxx.yyy

        WAAgent will not fail if this command fails, but the hostname will not
        be what the user set either. Currently we do not set the hostname when
        WAAgent starts up, so I am passing on setting it here too.

        :param hostname: The hostname to set on the device
        """"""
        return None

    def set_dhcp_hostname(self, hostname):
        """"""Sets the DHCP hostname

        See `set_hostname` for an explanation of why I pass here

        :param hostname: The hostname to set on the device
        """"""
        return None

    def useradd(self, username, expiration=None):
        """"""Create user account using tmsh

        Our policy is to create two accounts when booting a BIG-IP instance.
        The first account is the one that the user specified when they did
        the instance creation. The second one is the admin account that is,
        or should be, built in to the system.

        :param username: The username that you want to add to the system
        :param expiration: The expiration date to use. We do not use this
                           value.
        """"""
        if self.get_userentry(username):
            logger.info(""User {0} already exists, skip useradd"", username)
            return None

        cmd = ""/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash"" % (username)
        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)
        if retcode != 0:
            raise OSUtilError(
                ""Failed to create user account:{0}, retcode:{1}, output:{2}"".format(username, retcode, out)
            )
        self._save_sys_config()
        return retcode

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        """"""Change a user's password with tmsh

        Since we are creating the user specified account and additionally
        changing the password of the built-in 'admin' account, both must
        be modified in this method.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        :param username: The username whose password to change
        :param password: The unencrypted password to set for the user
        :param crypt_id: If encrypting the password, the crypt_id that was used
        :param salt_len: If encrypting the password, the length of the salt
                         value used to do it.
        """"""

        # Start by setting the password of the user provided account
        cmd = ""/usr/bin/tmsh modify auth user {0} password '{1}'"".format(username, password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for {0}: {1}"".format(username, output)
            )

        # Next, set the password of the built-in 'admin' account to be have
        # the same password as the user provided account
        userentry = self.get_userentry('admin')
        if userentry is None:
            raise OSUtilError(""The 'admin' user account was not found!"")

        cmd = ""/usr/bin/tmsh modify auth user 'admin' password '{0}'"".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                ""Failed to set password for 'admin': {0}"".format(output)
            )
        self._save_sys_config()
        return ret

    def del_account(self, username):
        """"""Deletes a user account.

        Note that the default method also checks for a ""system level"" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        We also don't use sudo, so we remove that method call as well.

        :param username:
        :return:
        """"""
        shellutil.run(""> /var/run/utmp"")
        shellutil.run(""/usr/bin/tmsh delete auth user "" + username)

    def get_dvd_device(self, dev_dir='/dev'):
        """"""Find BIG-IP's CD/DVD device

        This device is almost certainly /dev/cdrom so I added the ? to this pattern.
        Note that this method will return upon the first device found, but in my
        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the
        correct CD/DVD device though.

        :todo: Consider just always returning ""/dev/cdrom"" here if that device device
               exists on all platforms that are supported on Azure(Stack)
        :param dev_dir: The root directory from which to look for devices
        """"""
        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'
        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return ""/dev/{0}"".format(dvd.group(0))
        raise OSUtilError(""Failed to get dvd device"")

    def mount_dvd(self, **kwargs):
        """"""Mount the DVD containing the provisioningiso.iso file

        This is the _first_ hook that WAAgent provides for us, so this is the
        point where we should wait for mcpd to load. I am just overloading
        this method to add the mcpd wait. Then I proceed with the stock code.

        :param max_retry: Maximum number of retries waagent will make when
                          mounting the provisioningiso.iso DVD
        :param chk_err: Whether to check for errors or not in the mounting
                        commands
        """"""
        self._wait_until_mcpd_is_initialized()
        return super(BigIpOSUtil, self).mount_dvd(**kwargs)

    def eject_dvd(self, chk_err=True):
        """"""Runs the eject command to eject the provisioning DVD

        BIG-IP does not include an eject command. It is sufficient to just
        umount the DVD disk. But I will log that we do not support this for
        future reference.

        :param chk_err: Whether or not to check for errors raised by the eject
                        command
        """"""
        logger.warn(""Eject is not supported on this platform"")

    def get_first_if(self):
        """"""Return the interface name, and ip addr of the management interface.

        We need to add a struct_size check here because, curiously, our 64bit
        platform is identified by python in Azure(Stack) as 32 bit and without
        adjusting the struct_size, we can't get the information we need.

        I believe this may be caused by only python i686 being shipped with
        BIG-IP instead of python x86_64??
        """"""
        iface = ''
        expected = 16  # how many devices should I expect...

        python_arc = platform.architecture()[0]
        if python_arc == '64bit':
            struct_size = 40  # for 64bit the size is 40 bytes
        else:
            struct_size = 32  # for 32bit the size is 32 bytes
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        buff = array.array('B', b'\0' * (expected * struct_size))
        param = struct.pack('iL',
                            expected*struct_size,
                            buff.buffer_info()[0])
        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)
        retsize = (struct.unpack('iL', ret)[0])
        if retsize == (expected * struct_size):
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)
        sock = buff.tostring()
        for i in range(0, struct_size * expected, struct_size):
            iface = self._format_single_interface_name(sock, i)

            # Azure public was returning ""lo:1"" when deploying WAF
            if b'lo' in iface:
                continue
            else:
                break
        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])

    def _format_single_interface_name(self, sock, offset):
        return sock[offset:offset+16].split(b'\0', 1)[0]

    def route_add(self, net, mask, gateway):
        """"""Add specified route using tmsh.

        :param net:
        :param mask:
        :param gateway:
        :return:
        """"""
        cmd = (""/usr/bin/tmsh create net route ""
               ""{0}/{1} gw {2}"").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def device_for_ide_port(self, port_id):
        """"""Return device name attached to ide port 'n'.

        Include a wait in here because BIG-IP may not have yet initialized
        this list of devices.

        :param port_id:
        :return:
        """"""
        for retries in range(1, 100):
            # Retry until devices are ready
            if os.path.exists(""/sys/bus/vmbus/devices/""):
                break
            else:
                time.sleep(10)
        return super(BigIpOSUtil, self).device_for_ide_port(port_id)
/n/n/n/azurelinuxagent/common/osutil/freebsd.py/n/n# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.future import ustr

class FreeBSDOSUtil(DefaultOSUtil):
    def __init__(self):
        super(FreeBSDOSUtil, self).__init__()
        self._scsi_disks_timeout_set = False

    def set_hostname(self, hostname):
        rc_file_path = '/etc/rc.conf'
        conf_file = fileutil.read_file(rc_file_path).split(""\n"")
        textutil.set_ini_config(conf_file, ""hostname"", hostname)
        fileutil.write_file(rc_file_path, ""\n"".join(conf_file))
        shellutil.run(""hostname {0}"".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('service sshd restart', chk_err=False)

    def useradd(self, username, expiration=None):
        """"""
        Create user account with 'username'
        """"""
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.warn(""User {0} already exists, skip useradd"", username)
            return

        if expiration is not None:
            cmd = ""pw useradd {0} -e {1} -m"".format(username, expiration)
        else:
            cmd = ""pw useradd {0} -m"".format(username)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError((""Failed to create user account:{0}, ""
                               ""retcode:{1}, ""
                               ""output:{2}"").format(username, retcode, out))

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error(""{0} is a system user. Will not delete it."", username)
        shellutil.run('> /var/run/utx.active')
        shellutil.run('rmuser -y ' + username)
        self.conf_sudoer(username, remove=True)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError((""User {0} is a system user, ""
                               ""will not set password."").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = ""echo '{0}'|pw usermod {1} -H 0 "".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError((""Failed to set password for {0}: {1}""
                               """").format(username, output))

    def del_root_password(self):
        err = shellutil.run('pw usermod root -h -')
        if err:
            raise OSUtilError(""Failed to delete root password: Failed to update password database."")

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        """"""
        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to
        specify the route manually to get it work in a VNET environment.
        SEE ALSO: man ip(4) IP_ONESBCAST,
        """"""
        return True

    def is_dhcp_enabled(self):
        return True

    def start_dhcp_service(self):
        shellutil.run(""/etc/rc.d/dhclient start {0}"".format(self.get_if_name()), chk_err=False)

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run(""route add 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run(""route delete 255.255.255.255 -iface {0}"".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output(""pgrep -n dhclient"", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run(""cdcontrol -f {0} eject"".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError(""Failed to eject dvd: ret={0}"".format(retcode))

    def restart_if(self, ifname):
        # Restart dhclient only to publish hostname
        shellutil.run(""/etc/rc.d/dhclient restart {0}"".format(ifname), chk_err=False)

    def get_total_mem(self):
        cmd = ""sysctl hw.physmem |awk '{print $2}'""
        ret, output = shellutil.run_get_output(cmd)
        if ret:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output(""sysctl hw.ncpu |awk '{print $2}'"")
        if ret:
            raise OSUtilError(""Failed to get processor cores."")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError(""Failed to get total memory: {0}"".format(output))

    def set_scsi_disks_timeout(self, timeout):
        if self._scsi_disks_timeout_set:
            return

        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))
        if ret:
            raise OSUtilError(""Failed set SCSI disks timeout: {0}"".format(output))
        self._scsi_disks_timeout_set = True

    def check_pid_alive(self, pid):
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """"""
        There is no SIOCGIFCONF
        on freeBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """"""
        iface = ''
        inet = ''
        mac = ''

        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)
        if err:
            raise OSUtilError(""Can't find ether interface:{0}"".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError(""Can't find ether interface."")
        iface = ifaces[0]

        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)
        if err:
            raise OSUtilError(""Can't get info for interface:{0}"".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('ether ') != -1:
                mac = line.split()[1]
        logger.verbose(""Interface info: ({0},{1},{2})"", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """"""
        Return device name attached to ide port 'n'.
        """"""
        if port_id > 3:
            return None
        g0 = ""00000000""
        if port_id > 1:
            g0 = ""00000001""
            port_id = port_id - 2
        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')
        if err:
            return None
        g1 = ""000"" + ustr(port_id)
        g0g1 = ""{0}-{1}"".format(g0, g1)
        """"""
        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'
        """"""
        cmd_search_ide = ""sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}"".format(g0g1)
        err, output = shellutil.run_get_output(cmd_search_ide)
        if err:
            return None
        cmd_extract_id = cmd_search_ide + ""|awk -F . '{print $3}'""
        err, output = shellutil.run_get_output(cmd_extract_id)
        """"""
        try to search 'blkvscX' and 'storvscX' to find device name
        """"""
        output = output.rstrip()
        cmd_search_blkvsc = ""camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_blkvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible

        cmd_search_storvsc = ""camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'"".format(output)
        err, output = shellutil.run_get_output(cmd_search_storvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev=""camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'"".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible
        return None

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0
/n/n/n",1
44,44,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
45,45,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', '', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', '', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
56,56,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Kster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Kster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Kster""]
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
57,57,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Kster""
__copyright__ = ""Copyright 2015, Johannes Kster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
