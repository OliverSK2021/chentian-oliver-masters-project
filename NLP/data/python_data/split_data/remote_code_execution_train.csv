,Unnamed: 0,id,code,label
6,6,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
import re
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.text import encode_for_xml
from invenio.utils.filedownload import download_url

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""
    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        line_nb = len(text_lines)
        word_nb = 0
        for line in text_lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))
        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            local_file = download_url(entry)
            text_lines = extractor.text_lines_from_local_file(local_file)
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Output keywords reading a local file.

    Arguments and output are the same as for :see: get_keywords_from_text().
    """"""
    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings.

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""
    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext.

    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/ninvenio/legacy/bibclassify/ontology_reader.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify ontology reader.

The ontology reader reads currently either a RDF/SKOS taxonomy or a
simple controlled vocabulary file (1 word per line). The first role of
this module is to manage the cached version of the ontology file. The
second role is to hold all methods responsible for the creation of
regular expressions. These methods are grammatically related as we take
care of different forms of the same words.  The grammatical rules can be
configured via the configuration file.

The main method from this module is get_regular_expressions.
""""""

from __future__ import print_function

from datetime import datetime, timedelta
from six import iteritems
from six.moves import cPickle

import os
import re
import sys
import tempfile
import time
import urllib2
import traceback
import xml.sax
import thread
import rdflib

from invenio.legacy.bibclassify import config as bconfig
from invenio.modules.classifier.errors import TaxonomyError

log = bconfig.get_logger(""bibclassify.ontology_reader"")
from invenio import config

from invenio.modules.classifier.registry import taxonomies

# only if not running in a stanalone mode
if bconfig.STANDALONE:
    dbquery = None
    from urllib2 import urlopen
else:
    from invenio.legacy import dbquery
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

_contains_digit = re.compile(""\d"")
_starts_with_non = re.compile(""(?i)^non[a-z]"")
_starts_with_anti = re.compile(""(?i)^anti[a-z]"")
_split_by_punctuation = re.compile(""(\W+)"")

_CACHE = {}


def get_cache(taxonomy_id):
    """"""Return thread-safe cache for the given taxonomy id.

    :param taxonomy_id: identifier of the taxonomy
    :type taxonomy_id: str

    :return: dictionary object (empty if no taxonomy_id
        is found), you must not change anything inside it.
        Create a new dictionary and use set_cache if you want
        to update the cache!
    """"""
    # Because of a standalone mode, we don't use the
    # invenio.data_cacher.DataCacher, but it has no effect
    # on proper functionality.

    if taxonomy_id in _CACHE:
        ctime, taxonomy = _CACHE[taxonomy_id]

        # check it is fresh version
        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)
        cache_path = _get_cache_path(onto_name)

        # if source exists and is newer than the cache hold in memory
        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:
            log.info('Forcing taxonomy rebuild as cached'
                     ' version is newer/updated.')
            return {}  # force cache rebuild

        # if cache exists and is newer than the cache hold in memory
        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:
            log.info('Forcing taxonomy rebuild as source'
                     ' file is newer/updated.')
            return {}
        log.info('Taxonomy retrieved from cache')
        return taxonomy
    return {}


def set_cache(taxonomy_id, contents):
    """"""Update cache in a thread-safe manner.""""""
    lock = thread.allocate_lock()
    lock.acquire()
    try:
        _CACHE[taxonomy_id] = (time.time(), contents)
    finally:
        lock.release()


def get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):
    """"""Return a list of patterns compiled from the RDF/SKOS ontology.

    Uses cache if it exists and if the taxonomy hasn't changed.
    """"""
    # Translate the ontology name into a local path. Check if the name
    # relates to an existing ontology.
    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)
    if not onto_path:
        raise TaxonomyError(""Unable to locate the taxonomy: '%s'.""
                            % taxonomy_name)

    cache_path = _get_cache_path(onto_name)
    log.debug('Taxonomy discovered, now we load it '
              '(from cache: %s, onto_path: %s, cache_path: %s)'
              % (not no_cache, onto_path, cache_path))

    if os.access(cache_path, os.R_OK):
        if os.access(onto_path, os.R_OK):
            if rebuild or no_cache:
                log.debug(""Cache generation was manually forced."")
                return _build_cache(onto_path, skip_cache=no_cache)
        else:
            # ontology file not found. Use the cache instead.
            log.warning(""The ontology couldn't be located. However ""
                        ""a cached version of it is available. Using it as a ""
                        ""reference."")
            return _get_cache(cache_path, source_file=onto_path)

        if (os.path.getmtime(cache_path) >
                os.path.getmtime(onto_path)):
            # Cache is more recent than the ontology: use cache.
            log.debug(""Normal situation, cache is older than ontology,""
                      "" so we load it from cache"")
            return _get_cache(cache_path, source_file=onto_path)
        else:
            # Ontology is more recent than the cache: rebuild cache.
            log.warning(""Cache '%s' is older than '%s'. ""
                        ""We will rebuild the cache"" %
                        (cache_path, onto_path))
            return _build_cache(onto_path, skip_cache=no_cache)

    elif os.access(onto_path, os.R_OK):
        if not no_cache and\
                os.path.exists(cache_path) and\
                not os.access(cache_path, os.W_OK):
            raise TaxonomyError('We cannot read/write into: %s. '
                                'Aborting!' % cache_path)
        elif not no_cache and os.path.exists(cache_path):
            log.warning('Cache %s exists, but is not readable!' % cache_path)
        log.info(""Cache not available. Building it now: %s"" % onto_path)
        return _build_cache(onto_path, skip_cache=no_cache)

    else:
        raise TaxonomyError(""We miss both source and cache""
                            "" of the taxonomy: %s"" % taxonomy_name)


def _get_remote_ontology(onto_url, time_difference=None):
    """"""Check if the online ontology is more recent than the local ontology.

    If yes, try to download and store it in Invenio's cache directory.

    Return a boolean describing the success of the operation.

    :return: path to the downloaded ontology.
    """"""
    if onto_url is None:
        return False

    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +
              ""bibclassify"" + os.sep)
    if not os.path.exists(dl_dir):
        os.mkdir(dl_dir)

    local_file = dl_dir + os.path.basename(onto_url)
    remote_modif_time = _get_last_modification_date(onto_url)
    try:
        local_modif_seconds = os.path.getmtime(local_file)
    except OSError:
        # The local file does not exist. Download the ontology.
        download = True
        log.info(""The local ontology could not be found."")
    else:
        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])
        # Let's set a time delta of 1 hour and 10 minutes.
        time_difference = time_difference or timedelta(hours=1, minutes=10)
        download = remote_modif_time > local_modif_time + time_difference
        if download:
            log.info(""The remote ontology '%s' is more recent ""
                     ""than the local ontology."" % onto_url)

    if download:
        if not _download_ontology(onto_url, local_file):
            log.warning(""Error downloading the ontology from: %s"" % onto_url)

    return local_file


def _get_ontology(ontology):
    """"""Return the (name, path, url) to the short ontology name.

    :param ontology: name of the ontology or path to the file or url.
    """"""
    onto_name = onto_path = onto_url = None

    # first assume we got the path to the file
    if os.path.exists(ontology):
        onto_name = os.path.split(os.path.abspath(ontology))[1]
        onto_path = os.path.abspath(ontology)
        onto_url = """"
    else:
        # if not, try to find it in a known locations
        discovered_file = _discover_ontology(ontology)
        if discovered_file:
            onto_name = os.path.split(discovered_file)[1]
            onto_path = discovered_file
            # i know, this sucks
            x = ontology.lower()
            if ""http:"" in x or ""https:"" in x or ""ftp:"" in x or ""file:"" in x:
                onto_url = ontology
            else:
                onto_url = """"
        else:
            # not found, look into a database
            # (it is last because when bibclassify
            # runs in a standalone mode,
            # it has no database - [rca, old-heritage]
            if not bconfig.STANDALONE:
                result = dbquery.run_sql(""SELECT name, location from clsMETHOD WHERE name LIKE %s"",
                                         ('%' + ontology + '%',))
                for onto_short_name, url in result:
                    onto_name = onto_short_name
                    onto_path = _get_remote_ontology(url)
                    onto_url = url

    return (onto_name, onto_path, onto_url)


def _discover_ontology(ontology_name):
    """"""Look for the file in a known places.

    Inside invenio/etc/bibclassify and a few other places
    like current directory.

    :param ontology: name or path name or url
    :type ontology: str

    :return: absolute path of a file if found, or None
    """"""
    last_part = os.path.split(os.path.abspath(ontology_name))[1]
    if last_part in taxonomies:
        return taxonomies.get(last_part)
    elif last_part + "".rdf"" in taxonomies:
        return taxonomies.get(last_part + "".rdf"")
    else:
        log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)

    # LEGACY
    possible_patterns = [last_part, last_part.lower()]
    if not last_part.endswith('.rdf'):
        possible_patterns.append(last_part + '.rdf')
    places = [config.CFG_CACHEDIR,
              config.CFG_ETCDIR,
              os.path.join(config.CFG_CACHEDIR, ""bibclassify""),
              os.path.join(config.CFG_ETCDIR, ""bibclassify""),
              os.path.abspath('.'),
              os.path.abspath(os.path.join(os.path.dirname(__file__),
                                           ""../../../etc/bibclassify"")),
              os.path.join(os.path.dirname(__file__), ""bibclassify""),
              config.CFG_WEBDIR]

    log.debug(""Searching for taxonomy using string: %s"" % last_part)
    log.debug(""Possible patterns: %s"" % possible_patterns)
    for path in places:

        try:
            if os.path.isdir(path):
                log.debug(""Listing: %s"" % path)
                for filename in os.listdir(path):
                    #log.debug('Testing: %s' % filename)
                    for pattern in possible_patterns:
                        filename_lc = filename.lower()
                        if pattern == filename_lc and\
                                os.path.exists(os.path.join(path, filename)):
                            filepath = os.path.abspath(os.path.join(path,
                                                                    filename))
                            if (os.access(filepath, os.R_OK)):
                                log.debug(""Found taxonomy at: %s"" % filepath)
                                return filepath
                            else:
                                log.warning('Found taxonony at: %s, but it is'
                                            ' not readable. '
                                            'Continue searching...'
                                            % filepath)
        except OSError, os_error_msg:
            log.warning('OS Error when listing path '
                        '""%s"": %s' % (str(path), str(os_error_msg)))
    log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)


class KeywordToken:

    """"""KeywordToken is a class used for the extracted keywords.

    It can be initialized with values from RDF store or from
    simple strings. Specialty of this class is that objects are
    hashable by subject - so in the dictionary two objects with the
    same subject appears as one -- :see: self.__hash__ and self.__cmp__.
    """"""

    def __init__(self, subject, store=None, namespace=None, type='HEP'):
        """"""Initialize KeywordToken with a subject.

        :param subject: string or RDF object
        :param store: RDF graph object
                      (will be used to get info about the subject)
        :param namespace: RDF namespace object, used together with store
        :param type: type of this keyword.
        """"""
        self.id = subject
        self.type = type
        self.short_id = subject
        self.concept = """"
        self.regex = []
        self.nostandalone = False
        self.spires = False
        self.fieldcodes = []
        self.compositeof = []
        self.core = False
        # True means composite keyword
        self._composite = '#Composite' in subject
        self.__hash = None

        # the tokens are coming possibly from a normal text file
        if store is None:
            subject = subject.strip()
            self.concept = subject
            self.regex = _get_searchable_regex(basic=[subject])
            self.nostandalone = False
            self.fieldcodes = []
            self.core = False
            if subject.find(' ') > -1:
                self._composite = True

        # definitions from rdf
        else:
            self.short_id = self.short_id.split('#')[-1]

            # find alternate names for this label
            basic_labels = []

            # turn those patterns into regexes only for simple keywords
            if self._composite is False:
                try:
                    for label in store.objects(subject,
                                               namespace[""prefLabel""]):
                        # XXX shall i make it unicode?
                        basic_labels.append(str(label))
                except TypeError:
                    pass
                self.concept = basic_labels[0]
            else:
                try:
                    self.concept = str(store.value(subject,
                                                   namespace[""prefLabel""],
                                                   any=True))
                except KeyError:
                    log.warning(""Keyword with subject %s has no prefLabel.""
                                "" We use raw name"" %
                                self.short_id)
                    self.concept = self.short_id

            # this is common both to composite and simple keywords
            try:
                for label in store.objects(subject, namespace[""altLabel""]):
                    basic_labels.append(str(label))
            except TypeError:
                pass

            # hidden labels are special (possibly regex) codes
            hidden_labels = []
            try:
                for label in store.objects(subject, namespace[""hiddenLabel""]):
                    hidden_labels.append(unicode(label))
            except TypeError:
                pass

            # compile regular expression that will identify this token
            self.regex = _get_searchable_regex(basic_labels, hidden_labels)

            try:
                for note in map(lambda s: str(s).lower().strip(),
                                store.objects(subject, namespace[""note""])):
                    if note == 'core':
                        self.core = True
                    elif note in (""nostandalone"", ""nonstandalone""):
                        self.nostandalone = True
                    elif 'fc:' in note:
                        self.fieldcodes.append(note[3:].strip())
            except TypeError:
                pass

            # spiresLabel does not have multiple values
            spires_label = store.value(subject, namespace[""spiresLabel""])
            if spires_label:
                self.spires = str(spires_label)

        # important for comparisons
        self.__hash = hash(self.short_id)

        # extract composite parts ids
        if store is not None and self.isComposite():
            small_subject = self.id.split(""#Composite."")[-1]
            component_positions = []
            for label in store.objects(self.id, namespace[""compositeOf""]):
                strlabel = str(label).split(""#"")[-1]
                component_name = label.split(""#"")[-1]
                component_positions.append((small_subject.find(component_name),
                                            strlabel))
            component_positions.sort()
            if not component_positions:
                log.error(""Keyword is marked as composite, ""
                          ""but no composite components refs found: %s""
                          % self.short_id)
            else:
                self.compositeof = map(lambda x: x[1], component_positions)

    def refreshCompositeOf(self, single_keywords, composite_keywords,
                           store=None, namespace=None):
        """"""Re-check sub-parts of this keyword.

        This should be called after the whole RDF was processed, because
        it is using a cache of single keywords and if that
        one is incomplete, you will not identify all parts.
        """"""
        def _get_ckw_components(new_vals, label):
            if label in single_keywords:
                new_vals.append(single_keywords[label])
            elif ('Composite.%s' % label) in composite_keywords:
                for l in composite_keywords['Composite.%s' % label].compositeof:
                    _get_ckw_components(new_vals, l)
            elif label in composite_keywords:
                for l in composite_keywords[label].compositeof:
                    _get_ckw_components(new_vals, l)
            else:
                # One single or composite keyword is missing from the taxonomy.
                # This is due to an error in the taxonomy description.
                message = ""The composite term \""%s\""""\
                          "" should be made of single keywords,""\
                          "" but at least one is missing."" % self.id
                if store is not None:
                    message += ""Needed components: %s""\
                               % list(store.objects(self.id,
                                      namespace[""compositeOf""]))
                message += "" Missing is: %s"" % label
                raise TaxonomyError(message)

        if self.compositeof:
            new_vals = []
            try:
                for label in self.compositeof:
                    _get_ckw_components(new_vals, label)
                self.compositeof = new_vals
            except TaxonomyError as err:
                # the composites will be empty
                # (better than to have confusing, partial matches)
                self.compositeof = []
                log.error(err)

    def isComposite(self):
        """"""Return value of _composite.""""""
        return self._composite

    def getComponents(self):
        """"""Return value of compositeof.""""""
        return self.compositeof

    def getType(self):
        """"""Return value of type.""""""
        return self.type

    def setType(self, value):
        """"""Set value of value.""""""
        self.type = value

    def __hash__(self):
        """"""Return _hash.

        This might change in the future but for the moment we want to
        think that if the concept is the same, then it is the same
        keyword - this sucks, but it is sort of how it is necessary
        to use now.
        """"""
        return self.__hash

    def __cmp__(self, other):
        """"""Compare objects using _hash.""""""
        if self.__hash < other.__hash__():
            return -1
        elif self.__hash == other.__hash__():
            return 0
        else:
            return 1

    def __str__(self, spires=False):
        """"""Return the best output for the keyword.""""""
        if spires:
            if self.spires:
                return self.spires
            elif self._composite:
                return self.concept.replace(':', ',')
            # default action
        return self.concept

    def output(self, spires=False):
        """"""Return string representation with spires value.""""""
        return self.__str__(spires=spires)

    def __repr__(self):
        """"""Class representation.""""""
        return ""<KeywordToken: %s>"" % self.short_id


def _build_cache(source_file, skip_cache=False):
    """"""Build the cached data.

    Either by parsing the RDF taxonomy file or a vocabulary file.

    :param source_file: source file of the taxonomy, RDF file
    :param skip_cache: if True, build cache will not be
        saved (pickled) - it is saved as <source_file.db>
    """"""
    store = rdflib.ConjunctiveGraph()

    if skip_cache:
        log.info(""You requested not to save the cache to disk."")
    else:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        # Make sure we have a cache_dir readable and writable.
        try:
            os.makedirs(cache_dir)
        except:
            pass
        if os.access(cache_dir, os.R_OK):
            if not os.access(cache_dir, os.W_OK):
                raise TaxonomyError(""Cache directory exists but is not""
                                    "" writable. Check your permissions""
                                    "" for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    timer_start = time.clock()

    namespace = None
    single_keywords, composite_keywords = {}, {}

    try:
        log.info(""Building RDFLib's conjunctive graph from: %s"" % source_file)
        try:
            store.parse(source_file)
        except urllib2.URLError:
            if source_file[0] == '/':
                store.parse(""file://"" + source_file)
            else:
                store.parse(""file:///"" + source_file)

    except rdflib.exceptions.Error as e:
        log.error(""Serious error reading RDF file"")
        log.error(e)
        log.error(traceback.format_exc())
        raise rdflib.exceptions.Error(e)

    except (xml.sax.SAXParseException, ImportError) as e:
        # File is not a RDF file. We assume it is a controlled vocabulary.
        log.error(e)
        log.warning(""The ontology file is probably not a valid RDF file. \
            Assuming it is a controlled vocabulary file."")

        filestream = open(source_file, ""r"")
        for line in filestream:
            keyword = line.strip()
            kt = KeywordToken(keyword)
            single_keywords[kt.short_id] = kt
        if not len(single_keywords):
            raise TaxonomyError('The ontology file is not well formated')

    else:  # ok, no exception happened
        log.info(""Now building cache of keywords"")
        # File is a RDF file.
        namespace = rdflib.Namespace(""http://www.w3.org/2004/02/skos/core#"")

        single_count = 0
        composite_count = 0

        subject_objects = store.subject_objects(namespace[""prefLabel""])
        for subject, pref_label in subject_objects:
            kt = KeywordToken(subject, store=store, namespace=namespace)
            if kt.isComposite():
                composite_count += 1
                composite_keywords[kt.short_id] = kt
            else:
                single_keywords[kt.short_id] = kt
                single_count += 1

    cached_data = {}
    cached_data[""single""] = single_keywords
    cached_data[""composite""] = composite_keywords
    cached_data[""creation_time""] = time.gmtime()
    cached_data[""version_info""] = {'rdflib': rdflib.__version__,
                                   'bibclassify': bconfig.VERSION}
    log.debug(""Building taxonomy... %d terms built in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    log.info(""Total count of single keywords: %d ""
             % len(single_keywords))
    log.info(""Total count of composite keywords: %d ""
             % len(composite_keywords))

    if not skip_cache:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        log.debug(""Writing the cache into: %s"" % cache_path)
        # test again, it could have changed
        if os.access(cache_dir, os.R_OK):
            if os.access(cache_dir, os.W_OK):
                # Serialize.
                filestream = None
                try:
                    filestream = open(cache_path, ""wb"")
                except IOError as msg:
                    # Impossible to write the cache.
                    log.error(""Impossible to write cache to '%s'.""
                              % cache_path)
                    log.error(msg)
                else:
                    log.debug(""Writing cache to file %s"" % cache_path)
                    cPickle.dump(cached_data, filestream, 1)
                if filestream:
                    filestream.close()

            else:
                raise TaxonomyError(""Cache directory exists but is not ""
                                    ""writable. Check your permissions ""
                                    ""for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    # now when the whole taxonomy was parsed,
    # find sub-components of the composite kws
    # it is important to keep this call after the taxonomy was saved,
    # because we don't  want to pickle regexes multiple times
    # (as they are must be re-compiled at load time)
    for kt in composite_keywords.values():
        kt.refreshCompositeOf(single_keywords, composite_keywords,
                              store=store, namespace=namespace)

    # house-cleaning
    if store:
        store.close()

    return (single_keywords, composite_keywords)


def _capitalize_first_letter(word):
    """"""Return a regex pattern with the first letter.

    Accepts both lowercase and uppercase.
    """"""
    if word[0].isalpha():
        # These two cases are necessary in order to get a regex pattern
        # starting with '[xX]' and not '[Xx]'. This allows to check for
        # colliding regex afterwards.
        if word[0].isupper():
            return ""["" + word[0].swapcase() + word[0] + ""]"" + word[1:]
        else:
            return ""["" + word[0] + word[0].swapcase() + ""]"" + word[1:]
    return word


def _convert_punctuation(punctuation, conversion_table):
    """"""Return a regular expression for a punctuation string.""""""
    if punctuation in conversion_table:
        return conversion_table[punctuation]
    return re.escape(punctuation)


def _convert_word(word):
    """"""Return the plural form of the word if it exists.

    Otherwise return the word itself.
    """"""
    out = None

    # Acronyms.
    if word.isupper():
        out = word + ""s?""
    # Proper nouns or word with digits.
    elif word.istitle():
        out = word + ""('?s)?""
    elif _contains_digit.search(word):
        out = word

    if out is not None:
        return out

    # Words with non or anti prefixes.
    if _starts_with_non.search(word):
        word = ""non-?"" + _capitalize_first_letter(_convert_word(word[3:]))
    elif _starts_with_anti.search(word):
        word = ""anti-?"" + _capitalize_first_letter(_convert_word(word[4:]))

    if out is not None:
        return _capitalize_first_letter(out)

    # A few invariable words.
    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:
        return _capitalize_first_letter(word)

    # Some exceptions that would not produce good results with the set of
    # general_regular_expressions.
    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS
    if word in regexes:
        return _capitalize_first_letter(regexes[word])

    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS
    for regex in regexes:
        if regex.search(word) is not None:
            return _capitalize_first_letter(word)

    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS
    for regex, replacement in regexes:
        stemmed = regex.sub(replacement, word)
        if stemmed != word:
            return _capitalize_first_letter(stemmed)

    return _capitalize_first_letter(word + ""s?"")


def _get_cache(cache_file, source_file=None):
    """"""Get cached taxonomy using the cPickle module.

    No check is done at that stage.

    :param cache_file: full path to the file holding pickled data
    :param source_file: if we discover the cache is obsolete, we
        will build a new cache, therefore we need the source path
        of the cache
    :return: (single_keywords, composite_keywords).
    """"""
    timer_start = time.clock()

    filestream = open(cache_file, ""rb"")
    try:
        cached_data = cPickle.load(filestream)
        version_info = cached_data['version_info']
        if version_info['rdflib'] != rdflib.__version__\
                or version_info['bibclassify'] != bconfig.VERSION:
            raise KeyError
    except (cPickle.UnpicklingError, ImportError,
            AttributeError, DeprecationWarning, EOFError):
        log.warning(""The existing cache in %s is not readable. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        return _build_cache(source_file)
    except KeyError:
        log.warning(""The existing cache %s is not up-to-date. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        if source_file and os.path.exists(source_file):
            return _build_cache(source_file)
        else:
            log.error(""The cache contains obsolete data (and it was deleted), ""
                      ""however I can't build a new cache, the source does not ""
                      ""exist or is inaccessible! - %s"" % source_file)
    filestream.close()

    single_keywords = cached_data[""single""]
    composite_keywords = cached_data[""composite""]

    # the cache contains only keys of the composite keywords, not the objects
    # so now let's resolve them into objects
    for kw in composite_keywords.values():
        kw.refreshCompositeOf(single_keywords, composite_keywords)

    log.debug(""Retrieved taxonomy from cache %s created on %s"" %
              (cache_file, time.asctime(cached_data[""creation_time""])))

    log.debug(""%d terms read in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    return (single_keywords, composite_keywords)


def _get_cache_path(source_file):
    """"""Return the path where the cache should be written/located.

    :param onto_name: name of the ontology or the full path
    :return: string, abs path to the cache file in the tmpdir/bibclassify
    """"""
    local_name = os.path.basename(source_file)
    cache_name = local_name + "".db""
    cache_dir = os.path.join(config.CFG_CACHEDIR, ""bibclassify"")

    if not os.path.isdir(cache_dir):
        os.makedirs(cache_dir)

    return os.path.abspath(os.path.join(cache_dir, cache_name))


def _get_last_modification_date(url):
    """"""Get the last modification date of the ontology.""""""
    request = urllib2.Request(url)
    request.get_method = lambda: ""HEAD""
    http_file = urlopen(request)
    date_string = http_file.headers[""last-modified""]
    parsed = time.strptime(date_string, ""%a, %d %b %Y %H:%M:%S %Z"")
    return datetime(*(parsed)[0:6])


def _download_ontology(url, local_file):
    """"""Download the ontology and stores it in CFG_CACHEDIR.""""""
    log.debug(""Copying remote ontology '%s' to file '%s'."" % (url,
                                                              local_file))
    try:
        url_desc = urlopen(url)
        file_desc = open(local_file, 'w')
        file_desc.write(url_desc.read())
        file_desc.close()
    except IOError as e:
        print(e)
        return False
    except:
        log.warning(""Unable to download the ontology. '%s'"" %
                    sys.exc_info()[0])
        return False
    else:
        log.debug(""Done copying."")
        return True


def _get_searchable_regex(basic=None, hidden=None):
    """"""Return the searchable regular expressions for the single keyword.""""""
    # Hidden labels are used to store regular expressions.
    basic = basic or []
    hidden = hidden or []

    hidden_regex_dict = {}
    for hidden_label in hidden:
        if _is_regex(hidden_label):
            hidden_regex_dict[hidden_label] = \
                re.compile(
                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]
                )
        else:
            pattern = _get_regex_pattern(hidden_label)
            hidden_regex_dict[hidden_label] = re.compile(
                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
            )

    # We check if the basic label (preferred or alternative) is matched
    # by a hidden label regex. If yes, discard it.
    regex_dict = {}
    # Create regex for plural forms and add them to the hidden labels.
    for label in basic:
        pattern = _get_regex_pattern(label)
        regex_dict[label] = re.compile(
            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
        )

    # Merge both dictionaries.
    regex_dict.update(hidden_regex_dict)

    return regex_dict.values()


def _get_regex_pattern(label):
    """"""Return a regular expression of the label.

    This takes care of plural and different kinds of separators.
    """"""
    parts = _split_by_punctuation.split(label)

    for index, part in enumerate(parts):
        if index % 2 == 0:
            # Word
            if not parts[index].isdigit() and len(parts[index]) > 1:
                parts[index] = _convert_word(parts[index])
        else:
            # Punctuation
            if not parts[index + 1]:
                # The separator is not followed by another word. Treat
                # it as a symbol.
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SYMBOLS
                )
            else:
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SEPARATORS
                )

    return """".join(parts)


def _is_regex(string):
    """"""Check if a concept is a regular expression.""""""
    return string[0] == ""/"" and string[-1] == ""/""


def check_taxonomy(taxonomy):
    """"""Check the consistency of the taxonomy.

    Outputs a list of errors and warnings.
    """"""
    log.info(""Building graph with Python RDFLib version %s"" %
             rdflib.__version__)

    store = rdflib.ConjunctiveGraph()

    try:
        store.parse(taxonomy)
    except:
        log.error(""The taxonomy is not a valid RDF file. Are you ""
                  ""trying to check a controlled vocabulary?"")
        raise TaxonomyError('Error in RDF file')

    log.info(""Graph was successfully built."")

    prefLabel = ""prefLabel""
    hiddenLabel = ""hiddenLabel""
    altLabel = ""altLabel""
    composite = ""composite""
    compositeOf = ""compositeOf""
    note = ""note""

    both_skw_and_ckw = []

    # Build a dictionary we will reason on later.
    uniq_subjects = {}
    for subject in store.subjects():
        uniq_subjects[subject] = None

    subjects = {}
    for subject in uniq_subjects:
        strsubject = str(subject).split(""#Composite."")[-1]
        strsubject = strsubject.split(""#"")[-1]
        if (strsubject == ""http://cern.ch/thesauri/HEPontology.rdf"" or
           strsubject == ""compositeOf""):
            continue
        components = {}
        for predicate, value in store.predicate_objects(subject):
            strpredicate = str(predicate).split(""#"")[-1]
            strobject = str(value).split(""#Composite."")[-1]
            strobject = strobject.split(""#"")[-1]
            components.setdefault(strpredicate, []).append(strobject)
        if strsubject in subjects:
            both_skw_and_ckw.append(strsubject)
        else:
            subjects[strsubject] = components

    log.info(""Taxonomy contains %s concepts."" % len(subjects))

    no_prefLabel = []
    multiple_prefLabels = []
    bad_notes = []
    # Subjects with no composite or compositeOf predicate
    lonely = []
    both_composites = []
    bad_hidden_labels = {}
    bad_alt_labels = {}
    # Problems with composite keywords
    composite_problem1 = []
    composite_problem2 = []
    composite_problem3 = []
    composite_problem4 = {}
    composite_problem5 = []
    composite_problem6 = []

    stemming_collisions = []
    interconcept_collisions = {}

    for subject, predicates in iteritems(subjects):
        # No prefLabel or multiple prefLabels
        try:
            if len(predicates[prefLabel]) > 1:
                multiple_prefLabels.append(subject)
        except KeyError:
            no_prefLabel.append(subject)

        # Lonely and both composites.
        if composite not in predicates and compositeOf not in predicates:
            lonely.append(subject)
        elif composite in predicates and compositeOf in predicates:
            both_composites.append(subject)

        # Multiple or bad notes
        if note in predicates:
            bad_notes += [(subject, n) for n in predicates[note]
                          if n not in ('nostandalone', 'core')]

        # Bad hidden labels
        if hiddenLabel in predicates:
            for lbl in predicates[hiddenLabel]:
                if lbl.startswith(""/"") ^ lbl.endswith(""/""):
                    bad_hidden_labels.setdefault(subject, []).append(lbl)

        # Bad alt labels
        if altLabel in predicates:
            for lbl in predicates[altLabel]:
                if len(re.findall(""/"", lbl)) >= 2 or "":"" in lbl:
                    bad_alt_labels.setdefault(subject, []).append(lbl)

        # Check composite
        if composite in predicates:
            for ckw in predicates[composite]:
                if ckw in subjects:
                    if compositeOf in subjects[ckw]:
                        if subject not in subjects[ckw][compositeOf]:
                            composite_problem3.append((subject, ckw))
                    else:
                        if ckw not in both_skw_and_ckw:
                            composite_problem2.append((subject, ckw))
                else:
                    composite_problem1.append((subject, ckw))

        # Check compositeOf
        if compositeOf in predicates:
            for skw in predicates[compositeOf]:
                if skw in subjects:
                    if composite in subjects[skw]:
                        if subject not in subjects[skw][composite]:
                            composite_problem6.append((subject, skw))
                    else:
                        if skw not in both_skw_and_ckw:
                            composite_problem5.append((subject, skw))
                else:
                    composite_problem4.setdefault(skw, []).append(subject)

        # Check for stemmed labels
        if compositeOf in predicates:
            labels = (altLabel, hiddenLabel)
        else:
            labels = (prefLabel, altLabel, hiddenLabel)

        patterns = {}
        for label in [lbl for lbl in labels if lbl in predicates]:
            for expression in [expr for expr in predicates[label]
                               if not _is_regex(expr)]:
                pattern = _get_regex_pattern(expression)
                interconcept_collisions.setdefault(pattern, []).\
                    append((subject, label))
                if pattern in patterns:
                    stemming_collisions.append(
                        (subject,
                         patterns[pattern],
                         (label, expression)
                         )
                    )
                else:
                    patterns[pattern] = (label, expression)

    print(""\n==== ERRORS ===="")

    if no_prefLabel:
        print(""\nConcepts with no prefLabel: %d"" % len(no_prefLabel))
        print(""\n"".join([""   %s"" % subj for subj in no_prefLabel]))
    if multiple_prefLabels:
        print((""\nConcepts with multiple prefLabels: %d"" %
               len(multiple_prefLabels)))
        print(""\n"".join([""   %s"" % subj for subj in multiple_prefLabels]))
    if both_composites:
        print((""\nConcepts with both composite properties: %d"" %
               len(both_composites)))
        print(""\n"".join([""   %s"" % subj for subj in both_composites]))
    if bad_hidden_labels:
        print(""\nConcepts with bad hidden labels: %d"" % len(bad_hidden_labels))
        for kw, lbls in iteritems(bad_hidden_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if bad_alt_labels:
        print(""\nConcepts with bad alt labels: %d"" % len(bad_alt_labels))
        for kw, lbls in iteritems(bad_alt_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if both_skw_and_ckw:
        print((""\nKeywords that are both skw and ckw: %d"" %
               len(both_skw_and_ckw)))
        print(""\n"".join([""   %s"" % subj for subj in both_skw_and_ckw]))

    print()

    if composite_problem1:
        print(""\n"".join([""SKW '%s' references an unexisting CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem1]))
    if composite_problem2:
        print(""\n"".join([""SKW '%s' references a SKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem2]))
    if composite_problem3:
        print(""\n"".join([""SKW '%s' is not composite of CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem3]))
    if composite_problem4:
        for skw, ckws in iteritems(composite_problem4):
            print(""SKW '%s' does not exist but is "" ""referenced by:"" % skw)
            print(""\n"".join([""    %s"" % ckw for ckw in ckws]))
    if composite_problem5:
        print(""\n"".join([""CKW '%s' references a CKW '%s'."" % kw
                         for kw in composite_problem5]))
    if composite_problem6:
        print(""\n"".join([""CKW '%s' is not composed by SKW '%s'."" % kw
                         for kw in composite_problem6]))

    print(""\n==== WARNINGS ===="")

    if bad_notes:
        print((""\nConcepts with bad notes: %d"" % len(bad_notes)))
        print(""\n"".join([""   '%s': '%s'"" % _note for _note in bad_notes]))
    if stemming_collisions:
        print(""\nFollowing keywords have unnecessary labels that have ""
              ""already been generated by BibClassify."")
        for subj in stemming_collisions:
            print(""   %s:\n     %s\n     and %s"" % subj)

    print(""\nFinished."")
    sys.exit(0)


def test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):
    """"""Test the cache lookup.""""""
    cache = get_cache(taxonomy_name)
    if not cache:
        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,
                                                         rebuild=rebuild_cache,
                                                         no_cache=no_cache))
        cache = get_cache(taxonomy_name)
    return (thread.get_ident(), cache)


log.info('Loaded ontology reader')

if __name__ == '__main__':
    test_cache()
/n/n/ninvenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re

from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Check if a document is a PDF file and returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[-1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    # os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Return the fulltext of the local file.

    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""
    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    # Discard lines that do not contain at least one word.
    return [line for line in lines if _ONE_WORD.search(line) is not None]


def executable_exists(executable):
    """"""Test if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False
/n/n/n",0
7,7,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"/invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,
                                                       user_agent=make_user_agent_string(""BibClassify""))
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Outputs keywords reading a local file. Arguments and output are the same
    as for :see: get_keywords_from_text() """"""

    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/n/invenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""
BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re
import tempfile
import urllib2
from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Checks if a document is a PDF file. Returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported. Please report this to cds.support@cern.ch."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    #os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Returns the fulltext of the local file.
    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""

    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    if not _is_english_text('\n'.join(lines)):
        log.warning(""It seems the file '%s' is unvalid and doesn't ""
                    ""contain text. Please communicate this file to the Invenio ""
                    ""team."" % document)

    line_nb = len(lines)
    word_nb = 0
    for line in lines:
        word_nb += len(re.findall(""\S+"", line))

    # Discard lines that do not contain at least one word.
    lines = [line for line in lines if _ONE_WORD.search(line) is not None]

    if not remote:
        log.info(""Local file has %d lines and %d words."" % (line_nb, word_nb))

    return lines


def _is_english_text(text):
    """"""
    Checks if a text is correct english.
    Computes the number of words in the text and compares it to the
    expected number of words (based on an average size of words of 5.1
    letters).

    @param text_lines: the text to analyze
    @type text_lines:  string
    @return:           True if the text is English, False otherwise
    @rtype:            Boolean
    """"""
    # Consider one word and one space.
    avg_word_length = 2.55 + 1
    expected_word_number = float(len(text)) / avg_word_length

    words = [word
             for word in re.split('\W', text)
             if word.isalpha()]

    word_number = len(words)

    return word_number > expected_word_number


def text_lines_from_url(url, user_agent=""""):
    """"""Returns the fulltext of the file found at the URL.""""""
    request = urllib2.Request(url)
    if user_agent:
        request.add_header(""User-Agent"", user_agent)
    try:
        distant_stream = urlopen(request)
        # Write the URL content to a temporary file.
        local_file = tempfile.mkstemp(prefix=""bibclassify."")[1]
        local_stream = open(local_file, ""w"")
        local_stream.write(distant_stream.read())
        local_stream.close()
    except:
        log.error(""Unable to read from URL %s."" % url)
        return None
    else:
        # Read lines from the temporary file.
        lines = text_lines_from_local_file(local_file, remote=True)
        os.remove(local_file)

        line_nb = len(lines)
        word_nb = 0
        for line in lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))

        return lines


def executable_exists(executable):
    """"""Tests if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False


/n/n/n",1
38,38,d7c7d42b2b3e4c024a624c2cf21b07dede26bce0,"dciagent/plugins/ansibleplugin.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016 Red Hat, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ansible import inventory
from ansible import vars
from ansible.executor import playbook_executor
from ansible.parsing import dataloader

from ansible.utils.display import Display

from dciclient.v1 import helper as dci_helper
from dciagent.plugins import plugin


import jinja2
import os
import subprocess


class Options(object):
    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,
                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,
                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,
                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,
                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,
                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,
                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):
        self.verbosity = verbosity
        self.inventory = inventory
        self.listhosts = listhosts
        self.subset = subset
        self.module_paths = module_paths
        self.extra_vars = extra_vars
        self.forks = forks
        self.ask_vault_pass = ask_vault_pass
        self.vault_password_files = vault_password_files
        self.new_vault_password_file = new_vault_password_file
        self.output_file = output_file
        self.tags = tags
        self.skip_tags = skip_tags
        self.one_line = one_line
        self.tree = tree
        self.ask_sudo_pass = ask_sudo_pass
        self.ask_su_pass = ask_su_pass
        self.sudo = sudo
        self.sudo_user = sudo_user
        self.become = become
        self.become_method = become_method
        self.become_user = become_user
        self.become_ask_pass = become_ask_pass
        self.ask_pass = ask_pass
        self.private_key_file = private_key_file
        self.remote_user = remote_user
        self.connection = connection
        self.timeout = timeout
        self.ssh_common_args = ssh_common_args
        self.sftp_extra_args = sftp_extra_args
        self.scp_extra_args = scp_extra_args
        self.ssh_extra_args = ssh_extra_args
        self.poll_interval = poll_interval
        self.seconds = seconds
        self.check = check
        self.syntax = syntax
        self.diff = diff
        self.force_handlers = force_handlers
        self.flush_cache = flush_cache
        self.listtasks = listtasks
        self.listtags = listtags
        self.module_path = module_path


class Runner(object):

    def __init__(self, playbook, options=None, verbosity=0):

        if options is None:
            self.options = Options()
            self.options.verbosity = verbosity
            self.options.connection = 'ssh'
            self.options.become = True
            self.options.become_method = 'sudo'
            self.options.become_user = 'root'

        self.loader = dataloader.DataLoader()
        self.variable_manager = vars.VariableManager()

        self.inventory = inventory.Inventory(
            loader=self.loader,
            variable_manager=self.variable_manager,
            host_list='/etc/ansible/hosts'
        )
        self.variable_manager.set_inventory(self.inventory)

        # Playbook to run, from the current working directory.
        pb_dir = os.path.abspath('.')
        playbook_path = ""%s/%s"" % (pb_dir, playbook)

        self.pbex = playbook_executor.PlaybookExecutor(
            playbooks=[playbook],
            inventory=self.inventory,
            variable_manager=self.variable_manager,
            loader=self.loader,
            options=self.options,
            passwords={})

    def run(self, job_id):
        """"""Run the playbook and returns the playbook's stats.""""""

        self.variable_manager.extra_vars = {'job_id': job_id}
        self.pbex.run()
        return self.pbex._tqm._stats


class AnsiblePlugin(plugin.Plugin):

    def __init__(self, conf):
        super(AnsiblePlugin, self).__init__(conf)


    def generate_ansible_playbook_from_template(self, template_file, data):

        templateLoader = jinja2.FileSystemLoader( searchpath=""/"" )
        templateEnv = jinja2.Environment( loader=templateLoader )
        template = templateEnv.get_template( template_file )
        outputText = template.render( data )

        return outputText


    def run(self, state, data=None, context=None):
        """"""Run ansible-playbook on the specified playbook. """"""

        playbook = None
        log_file = None
        template = None

        if state in self.conf:
            if 'playbook' in self.conf[state]:
                playbook = self.conf[state]['playbook']
            if 'log_file' in self.conf[state]:
                log_file = self.conf[state]['log_file']
            if 'template' in self.conf[state]:
                template = self.conf[state]['template']

        if playbook is None:
            playbook = self.conf['playbook']
        if template is None and template in self.conf:
            template = self.conf['template']

        if log_file is None:
            if 'log_file' in self.conf:
                log_file = self.conf['log_file']
            else:
                log_file = open(os.devnull, 'w')

        if template:
            open(playbook, 'w').write(
                self.generate_ansible_playbook_from_template(template, data)
            )
            
        runner = Runner(playbook=playbook, verbosity=0)
        stats = runner.run(job_id=context.last_job_id)
/n/n/n",0
39,39,d7c7d42b2b3e4c024a624c2cf21b07dede26bce0,"/dciagent/plugins/ansibleplugin.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016 Red Hat, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ansible import inventory
from ansible import vars
from ansible.executor import playbook_executor
from ansible.parsing import dataloader

from ansible.utils.display import Display

from dciclient.v1 import helper as dci_helper
from dciagent.plugins import plugin


import jinja2
import os
import subprocess

display = Display()

class Options(object):
    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,
                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,
                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,
                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,
                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,
                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,
                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):
        self.verbosity = verbosity
        self.inventory = inventory
        self.listhosts = listhosts
        self.subset = subset
        self.module_paths = module_paths
        self.extra_vars = extra_vars
        self.forks = forks
        self.ask_vault_pass = ask_vault_pass
        self.vault_password_files = vault_password_files
        self.new_vault_password_file = new_vault_password_file
        self.output_file = output_file
        self.tags = tags
        self.skip_tags = skip_tags
        self.one_line = one_line
        self.tree = tree
        self.ask_sudo_pass = ask_sudo_pass
        self.ask_su_pass = ask_su_pass
        self.sudo = sudo
        self.sudo_user = sudo_user
        self.become = become
        self.become_method = become_method
        self.become_user = become_user
        self.become_ask_pass = become_ask_pass
        self.ask_pass = ask_pass
        self.private_key_file = private_key_file
        self.remote_user = remote_user
        self.connection = connection
        self.timeout = timeout
        self.ssh_common_args = ssh_common_args
        self.sftp_extra_args = sftp_extra_args
        self.scp_extra_args = scp_extra_args
        self.ssh_extra_args = ssh_extra_args
        self.poll_interval = poll_interval
        self.seconds = seconds
        self.check = check
        self.syntax = syntax
        self.diff = diff
        self.force_handlers = force_handlers
        self.flush_cache = flush_cache
        self.listtasks = listtasks
        self.listtags = listtags
        self.module_path = module_path


class Runner(object):

    def __init__(self, playbook, options=None, verbosity=0):

        if options is None:
            self.options = Options()
            self.options.verbosity = verbosity

        self.loader = dataloader.DataLoader()
        self.variable_manager = vars.VariableManager()

        self.inventory = inventory.Inventory(
            loader=self.loader,
            variable_manager=self.variable_manager,
            host_list='/etc/ansible/hosts'
        )
        self.variable_manager.set_inventory(self.inventory)

        # Playbook to run, from the current working directory.
        pb_dir = os.path.abspath('.')
        playbook_path = ""%s/%s"" % (pb_dir, playbook)
        display.verbosity = self.options.verbosity

        self.pbex = playbook_executor.PlaybookExecutor(
            #playbooks=[playbook_path],
            playbooks=[playbook],
            inventory=self.inventory,
            variable_manager=self.variable_manager,
            loader=self.loader,
            options=self.options,
            passwords={})

    def run(self, job_id):
        """"""Run the playbook and returns the playbook's stats.""""""
        self.variable_manager.extra_vars = {'job_id': job_id}
        self.pbex.run()
        return self.pbex._tqm._stats


class AnsiblePlugin(plugin.Plugin):

    def __init__(self, conf):
        super(AnsiblePlugin, self).__init__(conf)


    def generate_ansible_playbook_from_template(self, template_file, data):

        templateLoader = jinja2.FileSystemLoader( searchpath=""/"" )
        templateEnv = jinja2.Environment( loader=templateLoader )
        template = templateEnv.get_template( template_file )
        outputText = template.render( data )

        return outputText





    def run(self, state, data=None, context=None):
        """"""Run ansible-playbook on the specified playbook. """"""

        playbook = None
        log_file = None
        template = None

        if state in self.conf:
            if 'playbook' in self.conf[state]:
                playbook = self.conf[state]['playbook']
            if 'log_file' in self.conf[state]:
                log_file = self.conf[state]['log_file']
            if 'template' in self.conf[state]:
                template = self.conf[state]['template']

        if playbook is None:
            playbook = self.conf['playbook']
        if template is None and template in self.conf:
            template = self.conf['template']

        if log_file is None:
            if 'log_file' in self.conf:
                log_file = self.conf['log_file']
            else:
                log_file = open(os.devnull, 'w')

        if template:
            open(playbook, 'w').write(
                self.generate_ansible_playbook_from_template(template, data)
            )
            
        runner = Runner(playbook=playbook, verbosity=0)
        stats = runner.run(job_id=context.last_job_id)
/n/n/n",1
58,58,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Köster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Köster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Köster""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
59,59,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
68,68,22e3ab28b73a4de7a2a065d657b017ccbac352d8,"lib/galaxy/jobs/runners/lwr.py/n/nimport logging

from galaxy import model
from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner
from galaxy.jobs import ComputeEnvironment
from galaxy.jobs import JobDestination
from galaxy.jobs.command_factory import build_command
from galaxy.tools.deps import dependencies
from galaxy.util import string_as_bool_or_none
from galaxy.util.bunch import Bunch

import errno
from time import sleep
import os

from .lwr_client import build_client_manager
from .lwr_client import url_to_destination_params
from .lwr_client import finish_job as lwr_finish_job
from .lwr_client import submit_job as lwr_submit_job
from .lwr_client import ClientJobDescription
from .lwr_client import LwrOutputs
from .lwr_client import ClientOutputs
from .lwr_client import PathMapper

log = logging.getLogger( __name__ )

__all__ = [ 'LwrJobRunner' ]

NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = ""LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.""
NO_REMOTE_DATATYPES_CONFIG = ""LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.""

# Is there a good way to infer some default for this? Can only use
# url_for from web threads. https://gist.github.com/jmchilton/9098762
DEFAULT_GALAXY_URL = ""http://localhost:8080""


class LwrJobRunner( AsynchronousJobRunner ):
    """"""
    LWR Job Runner
    """"""
    runner_name = ""LWRRunner""

    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):
        """"""Start the job runner """"""
        super( LwrJobRunner, self ).__init__( app, nworkers )
        self.async_status_updates = dict()
        self._init_monitor_thread()
        self._init_worker_threads()
        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), ""url"": url}
        self.galaxy_url = galaxy_url
        self.client_manager = build_client_manager(**client_manager_kwargs)

    def url_to_destination( self, url ):
        """"""Convert a legacy URL to a job destination""""""
        return JobDestination( runner=""lwr"", params=url_to_destination_params( url ) )

    def check_watched_item(self, job_state):
        try:
            client = self.get_client_from_state(job_state)

            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):
                # Message queue implementation.

                # TODO: Very hacky now, refactor after Dannon merges in his
                # message queue work, runners need the ability to disable
                # check_watched_item like this and instead a callback needs to
                # be issued post job recovery allowing a message queue
                # consumer to be setup.
                self.client_manager.ensure_has_status_update_callback(self.__async_update)
                return job_state

            status = client.get_status()
        except Exception:
            # An orphaned job was put into the queue at app startup, so remote server went down
            # either way we are done I guess.
            self.mark_as_finished(job_state)
            return None
        job_state = self.__update_job_state_for_lwr_status(job_state, status)
        return job_state

    def __update_job_state_for_lwr_status(self, job_state, lwr_status):
        if lwr_status == ""complete"":
            self.mark_as_finished(job_state)
            return None
        if lwr_status == ""running"" and not job_state.running:
            job_state.running = True
            job_state.job_wrapper.change_state( model.Job.states.RUNNING )
        return job_state

    def __async_update( self, full_status ):
        job_id = full_status[ ""job_id"" ]
        job_state = self.__find_watched_job( job_id )
        if not job_state:
            # Probably finished too quickly, sleep and try again.
            # Kind of a hack, why does monitor queue need to no wait
            # get and sleep instead of doing a busy wait that would
            # respond immediately.
            sleep( 2 )
            job_state = self.__find_watched_job( job_id )
        if not job_state:
            log.warn( ""Failed to find job corresponding to final status %s in %s"" % ( full_status, self.watched ) )
        else:
            self.__update_job_state_for_lwr_status(job_state, full_status[""status""])

    def __find_watched_job( self, job_id ):
        found_job = None
        for async_job_state in self.watched:
            if str( async_job_state.job_id ) == job_id:
                found_job = async_job_state
                break
        return found_job

    def queue_job(self, job_wrapper):
        job_destination = job_wrapper.job_destination

        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )

        if not command_line:
            return

        try:
            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )
            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )
            unstructured_path_rewrites = {}
            if compute_environment:
                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites

            client_job_description = ClientJobDescription(
                command_line=command_line,
                input_files=self.get_input_files(job_wrapper),
                client_outputs=self.__client_outputs(client, job_wrapper),
                working_directory=job_wrapper.working_directory,
                tool=job_wrapper.tool,
                config_files=job_wrapper.extra_filenames,
                dependencies_description=dependencies_description,
                env=client.env,
                rewrite_paths=rewrite_paths,
                arbitrary_files=unstructured_path_rewrites,
            )
            job_id = lwr_submit_job(client, client_job_description, remote_job_config)
            log.info(""lwr job submitted with job_id %s"" % job_id)
            job_wrapper.set_job_destination( job_destination, job_id )
            job_wrapper.change_state( model.Job.states.QUEUED )
        except Exception:
            job_wrapper.fail( ""failure running job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)
            return

        lwr_job_state = AsynchronousJobState()
        lwr_job_state.job_wrapper = job_wrapper
        lwr_job_state.job_id = job_id
        lwr_job_state.old_state = True
        lwr_job_state.running = False
        lwr_job_state.job_destination = job_destination
        self.monitor_job(lwr_job_state)

    def __prepare_job(self, job_wrapper, job_destination):
        """""" Build command-line and LWR client for this job. """"""
        command_line = None
        client = None
        remote_job_config = None
        compute_environment = None
        try:
            client = self.get_client_from_wrapper(job_wrapper)
            tool = job_wrapper.tool
            remote_job_config = client.setup(tool.id, tool.version)
            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )
            prepare_kwds = {}
            if rewrite_parameters:
                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )
                prepare_kwds[ 'compute_environment' ] = compute_environment
            job_wrapper.prepare( **prepare_kwds )
            self.__prepare_input_files_locally(job_wrapper)
            remote_metadata = LwrJobRunner.__remote_metadata( client )
            dependency_resolution = LwrJobRunner.__dependency_resolution( client )
            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)
            remote_command_params = dict(
                working_directory=remote_job_config['working_directory'],
                metadata_kwds=metadata_kwds,
                dependency_resolution=dependency_resolution,
            )
            command_line = build_command(
                self,
                job_wrapper=job_wrapper,
                include_metadata=remote_metadata,
                include_work_dir_outputs=False,
                remote_command_params=remote_command_params,
            )
        except Exception:
            job_wrapper.fail( ""failure preparing job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)

        # If we were able to get a command line, run the job
        if not command_line:
            job_wrapper.finish( '', '' )

        return command_line, client, remote_job_config, compute_environment

    def __prepare_input_files_locally(self, job_wrapper):
        """"""Run task splitting commands locally.""""""
        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)
        if prepare_input_files_cmds is not None:
            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files
                if 0 != os.system(cmd):
                    raise Exception('Error running file staging command: %s' % cmd)
            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line

    def get_output_files(self, job_wrapper):
        output_paths = job_wrapper.get_output_fnames()
        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.

    def get_input_files(self, job_wrapper):
        input_paths = job_wrapper.get_input_paths()
        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.

    def get_client_from_wrapper(self, job_wrapper):
        job_id = job_wrapper.job_id
        if hasattr(job_wrapper, 'task_id'):
            job_id = ""%s_%s"" % (job_id, job_wrapper.task_id)
        params = job_wrapper.job_destination.params.copy()
        for key, value in params.iteritems():
            if value:
                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )
        env = getattr( job_wrapper.job_destination, ""env"", [] )
        return self.get_client( params, job_id, env )

    def get_client_from_state(self, job_state):
        job_destination_params = job_state.job_destination.params
        job_id = job_state.job_id
        return self.get_client( job_destination_params, job_id )

    def get_client( self, job_destination_params, job_id, env=[] ):
        # Cannot use url_for outside of web thread.
        #files_endpoint = url_for( controller=""job_files"", job_id=encoded_job_id )

        encoded_job_id = self.app.security.encode_id(job_id)
        job_key = self.app.security.encode_id( job_id, kind=""jobs_files"" )
        files_endpoint = ""%s/api/jobs/%s/files?job_key=%s"" % (
            self.galaxy_url,
            encoded_job_id,
            job_key
        )
        get_client_kwds = dict(
            job_id=str( job_id ),
            files_endpoint=files_endpoint,
            env=env
        )
        return self.client_manager.get_client( job_destination_params, **get_client_kwds )

    def finish_job( self, job_state ):
        stderr = stdout = ''
        job_wrapper = job_state.job_wrapper
        try:
            client = self.get_client_from_state(job_state)
            run_results = client.full_status()

            stdout = run_results.get('stdout', '')
            stderr = run_results.get('stderr', '')
            exit_code = run_results.get('returncode', None)
            lwr_outputs = LwrOutputs.from_status_response(run_results)
            # Use LWR client code to transfer/copy files back
            # and cleanup job if needed.
            completed_normally = \
                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]
            cleanup_job = self.app.config.cleanup_job
            client_outputs = self.__client_outputs(client, job_wrapper)
            finish_args = dict( client=client,
                                job_completed_normally=completed_normally,
                                cleanup_job=cleanup_job,
                                client_outputs=client_outputs,
                                lwr_outputs=lwr_outputs )
            failed = lwr_finish_job( **finish_args )

            if failed:
                job_wrapper.fail(""Failed to find or download one or more job outputs from remote server."", exception=True)
        except Exception:
            message = ""Failed to communicate with remote job server.""
            job_wrapper.fail( message, exception=True )
            log.exception(""failure finishing job %d"" % job_wrapper.job_id)
            return
        if not LwrJobRunner.__remote_metadata( client ):
            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )
        # Finish the job
        try:
            job_wrapper.finish( stdout, stderr, exit_code )
        except Exception:
            log.exception(""Job wrapper finish method failed"")
            job_wrapper.fail(""Unable to finish job"", exception=True)

    def fail_job( self, job_state ):
        """"""
        Seperated out so we can use the worker threads for it.
        """"""
        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )
        job_state.job_wrapper.fail( job_state.fail_message )

    def check_pid( self, pid ):
        try:
            os.kill( pid, 0 )
            return True
        except OSError, e:
            if e.errno == errno.ESRCH:
                log.debug( ""check_pid(): PID %d is dead"" % pid )
            else:
                log.warning( ""check_pid(): Got errno %s when attempting to check PID %d: %s"" % ( errno.errorcode[e.errno], pid, e.strerror ) )
            return False

    def stop_job( self, job ):
        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished
        job_ext_output_metadata = job.get_external_output_metadata()
        if job_ext_output_metadata:
            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them
            if pid in [ None, '' ]:
                log.warning( ""stop_job(): %s: no PID in database for job, unable to stop"" % job.id )
                return
            pid = int( pid )
            if not self.check_pid( pid ):
                log.warning( ""stop_job(): %s: PID %d was already dead or can't be signaled"" % ( job.id, pid ) )
                return
            for sig in [ 15, 9 ]:
                try:
                    os.killpg( pid, sig )
                except OSError, e:
                    log.warning( ""stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s"" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )
                    return  # give up
                sleep( 2 )
                if not self.check_pid( pid ):
                    log.debug( ""stop_job(): %s: PID %d successfully killed with signal %d"" % ( job.id, pid, sig ) )
                    return
                else:
                    log.warning( ""stop_job(): %s: PID %d refuses to die after signaling TERM/KILL"" % ( job.id, pid ) )
        else:
            # Remote kill
            lwr_url = job.job_runner_name
            job_id = job.job_runner_external_id
            log.debug(""Attempt remote lwr kill of job with url %s and id %s"" % (lwr_url, job_id))
            client = self.get_client(job.destination_params, job_id)
            client.kill()

    def recover( self, job, job_wrapper ):
        """"""Recovers jobs stuck in the queued/running state when Galaxy started""""""
        job_state = AsynchronousJobState()
        job_state.job_id = str( job.get_job_runner_external_id() )
        job_state.runner_url = job_wrapper.get_job_runner_url()
        job_state.job_destination = job_wrapper.job_destination
        job_wrapper.command_line = job.get_command_line()
        job_state.job_wrapper = job_wrapper
        state = job.get_state()
        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:
            log.debug( ""(LWR/%s) is still in running state, adding to the LWR queue"" % ( job.get_id()) )
            job_state.old_state = True
            job_state.running = state == model.Job.states.RUNNING
            self.monitor_queue.put( job_state )

    def shutdown( self ):
        super( LwrJobRunner, self ).shutdown()
        self.client_manager.shutdown()

    def __client_outputs( self, client, job_wrapper ):
        work_dir_outputs = self.get_work_dir_outputs( job_wrapper )
        output_files = self.get_output_files( job_wrapper )
        client_outputs = ClientOutputs(
            working_directory=job_wrapper.working_directory,
            work_dir_outputs=work_dir_outputs,
            output_files=output_files,
            version_file=job_wrapper.get_version_string_path(),
        )
        return client_outputs

    @staticmethod
    def __dependencies_description( lwr_client, job_wrapper ):
        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )
        remote_dependency_resolution = dependency_resolution == ""remote""
        if not remote_dependency_resolution:
            return None
        requirements = job_wrapper.tool.requirements or []
        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []
        return dependencies.DependenciesDescription(
            requirements=requirements,
            installed_tool_dependencies=installed_tool_dependencies,
        )

    @staticmethod
    def __dependency_resolution( lwr_client ):
        dependency_resolution = lwr_client.destination_params.get( ""dependency_resolution"", ""local"" )
        if dependency_resolution not in [""none"", ""local"", ""remote""]:
            raise Exception(""Unknown dependency_resolution value encountered %s"" % dependency_resolution)
        return dependency_resolution

    @staticmethod
    def __remote_metadata( lwr_client ):
        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( ""remote_metadata"", False ) )
        return remote_metadata

    @staticmethod
    def __use_remote_datatypes_conf( lwr_client ):
        """""" When setting remote metadata, use integrated datatypes from this
        Galaxy instance or use the datatypes config configured via the remote
        LWR.

        Both options are broken in different ways for same reason - datatypes
        may not match. One can push the local datatypes config to the remote
        server - but there is no guarentee these datatypes will be defined
        there. Alternatively, one can use the remote datatype config - but
        there is no guarentee that it will contain all the datatypes available
        to this Galaxy.
        """"""
        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( ""use_remote_datatypes"", False ) )
        return use_remote_datatypes

    @staticmethod
    def __rewrite_parameters( lwr_client ):
        return string_as_bool_or_none( lwr_client.destination_params.get( ""rewrite_parameters"", False ) ) or False

    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):
        metadata_kwds = {}
        if remote_metadata:
            remote_system_properties = remote_job_config.get(""system_properties"", {})
            remote_galaxy_home = remote_system_properties.get(""galaxy_home"", None)
            if not remote_galaxy_home:
                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)
            metadata_kwds['exec_dir'] = remote_galaxy_home
            outputs_directory = remote_job_config['outputs_directory']
            configs_directory = remote_job_config['configs_directory']
            working_directory = remote_job_config['working_directory']
            # For metadata calculation, we need to build a list of of output
            # file objects with real path indicating location on Galaxy server
            # and false path indicating location on compute server. Since the
            # LWR disables from_work_dir copying as part of the job command
            # line we need to take the list of output locations on the LWR
            # server (produced by self.get_output_files(job_wrapper)) and for
            # each work_dir output substitute the effective path on the LWR
            # server relative to the remote working directory as the
            # false_path to send the metadata command generation module.
            work_dir_outputs = self.get_work_dir_outputs(job_wrapper, job_working_directory=working_directory)
            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]
            for output in outputs:
                for lwr_workdir_path, real_path in work_dir_outputs:
                    if real_path == output.real_path:
                        output.false_path = lwr_workdir_path
            metadata_kwds['output_fnames'] = outputs
            metadata_kwds['compute_tmp_dir'] = working_directory
            metadata_kwds['config_root'] = remote_galaxy_home
            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')
            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)
            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)
            if LwrJobRunner.__use_remote_datatypes_conf( client ):
                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)
                if not remote_datatypes_config:
                    log.warn(NO_REMOTE_DATATYPES_CONFIG)
                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')
                metadata_kwds['datatypes_config'] = remote_datatypes_config
            else:
                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs
                # Ensure this file gets pushed out to the remote config dir.
                job_wrapper.extra_filenames.append(integrates_datatypes_config)

                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))
        return metadata_kwds


class LwrComputeEnvironment( ComputeEnvironment ):

    def __init__( self, lwr_client, job_wrapper, remote_job_config ):
        self.lwr_client = lwr_client
        self.job_wrapper = job_wrapper
        self.local_path_config = job_wrapper.default_compute_environment()
        self.unstructured_path_rewrites = {}
        # job_wrapper.prepare is going to expunge the job backing the following
        # computations, so precalculate these paths.
        self._wrapper_input_paths = self.local_path_config.input_paths()
        self._wrapper_output_paths = self.local_path_config.output_paths()
        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())
        self._config_directory = remote_job_config[ ""configs_directory"" ]
        self._working_directory = remote_job_config[ ""working_directory"" ]
        self._sep = remote_job_config[ ""system_properties"" ][ ""separator"" ]
        self._tool_dir = remote_job_config[ ""tools_directory"" ]
        version_path = self.local_path_config.version_path()
        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)
        if new_version_path:
            version_path = new_version_path
        self._version_path = version_path

    def output_paths( self ):
        local_output_paths = self._wrapper_output_paths

        results = []
        for local_output_path in local_output_paths:
            wrapper_path = str( local_output_path )
            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_output_path, remote_path ) )
        return results

    def input_paths( self ):
        local_input_paths = self._wrapper_input_paths

        results = []
        for local_input_path in local_input_paths:
            wrapper_path = str( local_input_path )
            # This will over-copy in some cases. For instance in the case of task
            # splitting, this input will be copied even though only the work dir
            # input will actually be used.
            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_input_path, remote_path ) )
        return results

    def _dataset_path( self, local_dataset_path, remote_path ):
        remote_extra_files_path = None
        if remote_path:
            remote_extra_files_path = ""%s_files"" % remote_path[ 0:-len( "".dat"" ) ]
        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )

    def working_directory( self ):
        return self._working_directory

    def config_directory( self ):
        return self._config_directory

    def new_file_path( self ):
        return self.working_directory()  # Problems with doing this?

    def sep( self ):
        return self._sep

    def version_path( self ):
        return self._version_path

    def rewriter( self, parameter_value ):
        unstructured_path_rewrites = self.unstructured_path_rewrites
        if parameter_value in unstructured_path_rewrites:
            # Path previously mapped, use previous mapping.
            return unstructured_path_rewrites[ parameter_value ]
        if parameter_value in unstructured_path_rewrites.itervalues():
            # Path is a rewritten remote path (this might never occur,
            # consider dropping check...)
            return parameter_value

        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )
        if rewrite:
            unstructured_path_rewrites.update(new_unstructured_path_rewrites)
            return rewrite
        else:
            # Did need to rewrite, use original path or value.
            return parameter_value

    def unstructured_path_rewriter( self ):
        return self.rewriter
/n/n/n",0
69,69,22e3ab28b73a4de7a2a065d657b017ccbac352d8,"/lib/galaxy/jobs/runners/lwr.py/n/nimport logging

from galaxy import model
from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner
from galaxy.jobs import ComputeEnvironment
from galaxy.jobs import JobDestination
from galaxy.jobs.command_factory import build_command
from galaxy.tools.deps import dependencies
from galaxy.util import string_as_bool_or_none
from galaxy.util.bunch import Bunch

import errno
from time import sleep
import os

from .lwr_client import build_client_manager
from .lwr_client import url_to_destination_params
from .lwr_client import finish_job as lwr_finish_job
from .lwr_client import submit_job as lwr_submit_job
from .lwr_client import ClientJobDescription
from .lwr_client import LwrOutputs
from .lwr_client import ClientOutputs
from .lwr_client import PathMapper

log = logging.getLogger( __name__ )

__all__ = [ 'LwrJobRunner' ]

NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = ""LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory.""
NO_REMOTE_DATATYPES_CONFIG = ""LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml.""

# Is there a good way to infer some default for this? Can only use
# url_for from web threads. https://gist.github.com/jmchilton/9098762
DEFAULT_GALAXY_URL = ""http://localhost:8080""


class LwrJobRunner( AsynchronousJobRunner ):
    """"""
    LWR Job Runner
    """"""
    runner_name = ""LWRRunner""

    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):
        """"""Start the job runner """"""
        super( LwrJobRunner, self ).__init__( app, nworkers )
        self.async_status_updates = dict()
        self._init_monitor_thread()
        self._init_worker_threads()
        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), ""url"": url}
        self.galaxy_url = galaxy_url
        self.client_manager = build_client_manager(**client_manager_kwargs)

    def url_to_destination( self, url ):
        """"""Convert a legacy URL to a job destination""""""
        return JobDestination( runner=""lwr"", params=url_to_destination_params( url ) )

    def check_watched_item(self, job_state):
        try:
            client = self.get_client_from_state(job_state)

            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):
                # Message queue implementation.

                # TODO: Very hacky now, refactor after Dannon merges in his
                # message queue work, runners need the ability to disable
                # check_watched_item like this and instead a callback needs to
                # be issued post job recovery allowing a message queue
                # consumer to be setup.
                self.client_manager.ensure_has_status_update_callback(self.__async_update)
                return job_state

            status = client.get_status()
        except Exception:
            # An orphaned job was put into the queue at app startup, so remote server went down
            # either way we are done I guess.
            self.mark_as_finished(job_state)
            return None
        job_state = self.__update_job_state_for_lwr_status(job_state, status)
        return job_state

    def __update_job_state_for_lwr_status(self, job_state, lwr_status):
        if lwr_status == ""complete"":
            self.mark_as_finished(job_state)
            return None
        if lwr_status == ""running"" and not job_state.running:
            job_state.running = True
            job_state.job_wrapper.change_state( model.Job.states.RUNNING )
        return job_state

    def __async_update( self, full_status ):
        job_id = full_status[ ""job_id"" ]
        job_state = self.__find_watched_job( job_id )
        if not job_state:
            # Probably finished too quickly, sleep and try again.
            # Kind of a hack, why does monitor queue need to no wait
            # get and sleep instead of doing a busy wait that would
            # respond immediately.
            sleep( 2 )
            job_state = self.__find_watched_job( job_id )
        if not job_state:
            log.warn( ""Failed to find job corresponding to final status %s in %s"" % ( full_status, self.watched ) )
        else:
            self.__update_job_state_for_lwr_status(job_state, full_status[""status""])

    def __find_watched_job( self, job_id ):
        found_job = None
        for async_job_state in self.watched:
            if str( async_job_state.job_id ) == job_id:
                found_job = async_job_state
                break
        return found_job

    def queue_job(self, job_wrapper):
        job_destination = job_wrapper.job_destination

        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )

        if not command_line:
            return

        try:
            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )
            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )
            unstructured_path_rewrites = {}
            if compute_environment:
                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites

            client_job_description = ClientJobDescription(
                command_line=command_line,
                input_files=self.get_input_files(job_wrapper),
                client_outputs=self.__client_outputs(client, job_wrapper),
                working_directory=job_wrapper.working_directory,
                tool=job_wrapper.tool,
                config_files=job_wrapper.extra_filenames,
                dependencies_description=dependencies_description,
                env=client.env,
                rewrite_paths=rewrite_paths,
                arbitrary_files=unstructured_path_rewrites,
            )
            job_id = lwr_submit_job(client, client_job_description, remote_job_config)
            log.info(""lwr job submitted with job_id %s"" % job_id)
            job_wrapper.set_job_destination( job_destination, job_id )
            job_wrapper.change_state( model.Job.states.QUEUED )
        except Exception:
            job_wrapper.fail( ""failure running job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)
            return

        lwr_job_state = AsynchronousJobState()
        lwr_job_state.job_wrapper = job_wrapper
        lwr_job_state.job_id = job_id
        lwr_job_state.old_state = True
        lwr_job_state.running = False
        lwr_job_state.job_destination = job_destination
        self.monitor_job(lwr_job_state)

    def __prepare_job(self, job_wrapper, job_destination):
        """""" Build command-line and LWR client for this job. """"""
        command_line = None
        client = None
        remote_job_config = None
        compute_environment = None
        try:
            client = self.get_client_from_wrapper(job_wrapper)
            tool = job_wrapper.tool
            remote_job_config = client.setup(tool.id, tool.version)
            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )
            prepare_kwds = {}
            if rewrite_parameters:
                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )
                prepare_kwds[ 'compute_environment' ] = compute_environment
            job_wrapper.prepare( **prepare_kwds )
            self.__prepare_input_files_locally(job_wrapper)
            remote_metadata = LwrJobRunner.__remote_metadata( client )
            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
            dependency_resolution = LwrJobRunner.__dependency_resolution( client )
            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)
            remote_command_params = dict(
                working_directory=remote_job_config['working_directory'],
                metadata_kwds=metadata_kwds,
                dependency_resolution=dependency_resolution,
            )
            command_line = build_command(
                self,
                job_wrapper=job_wrapper,
                include_metadata=remote_metadata,
                include_work_dir_outputs=remote_work_dir_copy,
                remote_command_params=remote_command_params,
            )
        except Exception:
            job_wrapper.fail( ""failure preparing job"", exception=True )
            log.exception(""failure running job %d"" % job_wrapper.job_id)

        # If we were able to get a command line, run the job
        if not command_line:
            job_wrapper.finish( '', '' )

        return command_line, client, remote_job_config, compute_environment

    def __prepare_input_files_locally(self, job_wrapper):
        """"""Run task splitting commands locally.""""""
        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)
        if prepare_input_files_cmds is not None:
            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files
                if 0 != os.system(cmd):
                    raise Exception('Error running file staging command: %s' % cmd)
            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line

    def get_output_files(self, job_wrapper):
        output_paths = job_wrapper.get_output_fnames()
        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.

    def get_input_files(self, job_wrapper):
        input_paths = job_wrapper.get_input_paths()
        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.

    def get_client_from_wrapper(self, job_wrapper):
        job_id = job_wrapper.job_id
        if hasattr(job_wrapper, 'task_id'):
            job_id = ""%s_%s"" % (job_id, job_wrapper.task_id)
        params = job_wrapper.job_destination.params.copy()
        for key, value in params.iteritems():
            if value:
                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )
        env = getattr( job_wrapper.job_destination, ""env"", [] )
        return self.get_client( params, job_id, env )

    def get_client_from_state(self, job_state):
        job_destination_params = job_state.job_destination.params
        job_id = job_state.job_id
        return self.get_client( job_destination_params, job_id )

    def get_client( self, job_destination_params, job_id, env=[] ):
        # Cannot use url_for outside of web thread.
        #files_endpoint = url_for( controller=""job_files"", job_id=encoded_job_id )

        encoded_job_id = self.app.security.encode_id(job_id)
        job_key = self.app.security.encode_id( job_id, kind=""jobs_files"" )
        files_endpoint = ""%s/api/jobs/%s/files?job_key=%s"" % (
            self.galaxy_url,
            encoded_job_id,
            job_key
        )
        get_client_kwds = dict(
            job_id=str( job_id ),
            files_endpoint=files_endpoint,
            env=env
        )
        return self.client_manager.get_client( job_destination_params, **get_client_kwds )

    def finish_job( self, job_state ):
        stderr = stdout = ''
        job_wrapper = job_state.job_wrapper
        try:
            client = self.get_client_from_state(job_state)
            run_results = client.full_status()

            stdout = run_results.get('stdout', '')
            stderr = run_results.get('stderr', '')
            exit_code = run_results.get('returncode', None)
            lwr_outputs = LwrOutputs.from_status_response(run_results)
            # Use LWR client code to transfer/copy files back
            # and cleanup job if needed.
            completed_normally = \
                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]
            cleanup_job = self.app.config.cleanup_job
            client_outputs = self.__client_outputs(client, job_wrapper)
            finish_args = dict( client=client,
                                job_completed_normally=completed_normally,
                                cleanup_job=cleanup_job,
                                client_outputs=client_outputs,
                                lwr_outputs=lwr_outputs )
            failed = lwr_finish_job( **finish_args )

            if failed:
                job_wrapper.fail(""Failed to find or download one or more job outputs from remote server."", exception=True)
        except Exception:
            message = ""Failed to communicate with remote job server.""
            job_wrapper.fail( message, exception=True )
            log.exception(""failure finishing job %d"" % job_wrapper.job_id)
            return
        if not LwrJobRunner.__remote_metadata( client ):
            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )
        # Finish the job
        try:
            job_wrapper.finish( stdout, stderr, exit_code )
        except Exception:
            log.exception(""Job wrapper finish method failed"")
            job_wrapper.fail(""Unable to finish job"", exception=True)

    def fail_job( self, job_state ):
        """"""
        Seperated out so we can use the worker threads for it.
        """"""
        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )
        job_state.job_wrapper.fail( job_state.fail_message )

    def check_pid( self, pid ):
        try:
            os.kill( pid, 0 )
            return True
        except OSError, e:
            if e.errno == errno.ESRCH:
                log.debug( ""check_pid(): PID %d is dead"" % pid )
            else:
                log.warning( ""check_pid(): Got errno %s when attempting to check PID %d: %s"" % ( errno.errorcode[e.errno], pid, e.strerror ) )
            return False

    def stop_job( self, job ):
        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished
        job_ext_output_metadata = job.get_external_output_metadata()
        if job_ext_output_metadata:
            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them
            if pid in [ None, '' ]:
                log.warning( ""stop_job(): %s: no PID in database for job, unable to stop"" % job.id )
                return
            pid = int( pid )
            if not self.check_pid( pid ):
                log.warning( ""stop_job(): %s: PID %d was already dead or can't be signaled"" % ( job.id, pid ) )
                return
            for sig in [ 15, 9 ]:
                try:
                    os.killpg( pid, sig )
                except OSError, e:
                    log.warning( ""stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s"" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )
                    return  # give up
                sleep( 2 )
                if not self.check_pid( pid ):
                    log.debug( ""stop_job(): %s: PID %d successfully killed with signal %d"" % ( job.id, pid, sig ) )
                    return
                else:
                    log.warning( ""stop_job(): %s: PID %d refuses to die after signaling TERM/KILL"" % ( job.id, pid ) )
        else:
            # Remote kill
            lwr_url = job.job_runner_name
            job_id = job.job_runner_external_id
            log.debug(""Attempt remote lwr kill of job with url %s and id %s"" % (lwr_url, job_id))
            client = self.get_client(job.destination_params, job_id)
            client.kill()

    def recover( self, job, job_wrapper ):
        """"""Recovers jobs stuck in the queued/running state when Galaxy started""""""
        job_state = AsynchronousJobState()
        job_state.job_id = str( job.get_job_runner_external_id() )
        job_state.runner_url = job_wrapper.get_job_runner_url()
        job_state.job_destination = job_wrapper.job_destination
        job_wrapper.command_line = job.get_command_line()
        job_state.job_wrapper = job_wrapper
        state = job.get_state()
        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:
            log.debug( ""(LWR/%s) is still in running state, adding to the LWR queue"" % ( job.get_id()) )
            job_state.old_state = True
            job_state.running = state == model.Job.states.RUNNING
            self.monitor_queue.put( job_state )

    def shutdown( self ):
        super( LwrJobRunner, self ).shutdown()
        self.client_manager.shutdown()

    def __client_outputs( self, client, job_wrapper ):
        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
        if not remote_work_dir_copy:
            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )
        else:
            # They have already been copied over to look like regular outputs remotely,
            # no need to handle them differently here.
            work_dir_outputs = []
        output_files = self.get_output_files( job_wrapper )
        client_outputs = ClientOutputs(
            working_directory=job_wrapper.working_directory,
            work_dir_outputs=work_dir_outputs,
            output_files=output_files,
            version_file=job_wrapper.get_version_string_path(),
        )
        return client_outputs

    @staticmethod
    def __dependencies_description( lwr_client, job_wrapper ):
        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )
        remote_dependency_resolution = dependency_resolution == ""remote""
        if not remote_dependency_resolution:
            return None
        requirements = job_wrapper.tool.requirements or []
        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []
        return dependencies.DependenciesDescription(
            requirements=requirements,
            installed_tool_dependencies=installed_tool_dependencies,
        )

    @staticmethod
    def __dependency_resolution( lwr_client ):
        dependency_resolution = lwr_client.destination_params.get( ""dependency_resolution"", ""local"" )
        if dependency_resolution not in [""none"", ""local"", ""remote""]:
            raise Exception(""Unknown dependency_resolution value encountered %s"" % dependency_resolution)
        return dependency_resolution

    @staticmethod
    def __remote_metadata( lwr_client ):
        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( ""remote_metadata"", False ) )
        return remote_metadata

    @staticmethod
    def __remote_work_dir_copy( lwr_client ):
        # Right now remote metadata handling assumes from_work_dir outputs
        # have been copied over before it runs. So do that remotely. This is
        # not the default though because adding it to the command line is not
        # cross-platform (no cp on Windows) and it's un-needed work outside
        # the context of metadata settting (just as easy to download from
        # either place.)
        return LwrJobRunner.__remote_metadata( lwr_client )

    @staticmethod
    def __use_remote_datatypes_conf( lwr_client ):
        """""" When setting remote metadata, use integrated datatypes from this
        Galaxy instance or use the datatypes config configured via the remote
        LWR.

        Both options are broken in different ways for same reason - datatypes
        may not match. One can push the local datatypes config to the remote
        server - but there is no guarentee these datatypes will be defined
        there. Alternatively, one can use the remote datatype config - but
        there is no guarentee that it will contain all the datatypes available
        to this Galaxy.
        """"""
        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( ""use_remote_datatypes"", False ) )
        return use_remote_datatypes

    @staticmethod
    def __rewrite_parameters( lwr_client ):
        return string_as_bool_or_none( lwr_client.destination_params.get( ""rewrite_parameters"", False ) ) or False

    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):
        metadata_kwds = {}
        if remote_metadata:
            remote_system_properties = remote_job_config.get(""system_properties"", {})
            remote_galaxy_home = remote_system_properties.get(""galaxy_home"", None)
            if not remote_galaxy_home:
                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)
            metadata_kwds['exec_dir'] = remote_galaxy_home
            outputs_directory = remote_job_config['outputs_directory']
            configs_directory = remote_job_config['configs_directory']
            working_directory = remote_job_config['working_directory']
            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]
            metadata_kwds['output_fnames'] = outputs
            metadata_kwds['compute_tmp_dir'] = working_directory
            metadata_kwds['config_root'] = remote_galaxy_home
            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')
            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)
            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)
            if LwrJobRunner.__use_remote_datatypes_conf( client ):
                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)
                if not remote_datatypes_config:
                    log.warn(NO_REMOTE_DATATYPES_CONFIG)
                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')
                metadata_kwds['datatypes_config'] = remote_datatypes_config
            else:
                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs
                # Ensure this file gets pushed out to the remote config dir.
                job_wrapper.extra_filenames.append(integrates_datatypes_config)

                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))
        return metadata_kwds


class LwrComputeEnvironment( ComputeEnvironment ):

    def __init__( self, lwr_client, job_wrapper, remote_job_config ):
        self.lwr_client = lwr_client
        self.job_wrapper = job_wrapper
        self.local_path_config = job_wrapper.default_compute_environment()
        self.unstructured_path_rewrites = {}
        # job_wrapper.prepare is going to expunge the job backing the following
        # computations, so precalculate these paths.
        self._wrapper_input_paths = self.local_path_config.input_paths()
        self._wrapper_output_paths = self.local_path_config.output_paths()
        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())
        self._config_directory = remote_job_config[ ""configs_directory"" ]
        self._working_directory = remote_job_config[ ""working_directory"" ]
        self._sep = remote_job_config[ ""system_properties"" ][ ""separator"" ]
        self._tool_dir = remote_job_config[ ""tools_directory"" ]
        version_path = self.local_path_config.version_path()
        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)
        if new_version_path:
            version_path = new_version_path
        self._version_path = version_path

    def output_paths( self ):
        local_output_paths = self._wrapper_output_paths

        results = []
        for local_output_path in local_output_paths:
            wrapper_path = str( local_output_path )
            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_output_path, remote_path ) )
        return results

    def input_paths( self ):
        local_input_paths = self._wrapper_input_paths

        results = []
        for local_input_path in local_input_paths:
            wrapper_path = str( local_input_path )
            # This will over-copy in some cases. For instance in the case of task
            # splitting, this input will be copied even though only the work dir
            # input will actually be used.
            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_input_path, remote_path ) )
        return results

    def _dataset_path( self, local_dataset_path, remote_path ):
        remote_extra_files_path = None
        if remote_path:
            remote_extra_files_path = ""%s_files"" % remote_path[ 0:-len( "".dat"" ) ]
        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )

    def working_directory( self ):
        return self._working_directory

    def config_directory( self ):
        return self._config_directory

    def new_file_path( self ):
        return self.working_directory()  # Problems with doing this?

    def sep( self ):
        return self._sep

    def version_path( self ):
        return self._version_path

    def rewriter( self, parameter_value ):
        unstructured_path_rewrites = self.unstructured_path_rewrites
        if parameter_value in unstructured_path_rewrites:
            # Path previously mapped, use previous mapping.
            return unstructured_path_rewrites[ parameter_value ]
        if parameter_value in unstructured_path_rewrites.itervalues():
            # Path is a rewritten remote path (this might never occur,
            # consider dropping check...)
            return parameter_value

        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )
        if rewrite:
            unstructured_path_rewrites.update(new_unstructured_path_rewrites)
            return rewrite
        else:
            # Did need to rewrite, use original path or value.
            return parameter_value

    def unstructured_path_rewriter( self ):
        return self.rewriter
/n/n/n",1
70,70,7ff203be36e439b535894764c37a8446351627ec,"lib/Shine/Commands/Base/Command.py/n/n# Command.py -- Base command class
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Support.Debug import Debug

from CommandRCDefs import *

import getopt


#
# Command exceptions are defined in Shine.Command.Exceptions
#

class Command(object):
    """"""
    The base class for command objects that can be added to the commands
    registry.
    """"""
    def __init__(self):
        self.options = {}
        self.getopt_string = """"
        self.params_desc = """"
        self.last_optional = 0
        self.arguments = None

        # All commands have debug support.
        self.debug_support = Debug(self)

    def is_hidden(self):
        """"""Return whether the command should not be displayed to user.""""""
        return False
    
    def get_name(self):
        raise NotImplementedError(""Derived classes must implement."")

    def get_desc(self):
        return ""Undocumented""

    def get_params_desc(self):
        pdesc = self.params_desc.strip()
        if self.has_subcommand():
            return ""%s %s"" % ('|'.join(self.get_subcommands()), pdesc)
        return pdesc

    def has_subcommand(self):
        """"""Return whether the command supports subcommand(s).""""""
        return False

    def get_subcommands(self):
        """"""Return the list of subcommand(s).""""""
        raise NotImplementedError(""Derived classes must implement."")
    
    def add_option(self, flag, arg, attr, cb=None):
        """"""
        Add an option for getopt with optional argument.
        """"""
        assert flag not in self.options

        optional = attr.get('optional', False)
        hidden = attr.get('hidden', False)

        if cb:
            self.options[flag] = cb

        object.__setattr__(self, ""opt_%s"" % flag, None)
            
        self.getopt_string += flag
        if optional:
            leftmark = '['
            rightmark = ']'
        else:
            leftmark = ''
            rightmark = ''

        if arg:
            self.getopt_string += "":""
            if not hidden:
                self.params_desc += ""%s-%s <%s>%s "" % (leftmark,
                    flag, arg, rightmark)
                self.last_optional = 0
        elif not hidden:
            if self.last_optional == 0:
                self.params_desc += ""%s-%s%s "" % (leftmark, flag, rightmark)
            else:
                self.params_desc = self.params_desc[:-2] + ""%s%s "" % (flag,
                    rightmark)
            
            if optional:
                self.last_optional = 1
            else:
                self.last_optional = 2

    def parse(self, args):
        """"""
        Parse command arguments.
        """"""
        options, arguments = getopt.gnu_getopt(args, self.getopt_string)
        self.arguments = arguments

        for opt, arg in options:
            trim_opt = opt[1:]
            callback = self.options.get(trim_opt)
            if callback:
                callback(trim_opt, arg)
            object.__setattr__(self, ""opt_%s"" % trim_opt, arg or True)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation.
        
        Return True when the user confirms the action, False otherwise.
        """"""
        i = raw_input(""%s (y)es/(N)o: "" % prompt)
        return i == 'y' or i == 'Y'


    def filter_rc(self, rc):
        """"""
        Allow derived classes to filter return codes.
        """"""
        # default is to not filter return code
        return rc

/n/n/nlib/Shine/Commands/Base/RemoteCommand.py/n/n# RemoteCommand.py -- Base command with remote capabilities
# Copyright (C) 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *
from Command import Command
from CommandRCDefs import *
from RemoteCallEventHandler import RemoteCallEventHandler
from Support.Nodes import Nodes
from Support.Yes import Yes

import socket


class RemoteCommand(Command):
    
    def __init__(self):
        Command.__init__(self)
        self.remote_call = False
        self.local_flag = False
        attr = { 'optional' : True, 'hidden' : True }
        self.add_option('L', None, attr, cb=self.parse_L)
        self.add_option('R', None, attr, cb=self.parse_R)
        self.nodes_support = Nodes(self)
        self.eventhandler = None

    def parse_L(self, opt, arg):
        self.local_flag = True

    def parse_R(self, opt, arg):
        self.remote_call = True

    def has_local_flag(self):
        return self.local_flag or self.remote_call

    def init_execute(self):
        """"""
        Initialize execution of remote command, if needed. Should be called
        first from derived classes before really executing the command.
        """"""
        # Limit the scope of the command if called with local flag (-L) or
        # called remotely (-R).
        if self.has_local_flag():
            self.opt_n = socket.gethostname().split('.', 1)[0]

    def install_eventhandler(self, local_eventhandler, global_eventhandler):
        """"""
        Select and install the appropriate event handler.
        """"""
        if self.remote_call:
            # When called remotely (-R), install a special event handler
            # that knows how to speak the Shine Proxy Protocol using pickle.
            self.eventhandler = RemoteCallEventHandler()
        elif self.local_flag:
            self.eventhandler = local_eventhandler
        else:
            self.eventhandler = global_eventhandler
        # return handler for convenience
        return self.eventhandler

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation. Overrides Command.ask_confirm to
        avoid confirmation when called remotely (-R).

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.remote_call or Command.ask_confirm(self, prompt)

    def filter_rc(self, rc):
        """"""
        When called remotely, return code are not used to handle shine action
        success or failure, nor for status info. To properly detect ssh or remote
        shine installation failures, we filter the return code here.
        """"""
        if self.remote_call:
            # Only errors of type RUNTIME ERROR are allowed to go up.
            rc &= RC_FLAG_RUNTIME_ERROR

        return Command.filter_rc(self, rc)


class RemoteCriticalCommand(RemoteCommand):

    def __init__(self):
        RemoteCommand.__init__(self)
        self.yes_support = Yes(self)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation if -y not specified.

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.yes_support.has_yes() or RemoteCommand.ask_confirm(self, prompt)

/n/n/nlib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        rc = command.execute()

        # Filter rc
        return command.filter_rc(rc)

/n/n/nlib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.CommandRCDefs import *
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes

from Exceptions import *

class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            raise CommandHelpException(""Lustre model file path (-m <model_file>) argument required."", self)
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return RC_OK

/n/n/nlib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Starting %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if not self.remote_call:
                if rc == RC_OK:
                    if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                        print ""Mount successful on %s"" % m_nodes
                elif rc == RC_RUNTIME_ERROR:
                    for nodes, msg in fs.proxy_errors:
                        print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.CommandRCDefs import *
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError %s"" % ex
            return RC_RUNTIME_ERROR

        return RC_OK
/n/n/nlib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

        return result
/n/n/nlib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)
            if not statusdict:
                continue

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if not self.remote_call and vlevel > 0:
                if view == ""fs"":
                    self.status_view_fs(fs)
                elif view.startswith(""target""):
                    self.status_view_targets(fs)
                elif view.startswith(""disk""):
                    self.status_view_disks(fs)
                else:
                    raise CommandBadParameterError(self.view_support.get_view(),
                            ""fs, targets, disks"")

        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/nlib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Stopping %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                    print ""Unmount successful on %s"" % m_nodes
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Configuration/FileSystem.py/n/n# FileSystem.py -- Lustre file system configuration
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$


from Globals import Globals
from Model import Model
from Exceptions import *
from TuningModel import TuningModel

from ClusterShell.NodeSet import NodeSet

from NidMap import NidMap
from TargetDevice import TargetDevice

import copy
import os
import sys


class FileSystem(Model):
    """"""
    Lustre File System Configuration class.
    """"""
    def __init__(self, fs_name=None, lmf=None, tuning_file=None):
        """""" Initialize File System config
        """"""
        self.backend = None

        globals = Globals()

        fs_conf_dir = os.path.expandvars(globals.get_conf_dir())
        fs_conf_dir = os.path.normpath(fs_conf_dir)

        # Load the file system from model or extended model
        if not fs_name and lmf:
            Model.__init__(self, lmf)

            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, self.get_one('fs_name'))

            self._setup_target_devices()

            # Reload
            self.set_filename(self.xmf_path)

        elif fs_name:
            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, fs_name)
            Model.__init__(self, self.xmf_path)

        self._setup_nid_map(self.get_one('nid_map'))

        self.fs_name = self.get_one('fs_name')
        
        # Initialize the tuning model to None if no special tuning configuration
        # is provided
        self.tuning_model = None
        
        if tuning_file:
            # It a tuning configuration file is provided load it
            self.tuning_model = TuningModel(tuning_file)
        else:
            self.tuning_model = TuningModel()

        #self._start_backend()

    def _start_backend(self):
        """"""
        Load and start backend subsystem once
        """"""
        if not self.backend:

            from Backend.BackendRegistry import BackendRegistry
            from Backend.Backend import Backend

            # Start the selected config backend system.
            self.backend = BackendRegistry().get_selected()
            if self.backend:
                self.backend.start()

        return self.backend

    def _setup_target_devices(self):
        """""" Generate the eXtended Model File XMF
        """"""
        self._start_backend()

        for target in [ 'mgt', 'mdt', 'ost' ]:

            if self.backend:

                # Returns a list of TargetDevices
                candidates = copy.copy(self.backend.get_target_devices(target))

                try:
                    # Save the model target selection
                    target_models = copy.copy(self.get(target))
                except KeyError, e:
                    raise ConfigException(""No %s target found"" %(target))

                # Delete it (to be replaced... see below)
                self.delete(target)
                 
                # Iterates on ModelDevices
                i = 0
                for target_model in target_models:
                    result = target_model.match_device(candidates)
                    if len(result) == 0 and not target == 'mgt' :
                        raise ConfigDeviceNotFoundError(target_model)
                    for matching in result:
                        candidates.remove(matching)
                        #
                        # target index is now mandatory in XMF files
                        if not matching.has_index():
                            matching.add_index(i)
                            i += 1

                        # `matching' is a TargetDevice, we want to add it to the
                        # underlying Model object. The current way to do this to
                        # create a configuration line string (performed by
                        # TargetDevice.getline()) and then call Model.add(). 
                        # TODO: add methods to Model/ModelDevice to avoid the use
                        #       of temporary configuration string line.
                        self.add(target, matching.getline())
            else:
                # no backend support

                devices = copy.copy(self.get_with_dict(target))

                self.delete(target)

                target_devices = []
                i = 0
                for dict in devices:
                    t = TargetDevice(target, dict)
                    if not t.has_index():
                        t.add_index(i)
                        i += 1
                    target_devices.append(TargetDevice(target, dict))
                    self.add(target, t.getline())

                if len(target_devices) == 0:
                    raise ConfigDeviceNotFoundError(self)




        # Save XMF
        self.save(self.xmf_path, ""Shine Lustre file system config file for %s"" % \
                self.get_one('fs_name'))
            
    def _setup_nid_map(self, maps):
        """"""
        Set self.nid_map using the NidMap helper class
        """"""
        #self.nid_map = NidMap().fromlist(maps)
        self.nid_map = NidMap(maps.get_one('nodes'), maps.get_one('nids'))

    def get_nid(self, node):
        try:
            return self.nid_map[node]
        except KeyError:
            raise ConfigException(""Cannot get NID for %s, aborting. Please verify `nid_map' configuration."" % node)

    def __str__(self):
        return "">> BACKEND:\n%s\n>> MODEL:\n%s"" % (self.backend, Model.__str__(self))

    def close(self):
        if self.backend:
            self.backend.stop()
            self.backend = None
    
    def register_client(self, node):
        """"""
        This function aims to register a new client that will be able to mount the
        file system.
        Parameters:
        @type node: string
        @param node : is the new client node name
        """"""
        if self._start_backend():
            self.backend.register_client(self.fs_name, node)
        
    def unregister_client(self, node):
        """"""
        This function aims to unregister a client of this  file system
        Parameters:
        @type node: string
        @param node : is name of the client node to unregister
        """"""
        if self._start_backend():
            self.backend.unregister_client(self.fs_name, node)
    
    def set_status_client_mount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                    self.backend.MOUNT_COMPLETE, options)

    def set_status_client_mount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_FAILED, options)

    def set_status_client_mount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_WARNING, options)

    def set_status_client_umount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_COMPLETE, options)

    def set_status_client_umount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_FAILED, options)

    def set_status_client_umount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_WARNING, options)

    def get_status_clients(self):
        if self._start_backend():
            return self.backend.get_status_clients(self.fs_name)

    def set_status_target_unknown(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNKNOWN
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, node, 
                self.backend.TARGET_UNKNOWN, options)

    def set_status_target_ko(self, target, options):
        """"""
        This function is used to set the specified target status
        to KO
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                backend.TARGET_KO, options)

    def set_status_target_available(self, target, options):
        """"""
        This function is used to set the specified target status
        to AVAILABLE
        """"""
        if self._start_backend():
            # Set the fs_name to Free since these targets are availble
            # which means not used by any file system.
            self.backend.set_status_target(None, target,
                self.backend.TARGET_AVAILABLE, options)

    def set_status_target_formating(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATING, options)

    def set_status_target_format_failed(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMAT_FAILED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMAT_FAILED, options)

    def set_status_target_formated(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATED, options)

    def set_status_target_offline(self, target, options):
        """"""
        This function is used to set the specified target status
        to OFFLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_OFFLINE, options)

    def set_status_target_starting(self, target, options):
        """"""
        This function is used to set the specified target status
        to STARTING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STARTING, options)

    def set_status_target_online(self, target, options):
        """"""
        This function is used to set the specified target status
        to ONLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_ONLINE, options)

    def set_status_target_critical(self, target, options):
        """"""
        This function is used to set the specified target status
        to CRITICAL
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_CRITICAL, options)

    def set_status_target_stopping(self, target, options):
        """"""
        This function is used to set the specified target status
        to STOPPING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STOPPING, options)

    def set_status_target_unreachable(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNREACHABLE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_UNREACHABLE, options)

    def get_status_targets(self):
        """"""
        This function returns the status of each targets
        involved in the current file system.
        """"""
        if self._start_backend():
            return self.backend.get_status_targets(self.fs_name)

    def register(self):
        """"""
        This function aims to register the file system configuration
        to the backend.
        """"""
        if self._start_backend():
            return self.backend.register_fs(self)

    def unregister(self):
        """"""
        This function aims to remove a file system configuration from
        the backend.        
        """"""
        result = 0
        if self._start_backend():
            result = self.backend.unregister_fs(self)

        if not result:
            os.unlink(self.xmf_path)

        return result
/n/n/nlib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
        except KeyError:
            raise
        
        return RC_RUNTIME_ERROR


/n/n/nlib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # some common remote errors:
            # rc 127 = command not found
            # rc 126 = found but not executable
            # rc 1 = python failure...
            if rc != 0:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/nlib/Shine/Lustre/FileSystem.py/n/n# FileSystem.py -- Lustre FS
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Lustre FileSystem class.

Represents a Lustre FS.
""""""

import copy
from sets import Set
import socket

from ClusterShell.NodeSet import NodeSet, RangeSet

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

# Action exceptions
from Actions.Action import ActionErrorException
from Actions.Proxies.ProxyAction import *

from Actions.Install import Install
from Actions.Proxies.Preinstall import Preinstall
from Actions.Proxies.FSProxyAction import FSProxyAction
from Actions.Proxies.FSClientProxyAction import FSClientProxyAction

from EventHandler import *
from Client import *
from Server import *
from Target import *


class FSException(Exception):
    def __init__(self, message):
        self.message = message
    def __str__(self):
        return self.message

class FSError(FSException):
    """"""
    Base FileSystem error exception.
    """"""

class FSSyntaxError(FSError):
    def __init__(self, message):
        self.message = ""Syntax error: \""%s\"""" % (message)
    def __str__(self):
        return self.message

class FSBadTargetError(FSSyntaxError):
    def __init__(self, target_name):
        self.message = ""Syntax error: unrecognized target \""%s\"""" % (target_name)

class FSStructureError(FSError):
    """"""
    Lustre file system structure error, raised after an invalid configuration
    is encountered. For example, you will get this error if you try to assign
    two targets `MGT' to a filesystem.
    """"""

class FSRemoteError(FSError):
    """"""
    Remote host(s) not available, or a remote operation failed.
    """"""
    def __init__(self, nodes, rc, message):
        FSError.__init__(self, message)
        self.nodes = nodes
        self.rc = int(rc)

    def __str__(self):
        return ""%s: %s [rc=%d]"" % (self.nodes, self.message, self.rc)


STATUS_SERVERS      = 0x01
STATUS_HASERVERS    = 0x02
STATUS_CLIENTS      = 0x10
STATUS_ANY          = 0xff


class FileSystem:
    """"""
    The Lustre FileSystem abstract class.
    """"""

    def __init__(self, fs_name, event_handler=None):
        self.fs_name = fs_name
        self.debug = False
        self.set_eventhandler(event_handler)
        self.proxy_errors = []

        self.local_hostname = socket.gethostname()
        self.local_hostname_short = self.local_hostname.split('.', 1)[0]

        # file system MGT
        self.mgt = None

        # All FS server targets (MGT, MDT, OST...)
        self.targets = []

        # All FS clients
        self.clients = []

        # filled after successful install
        self.mgt_servers = NodeSet()
        self.mgt_count = 0

        self.mdt_servers = NodeSet()
        self.mdt_count = 0

        self.ost_servers = NodeSet()
        self.ost_count = 0

        self.target_count = 0
        self.target_servers = NodeSet()

    def set_debug(self, debug):
        self.debug = debug

    #
    # file system event handling
    #

    def _invoke_event(self, event, **kwargs):
        if 'target' in kwargs or 'client' in kwargs:
            kwargs.setdefault('node', None)
        getattr(self.event_handler, event)(**kwargs)

    def _invoke_dummy(self, event, **kwargs):
        pass

    def set_eventhandler(self, event_handler):
        self.event_handler = event_handler
        if self.event_handler is None:
            self._invoke = self._invoke_dummy
        else:
            self._invoke = self._invoke_event

    def _handle_shine_event(self, event, node, **params):
        #print ""_handle_shine_event %s %s"" % (event, params)
        target = params.get('target')
        if target:
            found = False
            for t in self.targets:
                if t.match(target):
                    # perform sanity checks here
                    old_nids = t.get_nids()
                    if old_nids != target.get_nids():
                        print ""NIDs mismatch %s -> %s"" % \
                                (','.join(old.nids), ','.join(target.get_nids))
                    # update target from remote one
                    t.update(target)
                    # substitute target parameter by local one
                    params['target'] = t
                    found = True
            if not found:
                print ""Target Update FAILED (%s)"" % target
        
        client = params.get('client')
        if client:
            found = False
            for c in self.clients:
                if c.match(client):
                    # update client from remote one
                    c.update(client)
                    # substitute client parameter
                    params['client'] = c
                    found = True
            if not found:
                print ""Client Update FAILED (%s)"" % client

        self._invoke(event, node=node, **params)

    def _handle_shine_proxy_error(self, nodes, message):
        self.proxy_errors.append((NodeSet(nodes), message))

    #
    # file system construction
    #

    def _attach_target(self, target):
        self.targets.append(target)
        if target.type == 'mgt':
            self.mgt = target
        self._update_structure()

    def _attach_client(self, client):
        self.clients.append(client)
        self._update_structure()

    def new_target(self, server, type, index, dev, jdev=None, group=None,
            tag=None, enabled=True):
        """"""
        Create a new attached target.
        """"""
        #print ""new_target on %s type %s (enabled=%s)"" % (server, type, enabled)

        if type == 'mgt' and self.mgt and len(self.mgt.get_nids()) > 0:
            raise FSStructureError(""A Lustre FS has only one MGT."")

        # Instantiate matching target class (eg. 'ost' -> OST).
        target = getattr(sys.modules[self.__class__.__module__], type.upper())(fs=self,
                server=server, index=index, dev=dev, jdev=jdev, group=group, tag=tag,
                enabled=enabled)
        
        return target

    def new_client(self, server, mount_path, enabled=True):
        """"""
        Create a new attached client.
        """"""
        client = Client(self, server, mount_path, enabled)

        return client

    def get_mgs_nids(self):
        return self.mgt.get_nids()
    
    def get_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients])

    def get_enabled_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients if c.action_enabled])

    def get_enabled_target_servers(self):
        return NodeSet.fromlist([t.server for t in self.targets if t.action_enabled])

    def get_client_statecounters(self):
        """"""
        Get (ignored, offline, error, runtime_error, mounted) client state counters tuple.
        """"""
        ignored = 0
        states = {}
        for client in self.clients:
            if client.action_enabled:
                state = states.setdefault(client.state, 0)
                states[client.state] = state + 1
            else:
                ignored += 1
        
        return (ignored,
                states.get(OFFLINE, 0),
                states.get(CLIENT_ERROR, 0),
                states.get(RUNTIME_ERROR, 0),
                states.get(MOUNTED, 0))

    def targets_by_state(self, state):
        for target in self.targets:
            #print target, target.state
            if target.action_enabled and target.state == state:
                yield target

    def target_servers_by_state(self, state):
        servers = NodeSet()
        for target in self.targets_by_state(state):
            #print ""OK %s"" % target
            servers.add(target.servers[0])
        return servers

    def _distant_action_by_server(self, action_class, servers, **kwargs):

        task = task_self()

        # filter local server
        if self.local_hostname in servers:
            distant_servers = servers.difference(self.local_hostname)
        elif self.local_hostname_short in servers:
            distant_servers = servers.difference(self.local_hostname_short)
        else:
            distant_servers = servers

        # perform action on distant servers
        if len(distant_servers) > 0:
            action = action_class(nodes=distant_servers, fs=self, **kwargs)
            action.launch()
            task.resume()

    def install(self, fs_config_file, nodes=None):
        """"""
        Install FS config files.
        """"""
        servers = NodeSet()

        for target in self.targets:
            # install on failover partners too
            for s in target.servers:
                if not nodes or s in nodes:
                    servers.add(s)

        for client in self.clients:
            # install on failover partners too
            if not nodes or client.server in nodes:
                servers.add(client.server)

        assert len(servers) > 0, ""no servers?""

        try:
            self._distant_action_by_server(Preinstall, servers)
            self._distant_action_by_server(Install, servers, config_file=fs_config_file)
        except ProxyActionError, e:
            # switch to public exception
            raise FSRemoteError(e.nodes, e.rc, e.message)
        
    def remove(self):
        """"""
        Remove FS config files.
        """"""

        result = 0

        servers = NodeSet()

        self.action_refcnt = 0
        self.proxy_errors = []

        # iterate over lustre servers
        for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
            if not e_s_targets:
                continue

            if server.is_local():
                # remove local fs configuration file
                conf_dir_path = Globals().get_conf_dir()
                fs_file = os.path.join(Globals().get_conf_dir(), ""%s.xmf"" % self.fs_name)
                rc = os.unlink(fs_file)
                result = max(result, rc)
            else:
                servers.add(server)

        if len(servers) > 0:
            # Perform the remove operations on all targets for these nodes.
            action = FSProxyAction(self, 'remove', servers, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR
        
        return result

    def _update_structure(self):
        # convenience
        for type, targets, servers in self._iter_targets_servers_by_type():
            if type == 'ost':
                self.ost_count = len(targets)
                self.ost_servers = NodeSet(servers)
            elif type == 'mdt':
                self.mdt_count = len(targets)
                self.mdt_servers = NodeSet(servers)
            elif type == 'mgt':
                self.mgt_count = len(targets)
                self.mgt_servers = NodeSet(servers)

        self.target_count = self.mgt_count + self.mdt_count + self.ost_count
        self.target_servers = self.mgt_servers | self.mdt_servers | self.ost_servers

    def _iter_targets_servers_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns a tuple (list of targets,
        list of servers) per target type.
        """"""
        last_target_type = None
        servers = NodeSet()
        targets = Set()

        #self.targets.sort()

        if reverse:
            self.targets.reverse()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(targets) > 0:
                    yield last_target_type, targets, servers
                    servers.clear()     # ClusterShell 1.1+ needed (sorry)
                    targets.clear()

            if target.action_enabled:
                targets.add(target)
                # select server: change master_server for -F node
                servers.add(target.get_selected_server())
            last_target_type = target.type

        if len(targets) > 0:
            yield last_target_type, targets, servers

    def targets_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns the following tuple:
        (type, (list of all targets of this type, list of enabled targets))
        per target type.
        """"""
        last_target_type = None
        a_targets = Set()
        e_targets = Set()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(a_targets) > 0:
                    yield last_target_type, (a_targets, e_targets)
                    a_targets.clear()
                    e_targets.clear()

            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)
            last_target_type = target.type

        if len(a_targets) > 0:
            yield last_target_type, (a_targets, e_targets)

    def _iter_targets_by_server(self):
        """"""
        Per server of target iterator : returns the following tuple:
        (server, (list of all server targets, list of enabled targets))
        per target server.
        """"""
        servers = {}
        for target in self.targets:
            a_targets, e_targets = servers.setdefault(target.get_selected_server(), (Set(), Set()))
            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)

        return servers.iteritems()


    def _iter_type_idx_for_targets(self, targets):
        last_target_type = None

        indexes = RangeSet(autostep=3)

        #self.targets.sort()

        for target in targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(indexes) > 0:
                    yield last_target_type, indexes
                    indexes.clear()     # CS 1.1+
            indexes.add(int(target.index))
            last_target_type = target.type

        if len(indexes) > 0:
            yield last_target_type, indexes

    def format(self, **kwargs):

        # Remember format launched, so we can check their status once
        # all operations are done.
        format_launched = Set()

        servers_formatall = NodeSet()

        self.proxy_errors = []
        self.action_refcnt = 0

        for server, (a_targets, e_targets) in self._iter_targets_by_server():

            if server.is_local():
                # local server
                for target in e_targets:
                    target.format(**kwargs)
                    self.action_refcnt += 1

                format_launched.update(e_targets)

            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action if ""format all targets on this server""
                    # is detected
                    servers_formatall.add(server)
                else:
                    # otherwise, format per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'format',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

                format_launched.update(e_targets)

        if len(servers_formatall) > 0:
            action = FSProxyAction(self, 'format', servers_formatall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check.
        for target in format_launched:
            if target.state != OFFLINE:
                return target.state

        return OFFLINE

    def status(self, flags=STATUS_ANY):
        """"""
        Get status of filesystem.
        """"""

        status_target_launched = Set()
        status_client_launched = Set()
        servers_statusall = NodeSet()
        self.action_refcnt = 0
        self.proxy_errors = []

        # prepare servers status checks
        if flags & STATUS_SERVERS:
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                if len(e_s_targets) == 0:
                    continue

                if server.is_local():
                    for target in e_s_targets:
                        target.status()
                        self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)
                else:
                    # distant server: check if all server targets have been selected
                    if len(a_s_targets) == len(e_s_targets):
                        # ""status on all targets for this server"" detected
                        servers_statusall.add(server)
                    else:
                        # status per selected targets on this server
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(e_s_targets):
                            action = FSProxyAction(self, 'status',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)

        # prepare clients status checks
        if flags & STATUS_CLIENTS:
            for client in self.clients:
                if client.action_enabled:
                    server = client.server
                    if server.is_local():
                        client.status()
                        self.action_refcnt += 1
                    elif server not in servers_statusall:
                        servers_statusall.add(server)
                    status_client_launched.add(client)

        # launch distant actions
        if len(servers_statusall) > 0:
            action = FSProxyAction(self, 'status', servers_statusall, self.debug)
            action.launch()
            self.action_refcnt += 1

        # run loop
        task_self().resume()
        
        # return a dict of {state : target list}
        rdict = {}

        # all launched targets+clients
        launched = (status_target_launched | status_client_launched)
        if self.proxy_errors:
            # find targets/clients affected by the runtime error(s)
            for target in launched:
                for nodes, msg in self.proxy_errors:
                    if target.server in nodes:
                        target.state = RUNTIME_ERROR

        for target in launched:
            if target.state == None:
                print target, target.server
            assert target.state != None
            targets = rdict.setdefault(target.state, [])
            targets.append(target)
        return rdict

    def status_target(self, target):
        """"""
        Launch a status request for a specific local or remote target.
        """"""

        # Don't call me if the target itself is not enabled.
        assert target.action_enabled

        server = target.get_selected_server()

        if server.is_local():
            # Target is local
            target.status()
        else:
            action = FSProxyAction(self, 'status', NodeSet(server), self.debug,
                    target.type, RangeSet(str(target.index)))
            action.launch()

        self.action_refcnt = 1
        task_self().resume()

    def start(self, **kwargs):
        """"""
        Start Lustre file system servers.
        """"""
        self.proxy_errors = []

        # What starting order to use?
        for target in self.targets:
            if isinstance(target, MDT) and target.action_enabled:
                # Found enabled MDT: perform writeconf check.
                self.status_target(target)
                if target.has_first_time_flag() or target.has_writeconf_flag():
                    # first_time or writeconf flag found, start MDT before OSTs
                    MDT.target_order = 2 # change MDT class variable order

        self.targets.sort()

        # servers_startall is used for optimization, it contains nodes
        # where we have to perform the start operation on all targets
        # found for this FS. This will limit the number of FSProxyAction
        # to spawn.
        servers_startall = NodeSet()

        # Remember targets launched, so we can check their status once
        # all operations are done (here, status are checked after all
        # targets of the same type have completed the start operation -
        # with possible failure).
        targets_launched = Set()

        # Keep number of actions in order to abort task correctly in
        # action's ev_close.
        self.action_refcnt = 0

        result = 0

        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():
            
            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():

                # To summary, we keep targets that are:
                # 1. enabled
                # 2. of according type
                # 3. on this server
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Start targets if we are on the good server.
                    for target in type_e_targets:
                        # Note that target.start() should never block here:
                        # it will perform necessary non-blocking actions and
                        # (when needed) will start local ClusterShell workers.
                        target.start(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""start all FS targets on this server"" detected
                        servers_startall.add(server)
                    else:
                        # Start per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'start',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched targets of this server for late status check.
                targets_launched.update(type_e_targets)

            if len(servers_startall) > 0:
                # Perform the start operations on all targets for these nodes.
                action = FSProxyAction(self, 'start', servers_startall, self.debug)
                action.launch()
                self.action_refcnt += 1

            # Resume current task, ie. start runloop, process workers events
            # and also act as a target-type barrier.
            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_launched:
                if target.state > result:
                    result = target.state
                    if result > RECOVERING:
                        # Avoid broken cascading starts, so we break now if
                        # a target of the previous type failed to start.
                        return result

            # Some needed cleanup before next target type.
            servers_startall.clear()
            targets_launched.clear()

        return result


    def stop(self, **kwargs):
        """"""
        Stop file system.
        """"""
        rc = MOUNTED

        # Stop: reverse order
        self.targets.sort()
        self.targets.reverse()

        # servers_stopall is used for optimization, see the comment in
        # start() for servers_startall.
        servers_stopall = NodeSet()

        # Remember targets when stop was launched.
        targets_stopping = Set()

        self.action_refcnt = 0
        self.proxy_errors = []

        # We use a similar logic than start(): see start() for comments.
        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():

            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Stop targets if we are on the good server.
                    for target in type_e_targets:
                        target.stop(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""stop all FS targets on this server"" detected
                        servers_stopall.add(server)
                    else:
                        # Stop per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'stop',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched stopping targets of this server for late status check.
                targets_stopping.update(type_e_targets)

            if len(servers_stopall) > 0:
                # Perform the stop operations on all targets for these nodes.
                action = FSProxyAction(self, 'stop', servers_stopall, self.debug)
                action.launch()
                self.action_refcnt += 1

            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_stopping:
                if target.state > rc:
                    rc = target.state

            # Some needed cleanup before next target type.
            servers_stopall.clear()
            targets_stopping.clear()

        return rc

    def mount(self, **kwargs):
        """"""
        Mount FS clients.
        """"""
        servers_mountall = NodeSet()
        clients_mounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.start(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_mountall.add(client.server)

            clients_mounting.add(client)

        if len(servers_mountall) > 0:
            action = FSClientProxyAction(self, 'mount', servers_mountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_mounting:
            if client.state != MOUNTED:
                return client.state

        return MOUNTED

    def umount(self, **kwargs):
        """"""
        Unmount FS clients.
        """"""
        servers_umountall = NodeSet()
        clients_umounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.stop(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_umountall.add(client.server)

            clients_umounting.add(client)

        if len(servers_umountall) > 0:
            action = FSClientProxyAction(self, 'umount', servers_umountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_umounting:
            if client.state != OFFLINE:
                return client.state

        return OFFLINE

    def info(self):
        pass

    def tune(self, tuning_model):
        """"""
        Tune server.
        """"""
        task = task_self()
        tune_all = NodeSet()
        type_map = { 'mgt': 'mgs', 'mdt': 'mds', 'ost' : 'oss' }
        self.action_refcnt = 0
        self.proxy_errors = []
        result = 0

        # Install tuning.conf on enabled distant servers
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if e_targets and not server.is_local():
                tune_all.add(server)
        if len(tune_all) > 0:
            self._distant_action_by_server(Install, tune_all, config_file=Globals().get_tuning_file())
            self.action_refcnt += 1
            task.resume()
            tune_all.clear()

        # Apply tunings
        self.action_refcnt = 0
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if not e_targets:
                continue
            if server.is_local():
                types = Set()
                for t in e_targets:
                    types.add(type_map[t.type])

                rc = server.tune(tuning_model, types, self.fs_name)
                result = max(result, rc)
            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action
                    tune_all.add(server)
                else:
                    # otherwise, tune per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'tune',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

        if len(tune_all) > 0:
            action = FSProxyAction(self, 'tune', tune_all, self.debug)
            action.launch()
            self.action_refcnt += 1

        task.resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        return result

/n/n/n",0
71,71,7ff203be36e439b535894764c37a8446351627ec,"/lib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()

/n/n/n/lib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            print ""Bad argument""
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return 0

/n/n/n/lib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Mount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError""
            raise

/n/n/n/lib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc
/n/n/n/lib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == ""fs"":
                self.status_view_fs(fs)
            elif view.startswith(""target""):
                self.status_view_targets(fs)
            elif view.startswith(""disk""):
                self.status_view_disks(fs)
            else:
                raise CommandBadParameterError(self.view_support.get_view(),
                        ""fs, targets, disks"")
        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/n/lib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Unmount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
            return RC_USER_ERROR
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
            return RC_RUNTIME_ERROR
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except KeyError:
            print ""Error - Unrecognized action""
            print
            raise
        
        return 1


/n/n/n/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/n",1
52,52,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Köster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Köster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Köster""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
53,53,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
62,62,e08c7a0b2dc5002a935737e661a6e8e8c9040de3,"openqml-pq/openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/nopenqml-sf/openqml_sf/fock.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CatState:': Catstate,
    'CoherentState': Coherent,
    'FockDensityMatrix': DensityMatrix,
    'DisplacedSqueezed': DisplacedSqueezed,
    'FockState': Fock,
    'FockStateVector': Ket,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'CrossKerr': CKgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'Kerr': Kgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeezing': Sgate,
    'CubicPhase': Vgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureFock': MeasureFock,
    # 'MeasureHomodyne': MeasureHomodyne
}


class StrawberryFieldsFock(Device):
    """"""StrawberryFields Fock device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):
        self.wires = wires
        self.cutoff = cutoff
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self.observe.params)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/nopenqml-sf/openqml_sf/gaussian.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CoherentState': Coherent,
    'DisplacedSqueezed': DisplacedSqueezed,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeeze': Sgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureHomodyne': MeasureHomodyne,
    # 'MeasureHeterodyne': MeasureHeterodyne
}



class StrawberryFieldsGaussian(Device):
    """"""StrawberryFields Gaussian device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, hbar=2):
        self.wires = wires
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('gaussian')

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self._observe.params)
        elif self._observe.name == 'Displacement':
            ex = self.state.displacement(modes=reg)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/nopenqml/device.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""

import abc
import logging


logging.getLogger()


class MethodFactory(type):
    """"""Metaclass that allows derived classes to dynamically instantiate
    new objects based on undefined methods. The dynamic methods pass their arguments
    directly to __init__ of the inheriting class.""""""
    def __getattr__(cls, name):
        """"""Get the attribute call via name""""""
        def new_object(*args, **kwargs):
            """"""Return a new object of the same class, passing the attribute name
            as the first parameter, along with any additional parameters.""""""
            return cls(name, *args, **kwargs)
        return new_object


class DeviceError(Exception):
    """"""Exception raised by a :class:`Device` when it encounters an illegal
    operation in the quantum circuit.
    """"""
    pass


class Device(abc.ABC):
    """"""Abstract base class for devices.""""""
    _current_context = None
    name = ''          #: str: official device plugin name
    short_name = ''    #: str: name used to load device plugin
    api_version = ''   #: str: version of OpenQML for which the plugin was made
    version = ''       #: str: version of the device plugin itself
    author = ''        #: str: plugin author(s)
    _capabilities = {} #: dict[str->*]: plugin capabilities
    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates
    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables
    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class

    def __init__(self, name, shots):
        self.name = name # the name of the device

        # number of circuit evaluations used to estimate
        # expectation values of observables. 0 means the exact ev is returned.
        self.shots = shots

        self._out = None  # this attribute stores the expectation output
        self._queue = []  # this list stores the operations to be queued to the device
        self._observe = None # the measurement operation to be performed

    def __repr__(self):
        """"""String representation.""""""
        return self.__module__ +'.' +self.__class__.__name__ +'\nInstance: ' +self.name

    def __str__(self):
        """"""Verbose string representation.""""""
        return self.__repr__() +'\nName: ' +self.name +'\nAPI version: ' +self.api_version\
            +'\nPlugin version: ' +self.version +'\nAuthor: ' +self.author +'\n'

    def __enter__(self):
        if Device._current_context is None:
            Device._current_context = self
            self.reset()
        else:
            raise DeviceError('Only one device can be active at a time.')
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if self._observe is None:
            raise DeviceError('A qfunc must always conclude with a classical expectation value.')
        Device._current_context = None
        self.execute()

    @property
    def gates(self):
        """"""Get the supported gate set.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._gates

    @property
    def observables(self):
        """"""Get the supported observables.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._observables

    @property
    def templates(self):
        """"""Get the predefined circuit templates.

        .. todo:: rename to circuits?

        Returns:
          dict[str->Circuit]: circuit templates
        """"""
        return self._circuits

    @property
    def result(self):
        """"""Get the circuit result.

        Returns:
            float or int
        """"""
        return self._out

    @classmethod
    def capabilities(cls):
        """"""Get the other capabilities of the plugin.

        Measurements, batching etc.

        Returns:
          dict[str->*]: results
        """"""
        return cls._capabilities

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        self._out = self.execute_queued()

    @abc.abstractmethod
    def execute_queued(self):
        """"""Called during execute(). To be implemented by each plugin.

        Returns:
          float: expectation value(s) #todo: This should become an array type to handle multiple expectation values.
        """"""
        raise NotImplementedError

    @abc.abstractmethod
    def reset(self):
        """"""Reset the backend state.

        After the reset the backend should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        raise NotImplementedError
/n/n/nopenqml/plugins/default.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from scipy.linalg import expm, eigh

import openqml as qm
from openqml import Device, DeviceError, qfunc, QNode, Variable, __version__


# tolerance for numerical errors
tolerance = 1e-10


#========================================================
#  utilities
#========================================================

def spectral_decomposition_qubit(A):
    r""""""Spectral decomposition of a 2*2 Hermitian matrix.

    Args:
      A (array): 2*2 Hermitian matrix

    Returns:
      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors
        such that :math:`A = \sum_k a_k P_k`.
    """"""
    d, v = eigh(A)
    P = []
    for k in range(2):
        temp = v[:, k]
        P.append(np.outer(temp.conj(), temp))
    return d, P


#========================================================
#  fixed gates
#========================================================

I = np.eye(2)
# Pauli matrices
X = np.array([[0, 1], [1, 0]])
Y = np.array([[0, -1j], [1j, 0]])
Z = np.array([[1, 0], [0, -1]])
CNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])
SWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])


#========================================================
#  parametrized gates
#========================================================


def frx(theta):
    r""""""One-qubit rotation about the x axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_x \theta/2}`
    """"""
    return expm(-1j * theta/2 * X)


def fry(theta):
    r""""""One-qubit rotation about the y axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_y \theta/2}`
    """"""
    return expm(-1j * theta/2 * Y)


def frz(theta):
    r""""""One-qubit rotation about the z axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_z \theta/2}`
    """"""
    return expm(-1j * theta/2 * Z)


def fr3(a, b, c):
    r""""""Arbitrary one-qubit rotation using three Euler angles.

    Args:
        a,b,c (float): rotation angles
    Returns:
        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)
    """"""
    return frz(c) @ (fry(b) @ frz(a))


#========================================================
#  Arbitrary states and operators
#========================================================

def ket(*args):
    r""""""Input validation for an arbitary state vector.

    Args:
        args (array): NumPy array.

    Returns:
        array: normalised array.
    """"""
    state = np.asarray(args)
    return state/np.linalg.norm(state)


def unitary(*args):
    r""""""Input validation for an arbitary unitary operation.

    Args:
        args (array): square unitary matrix.

    Returns:
        array: square unitary matrix.
    """"""
    U = np.asarray(args[0])

    if U.shape[0] != U.shape[1]:
        raise ValueError(""Operator must be a square matrix."")

    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):
        raise ValueError(""Operator must be unitary."")

    return U


def hermitian(*args):
    r""""""Input validation for an arbitary Hermitian observable.

    Args:
        args (array): square hermitian matrix.

    Returns:
        array: square hermitian matrix.
    """"""
    A = np.asarray(args[0])

    if A.shape[0] != A.shape[1]:
        raise ValueError(""Observable must be a square matrix."")

    if not np.allclose(A, A.conj().T, atol=tolerance):
        raise ValueError(""Observable must be Hermitian."")
    return A


#========================================================
#  operator map
#========================================================


operator_map = {
    'QubitStateVector': ket,
    'QubitUnitary': unitary,
    'Hermitian': hermitian,
    'Identity': I,
    'PauliX': X,
    'PauliY': Y,
    'PauliZ': Z,
    'CNOT': CNOT,
    'SWAP': SWAP,
    'RX': frx,
    'RY': fry,
    'RZ': frz,
    'Rot': fr3
}


#========================================================
#  device
#========================================================


class DefaultQubit(Device):
    """"""Default qubit device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Default OpenQML plugin'
    short_name = 'default.qubit'
    api_version = '0.1.0'
    version = '0.1.0'
    author = 'Xanadu Inc.'
    _gates = set(operator_map.keys())
    _observables = {}
    _circuits = {}

    def __init__(self, wires, *, shots=0):
        self.wires = wires
        self.eng = None
        self._state = None
        super().__init__(self.short_name, shots)

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self._state is None:
            # init the state vector to |00..0>
            self._state = np.zeros(2**self.wires, dtype=complex)
            self._state[0] = 1
            self._out = np.full(self.wires, np.nan)

        # apply unitary operations U
        for operation in self._queue:
            if operation.name == 'QubitStateVector':
                state = np.asarray(operation.params[0])
                if state.ndim == 1 and state.shape[0] == 2**self.wires:
                    self._state = state
                else:
                    raise ValueError('State vector must be of length 2**wires.')
                continue

            U = DefaultQubit._get_operator_matrix(operation)

            if len(operation.wires) == 1:
                U = self.expand_one(U, operation.wires)
            elif len(operation.wires) == 2:
                U = self.expand_two(U, operation.wires)
            else:
                raise ValueError('This plugin supports only one- and two-qubit gates.')
            self._state = U @ self._state

        # measurement/expectation value <psi|A|psi>
        A = DefaultQubit._get_operator_matrix(self._observe)
        if self.shots == 0:
            # exact expectation value
            ev = self.ev(A, [self._observe.wires])
        else:
            # estimate the ev
            if 0:
                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
                ev = self.ev(A, self._observe.wires)
                var = self.ev(A**2, self._observe.wires) - ev**2  # variance
                ev = np.random.normal(ev, np.sqrt(var / self.shots))
            else:
                # sample Bernoulli distribution n_eval times / binomial distribution once
                a, P = spectral_decomposition_qubit(A)
                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]
                n0 = np.random.binomial(self.shots, p0)
                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots

        self._out = ev  # store the result

    @classmethod
    def _get_operator_matrix(cls, A):
        """"""Get the operator matrix for a given operation.

        Args:
            A (openqml.Operation or openqml.Expectation): operation/observable.

        Returns:
            array: matrix representation.
        """"""
        if A.name not in operator_map:
            raise DeviceError(""{} not supported by device {}"".format(A.name, cls.short_name))

        if not callable(operator_map[A.name]):
            return operator_map[A.name]

        # unpack variables
        p = [x.val if isinstance(x, Variable) else x for x in A.params]
        return operator_map[A.name](*p)

    def ev(self, A, wires):
        r""""""Expectation value of a one-qubit observable in the current state.

        Args:
          A (array): 2*2 hermitian matrix corresponding to the observable
          wires (Sequence[int]): target subsystem

        Returns:
          float: expectation value :math:`\expect{A} = \bra{\psi}A\ket{\psi}`
        """"""
        if A.shape != (2, 2):
            raise ValueError('2x2 matrix required.')

        A = self.expand_one(A, wires)
        expectation = np.vdot(self._state, A @ self._state)

        if np.abs(expectation.imag) > tolerance:
            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))
        return expectation.real

    def reset(self):
        """"""Reset the device""""""
        self._state  = None  #: array: state vector
        self._out = None  #: array: measurement results

    def expand_one(self, U, wires):
        """"""Expand a one-qubit operator into a full system operator.

        Args:
          U (array): 2*2 matrix
          wires (Sequence[int]): target subsystem

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (2, 2):
            raise ValueError('2x2 matrix required.')
        if len(wires) != 1:
            raise ValueError('One target subsystem required.')
        wires = wires[0]
        before = 2**wires
        after  = 2**(self.wires-wires-1)
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U

    def expand_two(self, U, wires):
        """"""Expand a two-qubit operator into a full system operator.

        Args:
          U (array): 4x4 matrix
          wires (Sequence[int]): two target subsystems (order matters!)

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (4, 4):
            raise ValueError('4x4 matrix required.')
        if len(wires) != 2:
            raise ValueError('Two target subsystems required.')
        wires = np.asarray(wires)
        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:
            raise ValueError('Bad target subsystems.')

        a = np.min(wires)
        b = np.max(wires)
        n_between = b-a-1  # number of qubits between a and b
        # dimensions of the untouched subsystems
        before  = 2**a
        after   = 2**(self.wires-b-1)
        between = 2**n_between

        U = np.kron(U, np.eye(between))
        # how U should be reordered
        if wires[0] < wires[1]:
            p = [0, 2, 1]
        else:
            p = [1, 2, 0]
        dim = [2, 2, between]
        p = np.array(p)
        perm = np.r_[p, p+3]
        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array
        temp = np.prod(dim)
        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U


#====================
# Default circuits
#====================


dev = DefaultQubit(wires=2)

def node(x, y, z):
    qm.RX(x, [0])
    qm.CNOT([0, 1])
    qm.RY(-1.6, [0])
    qm.RY(y, [1])
    qm.CNOT([1, 0])
    qm.RX(z, [0])
    qm.CNOT([0, 1])
    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)

circuits = {'demo_ev': QNode(node, dev)}
/n/n/n",0
63,63,e08c7a0b2dc5002a935737e661a6e8e8c9040de3,"/openqml-pq/openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute(self):
        """""" """"""
        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18
        self._out = self.execute_queued()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/n/openqml-sf/openqml_sf/fock.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CatState:': Catstate,
    'CoherentState': Coherent,
    'FockDensityMatrix': DensityMatrix,
    'DisplacedSqueezed': DisplacedSqueezed,
    'FockState': Fock,
    'FockStateVector': Ket,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'CrossKerr': CKgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'Kerr': Kgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeezing': Sgate,
    'CubicPhase': Vgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureFock': MeasureFock,
    # 'MeasureHomodyne': MeasureHomodyne
}


class StrawberryFieldsFock(Device):
    """"""StrawberryFields Fock device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):
        self.wires = wires
        self.cutoff = cutoff
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self.observe.params)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/n/openqml-sf/openqml_sf/gaussian.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CoherentState': Coherent,
    'DisplacedSqueezed': DisplacedSqueezed,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeeze': Sgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureHomodyne': MeasureHomodyne,
    # 'MeasureHeterodyne': MeasureHeterodyne
}



class StrawberryFieldsGaussian(Device):
    """"""StrawberryFields Gaussian device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, hbar=2):
        self.wires = wires
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('gaussian')

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self._observe.params)
        elif self._observe.name == 'Displacement':
            ex = self.state.displacement(modes=reg)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """"""Reset the device""""""
        if self.eng is not None:
            self.eng = None
            self.state = None
/n/n/n/openqml/device.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""

import abc
import logging


logging.getLogger()


class MethodFactory(type):
    """"""Metaclass that allows derived classes to dynamically instantiate
    new objects based on undefined methods. The dynamic methods pass their arguments
    directly to __init__ of the inheriting class.""""""
    def __getattr__(cls, name):
        """"""Get the attribute call via name""""""
        def new_object(*args, **kwargs):
            """"""Return a new object of the same class, passing the attribute name
            as the first parameter, along with any additional parameters.""""""
            return cls(name, *args, **kwargs)
        return new_object


class DeviceError(Exception):
    """"""Exception raised by a :class:`Device` when it encounters an illegal
    operation in the quantum circuit.
    """"""
    pass


class Device(abc.ABC):
    """"""Abstract base class for devices.""""""
    _current_context = None
    name = ''          #: str: official device plugin name
    short_name = ''    #: str: name used to load device plugin
    api_version = ''   #: str: version of OpenQML for which the plugin was made
    version = ''       #: str: version of the device plugin itself
    author = ''        #: str: plugin author(s)
    _capabilities = {} #: dict[str->*]: plugin capabilities
    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates
    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables
    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class

    def __init__(self, name, shots):
        self.name = name # the name of the device

        # number of circuit evaluations used to estimate
        # expectation values of observables. 0 means the exact ev is returned.
        self.shots = shots

        self._out = None  # this attribute stores the expectation output
        self._queue = []  # this list stores the operations to be queued to the device
        self._observe = None # the measurement operation to be performed

    def __repr__(self):
        """"""String representation.""""""
        return self.__module__ +'.' +self.__class__.__name__ +'\nInstance: ' +self.name

    def __str__(self):
        """"""Verbose string representation.""""""
        return self.__repr__() +'\nName: ' +self.name +'\nAPI version: ' +self.api_version\
            +'\nPlugin version: ' +self.version +'\nAuthor: ' +self.author +'\n'

    def __enter__(self):
        if Device._current_context is None:
            Device._current_context = self
            self.reset()
        else:
            raise DeviceError('Only one device can be active at a time.')
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if self._observe is None:
            raise DeviceError('A qfunc must always conclude with a classical expectation value.')
        Device._current_context = None
        self.execute()

    @property
    def gates(self):
        """"""Get the supported gate set.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._gates

    @property
    def observables(self):
        """"""Get the supported observables.

        Returns:
          dict[str->GateSpec]:
        """"""
        return self._observables

    @property
    def templates(self):
        """"""Get the predefined circuit templates.

        .. todo:: rename to circuits?

        Returns:
          dict[str->Circuit]: circuit templates
        """"""
        return self._circuits

    @property
    def result(self):
        """"""Get the circuit result.

        Returns:
            float or int
        """"""
        return self._out

    @classmethod
    def capabilities(cls):
        """"""Get the other capabilities of the plugin.

        Measurements, batching etc.

        Returns:
          dict[str->*]: results
        """"""
        return cls._capabilities

    @abc.abstractmethod
    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        raise NotImplementedError

    @abc.abstractmethod
    def reset(self):
        """"""Reset the backend state.

        After the reset the backend should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        raise NotImplementedError
/n/n/n/openqml/plugins/default.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""This module contains the device class and context manager""""""
import numpy as np
from scipy.linalg import expm, eigh

import openqml as qm
from openqml import Device, DeviceError, qfunc, QNode, Variable, __version__


# tolerance for numerical errors
tolerance = 1e-10


#========================================================
#  utilities
#========================================================

def spectral_decomposition_qubit(A):
    r""""""Spectral decomposition of a 2*2 Hermitian matrix.

    Args:
      A (array): 2*2 Hermitian matrix

    Returns:
      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors
        such that :math:`A = \sum_k a_k P_k`.
    """"""
    d, v = eigh(A)
    P = []
    for k in range(2):
        temp = v[:, k]
        P.append(np.outer(temp.conj(), temp))
    return d, P


#========================================================
#  fixed gates
#========================================================

I = np.eye(2)
# Pauli matrices
X = np.array([[0, 1], [1, 0]])
Y = np.array([[0, -1j], [1j, 0]])
Z = np.array([[1, 0], [0, -1]])
CNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])
SWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])


#========================================================
#  parametrized gates
#========================================================


def frx(theta):
    r""""""One-qubit rotation about the x axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_x \theta/2}`
    """"""
    return expm(-1j * theta/2 * X)


def fry(theta):
    r""""""One-qubit rotation about the y axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_y \theta/2}`
    """"""
    return expm(-1j * theta/2 * Y)


def frz(theta):
    r""""""One-qubit rotation about the z axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_z \theta/2}`
    """"""
    return expm(-1j * theta/2 * Z)


def fr3(a, b, c):
    r""""""Arbitrary one-qubit rotation using three Euler angles.

    Args:
        a,b,c (float): rotation angles
    Returns:
        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)
    """"""
    return frz(c) @ (fry(b) @ frz(a))


#========================================================
#  Arbitrary states and operators
#========================================================

def ket(*args):
    r""""""Input validation for an arbitary state vector.

    Args:
        args (array): NumPy array.

    Returns:
        array: normalised array.
    """"""
    state = np.asarray(args)
    return state/np.linalg.norm(state)


def unitary(*args):
    r""""""Input validation for an arbitary unitary operation.

    Args:
        args (array): square unitary matrix.

    Returns:
        array: square unitary matrix.
    """"""
    U = np.asarray(args[0])

    if U.shape[0] != U.shape[1]:
        raise ValueError(""Operator must be a square matrix."")

    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):
        raise ValueError(""Operator must be unitary."")

    return U


def hermitian(*args):
    r""""""Input validation for an arbitary Hermitian observable.

    Args:
        args (array): square hermitian matrix.

    Returns:
        array: square hermitian matrix.
    """"""
    A = np.asarray(args[0])

    if A.shape[0] != A.shape[1]:
        raise ValueError(""Observable must be a square matrix."")

    if not np.allclose(A, A.conj().T, atol=tolerance):
        raise ValueError(""Observable must be Hermitian."")
    return A


#========================================================
#  operator map
#========================================================


operator_map = {
    'QubitStateVector': ket,
    'QubitUnitary': unitary,
    'Hermitian': hermitian,
    'Identity': I,
    'PauliX': X,
    'PauliY': Y,
    'PauliZ': Z,
    'CNOT': CNOT,
    'SWAP': SWAP,
    'RX': frx,
    'RY': fry,
    'RZ': frz,
    'Rot': fr3
}


#========================================================
#  device
#========================================================


class DefaultQubit(Device):
    """"""Default qubit device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """"""
    name = 'Default OpenQML plugin'
    short_name = 'default.qubit'
    api_version = '0.1.0'
    version = '0.1.0'
    author = 'Xanadu Inc.'
    _gates = set(operator_map.keys())
    _observables = {}
    _circuits = {}

    def __init__(self, wires, *, shots=0):
        self.wires = wires
        self.eng = None
        self._state = None
        super().__init__(self.short_name, shots)

    def execute(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        if self._state is None:
            # init the state vector to |00..0>
            self._state = np.zeros(2**self.wires, dtype=complex)
            self._state[0] = 1
            self._out = np.full(self.wires, np.nan)

        # apply unitary operations U
        for operation in self._queue:
            if operation.name == 'QubitStateVector':
                state = np.asarray(operation.params[0])
                if state.ndim == 1 and state.shape[0] == 2**self.wires:
                    self._state = state
                else:
                    raise ValueError('State vector must be of length 2**wires.')
                continue

            U = DefaultQubit._get_operator_matrix(operation)

            if len(operation.wires) == 1:
                U = self.expand_one(U, operation.wires)
            elif len(operation.wires) == 2:
                U = self.expand_two(U, operation.wires)
            else:
                raise ValueError('This plugin supports only one- and two-qubit gates.')
            self._state = U @ self._state

        # measurement/expectation value <psi|A|psi>
        A = DefaultQubit._get_operator_matrix(self._observe)
        if self.shots == 0:
            # exact expectation value
            ev = self.ev(A, [self._observe.wires])
        else:
            # estimate the ev
            if 0:
                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
                ev = self.ev(A, self._observe.wires)
                var = self.ev(A**2, self._observe.wires) - ev**2  # variance
                ev = np.random.normal(ev, np.sqrt(var / self.shots))
            else:
                # sample Bernoulli distribution n_eval times / binomial distribution once
                a, P = spectral_decomposition_qubit(A)
                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]
                n0 = np.random.binomial(self.shots, p0)
                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots

        self._out = ev  # store the result

    @classmethod
    def _get_operator_matrix(cls, A):
        """"""Get the operator matrix for a given operation.

        Args:
            A (openqml.Operation or openqml.Expectation): operation/observable.

        Returns:
            array: matrix representation.
        """"""
        if A.name not in operator_map:
            raise DeviceError(""{} not supported by device {}"".format(A.name, cls.short_name))

        if not callable(operator_map[A.name]):
            return operator_map[A.name]

        # unpack variables
        p = [x.val if isinstance(x, Variable) else x for x in A.params]
        return operator_map[A.name](*p)

    def ev(self, A, wires):
        r""""""Expectation value of a one-qubit observable in the current state.

        Args:
          A (array): 2*2 hermitian matrix corresponding to the observable
          wires (Sequence[int]): target subsystem

        Returns:
          float: expectation value :math:`\expect{A} = \bra{\psi}A\ket{\psi}`
        """"""
        if A.shape != (2, 2):
            raise ValueError('2x2 matrix required.')

        A = self.expand_one(A, wires)
        expectation = np.vdot(self._state, A @ self._state)

        if np.abs(expectation.imag) > tolerance:
            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))
        return expectation.real

    def reset(self):
        """"""Reset the device""""""
        self._state  = None  #: array: state vector
        self._out = None  #: array: measurement results

    def expand_one(self, U, wires):
        """"""Expand a one-qubit operator into a full system operator.

        Args:
          U (array): 2*2 matrix
          wires (Sequence[int]): target subsystem

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (2, 2):
            raise ValueError('2x2 matrix required.')
        if len(wires) != 1:
            raise ValueError('One target subsystem required.')
        wires = wires[0]
        before = 2**wires
        after  = 2**(self.wires-wires-1)
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U

    def expand_two(self, U, wires):
        """"""Expand a two-qubit operator into a full system operator.

        Args:
          U (array): 4x4 matrix
          wires (Sequence[int]): two target subsystems (order matters!)

        Returns:
          array: 2^n*2^n matrix
        """"""
        if U.shape != (4, 4):
            raise ValueError('4x4 matrix required.')
        if len(wires) != 2:
            raise ValueError('Two target subsystems required.')
        wires = np.asarray(wires)
        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:
            raise ValueError('Bad target subsystems.')

        a = np.min(wires)
        b = np.max(wires)
        n_between = b-a-1  # number of qubits between a and b
        # dimensions of the untouched subsystems
        before  = 2**a
        after   = 2**(self.wires-b-1)
        between = 2**n_between

        U = np.kron(U, np.eye(between))
        # how U should be reordered
        if wires[0] < wires[1]:
            p = [0, 2, 1]
        else:
            p = [1, 2, 0]
        dim = [2, 2, between]
        p = np.array(p)
        perm = np.r_[p, p+3]
        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array
        temp = np.prod(dim)
        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U


#====================
# Default circuits
#====================


dev = DefaultQubit(wires=2)

def node(x, y, z):
    qm.RX(x, [0])
    qm.CNOT([0, 1])
    qm.RY(-1.6, [0])
    qm.RY(y, [1])
    qm.CNOT([1, 0])
    qm.RX(z, [0])
    qm.CNOT([0, 1])
    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)

circuits = {'demo_ev': QNode(node, dev)}
/n/n/n",1
14,14,a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7,"src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py/n/n""""""monconfgenerator

Creates an Icinga monitoring configuration. It does it by querying an URL from
which it receives a specially formatted yaml file. This file is transformed into
a valid Icinga configuration file.
If no URL is given it reads it's default configuration from file system. The
configuration file is: /etc/monitoring_config_generator/config.yaml'

Usage:
  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]
  monconfgenerator -h

Options:
  -h                Show this message.
  --debug           Print additional information.
  --targetdir=DIR   The generated Icinga monitoring configuration is written
                    into this directory. If no target directory is given its
                    value is read from /etc/monitoring_config_generator/config.yaml
  --skip-checks     Do not run checks on the yaml file received from the URL.

""""""
from datetime import datetime
import logging
import os
import sys

from docopt import docopt

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \
    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException
from monitoring_config_generator import set_log_level_to_debug
from monitoring_config_generator.yaml_tools.readers import Header, read_config
from monitoring_config_generator.yaml_tools.config import YamlConfig
from monitoring_config_generator.settings import CONFIG


EXIT_CODE_CONFIG_WRITTEN = 0
EXIT_CODE_ERROR = 1
EXIT_CODE_NOT_WRITTEN = 2

LOG = logging.getLogger(""monconfgenerator"")


class MonitoringConfigGenerator(object):
    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):
        self.skip_checks = skip_checks
        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']
        self.source = url

        if debug_enabled:
            set_log_level_to_debug()

        if not self.target_dir or not os.path.isdir(self.target_dir):
            raise MonitoringConfigGeneratorException(""%s is not a directory"" % self.target_dir)

        LOG.debug(""Using %s as target dir"" % self.target_dir)
        LOG.debug(""Using URL: %s"" % self.source)
        LOG.debug(""MonitoringConfigGenerator start: reading from %s, writing to %s"" %
                  (self.source, self.target_dir))

    def _is_newer(self, header_source, hostname):
        if not hostname:
            raise NoSuchHostname('hostname not found')
        output_path = self.output_path(self.create_filename(hostname))
        old_header = Header.parse(output_path)
        return header_source.is_newer_than(old_header)

    def output_path(self, file_name):
        return os.path.join(self.target_dir, file_name)

    def write_output(self, file_name, yaml_icinga):
        lines = yaml_icinga.icinga_lines
        output_writer = OutputWriter(self.output_path(file_name))
        output_writer.write_lines(lines)

    @staticmethod
    def create_filename(hostname):
        name = '%s.cfg' % hostname
        if name != os.path.basename(name):
            msg = ""Directory traversal attempt detected for host name %r""
            raise Exception(msg % hostname)
        return name

    def generate(self):
        file_name = None
        raw_yaml_config, header_source = read_config(self.source)

        if raw_yaml_config is None:
            raise SystemExit(""Raw yaml config from source '%s' is 'None'."" % self.source)

        yaml_config = YamlConfig(raw_yaml_config,
                                 skip_checks=self.skip_checks)

        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):
            file_name = self.create_filename(yaml_config.host_name)
            yaml_icinga = YamlToIcinga(yaml_config, header_source)
            self.write_output(file_name, yaml_icinga)

        if file_name:
            LOG.info(""Icinga config file '%s' created."" % file_name)

        return file_name

class YamlToIcinga(object):
    def __init__(self, yaml_config, header):
        self.icinga_lines = []
        self.indent = CONFIG['INDENT']
        self.icinga_lines.extend(header.serialize())
        self.write_section('host', yaml_config.host)
        for service in yaml_config.services:
            self.write_section('service', service)

    def write_line(self, line):
        self.icinga_lines.append(line)

    def write_section(self, section_name, section_data):
        self.write_line("""")
        self.write_line(""define %s {"" % section_name)
        sorted_keys = section_data.keys()
        sorted_keys.sort()
        for key in sorted_keys:
            value = self.value_to_icinga(section_data[key])
            icinga_line = ""%s%-45s%s"" % (self.indent, key, value)

            if ""\n"" in icinga_line or ""}"" in icinga_line:
                msg = ""Found forbidden newline or '}' character in section %r.""
                raise Exception(msg % section_name)

            self.icinga_lines.append(icinga_line)
        self.write_line(""}"")

    @staticmethod
    def value_to_icinga(value):
        """"""Convert a scalar or list to Icinga value format. Lists are concatenated by ,
        and empty (None) values produce an empty string""""""
        if isinstance(value, list):
            # explicitly set None values to empty string
            return "","".join([str(x) if (x is not None) else """" for x in value])
        else:
            return str(value)


class OutputWriter(object):
    def __init__(self, output_file):
        self.output_file = output_file

    def write_lines(self, lines):
        with open(self.output_file, 'w') as f:
            for line in lines:
                f.write(line + ""\n"")
        LOG.debug(""Created %s"" % self.output_file)


def generate_config():
    arg = docopt(__doc__, version='0.1.0')
    start_time = datetime.now()
    try:
        file_name = MonitoringConfigGenerator(arg['URL'],
                                              arg['--debug'],
                                              arg['--targetdir'],
                                              arg['--skip-checks']).generate()
        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN
    except HostUnreachableException:
        LOG.warn(""Target url {0} unreachable. Could not get yaml config!"".format(arg['URL']))
        exit_code = EXIT_CODE_NOT_WRITTEN
    except ConfigurationContainsUndefinedVariables:
        LOG.error(""Configuration contained undefined variables!"")
        exit_code = EXIT_CODE_ERROR
    except SystemExit as e:
        exit_code = e.code
    except BaseException as e:
        LOG.error(e)
        exit_code = EXIT_CODE_ERROR
    finally:
        stop_time = datetime.now()
        LOG.info(""finished in %s"" % (stop_time - start_time))
    sys.exit(exit_code)


if __name__ == '__main__':
    generate_config()
/n/n/nsrc/unittest/python/YamlToIcinga_tests.py/n/nimport os
import unittest
from mock import Mock

os.environ['MONITORING_CONFIG_GENERATOR_CONFIG'] = ""testdata/testconfig.yaml""
from monitoring_config_generator.MonitoringConfigGenerator import YamlToIcinga


class Test(unittest.TestCase):

    def test_text_to_cvs(self):
        self.assertEquals("""", YamlToIcinga.value_to_icinga(""""))
        self.assertEquals(""text"", YamlToIcinga.value_to_icinga(""text""))

    def test_number_to_cvs(self):
        self.assertEquals(""42"", YamlToIcinga.value_to_icinga(42))
        self.assertEquals(""-1"", YamlToIcinga.value_to_icinga(-1))
        self.assertEquals(""-1.6"", YamlToIcinga.value_to_icinga(-1.6))

    def test_list_to_cvs(self):
        self.assertEquals("""", YamlToIcinga.value_to_icinga([]))
        self.assertEquals(""a"", YamlToIcinga.value_to_icinga([""a""]))
        self.assertEquals(""a,b"", YamlToIcinga.value_to_icinga([""a"", ""b""]))
        self.assertEquals(""a,,b"", YamlToIcinga.value_to_icinga([""a"", None, ""b""]))
        self.assertEquals("",,,"", YamlToIcinga.value_to_icinga([None, None, None, None]))
        self.assertEquals("",23,42,"", YamlToIcinga.value_to_icinga([None, ""23"", 42, None]))

    def _get_config_mock(self, host=None, services=None):
        config = Mock()
        config.host = host or {}
        config.services = services or {}
        return config

    def test_write_section_forbidden_characters(self):
        # Malicious hosts may try to insert new sections, e.g. by setting a
        # value to  ""42\n}\n define command {\n ......"" which would lead to
        # arbitrary code execution. Therefore, certain characters must be
        # forbidden.
        header = Mock()
        header.serialize.return_value = ""the header""

        for forbidden in '\n', '}':
            # Forbidden character in 'host' section.
            config = self._get_config_mock(host={'key': 'xx%syy' % forbidden})
            self.assertRaises(Exception, YamlToIcinga, config, header)
            config = self._get_config_mock(host={'xx%syy' % forbidden: ""value""})
            self.assertRaises(Exception, YamlToIcinga, config, header)

            config = self._get_config_mock(services={'foo': 'xx%syy' % forbidden})
            self.assertRaises(Exception, YamlToIcinga, config, header)
            config = self._get_config_mock(services={'xx%syy' % forbidden: ""value""})
            self.assertRaises(Exception, YamlToIcinga, config, header)
/n/n/n",0
15,15,a4b01b72d2e3d6ec2600c384a77f675fa9bbf6b7,"/src/main/python/monitoring_config_generator/MonitoringConfigGenerator.py/n/n""""""monconfgenerator

Creates an Icinga monitoring configuration. It does it by querying an URL from
which it receives a specially formatted yaml file. This file is transformed into
a valid Icinga configuration file.
If no URL is given it reads it's default configuration from file system. The
configuration file is: /etc/monitoring_config_generator/config.yaml'

Usage:
  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]
  monconfgenerator -h

Options:
  -h                Show this message.
  --debug           Print additional information.
  --targetdir=DIR   The generated Icinga monitoring configuration is written
                    into this directory. If no target directory is given its
                    value is read from /etc/monitoring_config_generator/config.yaml
  --skip-checks     Do not run checks on the yaml file received from the URL.

""""""
from datetime import datetime
import logging
import os
import sys

from docopt import docopt

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \
    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException
from monitoring_config_generator import set_log_level_to_debug
from monitoring_config_generator.yaml_tools.readers import Header, read_config
from monitoring_config_generator.yaml_tools.config import YamlConfig
from monitoring_config_generator.settings import CONFIG


EXIT_CODE_CONFIG_WRITTEN = 0
EXIT_CODE_ERROR = 1
EXIT_CODE_NOT_WRITTEN = 2

LOG = logging.getLogger(""monconfgenerator"")


class MonitoringConfigGenerator(object):
    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):
        self.skip_checks = skip_checks
        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']
        self.source = url

        if debug_enabled:
            set_log_level_to_debug()

        if not self.target_dir or not os.path.isdir(self.target_dir):
            raise MonitoringConfigGeneratorException(""%s is not a directory"" % self.target_dir)

        LOG.debug(""Using %s as target dir"" % self.target_dir)
        LOG.debug(""Using URL: %s"" % self.source)
        LOG.debug(""MonitoringConfigGenerator start: reading from %s, writing to %s"" %
                  (self.source, self.target_dir))

    def _is_newer(self, header_source, hostname):
        if not hostname:
            raise NoSuchHostname('hostname not found')
        output_path = self.output_path(self.create_filename(hostname))
        old_header = Header.parse(output_path)
        return header_source.is_newer_than(old_header)

    def output_path(self, file_name):
        return os.path.join(self.target_dir, file_name)

    def write_output(self, file_name, yaml_icinga):
        lines = yaml_icinga.icinga_lines
        output_writer = OutputWriter(self.output_path(file_name))
        output_writer.write_lines(lines)

    @staticmethod
    def create_filename(hostname):
        name = '%s.cfg' % hostname
        if name != os.path.basename(name):
            msg = ""Directory traversal attempt detected for host name %r""
            raise Exception(msg % hostname)
        return name

    def generate(self):
        file_name = None
        raw_yaml_config, header_source = read_config(self.source)

        if raw_yaml_config is None:
            raise SystemExit(""Raw yaml config from source '%s' is 'None'."" % self.source)

        yaml_config = YamlConfig(raw_yaml_config,
                                 skip_checks=self.skip_checks)

        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):
            file_name = self.create_filename(yaml_config.host_name)
            yaml_icinga = YamlToIcinga(yaml_config, header_source)
            self.write_output(file_name, yaml_icinga)

        if file_name:
            LOG.info(""Icinga config file '%s' created."" % file_name)

        return file_name

class YamlToIcinga(object):
    def __init__(self, yaml_config, header):
        self.icinga_lines = []
        self.indent = CONFIG['INDENT']
        self.icinga_lines.extend(header.serialize())
        self.write_section('host', yaml_config.host)
        for service in yaml_config.services:
            self.write_section('service', service)

    def write_line(self, line):
        self.icinga_lines.append(line)

    def write_section(self, section_name, section_data):
        self.write_line("""")
        self.write_line(""define %s {"" % section_name)
        sorted_keys = section_data.keys()
        sorted_keys.sort()
        for key in sorted_keys:
            value = section_data[key]
            self.icinga_lines.append((""%s%-45s%s"" % (self.indent, key, self.value_to_icinga(value))))
        self.write_line(""}"")

    @staticmethod
    def value_to_icinga(value):
        """"""Convert a scalar or list to Icinga value format. Lists are concatenated by ,
        and empty (None) values produce an empty string""""""
        if isinstance(value, list):
            # explicitly set None values to empty string
            return "","".join([str(x) if (x is not None) else """" for x in value])
        else:
            return str(value)


class OutputWriter(object):
    def __init__(self, output_file):
        self.output_file = output_file

    def write_lines(self, lines):
        with open(self.output_file, 'w') as f:
            for line in lines:
                f.write(line + ""\n"")
        LOG.debug(""Created %s"" % self.output_file)


def generate_config():
    arg = docopt(__doc__, version='0.1.0')
    start_time = datetime.now()
    try:
        file_name = MonitoringConfigGenerator(arg['URL'],
                                              arg['--debug'],
                                              arg['--targetdir'],
                                              arg['--skip-checks']).generate()
        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN
    except HostUnreachableException:
        LOG.warn(""Target url {0} unreachable. Could not get yaml config!"".format(arg['URL']))
        exit_code = EXIT_CODE_NOT_WRITTEN
    except ConfigurationContainsUndefinedVariables:
        LOG.error(""Configuration contained undefined variables!"")
        exit_code = EXIT_CODE_ERROR
    except SystemExit as e:
        exit_code = e.code
    except BaseException as e:
        LOG.error(e)
        exit_code = EXIT_CODE_ERROR
    finally:
        stop_time = datetime.now()
        LOG.info(""finished in %s"" % (stop_time - start_time))
    sys.exit(exit_code)


if __name__ == '__main__':
    generate_config()
/n/n/n",1
2,2,9b7805119938343fcac9dc929d8882f1d97cf14a,"vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    #subprocess escapes the username stopping code injection
    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",0
3,3,9b7805119938343fcac9dc929d8882f1d97cf14a,"/vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd 

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    os.system(""useradd -G docker,wheel -p ""+encPass+"" ""+username)

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",1
10,10,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
import re
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.text import encode_for_xml
from invenio.utils.filedownload import download_url

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""
    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        line_nb = len(text_lines)
        word_nb = 0
        for line in text_lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))
        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            local_file = download_url(entry)
            text_lines = extractor.text_lines_from_local_file(local_file)
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Output keywords reading a local file.

    Arguments and output are the same as for :see: get_keywords_from_text().
    """"""
    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings.

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""
    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext.

    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/ninvenio/legacy/bibclassify/ontology_reader.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify ontology reader.

The ontology reader reads currently either a RDF/SKOS taxonomy or a
simple controlled vocabulary file (1 word per line). The first role of
this module is to manage the cached version of the ontology file. The
second role is to hold all methods responsible for the creation of
regular expressions. These methods are grammatically related as we take
care of different forms of the same words.  The grammatical rules can be
configured via the configuration file.

The main method from this module is get_regular_expressions.
""""""

from __future__ import print_function

from datetime import datetime, timedelta
from six import iteritems
from six.moves import cPickle

import os
import re
import sys
import tempfile
import time
import urllib2
import traceback
import xml.sax
import thread
import rdflib

from invenio.legacy.bibclassify import config as bconfig
from invenio.modules.classifier.errors import TaxonomyError

log = bconfig.get_logger(""bibclassify.ontology_reader"")
from invenio import config

from invenio.modules.classifier.registry import taxonomies

# only if not running in a stanalone mode
if bconfig.STANDALONE:
    dbquery = None
    from urllib2 import urlopen
else:
    from invenio.legacy import dbquery
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

_contains_digit = re.compile(""\d"")
_starts_with_non = re.compile(""(?i)^non[a-z]"")
_starts_with_anti = re.compile(""(?i)^anti[a-z]"")
_split_by_punctuation = re.compile(""(\W+)"")

_CACHE = {}


def get_cache(taxonomy_id):
    """"""Return thread-safe cache for the given taxonomy id.

    :param taxonomy_id: identifier of the taxonomy
    :type taxonomy_id: str

    :return: dictionary object (empty if no taxonomy_id
        is found), you must not change anything inside it.
        Create a new dictionary and use set_cache if you want
        to update the cache!
    """"""
    # Because of a standalone mode, we don't use the
    # invenio.data_cacher.DataCacher, but it has no effect
    # on proper functionality.

    if taxonomy_id in _CACHE:
        ctime, taxonomy = _CACHE[taxonomy_id]

        # check it is fresh version
        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)
        cache_path = _get_cache_path(onto_name)

        # if source exists and is newer than the cache hold in memory
        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:
            log.info('Forcing taxonomy rebuild as cached'
                     ' version is newer/updated.')
            return {}  # force cache rebuild

        # if cache exists and is newer than the cache hold in memory
        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:
            log.info('Forcing taxonomy rebuild as source'
                     ' file is newer/updated.')
            return {}
        log.info('Taxonomy retrieved from cache')
        return taxonomy
    return {}


def set_cache(taxonomy_id, contents):
    """"""Update cache in a thread-safe manner.""""""
    lock = thread.allocate_lock()
    lock.acquire()
    try:
        _CACHE[taxonomy_id] = (time.time(), contents)
    finally:
        lock.release()


def get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):
    """"""Return a list of patterns compiled from the RDF/SKOS ontology.

    Uses cache if it exists and if the taxonomy hasn't changed.
    """"""
    # Translate the ontology name into a local path. Check if the name
    # relates to an existing ontology.
    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)
    if not onto_path:
        raise TaxonomyError(""Unable to locate the taxonomy: '%s'.""
                            % taxonomy_name)

    cache_path = _get_cache_path(onto_name)
    log.debug('Taxonomy discovered, now we load it '
              '(from cache: %s, onto_path: %s, cache_path: %s)'
              % (not no_cache, onto_path, cache_path))

    if os.access(cache_path, os.R_OK):
        if os.access(onto_path, os.R_OK):
            if rebuild or no_cache:
                log.debug(""Cache generation was manually forced."")
                return _build_cache(onto_path, skip_cache=no_cache)
        else:
            # ontology file not found. Use the cache instead.
            log.warning(""The ontology couldn't be located. However ""
                        ""a cached version of it is available. Using it as a ""
                        ""reference."")
            return _get_cache(cache_path, source_file=onto_path)

        if (os.path.getmtime(cache_path) >
                os.path.getmtime(onto_path)):
            # Cache is more recent than the ontology: use cache.
            log.debug(""Normal situation, cache is older than ontology,""
                      "" so we load it from cache"")
            return _get_cache(cache_path, source_file=onto_path)
        else:
            # Ontology is more recent than the cache: rebuild cache.
            log.warning(""Cache '%s' is older than '%s'. ""
                        ""We will rebuild the cache"" %
                        (cache_path, onto_path))
            return _build_cache(onto_path, skip_cache=no_cache)

    elif os.access(onto_path, os.R_OK):
        if not no_cache and\
                os.path.exists(cache_path) and\
                not os.access(cache_path, os.W_OK):
            raise TaxonomyError('We cannot read/write into: %s. '
                                'Aborting!' % cache_path)
        elif not no_cache and os.path.exists(cache_path):
            log.warning('Cache %s exists, but is not readable!' % cache_path)
        log.info(""Cache not available. Building it now: %s"" % onto_path)
        return _build_cache(onto_path, skip_cache=no_cache)

    else:
        raise TaxonomyError(""We miss both source and cache""
                            "" of the taxonomy: %s"" % taxonomy_name)


def _get_remote_ontology(onto_url, time_difference=None):
    """"""Check if the online ontology is more recent than the local ontology.

    If yes, try to download and store it in Invenio's cache directory.

    Return a boolean describing the success of the operation.

    :return: path to the downloaded ontology.
    """"""
    if onto_url is None:
        return False

    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +
              ""bibclassify"" + os.sep)
    if not os.path.exists(dl_dir):
        os.mkdir(dl_dir)

    local_file = dl_dir + os.path.basename(onto_url)
    remote_modif_time = _get_last_modification_date(onto_url)
    try:
        local_modif_seconds = os.path.getmtime(local_file)
    except OSError:
        # The local file does not exist. Download the ontology.
        download = True
        log.info(""The local ontology could not be found."")
    else:
        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])
        # Let's set a time delta of 1 hour and 10 minutes.
        time_difference = time_difference or timedelta(hours=1, minutes=10)
        download = remote_modif_time > local_modif_time + time_difference
        if download:
            log.info(""The remote ontology '%s' is more recent ""
                     ""than the local ontology."" % onto_url)

    if download:
        if not _download_ontology(onto_url, local_file):
            log.warning(""Error downloading the ontology from: %s"" % onto_url)

    return local_file


def _get_ontology(ontology):
    """"""Return the (name, path, url) to the short ontology name.

    :param ontology: name of the ontology or path to the file or url.
    """"""
    onto_name = onto_path = onto_url = None

    # first assume we got the path to the file
    if os.path.exists(ontology):
        onto_name = os.path.split(os.path.abspath(ontology))[1]
        onto_path = os.path.abspath(ontology)
        onto_url = """"
    else:
        # if not, try to find it in a known locations
        discovered_file = _discover_ontology(ontology)
        if discovered_file:
            onto_name = os.path.split(discovered_file)[1]
            onto_path = discovered_file
            # i know, this sucks
            x = ontology.lower()
            if ""http:"" in x or ""https:"" in x or ""ftp:"" in x or ""file:"" in x:
                onto_url = ontology
            else:
                onto_url = """"
        else:
            # not found, look into a database
            # (it is last because when bibclassify
            # runs in a standalone mode,
            # it has no database - [rca, old-heritage]
            if not bconfig.STANDALONE:
                result = dbquery.run_sql(""SELECT name, location from clsMETHOD WHERE name LIKE %s"",
                                         ('%' + ontology + '%',))
                for onto_short_name, url in result:
                    onto_name = onto_short_name
                    onto_path = _get_remote_ontology(url)
                    onto_url = url

    return (onto_name, onto_path, onto_url)


def _discover_ontology(ontology_name):
    """"""Look for the file in a known places.

    Inside invenio/etc/bibclassify and a few other places
    like current directory.

    :param ontology: name or path name or url
    :type ontology: str

    :return: absolute path of a file if found, or None
    """"""
    last_part = os.path.split(os.path.abspath(ontology_name))[1]
    if last_part in taxonomies:
        return taxonomies.get(last_part)
    elif last_part + "".rdf"" in taxonomies:
        return taxonomies.get(last_part + "".rdf"")
    else:
        log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)

    # LEGACY
    possible_patterns = [last_part, last_part.lower()]
    if not last_part.endswith('.rdf'):
        possible_patterns.append(last_part + '.rdf')
    places = [config.CFG_CACHEDIR,
              config.CFG_ETCDIR,
              os.path.join(config.CFG_CACHEDIR, ""bibclassify""),
              os.path.join(config.CFG_ETCDIR, ""bibclassify""),
              os.path.abspath('.'),
              os.path.abspath(os.path.join(os.path.dirname(__file__),
                                           ""../../../etc/bibclassify"")),
              os.path.join(os.path.dirname(__file__), ""bibclassify""),
              config.CFG_WEBDIR]

    log.debug(""Searching for taxonomy using string: %s"" % last_part)
    log.debug(""Possible patterns: %s"" % possible_patterns)
    for path in places:

        try:
            if os.path.isdir(path):
                log.debug(""Listing: %s"" % path)
                for filename in os.listdir(path):
                    #log.debug('Testing: %s' % filename)
                    for pattern in possible_patterns:
                        filename_lc = filename.lower()
                        if pattern == filename_lc and\
                                os.path.exists(os.path.join(path, filename)):
                            filepath = os.path.abspath(os.path.join(path,
                                                                    filename))
                            if (os.access(filepath, os.R_OK)):
                                log.debug(""Found taxonomy at: %s"" % filepath)
                                return filepath
                            else:
                                log.warning('Found taxonony at: %s, but it is'
                                            ' not readable. '
                                            'Continue searching...'
                                            % filepath)
        except OSError, os_error_msg:
            log.warning('OS Error when listing path '
                        '""%s"": %s' % (str(path), str(os_error_msg)))
    log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)


class KeywordToken:

    """"""KeywordToken is a class used for the extracted keywords.

    It can be initialized with values from RDF store or from
    simple strings. Specialty of this class is that objects are
    hashable by subject - so in the dictionary two objects with the
    same subject appears as one -- :see: self.__hash__ and self.__cmp__.
    """"""

    def __init__(self, subject, store=None, namespace=None, type='HEP'):
        """"""Initialize KeywordToken with a subject.

        :param subject: string or RDF object
        :param store: RDF graph object
                      (will be used to get info about the subject)
        :param namespace: RDF namespace object, used together with store
        :param type: type of this keyword.
        """"""
        self.id = subject
        self.type = type
        self.short_id = subject
        self.concept = """"
        self.regex = []
        self.nostandalone = False
        self.spires = False
        self.fieldcodes = []
        self.compositeof = []
        self.core = False
        # True means composite keyword
        self._composite = '#Composite' in subject
        self.__hash = None

        # the tokens are coming possibly from a normal text file
        if store is None:
            subject = subject.strip()
            self.concept = subject
            self.regex = _get_searchable_regex(basic=[subject])
            self.nostandalone = False
            self.fieldcodes = []
            self.core = False
            if subject.find(' ') > -1:
                self._composite = True

        # definitions from rdf
        else:
            self.short_id = self.short_id.split('#')[-1]

            # find alternate names for this label
            basic_labels = []

            # turn those patterns into regexes only for simple keywords
            if self._composite is False:
                try:
                    for label in store.objects(subject,
                                               namespace[""prefLabel""]):
                        # XXX shall i make it unicode?
                        basic_labels.append(str(label))
                except TypeError:
                    pass
                self.concept = basic_labels[0]
            else:
                try:
                    self.concept = str(store.value(subject,
                                                   namespace[""prefLabel""],
                                                   any=True))
                except KeyError:
                    log.warning(""Keyword with subject %s has no prefLabel.""
                                "" We use raw name"" %
                                self.short_id)
                    self.concept = self.short_id

            # this is common both to composite and simple keywords
            try:
                for label in store.objects(subject, namespace[""altLabel""]):
                    basic_labels.append(str(label))
            except TypeError:
                pass

            # hidden labels are special (possibly regex) codes
            hidden_labels = []
            try:
                for label in store.objects(subject, namespace[""hiddenLabel""]):
                    hidden_labels.append(unicode(label))
            except TypeError:
                pass

            # compile regular expression that will identify this token
            self.regex = _get_searchable_regex(basic_labels, hidden_labels)

            try:
                for note in map(lambda s: str(s).lower().strip(),
                                store.objects(subject, namespace[""note""])):
                    if note == 'core':
                        self.core = True
                    elif note in (""nostandalone"", ""nonstandalone""):
                        self.nostandalone = True
                    elif 'fc:' in note:
                        self.fieldcodes.append(note[3:].strip())
            except TypeError:
                pass

            # spiresLabel does not have multiple values
            spires_label = store.value(subject, namespace[""spiresLabel""])
            if spires_label:
                self.spires = str(spires_label)

        # important for comparisons
        self.__hash = hash(self.short_id)

        # extract composite parts ids
        if store is not None and self.isComposite():
            small_subject = self.id.split(""#Composite."")[-1]
            component_positions = []
            for label in store.objects(self.id, namespace[""compositeOf""]):
                strlabel = str(label).split(""#"")[-1]
                component_name = label.split(""#"")[-1]
                component_positions.append((small_subject.find(component_name),
                                            strlabel))
            component_positions.sort()
            if not component_positions:
                log.error(""Keyword is marked as composite, ""
                          ""but no composite components refs found: %s""
                          % self.short_id)
            else:
                self.compositeof = map(lambda x: x[1], component_positions)

    def refreshCompositeOf(self, single_keywords, composite_keywords,
                           store=None, namespace=None):
        """"""Re-check sub-parts of this keyword.

        This should be called after the whole RDF was processed, because
        it is using a cache of single keywords and if that
        one is incomplete, you will not identify all parts.
        """"""
        def _get_ckw_components(new_vals, label):
            if label in single_keywords:
                new_vals.append(single_keywords[label])
            elif ('Composite.%s' % label) in composite_keywords:
                for l in composite_keywords['Composite.%s' % label].compositeof:
                    _get_ckw_components(new_vals, l)
            elif label in composite_keywords:
                for l in composite_keywords[label].compositeof:
                    _get_ckw_components(new_vals, l)
            else:
                # One single or composite keyword is missing from the taxonomy.
                # This is due to an error in the taxonomy description.
                message = ""The composite term \""%s\""""\
                          "" should be made of single keywords,""\
                          "" but at least one is missing."" % self.id
                if store is not None:
                    message += ""Needed components: %s""\
                               % list(store.objects(self.id,
                                      namespace[""compositeOf""]))
                message += "" Missing is: %s"" % label
                raise TaxonomyError(message)

        if self.compositeof:
            new_vals = []
            try:
                for label in self.compositeof:
                    _get_ckw_components(new_vals, label)
                self.compositeof = new_vals
            except TaxonomyError as err:
                # the composites will be empty
                # (better than to have confusing, partial matches)
                self.compositeof = []
                log.error(err)

    def isComposite(self):
        """"""Return value of _composite.""""""
        return self._composite

    def getComponents(self):
        """"""Return value of compositeof.""""""
        return self.compositeof

    def getType(self):
        """"""Return value of type.""""""
        return self.type

    def setType(self, value):
        """"""Set value of value.""""""
        self.type = value

    def __hash__(self):
        """"""Return _hash.

        This might change in the future but for the moment we want to
        think that if the concept is the same, then it is the same
        keyword - this sucks, but it is sort of how it is necessary
        to use now.
        """"""
        return self.__hash

    def __cmp__(self, other):
        """"""Compare objects using _hash.""""""
        if self.__hash < other.__hash__():
            return -1
        elif self.__hash == other.__hash__():
            return 0
        else:
            return 1

    def __str__(self, spires=False):
        """"""Return the best output for the keyword.""""""
        if spires:
            if self.spires:
                return self.spires
            elif self._composite:
                return self.concept.replace(':', ',')
            # default action
        return self.concept

    def output(self, spires=False):
        """"""Return string representation with spires value.""""""
        return self.__str__(spires=spires)

    def __repr__(self):
        """"""Class representation.""""""
        return ""<KeywordToken: %s>"" % self.short_id


def _build_cache(source_file, skip_cache=False):
    """"""Build the cached data.

    Either by parsing the RDF taxonomy file or a vocabulary file.

    :param source_file: source file of the taxonomy, RDF file
    :param skip_cache: if True, build cache will not be
        saved (pickled) - it is saved as <source_file.db>
    """"""
    store = rdflib.ConjunctiveGraph()

    if skip_cache:
        log.info(""You requested not to save the cache to disk."")
    else:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        # Make sure we have a cache_dir readable and writable.
        try:
            os.makedirs(cache_dir)
        except:
            pass
        if os.access(cache_dir, os.R_OK):
            if not os.access(cache_dir, os.W_OK):
                raise TaxonomyError(""Cache directory exists but is not""
                                    "" writable. Check your permissions""
                                    "" for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    timer_start = time.clock()

    namespace = None
    single_keywords, composite_keywords = {}, {}

    try:
        log.info(""Building RDFLib's conjunctive graph from: %s"" % source_file)
        try:
            store.parse(source_file)
        except urllib2.URLError:
            if source_file[0] == '/':
                store.parse(""file://"" + source_file)
            else:
                store.parse(""file:///"" + source_file)

    except rdflib.exceptions.Error as e:
        log.error(""Serious error reading RDF file"")
        log.error(e)
        log.error(traceback.format_exc())
        raise rdflib.exceptions.Error(e)

    except (xml.sax.SAXParseException, ImportError) as e:
        # File is not a RDF file. We assume it is a controlled vocabulary.
        log.error(e)
        log.warning(""The ontology file is probably not a valid RDF file. \
            Assuming it is a controlled vocabulary file."")

        filestream = open(source_file, ""r"")
        for line in filestream:
            keyword = line.strip()
            kt = KeywordToken(keyword)
            single_keywords[kt.short_id] = kt
        if not len(single_keywords):
            raise TaxonomyError('The ontology file is not well formated')

    else:  # ok, no exception happened
        log.info(""Now building cache of keywords"")
        # File is a RDF file.
        namespace = rdflib.Namespace(""http://www.w3.org/2004/02/skos/core#"")

        single_count = 0
        composite_count = 0

        subject_objects = store.subject_objects(namespace[""prefLabel""])
        for subject, pref_label in subject_objects:
            kt = KeywordToken(subject, store=store, namespace=namespace)
            if kt.isComposite():
                composite_count += 1
                composite_keywords[kt.short_id] = kt
            else:
                single_keywords[kt.short_id] = kt
                single_count += 1

    cached_data = {}
    cached_data[""single""] = single_keywords
    cached_data[""composite""] = composite_keywords
    cached_data[""creation_time""] = time.gmtime()
    cached_data[""version_info""] = {'rdflib': rdflib.__version__,
                                   'bibclassify': bconfig.VERSION}
    log.debug(""Building taxonomy... %d terms built in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    log.info(""Total count of single keywords: %d ""
             % len(single_keywords))
    log.info(""Total count of composite keywords: %d ""
             % len(composite_keywords))

    if not skip_cache:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        log.debug(""Writing the cache into: %s"" % cache_path)
        # test again, it could have changed
        if os.access(cache_dir, os.R_OK):
            if os.access(cache_dir, os.W_OK):
                # Serialize.
                filestream = None
                try:
                    filestream = open(cache_path, ""wb"")
                except IOError as msg:
                    # Impossible to write the cache.
                    log.error(""Impossible to write cache to '%s'.""
                              % cache_path)
                    log.error(msg)
                else:
                    log.debug(""Writing cache to file %s"" % cache_path)
                    cPickle.dump(cached_data, filestream, 1)
                if filestream:
                    filestream.close()

            else:
                raise TaxonomyError(""Cache directory exists but is not ""
                                    ""writable. Check your permissions ""
                                    ""for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    # now when the whole taxonomy was parsed,
    # find sub-components of the composite kws
    # it is important to keep this call after the taxonomy was saved,
    # because we don't  want to pickle regexes multiple times
    # (as they are must be re-compiled at load time)
    for kt in composite_keywords.values():
        kt.refreshCompositeOf(single_keywords, composite_keywords,
                              store=store, namespace=namespace)

    # house-cleaning
    if store:
        store.close()

    return (single_keywords, composite_keywords)


def _capitalize_first_letter(word):
    """"""Return a regex pattern with the first letter.

    Accepts both lowercase and uppercase.
    """"""
    if word[0].isalpha():
        # These two cases are necessary in order to get a regex pattern
        # starting with '[xX]' and not '[Xx]'. This allows to check for
        # colliding regex afterwards.
        if word[0].isupper():
            return ""["" + word[0].swapcase() + word[0] + ""]"" + word[1:]
        else:
            return ""["" + word[0] + word[0].swapcase() + ""]"" + word[1:]
    return word


def _convert_punctuation(punctuation, conversion_table):
    """"""Return a regular expression for a punctuation string.""""""
    if punctuation in conversion_table:
        return conversion_table[punctuation]
    return re.escape(punctuation)


def _convert_word(word):
    """"""Return the plural form of the word if it exists.

    Otherwise return the word itself.
    """"""
    out = None

    # Acronyms.
    if word.isupper():
        out = word + ""s?""
    # Proper nouns or word with digits.
    elif word.istitle():
        out = word + ""('?s)?""
    elif _contains_digit.search(word):
        out = word

    if out is not None:
        return out

    # Words with non or anti prefixes.
    if _starts_with_non.search(word):
        word = ""non-?"" + _capitalize_first_letter(_convert_word(word[3:]))
    elif _starts_with_anti.search(word):
        word = ""anti-?"" + _capitalize_first_letter(_convert_word(word[4:]))

    if out is not None:
        return _capitalize_first_letter(out)

    # A few invariable words.
    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:
        return _capitalize_first_letter(word)

    # Some exceptions that would not produce good results with the set of
    # general_regular_expressions.
    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS
    if word in regexes:
        return _capitalize_first_letter(regexes[word])

    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS
    for regex in regexes:
        if regex.search(word) is not None:
            return _capitalize_first_letter(word)

    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS
    for regex, replacement in regexes:
        stemmed = regex.sub(replacement, word)
        if stemmed != word:
            return _capitalize_first_letter(stemmed)

    return _capitalize_first_letter(word + ""s?"")


def _get_cache(cache_file, source_file=None):
    """"""Get cached taxonomy using the cPickle module.

    No check is done at that stage.

    :param cache_file: full path to the file holding pickled data
    :param source_file: if we discover the cache is obsolete, we
        will build a new cache, therefore we need the source path
        of the cache
    :return: (single_keywords, composite_keywords).
    """"""
    timer_start = time.clock()

    filestream = open(cache_file, ""rb"")
    try:
        cached_data = cPickle.load(filestream)
        version_info = cached_data['version_info']
        if version_info['rdflib'] != rdflib.__version__\
                or version_info['bibclassify'] != bconfig.VERSION:
            raise KeyError
    except (cPickle.UnpicklingError, ImportError,
            AttributeError, DeprecationWarning, EOFError):
        log.warning(""The existing cache in %s is not readable. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        return _build_cache(source_file)
    except KeyError:
        log.warning(""The existing cache %s is not up-to-date. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        if source_file and os.path.exists(source_file):
            return _build_cache(source_file)
        else:
            log.error(""The cache contains obsolete data (and it was deleted), ""
                      ""however I can't build a new cache, the source does not ""
                      ""exist or is inaccessible! - %s"" % source_file)
    filestream.close()

    single_keywords = cached_data[""single""]
    composite_keywords = cached_data[""composite""]

    # the cache contains only keys of the composite keywords, not the objects
    # so now let's resolve them into objects
    for kw in composite_keywords.values():
        kw.refreshCompositeOf(single_keywords, composite_keywords)

    log.debug(""Retrieved taxonomy from cache %s created on %s"" %
              (cache_file, time.asctime(cached_data[""creation_time""])))

    log.debug(""%d terms read in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    return (single_keywords, composite_keywords)


def _get_cache_path(source_file):
    """"""Return the path where the cache should be written/located.

    :param onto_name: name of the ontology or the full path
    :return: string, abs path to the cache file in the tmpdir/bibclassify
    """"""
    local_name = os.path.basename(source_file)
    cache_name = local_name + "".db""
    cache_dir = os.path.join(config.CFG_CACHEDIR, ""bibclassify"")

    if not os.path.isdir(cache_dir):
        os.makedirs(cache_dir)

    return os.path.abspath(os.path.join(cache_dir, cache_name))


def _get_last_modification_date(url):
    """"""Get the last modification date of the ontology.""""""
    request = urllib2.Request(url)
    request.get_method = lambda: ""HEAD""
    http_file = urlopen(request)
    date_string = http_file.headers[""last-modified""]
    parsed = time.strptime(date_string, ""%a, %d %b %Y %H:%M:%S %Z"")
    return datetime(*(parsed)[0:6])


def _download_ontology(url, local_file):
    """"""Download the ontology and stores it in CFG_CACHEDIR.""""""
    log.debug(""Copying remote ontology '%s' to file '%s'."" % (url,
                                                              local_file))
    try:
        url_desc = urlopen(url)
        file_desc = open(local_file, 'w')
        file_desc.write(url_desc.read())
        file_desc.close()
    except IOError as e:
        print(e)
        return False
    except:
        log.warning(""Unable to download the ontology. '%s'"" %
                    sys.exc_info()[0])
        return False
    else:
        log.debug(""Done copying."")
        return True


def _get_searchable_regex(basic=None, hidden=None):
    """"""Return the searchable regular expressions for the single keyword.""""""
    # Hidden labels are used to store regular expressions.
    basic = basic or []
    hidden = hidden or []

    hidden_regex_dict = {}
    for hidden_label in hidden:
        if _is_regex(hidden_label):
            hidden_regex_dict[hidden_label] = \
                re.compile(
                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]
                )
        else:
            pattern = _get_regex_pattern(hidden_label)
            hidden_regex_dict[hidden_label] = re.compile(
                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
            )

    # We check if the basic label (preferred or alternative) is matched
    # by a hidden label regex. If yes, discard it.
    regex_dict = {}
    # Create regex for plural forms and add them to the hidden labels.
    for label in basic:
        pattern = _get_regex_pattern(label)
        regex_dict[label] = re.compile(
            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
        )

    # Merge both dictionaries.
    regex_dict.update(hidden_regex_dict)

    return regex_dict.values()


def _get_regex_pattern(label):
    """"""Return a regular expression of the label.

    This takes care of plural and different kinds of separators.
    """"""
    parts = _split_by_punctuation.split(label)

    for index, part in enumerate(parts):
        if index % 2 == 0:
            # Word
            if not parts[index].isdigit() and len(parts[index]) > 1:
                parts[index] = _convert_word(parts[index])
        else:
            # Punctuation
            if not parts[index + 1]:
                # The separator is not followed by another word. Treat
                # it as a symbol.
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SYMBOLS
                )
            else:
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SEPARATORS
                )

    return """".join(parts)


def _is_regex(string):
    """"""Check if a concept is a regular expression.""""""
    return string[0] == ""/"" and string[-1] == ""/""


def check_taxonomy(taxonomy):
    """"""Check the consistency of the taxonomy.

    Outputs a list of errors and warnings.
    """"""
    log.info(""Building graph with Python RDFLib version %s"" %
             rdflib.__version__)

    store = rdflib.ConjunctiveGraph()

    try:
        store.parse(taxonomy)
    except:
        log.error(""The taxonomy is not a valid RDF file. Are you ""
                  ""trying to check a controlled vocabulary?"")
        raise TaxonomyError('Error in RDF file')

    log.info(""Graph was successfully built."")

    prefLabel = ""prefLabel""
    hiddenLabel = ""hiddenLabel""
    altLabel = ""altLabel""
    composite = ""composite""
    compositeOf = ""compositeOf""
    note = ""note""

    both_skw_and_ckw = []

    # Build a dictionary we will reason on later.
    uniq_subjects = {}
    for subject in store.subjects():
        uniq_subjects[subject] = None

    subjects = {}
    for subject in uniq_subjects:
        strsubject = str(subject).split(""#Composite."")[-1]
        strsubject = strsubject.split(""#"")[-1]
        if (strsubject == ""http://cern.ch/thesauri/HEPontology.rdf"" or
           strsubject == ""compositeOf""):
            continue
        components = {}
        for predicate, value in store.predicate_objects(subject):
            strpredicate = str(predicate).split(""#"")[-1]
            strobject = str(value).split(""#Composite."")[-1]
            strobject = strobject.split(""#"")[-1]
            components.setdefault(strpredicate, []).append(strobject)
        if strsubject in subjects:
            both_skw_and_ckw.append(strsubject)
        else:
            subjects[strsubject] = components

    log.info(""Taxonomy contains %s concepts."" % len(subjects))

    no_prefLabel = []
    multiple_prefLabels = []
    bad_notes = []
    # Subjects with no composite or compositeOf predicate
    lonely = []
    both_composites = []
    bad_hidden_labels = {}
    bad_alt_labels = {}
    # Problems with composite keywords
    composite_problem1 = []
    composite_problem2 = []
    composite_problem3 = []
    composite_problem4 = {}
    composite_problem5 = []
    composite_problem6 = []

    stemming_collisions = []
    interconcept_collisions = {}

    for subject, predicates in iteritems(subjects):
        # No prefLabel or multiple prefLabels
        try:
            if len(predicates[prefLabel]) > 1:
                multiple_prefLabels.append(subject)
        except KeyError:
            no_prefLabel.append(subject)

        # Lonely and both composites.
        if composite not in predicates and compositeOf not in predicates:
            lonely.append(subject)
        elif composite in predicates and compositeOf in predicates:
            both_composites.append(subject)

        # Multiple or bad notes
        if note in predicates:
            bad_notes += [(subject, n) for n in predicates[note]
                          if n not in ('nostandalone', 'core')]

        # Bad hidden labels
        if hiddenLabel in predicates:
            for lbl in predicates[hiddenLabel]:
                if lbl.startswith(""/"") ^ lbl.endswith(""/""):
                    bad_hidden_labels.setdefault(subject, []).append(lbl)

        # Bad alt labels
        if altLabel in predicates:
            for lbl in predicates[altLabel]:
                if len(re.findall(""/"", lbl)) >= 2 or "":"" in lbl:
                    bad_alt_labels.setdefault(subject, []).append(lbl)

        # Check composite
        if composite in predicates:
            for ckw in predicates[composite]:
                if ckw in subjects:
                    if compositeOf in subjects[ckw]:
                        if subject not in subjects[ckw][compositeOf]:
                            composite_problem3.append((subject, ckw))
                    else:
                        if ckw not in both_skw_and_ckw:
                            composite_problem2.append((subject, ckw))
                else:
                    composite_problem1.append((subject, ckw))

        # Check compositeOf
        if compositeOf in predicates:
            for skw in predicates[compositeOf]:
                if skw in subjects:
                    if composite in subjects[skw]:
                        if subject not in subjects[skw][composite]:
                            composite_problem6.append((subject, skw))
                    else:
                        if skw not in both_skw_and_ckw:
                            composite_problem5.append((subject, skw))
                else:
                    composite_problem4.setdefault(skw, []).append(subject)

        # Check for stemmed labels
        if compositeOf in predicates:
            labels = (altLabel, hiddenLabel)
        else:
            labels = (prefLabel, altLabel, hiddenLabel)

        patterns = {}
        for label in [lbl for lbl in labels if lbl in predicates]:
            for expression in [expr for expr in predicates[label]
                               if not _is_regex(expr)]:
                pattern = _get_regex_pattern(expression)
                interconcept_collisions.setdefault(pattern, []).\
                    append((subject, label))
                if pattern in patterns:
                    stemming_collisions.append(
                        (subject,
                         patterns[pattern],
                         (label, expression)
                         )
                    )
                else:
                    patterns[pattern] = (label, expression)

    print(""\n==== ERRORS ===="")

    if no_prefLabel:
        print(""\nConcepts with no prefLabel: %d"" % len(no_prefLabel))
        print(""\n"".join([""   %s"" % subj for subj in no_prefLabel]))
    if multiple_prefLabels:
        print((""\nConcepts with multiple prefLabels: %d"" %
               len(multiple_prefLabels)))
        print(""\n"".join([""   %s"" % subj for subj in multiple_prefLabels]))
    if both_composites:
        print((""\nConcepts with both composite properties: %d"" %
               len(both_composites)))
        print(""\n"".join([""   %s"" % subj for subj in both_composites]))
    if bad_hidden_labels:
        print(""\nConcepts with bad hidden labels: %d"" % len(bad_hidden_labels))
        for kw, lbls in iteritems(bad_hidden_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if bad_alt_labels:
        print(""\nConcepts with bad alt labels: %d"" % len(bad_alt_labels))
        for kw, lbls in iteritems(bad_alt_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if both_skw_and_ckw:
        print((""\nKeywords that are both skw and ckw: %d"" %
               len(both_skw_and_ckw)))
        print(""\n"".join([""   %s"" % subj for subj in both_skw_and_ckw]))

    print()

    if composite_problem1:
        print(""\n"".join([""SKW '%s' references an unexisting CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem1]))
    if composite_problem2:
        print(""\n"".join([""SKW '%s' references a SKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem2]))
    if composite_problem3:
        print(""\n"".join([""SKW '%s' is not composite of CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem3]))
    if composite_problem4:
        for skw, ckws in iteritems(composite_problem4):
            print(""SKW '%s' does not exist but is "" ""referenced by:"" % skw)
            print(""\n"".join([""    %s"" % ckw for ckw in ckws]))
    if composite_problem5:
        print(""\n"".join([""CKW '%s' references a CKW '%s'."" % kw
                         for kw in composite_problem5]))
    if composite_problem6:
        print(""\n"".join([""CKW '%s' is not composed by SKW '%s'."" % kw
                         for kw in composite_problem6]))

    print(""\n==== WARNINGS ===="")

    if bad_notes:
        print((""\nConcepts with bad notes: %d"" % len(bad_notes)))
        print(""\n"".join([""   '%s': '%s'"" % _note for _note in bad_notes]))
    if stemming_collisions:
        print(""\nFollowing keywords have unnecessary labels that have ""
              ""already been generated by BibClassify."")
        for subj in stemming_collisions:
            print(""   %s:\n     %s\n     and %s"" % subj)

    print(""\nFinished."")
    sys.exit(0)


def test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):
    """"""Test the cache lookup.""""""
    cache = get_cache(taxonomy_name)
    if not cache:
        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,
                                                         rebuild=rebuild_cache,
                                                         no_cache=no_cache))
        cache = get_cache(taxonomy_name)
    return (thread.get_ident(), cache)


log.info('Loaded ontology reader')

if __name__ == '__main__':
    test_cache()
/n/n/ninvenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re

from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Check if a document is a PDF file and returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[-1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    # os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Return the fulltext of the local file.

    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""
    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    # Discard lines that do not contain at least one word.
    return [line for line in lines if _ONE_WORD.search(line) is not None]


def executable_exists(executable):
    """"""Test if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False
/n/n/n",0
11,11,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"/invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,
                                                       user_agent=make_user_agent_string(""BibClassify""))
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Outputs keywords reading a local file. Arguments and output are the same
    as for :see: get_keywords_from_text() """"""

    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/n/invenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""
BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re
import tempfile
import urllib2
from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Checks if a document is a PDF file. Returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported. Please report this to cds.support@cern.ch."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    #os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Returns the fulltext of the local file.
    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""

    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    if not _is_english_text('\n'.join(lines)):
        log.warning(""It seems the file '%s' is unvalid and doesn't ""
                    ""contain text. Please communicate this file to the Invenio ""
                    ""team."" % document)

    line_nb = len(lines)
    word_nb = 0
    for line in lines:
        word_nb += len(re.findall(""\S+"", line))

    # Discard lines that do not contain at least one word.
    lines = [line for line in lines if _ONE_WORD.search(line) is not None]

    if not remote:
        log.info(""Local file has %d lines and %d words."" % (line_nb, word_nb))

    return lines


def _is_english_text(text):
    """"""
    Checks if a text is correct english.
    Computes the number of words in the text and compares it to the
    expected number of words (based on an average size of words of 5.1
    letters).

    @param text_lines: the text to analyze
    @type text_lines:  string
    @return:           True if the text is English, False otherwise
    @rtype:            Boolean
    """"""
    # Consider one word and one space.
    avg_word_length = 2.55 + 1
    expected_word_number = float(len(text)) / avg_word_length

    words = [word
             for word in re.split('\W', text)
             if word.isalpha()]

    word_number = len(words)

    return word_number > expected_word_number


def text_lines_from_url(url, user_agent=""""):
    """"""Returns the fulltext of the file found at the URL.""""""
    request = urllib2.Request(url)
    if user_agent:
        request.add_header(""User-Agent"", user_agent)
    try:
        distant_stream = urlopen(request)
        # Write the URL content to a temporary file.
        local_file = tempfile.mkstemp(prefix=""bibclassify."")[1]
        local_stream = open(local_file, ""w"")
        local_stream.write(distant_stream.read())
        local_stream.close()
    except:
        log.error(""Unable to read from URL %s."" % url)
        return None
    else:
        # Read lines from the temporary file.
        lines = text_lines_from_local_file(local_file, remote=True)
        os.remove(local_file)

        line_nb = len(lines)
        word_nb = 0
        for line in lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))

        return lines


def executable_exists(executable):
    """"""Tests if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False


/n/n/n",1
80,80,18609fa3b8b1e8cca95f1021d60750628abf7433,"genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_aggregations = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_aggregations"" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[""benchmark_aggregations""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/ngenomics_benchmarks/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import allel
import zarr
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import os
import pandas as pd
from collections import OrderedDict
from genomics_benchmarks import config, data_service


class BenchmarkResultsData:
    run_number = None
    operation_name = None
    start_time = None
    exec_time = None

    def to_dict(self):
        return OrderedDict([(""Log Timestamp"", datetime.datetime.fromtimestamp(self.start_time)),
                            (""Run Number"", self.run_number),
                            (""Operation"", self.operation_name),
                            (""Execution Time"", self.exec_time)])

    def to_pandas(self):
        data = self.to_dict()
        df = pd.DataFrame(data, index=[1])
        df.index.name = '#'
        return df


class BenchmarkProfiler:
    benchmark_running = False

    def __init__(self, benchmark_label):
        self.results = BenchmarkResultsData()
        self.benchmark_label = benchmark_label

    def set_run_number(self, run_number):
        if not self.benchmark_running:
            self.results.run_number = run_number

    def start_benchmark(self, operation_name):
        if not self.benchmark_running:
            self.results.operation_name = operation_name

            self.benchmark_running = True

            # Start the benchmark timer
            self.results.start_time = time.time()

    def end_benchmark(self):
        if self.benchmark_running:
            end_time = time.time()

            # Calculate the execution time from start and end times
            self.results.exec_time = end_time - self.results.start_time

            # Save benchmark results
            self._record_runtime(self.results, ""{}.psv"".format(self.benchmark_label))

            self.benchmark_running = False

    def get_benchmark_results(self):
        return self.results

    def _record_runtime(self, benchmark_results, output_filename):
        """"""
        Records the benchmark results data entry to the specified PSV file.
        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data
        :param output_filename: Which file to output the benchmark results to
        :type benchmark_results: BenchmarkResultsData
        :type output_filename: str
        """"""
        output_filename = str(output_filename)

        psv_header = not os.path.isfile(output_filename)

        # Open the output file in append mode
        with open(output_filename, ""a"") as psv_file:
            pd_results = benchmark_results.to_pandas()
            pd_results.to_csv(psv_file, sep=""|"", header=psv_header, index=False)


class Benchmark:
    benchmark_zarr_dir = """"  # Directory for which to use data from for benchmark process
    benchmark_zarr_file = """"  # File within benchmark_zarr_dir for which to use for benchmark process

    def __init__(self, bench_conf, data_dirs, benchmark_label):
        """"""
        Sets up a Benchmark object which is used to execute benchmarks.
        :param bench_conf: Benchmark configuration data that controls the benchmark execution
        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories
        :param benchmark_label: label to use when saving benchmark results to file
        :type bench_conf: config.BenchmarkConfigurationRepresentation
        :type data_dirs: config.DataDirectoriesConfigurationRepresentation
        :type benchmark_label: str
        """"""
        self.bench_conf = bench_conf
        self.data_dirs = data_dirs
        self.benchmark_label = benchmark_label

        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)

    def run_benchmark(self):
        """"""
        Executes the benchmarking process.
        """"""
        if self.bench_conf is not None and self.data_dirs is not None:
            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):
                # Clear out existing files in Zarr benchmark directory
                # (Should be done every single run)
                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)

                # Update run number in benchmark profiler (for results tracking)
                self.benchmark_profiler.set_run_number(run_number)

                # Prepare data directory and file locations for benchmarks
                if self.bench_conf.benchmark_data_input == ""vcf"":
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark

                    # Convert VCF data to Zarr format as part of benchmark
                    self._benchmark_convert_to_zarr()

                elif self.bench_conf.benchmark_data_input == ""zarr"":
                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup
                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset

                else:
                    print(""[Exec] Error: Invalid option supplied for benchmark data input format."")
                    print(""  - Expected data input formats: vcf, zarr"")
                    print(""  - Provided data input format: {}"".format(self.bench_conf.benchmark_data_input))
                    exit(1)

                # Ensure Zarr dataset exists and can be used for upcoming benchmarks
                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)
                if (benchmark_zarr_path != """") and (os.path.isdir(benchmark_zarr_path)):
                    # Load Zarr dataset into memory
                    self._benchmark_load_zarr_dataset(benchmark_zarr_path)

                    if self.bench_conf.benchmark_aggregations:
                        self._benchmark_simple_aggregations(benchmark_zarr_path)
                else:
                    # Zarr dataset doesn't exist. Print error message and exit
                    print(""[Exec] Error: Zarr dataset could not be found for benchmarking."")
                    print(""  - Zarr dataset location: {}"".format(benchmark_zarr_path))
                    exit(1)

    def _benchmark_convert_to_zarr(self):
        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark
        input_vcf_file = self.bench_conf.benchmark_dataset
        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)

        if os.path.isfile(input_vcf_path):
            output_zarr_file = input_vcf_file
            output_zarr_file = output_zarr_file[
                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename
            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)

            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=self.bench_conf.vcf_to_zarr_config,
                                         benchmark_profiler=self.benchmark_profiler)

            self.benchmark_zarr_file = output_zarr_file
        else:
            print(""[Exec] Error: Dataset specified in configuration file does not exist. Exiting..."")
            print(""  - Dataset file specified in configuration: {}"".format(input_vcf_file))
            print(""  - Expected file location: {}"".format(input_vcf_path))
            exit(1)

    def _benchmark_load_zarr_dataset(self, zarr_path):
        self.benchmark_profiler.start_benchmark(operation_name=""Load Zarr Dataset"")
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)
        self.benchmark_profiler.end_benchmark()

    def _benchmark_simple_aggregations(self, zarr_path):
        # Load Zarr dataset
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)

        gtz = callset['calldata/GT']

        # Setup genotype Dask array for computations
        gt = allel.GenotypeDaskArray(gtz)

        # Run benchmark for allele count
        self.benchmark_profiler.start_benchmark(operation_name=""Allele Count (All Samples)"")
        gt.count_alleles().compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Variant"")
        gt.count_het(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Variant"")
        gt.count_hom(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Sample"")
        gt.count_het(axis=0).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Sample"")
        gt.count_hom(axis=0).compute()
        self.benchmark_profiler.end_benchmark()
/n/n/ngenomics_benchmarks/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import sys

# Support Python 2.x and 3.x
if sys.version_info[0] >= 3:
    from urllib.request import urlretrieve
else:
    from urllib import urlretrieve

from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc
from genomics_benchmarks import config

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    try:
        pathlib.Path(path).mkdir(parents=True)
    except OSError:  # Catch if directory already exists
        pass


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_profiler=None):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    :type benchmark_runner: core.BenchmarkProfiler
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get fields to extract (for unit testing only)
        fields = conversion_config.fields

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Read VCF file into memory for alt number"")

            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()

            numalt = callset['variants/numalt']

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Determine maximum alt number"")

            alt_number = np.max(numalt)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        if benchmark_profiler is not None:
            benchmark_profiler.start_benchmark(operation_name=""Convert VCF to Zarr"")

        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)

        if benchmark_profiler is not None:
            benchmark_profiler.end_benchmark()


GENOTYPE_ARRAY_NORMAL = 0
GENOTYPE_ARRAY_DASK = 1
GENOTYPE_ARRAY_CHUNKED = 2


def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):
    genotype_ref_name = ''

    # Ensure 'calldata' is within the callset
    if 'calldata' in callset:
        # Try to find either GT or genotype in calldata
        if 'GT' in callset['calldata']:
            genotype_ref_name = 'GT'
        elif 'genotype' in callset['calldata']:
            genotype_ref_name = 'genotype'
        else:
            return None
    else:
        return None

    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:
        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_DASK:
        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:
        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])
    else:
        return None
/n/n/ntests/test_cli.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run:  
    python -m unittest tests.test_cli 

""""""
import unittest
import sys

try:
    from unittest.mock import patch
except ImportError:
    from mock import patch
from genomics_benchmarks import cli


class TestCommandLineInterface(unittest.TestCase):

    def run_subparser_test(self, subparser_cmd, parameter, expected, default_key=None, default_value=None):
        """""" Tests subparsers for missing arguments and default values. """"""
        testargs = [""prog"", subparser_cmd, ""--"" + parameter, expected]
        with patch.object(sys, 'argv', testargs):
            args = cli.get_cli_arguments()
            self.assertEqual(args[parameter], expected,
                             subparser_cmd + "" subparser did not parse right config file arg."")
            self.assertEqual(args[""command""], subparser_cmd, subparser_cmd + "" command was not interpreted properly"")
            if default_key:
                self.assertEqual(args[default_key], default_value,
                                 subparser_cmd + "" command parser did not setup the right default key "" + default_key +
                                 "" to "" + default_value)

    def test_getting_command_arguments(self):
        """""" Tests for reading args and storing values for running all benchmark options from the command line.""""""
        # Test group 1 -- config
        self.run_subparser_test(""config"", ""output_config"", ""./benchmark.conf"")
        # Test group 2 -- setup
        self.run_subparser_test(""setup"", ""config_file"", ""./benhcmark.conf"")
        # Test group 3 - Tests if it the argparser is setting default values """"""
        self.run_subparser_test(""exec"",""config_file"",""./benchmark.conf"")

    def test_parser_expected_failing(self):
        """""" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (""something"") """"""
        testargs = [""prog""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the missing command line argument."")

        testargs = [""prog"", ""something""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the wrong command line argument."")


if __name__ == '__main__':
    unittest.main()
/n/n/ntests/test_core.py/n/nimport unittest
from genomics_benchmarks.core import *
from genomics_benchmarks.config import \
    BenchmarkConfigurationRepresentation, \
    VCFtoZarrConfigurationRepresentation, \
    DataDirectoriesConfigurationRepresentation
from time import sleep
import os
import shutil


class TestCoreBenchmark(unittest.TestCase):
    def test_benchmark_profiler_results(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_profiler_results'
        profiler = BenchmarkProfiler(profiler_label)

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = 'Sleep {} seconds'.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            # Grab benchmark results
            results = profiler.get_benchmark_results()
            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals
            results_operation_name = results.operation_name
            results_run_number = results.run_number

            # Ensure benchmark results match expected values
            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')
            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')
            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')

            i += 1

        # Delete *.psv file created when running benchmark
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_results_psv(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_results_psv'

        # Delete *.psv file created from any previous unit testing
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

        profiler = BenchmarkProfiler(profiler_label)

        operation_name_format = 'Sleep {} seconds'

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = operation_name_format.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            i += 1

        # Read results psv file
        psv_file = '{}.psv'.format(profiler_label)

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)
            num_lines = len(psv_lines)
            num_lines_expected = len(benchmark_times) + 1
            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')

            # Ensure header (first line) of psv file is correct
            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'
            header_actual = psv_lines[0]
            self.assertEqual(header_expected, header_actual)

            # Ensure contents (benchmark data) of psv file is correct
            i = 1
            for line_number in range(1, num_lines):
                content = psv_lines[line_number].split('|')

                # Ensure column count is correct
                num_columns = len(content)
                num_columns_expected = 4
                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')

                # Ensure run number is correct
                run_number_psv = int(content[1])
                run_number_expected = i
                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')

                # Ensure operation name is correct
                operation_name_psv = content[2]
                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])
                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')

                # Ensure execution time is correct
                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals
                execution_time_expected = benchmark_times[i - 1]
                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')

                i += 1

        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Delete *.psv file created when running benchmark
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_simple_aggregations(self):
        test_dir = './tests_temp/'
        benchmark_label = 'test_benchmark_simple_aggregations'
        psv_file = '{}.psv'.format(benchmark_label)

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from any previous unit tests
        if os.path.isfile(psv_file):
            os.remove(psv_file)

        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()
        vcf_to_zar_config.enabled = True

        bench_conf = BenchmarkConfigurationRepresentation()
        bench_conf.vcf_to_zarr_config = vcf_to_zar_config
        bench_conf.benchmark_number_runs = 1
        bench_conf.benchmark_data_input = 'vcf'
        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'
        bench_conf.benchmark_aggregations = True

        data_dirs = DataDirectoriesConfigurationRepresentation()
        data_dirs.vcf_dir = './tests/data/'
        data_dirs.zarr_dir_setup = './tests_temp/zarr/'
        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'
        data_dirs.temp_dir = './tests_temp/temp/'

        # Run the benchmark and ensure nothing fails
        benchmark = Benchmark(bench_conf=bench_conf,
                              data_dirs=data_dirs,
                              benchmark_label='test_benchmark_simple_aggregations')
        benchmark.run_benchmark()

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file
            num_lines = len(psv_lines)
            num_lines_expected = 10
            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')

            psv_operation_names = []

            for psv_line in psv_lines:
                line_split = psv_line.split('|')
                line_cols_actual = len(line_split)
                line_cols_expected = 4

                # Ensure correct number of data columns exist for current line of data
                self.assertEqual(line_cols_expected, line_cols_actual,
                                 msg='Unexpected number of columns in resulting psv file')

                operation_name = line_split[2]
                psv_operation_names.append(operation_name)

            # Ensure all aggregations were run
            test_operation_names = ['Allele Count (All Samples)',
                                    'Genotype Count: Heterozygous per Variant',
                                    'Genotype Count: Homozygous per Variant',
                                    'Genotype Count: Heterozygous per Sample',
                                    'Genotype Count: Homozygous per Sample']

            for test_operation_name in test_operation_names:
                if test_operation_name not in psv_operation_names:
                    self.fail(msg='Operation \""{}\"" was not run during the benchmark.'.format(test_operation_name))
        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from this unit test
        if os.path.isfile(psv_file):
            os.remove(psv_file)


if __name__ == ""__main__"":
    unittest.main()
/n/n/n",0
81,81,18609fa3b8b1e8cca95f1021d60750628abf7433,"/genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_allele_count = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_allele_count"" in runtime_config.benchmark:
                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[""benchmark_allele_count""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/n",1
46,46,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', 'º', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', 'º', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
47,47,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', 'º', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', 'º', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
50,50,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Köster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Köster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Köster""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
51,51,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
22,22,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
23,23,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
42,42,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', 'º', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', 'º', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
43,43,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', 'º', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', 'º', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
82,82,e5195bc7bcf1060f2f727acf9f0cee033262caaa,"genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]

PCA_DATA_SCALER_STANDARD = 0
PCA_DATA_SCALER_PATTERSON = 1
PCA_DATA_SCALER_NONE = 2
benchmark_pca_data_scaler_types = {PCA_DATA_SCALER_STANDARD: 'standard',
                                   PCA_DATA_SCALER_PATTERSON: 'patterson',
                                   PCA_DATA_SCALER_NONE: None}

GENOTYPE_ARRAY_NORMAL = 0
GENOTYPE_ARRAY_DASK = 1
GENOTYPE_ARRAY_CHUNKED = 2
benchmark_pca_genotype_array_types = {GENOTYPE_ARRAY_NORMAL,
                                      GENOTYPE_ARRAY_DASK,
                                      GENOTYPE_ARRAY_CHUNKED}


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_aggregations = False
    benchmark_pca = False
    vcf_to_zarr_config = None

    # PCA-specific settings
    pca_number_components = 10
    pca_data_scaler = benchmark_pca_data_scaler_types[PCA_DATA_SCALER_PATTERSON]
    pca_genotype_array_type = GENOTYPE_ARRAY_DASK
    pca_subset_size = 100000
    pca_ld_pruning_number_iterations = 2
    pca_ld_pruning_size = 100
    pca_ld_pruning_step = 20
    pca_ld_pruning_threshold = 0.01

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_aggregations"" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[""benchmark_aggregations""])
                if ""benchmark_pca"" in runtime_config.benchmark:
                    self.benchmark_pca = config_str_to_bool(runtime_config.benchmark[""benchmark_pca""])
                if ""pca_number_components"" in runtime_config.benchmark:
                    pca_number_components_str = runtime_config.benchmark[""pca_number_components""]
                    if isint(pca_number_components_str) and (int(pca_number_components_str) > 0):
                        self.pca_number_components = int(pca_number_components_str)
                    else:
                        raise ValueError(""Invalid value for pca_number_components in configuration.\n""
                                         ""pca_number_components must be a valid integer greater than 0."")
                if ""pca_data_scaler"" in runtime_config.benchmark:
                    pca_data_scaler_str = runtime_config.benchmark[""pca_data_scaler""]
                    if isint(pca_data_scaler_str) and (int(pca_data_scaler_str) in benchmark_pca_data_scaler_types):
                        self.pca_data_scaler = benchmark_pca_data_scaler_types[int(pca_data_scaler_str)]
                    else:
                        raise ValueError(""Invalid value for pca_data_scaler in configuration.\n""
                                         ""pca_data_scaler must be a valid integer between 0 and 2"")
                if ""pca_genotype_array_type"" in runtime_config.benchmark:
                    pca_genotype_array_type_str = runtime_config.benchmark[""pca_genotype_array_type""]
                    if isint(pca_genotype_array_type_str) and (
                            int(pca_genotype_array_type_str) in benchmark_pca_genotype_array_types):
                        self.pca_genotype_array_type = int(pca_genotype_array_type_str)
                    else:
                        raise ValueError(""Invalid value for pca_genotype_array_type in configuration.\n""
                                         ""pca_genotype_array_type must be a valid integer between 0 and 2"")
                if ""pca_subset_size"" in runtime_config.benchmark:
                    pca_subset_size_str = runtime_config.benchmark[""pca_subset_size""]
                    if isint(pca_subset_size_str) and (int(pca_subset_size_str) > 0):
                        self.pca_subset_size = int(pca_subset_size_str)
                    else:
                        raise ValueError(""Invalid value for pca_subset_size in configuration.\n""
                                         ""pca_subset_size must be a valid integer greater than 0."")
                if ""pca_ld_pruning_number_iterations"" in runtime_config.benchmark:
                    pca_ld_pruning_number_iterations_str = runtime_config.benchmark[""pca_ld_pruning_number_iterations""]
                    if isint(pca_ld_pruning_number_iterations_str) and (int(pca_ld_pruning_number_iterations_str) > 0):
                        self.pca_ld_pruning_number_iterations = int(pca_ld_pruning_number_iterations_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_number_iterations in configuration.\n""
                                         ""pca_ld_pruning_number_iterations must be a valid integer greater than 0."")
                if ""pca_ld_pruning_size"" in runtime_config.benchmark:
                    pca_ld_pruning_size_str = runtime_config.benchmark[""pca_ld_pruning_size""]
                    if isint(pca_ld_pruning_size_str) and (int(pca_ld_pruning_size_str) > 0):
                        self.pca_ld_pruning_size = int(pca_ld_pruning_size_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_size in configuration.\n""
                                         ""pca_ld_pruning_size must be a valid integer greater than 0."")
                if ""pca_ld_pruning_step"" in runtime_config.benchmark:
                    pca_ld_pruning_step_str = runtime_config.benchmark[""pca_ld_pruning_step""]
                    if isint(pca_ld_pruning_step_str) and (int(pca_ld_pruning_step_str) > 0):
                        self.pca_ld_pruning_step = int(pca_ld_pruning_step_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_step in configuration.\n""
                                         ""pca_ld_pruning_step must be a valid integer greater than 0."")
                if ""pca_ld_pruning_threshold"" in runtime_config.benchmark:
                    pca_ld_pruning_threshold_str = runtime_config.benchmark[""pca_ld_pruning_threshold""]
                    if isfloat(pca_ld_pruning_threshold_str) and (float(pca_ld_pruning_threshold_str) > 0):
                        self.pca_ld_pruning_threshold = float(pca_ld_pruning_threshold_str)
                    else:
                        raise ValueError(""Invalid value for pca_ld_pruning_threshold in configuration.\n""
                                         ""pca_ld_pruning_threshold must be a valid float greater than 0."")

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/ngenomics_benchmarks/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import allel
import zarr
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import numpy as np
import os
import pandas as pd
from collections import OrderedDict
from genomics_benchmarks import config, data_service


class BenchmarkResultsData:
    run_number = None
    operation_name = None
    start_time = None
    exec_time = None

    def to_dict(self):
        return OrderedDict([(""Log Timestamp"", datetime.datetime.fromtimestamp(self.start_time)),
                            (""Run Number"", self.run_number),
                            (""Operation"", self.operation_name),
                            (""Execution Time"", self.exec_time)])

    def to_pandas(self):
        data = self.to_dict()
        df = pd.DataFrame(data, index=[1])
        df.index.name = '#'
        return df


class BenchmarkProfiler:
    benchmark_running = False

    def __init__(self, benchmark_label):
        self.results = BenchmarkResultsData()
        self.benchmark_label = benchmark_label

    def set_run_number(self, run_number):
        if not self.benchmark_running:
            self.results.run_number = run_number

    def start_benchmark(self, operation_name):
        if not self.benchmark_running:
            self.results.operation_name = operation_name

            self.benchmark_running = True

            # Start the benchmark timer
            self.results.start_time = time.time()

    def end_benchmark(self):
        if self.benchmark_running:
            end_time = time.time()

            # Calculate the execution time from start and end times
            self.results.exec_time = end_time - self.results.start_time

            # Save benchmark results
            self._record_runtime(self.results, ""{}.psv"".format(self.benchmark_label))

            self.benchmark_running = False

    def get_benchmark_results(self):
        return self.results

    def _record_runtime(self, benchmark_results, output_filename):
        """"""
        Records the benchmark results data entry to the specified PSV file.
        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data
        :param output_filename: Which file to output the benchmark results to
        :type benchmark_results: BenchmarkResultsData
        :type output_filename: str
        """"""
        output_filename = str(output_filename)

        psv_header = not os.path.isfile(output_filename)

        # Open the output file in append mode
        with open(output_filename, ""a"") as psv_file:
            pd_results = benchmark_results.to_pandas()
            pd_results.to_csv(psv_file, sep=""|"", header=psv_header, index=False)


class Benchmark:
    benchmark_zarr_dir = """"  # Directory for which to use data from for benchmark process
    benchmark_zarr_file = """"  # File within benchmark_zarr_dir for which to use for benchmark process

    def __init__(self, bench_conf, data_dirs, benchmark_label):
        """"""
        Sets up a Benchmark object which is used to execute benchmarks.
        :param bench_conf: Benchmark configuration data that controls the benchmark execution
        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories
        :param benchmark_label: label to use when saving benchmark results to file
        :type bench_conf: config.BenchmarkConfigurationRepresentation
        :type data_dirs: config.DataDirectoriesConfigurationRepresentation
        :type benchmark_label: str
        """"""
        self.bench_conf = bench_conf
        self.data_dirs = data_dirs
        self.benchmark_label = benchmark_label

        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)

    def run_benchmark(self):
        """"""
        Executes the benchmarking process.
        """"""
        if self.bench_conf is not None and self.data_dirs is not None:
            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):
                # Clear out existing files in Zarr benchmark directory
                # (Should be done every single run)
                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)

                # Update run number in benchmark profiler (for results tracking)
                self.benchmark_profiler.set_run_number(run_number)

                # Prepare data directory and file locations for benchmarks
                if self.bench_conf.benchmark_data_input == ""vcf"":
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark

                    # Convert VCF data to Zarr format as part of benchmark
                    self._benchmark_convert_to_zarr()

                elif self.bench_conf.benchmark_data_input == ""zarr"":
                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup
                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset

                else:
                    print(""[Exec] Error: Invalid option supplied for benchmark data input format."")
                    print(""  - Expected data input formats: vcf, zarr"")
                    print(""  - Provided data input format: {}"".format(self.bench_conf.benchmark_data_input))
                    exit(1)

                # Ensure Zarr dataset exists and can be used for upcoming benchmarks
                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)
                if (benchmark_zarr_path != """") and (os.path.isdir(benchmark_zarr_path)):
                    # Load Zarr dataset into memory
                    self._benchmark_load_zarr_dataset(benchmark_zarr_path)

                    if self.bench_conf.benchmark_aggregations:
                        self._benchmark_simple_aggregations(benchmark_zarr_path)

                    if self.bench_conf.benchmark_pca:
                        self._benchmark_pca(benchmark_zarr_path)
                else:
                    # Zarr dataset doesn't exist. Print error message and exit
                    print(""[Exec] Error: Zarr dataset could not be found for benchmarking."")
                    print(""  - Zarr dataset location: {}"".format(benchmark_zarr_path))
                    exit(1)

    def _benchmark_convert_to_zarr(self):
        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark
        input_vcf_file = self.bench_conf.benchmark_dataset
        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)

        if os.path.isfile(input_vcf_path):
            output_zarr_file = input_vcf_file
            output_zarr_file = output_zarr_file[
                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename
            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)

            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=self.bench_conf.vcf_to_zarr_config,
                                         benchmark_profiler=self.benchmark_profiler)

            self.benchmark_zarr_file = output_zarr_file
        else:
            print(""[Exec] Error: Dataset specified in configuration file does not exist. Exiting..."")
            print(""  - Dataset file specified in configuration: {}"".format(input_vcf_file))
            print(""  - Expected file location: {}"".format(input_vcf_path))
            exit(1)

    def _benchmark_load_zarr_dataset(self, zarr_path):
        self.benchmark_profiler.start_benchmark(operation_name=""Load Zarr Dataset"")
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)
        self.benchmark_profiler.end_benchmark()

    def _benchmark_simple_aggregations(self, zarr_path):
        # Load Zarr dataset
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)

        gtz = callset['calldata/GT']

        # Setup genotype Dask array for computations
        gt = allel.GenotypeDaskArray(gtz)

        # Run benchmark for allele count
        self.benchmark_profiler.start_benchmark(operation_name=""Allele Count (All Samples)"")
        gt.count_alleles().compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Variant"")
        gt.count_het(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per variant)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Variant"")
        gt.count_hom(axis=1).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (heterozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Heterozygous per Sample"")
        gt.count_het(axis=0).compute()
        self.benchmark_profiler.end_benchmark()

        # Run benchmark for genotype count (homozygous per sample)
        self.benchmark_profiler.start_benchmark(operation_name=""Genotype Count: Homozygous per Sample"")
        gt.count_hom(axis=0).compute()
        self.benchmark_profiler.end_benchmark()

    def _benchmark_pca(self, zarr_path):
        # Load Zarr dataset
        store = zarr.DirectoryStore(zarr_path)
        callset = zarr.Group(store=store, read_only=True)

        # Get genotype data from data set
        genotype_array_type = self.bench_conf.pca_genotype_array_type
        g = data_service.get_genotype_data(callset=callset, genotype_array_type=genotype_array_type)

        # Count alleles at each variant
        self.benchmark_profiler.start_benchmark('PCA: Count alleles')
        ac = g.count_alleles()[:]
        self.benchmark_profiler.end_benchmark()

        # Count number of multiallelic SNPs
        self.benchmark_profiler.start_benchmark('PCA: Count multiallelic SNPs')
        num_multiallelic_snps = np.count_nonzero(ac.max_allele() > 1)
        self.benchmark_profiler.end_benchmark()

        # Count number of biallelic singletons
        self.benchmark_profiler.start_benchmark('PCA: Count biallelic singletons')
        num_biallelic_singletons = np.count_nonzero((ac.max_allele() == 1) & ac.is_singleton(1))
        self.benchmark_profiler.end_benchmark()

        # Apply filtering to remove singletons and multiallelic SNPs
        flt = (ac.max_allele() == 1) & (ac[:, :2].min(axis=1) > 1)
        flt_count = np.count_nonzero(flt)
        self.benchmark_profiler.start_benchmark('PCA: Remove singletons and multiallelic SNPs')
        if flt_count > 0:
            gf = g.compress(flt, axis=0)
        else:
            # Don't apply filtering
            print('[Exec][PCA] Cannot remove singletons and multiallelic SNPs as no data would remain. Skipping...')
            gf = g
        self.benchmark_profiler.end_benchmark()

        # Transform genotype data into 2-dim matrix
        self.benchmark_profiler.start_benchmark('PCA: Transform genotype data for PCA')
        gn = gf.to_n_alt()
        self.benchmark_profiler.end_benchmark()

        # Randomly choose subset of SNPs
        n = min(gn.shape[0], self.bench_conf.pca_subset_size)
        vidx = np.random.choice(gn.shape[0], n, replace=False)
        vidx.sort()
        gnr = gn.take(vidx, axis=0)

        # Apply LD pruning to subset of SNPs
        size = self.bench_conf.pca_ld_pruning_size
        step = self.bench_conf.pca_ld_pruning_step
        threshold = self.bench_conf.pca_ld_pruning_threshold
        n_iter = self.bench_conf.pca_ld_pruning_number_iterations

        self.benchmark_profiler.start_benchmark('PCA: Apply LD pruning')
        gnu = self._pca_ld_prune(gnr, size=size, step=step, threshold=threshold, n_iter=n_iter)
        self.benchmark_profiler.end_benchmark()

        # If data is chunked, move to memory for PCA
        self.benchmark_profiler.start_benchmark('PCA: Move data set to memory')
        gnu = gnu[:]
        self.benchmark_profiler.end_benchmark()

        # Run PCA analysis
        pca_num_components = self.bench_conf.pca_number_components
        scaler = self.bench_conf.pca_data_scaler

        # Run conventional PCA analysis
        self.benchmark_profiler.start_benchmark(
            'PCA: Run conventional PCA analysis (scaler: {})'.format(scaler if scaler is not None else 'none'))
        allel.pca(gnu, n_components=pca_num_components, scaler=scaler)
        self.benchmark_profiler.end_benchmark()

        # Run randomized PCA analysis
        self.benchmark_profiler.start_benchmark(
            'PCA: Run randomized PCA analysis (scaler: {})'.format(scaler if scaler is not None else 'none'))
        allel.randomized_pca(gnu, n_components=pca_num_components, scaler=scaler)
        self.benchmark_profiler.end_benchmark()

    @staticmethod
    def _pca_ld_prune(gn, size, step, threshold=.1, n_iter=1):
        blen = size * 10
        for i in range(n_iter):
            loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold, blen=blen)
            n = np.count_nonzero(loc_unlinked)
            n_remove = gn.shape[0] - n
            print(
                '[Exec][PCA][LD Prune] Iteration {}/{}: Retaining {} and removing {} variants.'.format(i + 1,
                                                                                                       n_iter,
                                                                                                       n,
                                                                                                       n_remove))
            gn = gn.compress(loc_unlinked, axis=0)
        return gn
/n/n/ngenomics_benchmarks/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import sys

# Support Python 2.x and 3.x
if sys.version_info[0] >= 3:
    from urllib.request import urlretrieve
else:
    from urllib import urlretrieve

from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc
from genomics_benchmarks import config

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    try:
        pathlib.Path(path).mkdir(parents=True)
    except OSError:  # Catch if directory already exists
        pass


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_profiler=None):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    :type benchmark_runner: core.BenchmarkProfiler
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get fields to extract (for unit testing only)
        fields = conversion_config.fields

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Read VCF file into memory for alt number"")

            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()

            numalt = callset['variants/numalt']

            if benchmark_profiler is not None:
                benchmark_profiler.start_benchmark(operation_name=""Determine maximum alt number"")

            alt_number = np.max(numalt)

            if benchmark_profiler is not None:
                benchmark_profiler.end_benchmark()
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        if benchmark_profiler is not None:
            benchmark_profiler.start_benchmark(operation_name=""Convert VCF to Zarr"")

        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)

        if benchmark_profiler is not None:
            benchmark_profiler.end_benchmark()


def get_genotype_data(callset, genotype_array_type=config.GENOTYPE_ARRAY_DASK):
    genotype_ref_name = ''

    # Ensure 'calldata' is within the callset
    if 'calldata' in callset:
        # Try to find either GT or genotype in calldata
        if 'GT' in callset['calldata']:
            genotype_ref_name = 'GT'
        elif 'genotype' in callset['calldata']:
            genotype_ref_name = 'genotype'
        else:
            return None
    else:
        return None

    if genotype_array_type == config.GENOTYPE_ARRAY_NORMAL:
        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == config.GENOTYPE_ARRAY_DASK:
        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == config.GENOTYPE_ARRAY_CHUNKED:
        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])
    else:
        return None
/n/n/ntests/test_cli.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run:  
    python -m unittest tests.test_cli 

""""""
import unittest
import sys

try:
    from unittest.mock import patch
except ImportError:
    from mock import patch
from genomics_benchmarks import cli


class TestCommandLineInterface(unittest.TestCase):

    def run_subparser_test(self, subparser_cmd, parameter, expected, default_key=None, default_value=None):
        """""" Tests subparsers for missing arguments and default values. """"""
        testargs = [""prog"", subparser_cmd, ""--"" + parameter, expected]
        with patch.object(sys, 'argv', testargs):
            args = cli.get_cli_arguments()
            self.assertEqual(args[parameter], expected,
                             subparser_cmd + "" subparser did not parse right config file arg."")
            self.assertEqual(args[""command""], subparser_cmd, subparser_cmd + "" command was not interpreted properly"")
            if default_key:
                self.assertEqual(args[default_key], default_value,
                                 subparser_cmd + "" command parser did not setup the right default key "" + default_key +
                                 "" to "" + default_value)

    def test_getting_command_arguments(self):
        """""" Tests for reading args and storing values for running all benchmark options from the command line.""""""
        # Test group 1 -- config
        self.run_subparser_test(""config"", ""output_config"", ""./benchmark.conf"")
        # Test group 2 -- setup
        self.run_subparser_test(""setup"", ""config_file"", ""./benhcmark.conf"")
        # Test group 3 - Tests if it the argparser is setting default values """"""
        self.run_subparser_test(""exec"", ""config_file"", ""./benchmark.conf"")

    def test_parser_expected_failing(self):
        """""" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (""something"") """"""
        testargs = [""prog""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the missing command line argument."")

        testargs = [""prog"", ""something""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises(SystemExit) as cm:
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code,
                                 ""CLI handler was supposed to fail on the wrong command line argument."")


if __name__ == '__main__':
    unittest.main()
/n/n/ntests/test_core.py/n/nimport unittest
from genomics_benchmarks.core import *
from genomics_benchmarks.config import \
    BenchmarkConfigurationRepresentation, \
    VCFtoZarrConfigurationRepresentation, \
    DataDirectoriesConfigurationRepresentation
from time import sleep
import os
import shutil


class TestCoreBenchmark(unittest.TestCase):
    def test_benchmark_profiler_results(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_profiler_results'
        profiler = BenchmarkProfiler(profiler_label)

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = 'Sleep {} seconds'.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            # Grab benchmark results
            results = profiler.get_benchmark_results()
            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals
            results_operation_name = results.operation_name
            results_run_number = results.run_number

            # Ensure benchmark results match expected values
            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')
            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')
            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')

            i += 1

        # Delete *.psv file created when running benchmark
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_results_psv(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_results_psv'

        # Delete *.psv file created from any previous unit testing
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

        profiler = BenchmarkProfiler(profiler_label)

        operation_name_format = 'Sleep {} seconds'

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = operation_name_format.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            i += 1

        # Read results psv file
        psv_file = '{}.psv'.format(profiler_label)

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)
            num_lines = len(psv_lines)
            num_lines_expected = len(benchmark_times) + 1
            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')

            # Ensure header (first line) of psv file is correct
            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'
            header_actual = psv_lines[0]
            self.assertEqual(header_expected, header_actual)

            # Ensure contents (benchmark data) of psv file is correct
            i = 1
            for line_number in range(1, num_lines):
                content = psv_lines[line_number].split('|')

                # Ensure column count is correct
                num_columns = len(content)
                num_columns_expected = 4
                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')

                # Ensure run number is correct
                run_number_psv = int(content[1])
                run_number_expected = i
                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')

                # Ensure operation name is correct
                operation_name_psv = content[2]
                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])
                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')

                # Ensure execution time is correct
                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals
                execution_time_expected = benchmark_times[i - 1]
                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')

                i += 1

        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Delete *.psv file created when running benchmark
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_simple_aggregations(self):
        test_dir = './tests_temp/'
        benchmark_label = 'test_benchmark_simple_aggregations'
        psv_file = '{}.psv'.format(benchmark_label)

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from any previous unit tests
        if os.path.isfile(psv_file):
            os.remove(psv_file)

        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()
        vcf_to_zar_config.enabled = True

        bench_conf = BenchmarkConfigurationRepresentation()
        bench_conf.vcf_to_zarr_config = vcf_to_zar_config
        bench_conf.benchmark_number_runs = 1
        bench_conf.benchmark_data_input = 'vcf'
        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'
        bench_conf.benchmark_aggregations = True

        data_dirs = DataDirectoriesConfigurationRepresentation()
        data_dirs.vcf_dir = './tests/data/'
        data_dirs.zarr_dir_setup = './tests_temp/zarr/'
        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'
        data_dirs.temp_dir = './tests_temp/temp/'

        # Run the benchmark and ensure nothing fails
        benchmark = Benchmark(bench_conf=bench_conf,
                              data_dirs=data_dirs,
                              benchmark_label='test_benchmark_simple_aggregations')
        benchmark.run_benchmark()

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file
            num_lines = len(psv_lines)
            num_lines_expected = 10
            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')

            psv_operation_names = []

            for psv_line in psv_lines:
                line_split = psv_line.split('|')
                line_cols_actual = len(line_split)
                line_cols_expected = 4

                # Ensure correct number of data columns exist for current line of data
                self.assertEqual(line_cols_expected, line_cols_actual,
                                 msg='Unexpected number of columns in resulting psv file')

                operation_name = line_split[2]
                psv_operation_names.append(operation_name)

            # Ensure all aggregations were run
            test_operation_names = ['Allele Count (All Samples)',
                                    'Genotype Count: Heterozygous per Variant',
                                    'Genotype Count: Homozygous per Variant',
                                    'Genotype Count: Heterozygous per Sample',
                                    'Genotype Count: Homozygous per Sample']

            for test_operation_name in test_operation_names:
                if test_operation_name not in psv_operation_names:
                    self.fail(msg='Operation \""{}\"" was not run during the benchmark.'.format(test_operation_name))
        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from this unit test
        if os.path.isfile(psv_file):
            os.remove(psv_file)

    def test_benchmark_pca(self):
        test_dir = './tests_temp/'
        benchmark_label = 'test_benchmark_pca'
        psv_file = '{}.psv'.format(benchmark_label)

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from any previous unit tests
        if os.path.isfile(psv_file):
            os.remove(psv_file)

        vcf_to_zar_config = VCFtoZarrConfigurationRepresentation()
        vcf_to_zar_config.enabled = True

        bench_conf = BenchmarkConfigurationRepresentation()
        bench_conf.vcf_to_zarr_config = vcf_to_zar_config
        bench_conf.benchmark_number_runs = 1
        bench_conf.benchmark_data_input = 'vcf'
        bench_conf.benchmark_dataset = 'trio.2010_06.ychr.genotypes.vcf'
        bench_conf.benchmark_pca = True
        bench_conf.pca_data_scaler = config.benchmark_pca_data_scaler_types[config.PCA_DATA_SCALER_PATTERSON]
        bench_conf.pca_genotype_array_type = config.GENOTYPE_ARRAY_CHUNKED

        data_dirs = DataDirectoriesConfigurationRepresentation()
        data_dirs.vcf_dir = './tests/data/'
        data_dirs.zarr_dir_setup = './tests_temp/zarr/'
        data_dirs.zarr_dir_benchmark = './tests_temp/zarr_benchmark/'
        data_dirs.temp_dir = './tests_temp/temp/'

        # Run the benchmark and ensure nothing fails
        benchmark = Benchmark(bench_conf=bench_conf,
                              data_dirs=data_dirs,
                              benchmark_label=benchmark_label)
        benchmark.run_benchmark()

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file
            num_lines = len(psv_lines)
            num_lines_expected = 14
            self.assertEqual(num_lines_expected, num_lines, msg='Unexpected line count in resulting psv file.')

            psv_operation_names = []

            for psv_line in psv_lines:
                line_split = psv_line.split('|')
                line_cols_actual = len(line_split)
                line_cols_expected = 4

                # Ensure correct number of data columns exist for current line of data
                self.assertEqual(line_cols_expected, line_cols_actual,
                                 msg='Unexpected number of columns in resulting psv file')

                operation_name = line_split[2]
                psv_operation_names.append(operation_name)

            # Ensure all aggregations were run
            test_operation_names = ['PCA: Count alleles',
                                    'PCA: Count multiallelic SNPs',
                                    'PCA: Count biallelic singletons',
                                    'PCA: Remove singletons and multiallelic SNPs',
                                    'PCA: Transform genotype data for PCA',
                                    'PCA: Apply LD pruning',
                                    'PCA: Move data set to memory',
                                    'PCA: Run conventional PCA analysis (scaler: patterson)',
                                    'PCA: Run randomized PCA analysis (scaler: patterson)']

            for test_operation_name in test_operation_names:
                if test_operation_name not in psv_operation_names:
                    self.fail(msg='Operation \""{}\"" was not run during the benchmark.'.format(test_operation_name))
        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Remove the test data directory from any previous unit tests
        if os.path.isdir(test_dir):
            shutil.rmtree(test_dir)

        # Remove the PSV file from this unit test
        if os.path.isfile(psv_file):
            os.remove(psv_file)


if __name__ == ""__main__"":
    unittest.main()
/n/n/n",0
83,83,e5195bc7bcf1060f2f727acf9f0cee033262caaa,"/genomics_benchmarks/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_aggregations = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_aggregations"" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark[""benchmark_aggregations""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print(""[Config] Configuration file has been generated successfully."")
        else:
            print(""[Config] Configuration file was not generated."")
/n/n/n",1
28,28,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
29,29,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
34,34,bd037e882d675ea27b96d41faf0deeac6563695c,"reddytt.py/n/n#!/usr/bin/env python3

################
# Imports
################

import os
import pickle
from bs4 import BeautifulSoup
import urllib3
import certifi
import re
import subprocess
import sys
import argparse as ap
#from argparse import ArgumentParser, REMINDER

################
# Functions
################

# Function to flatten a list
flatten = lambda l: [item for sublist in l for item in sublist]
# cheers to https://stackoverflow.com/a/952952

# Get and parse out links
def getytlinks(link):
    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())
    html_page = pm.request('GET', link)
    soup = BeautifulSoup(html_page.data, ""lxml"")
    links = [a.get('href') for a in soup('a') if a.get('href')]

    # Pick out youtube links
    new_links = [x for x in links if re.match(""^https://youtu\.be"", x)]
    newer_links = [x for x in links if re.match(""^https://www\.youtube\.com/watch"", x)]
    # the youtube.com links are not always well formatted for mpv, so we reformat them:
    for lk in newer_links:
        videolabel = re.search('v=([^&?]*)', lk)[1]
        if videolabel is None:
            print('Reddytt: skipping URL without video label:', lk)
            continue
        new_links.append('https://www.youtube.com/watch?v=' + videolabel)
    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html
    return new_links, links

################
# Main
################

if __name__ == '__main__':

    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')

    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')
    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')
    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')

    args = parser.parse_args()

    subreddit = args.subreddit
    depth = args.depth

    subreddit_link = ""https://reddit.com/r/"" + subreddit

    # Setup working directory
    work_dir = os.environ['HOME'] + ""/.reddytt""
    sr_dir = work_dir + ""/%s"" % subreddit
    seen_file = sr_dir + ""/seen""
    seen_links = []
    unseen_file = sr_dir + ""/unseen""
    unseen_links = []
    print(""Reddytt: Checking for reddytt working directory (%s)."" % work_dir)

    if not os.path.isdir(work_dir):
        print(""Reddytt: Working directory not found. Creating %s, and files."" % work_dir)
        os.mkdir(work_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    elif not os.path.isdir(sr_dir):
        print(""Reddytt: Working directory found, but no subreddit directory. Creating %s, and files."" % sr_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    else:
        print(""Reddytt: Working directory found. Loading variables."")
        with open(seen_file, 'rb') as f:
            seen_links = pickle.load(f)
        with open(unseen_file, 'rb') as f:
            unseen_links = pickle.load(f)

    new_links, links = getytlinks(subreddit_link)

    # Go deeper
    if depth > 0:
        for d in range(depth):
            link = """"
            for l in links:
                if re.search(""after="", l):
                    link = l
            if link == """":
                print(""Reddytt: Could not identify 'after'-variable to progress deeper."")
            else:
                newer_links, links = getytlinks(link)
                new_links += newer_links
                new_links = list(set(new_links))

    # we also want to watch the stored ones
    new_links += unseen_links
    new_links = list(set(new_links))

    # Start watching
    save_links = new_links
    for link in new_links:
        if link in seen_links:
            print(""Reddytt: Link seen. Skipping."")
        else:
            p = subprocess.Popen(['mpv', link] + args.mpv, shell=False)
            p.communicate()
            print(""Reddytt: That was: %s"" % link)
            if p.returncode == 0:
                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.
                # Store the video in seen_links.
                seen_links.append(link)
                save_links.remove(link)
            elif p.returncode == 4:
                # You made a hard exit, and want to stop. (Ctrl+C)
                # Store the links and exit the program.
                print(""Reddytt: Forced exit detected. Saving and exiting."")
                with open(seen_file, 'wb') as f:
                    pickle.dump(seen_links, f)
                with open(unseen_file, 'wb') as f:
                    pickle.dump(save_links, f)
                # Exit program.
                sys.exit()
            else:
                # Something else happened. Bad link perhaps.
                # Store in seen_links to avoid in the future.

                seen_links.append(link)
                save_links.remove(link)

    # The playlist is finished. Save everything.
    with open(seen_file, 'wb') as f:
        pickle.dump(seen_links, f)
    with open(unseen_file, 'wb') as f:
        pickle.dump(save_links, f)
/n/n/n",0
35,35,bd037e882d675ea27b96d41faf0deeac6563695c,"/reddytt.py/n/n#!/usr/bin/env python3

################
# Imports
################

import os
import pickle
from bs4 import BeautifulSoup
import urllib3
import certifi
import re
import sys
import argparse as ap
#from argparse import ArgumentParser, REMINDER

################
# Functions
################

# Function to flatten a list
flatten = lambda l: [item for sublist in l for item in sublist]
# cheers to https://stackoverflow.com/a/952952

# Get and parse out links
def getytlinks(link):
    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())
    html_page = pm.request('GET', link)
    soup = BeautifulSoup(html_page.data, ""lxml"")
    links = [a.get('href') for a in soup('a') if a.get('href')]

    # Pick out youtube links
    new_links = [x for x in links if re.match(""^https://youtu\.be"", x)]
    newer_links = [x for x in links if re.match(""^https://www\.youtube\.com/watch"", x)]
    # the youtube.com links are not always well formatted for mpv, so we reformat them:
    for lk in newer_links:
        videolabel = re.search('v=([^&?]*)', lk)[1]
        if videolabel is None:
            print('Reddytt: skipping URL without video label:', lk)
            continue
        new_links.append('https://www.youtube.com/watch?v=' + videolabel)
    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html
    return new_links, links

################
# Main
################

if __name__ == '__main__':

    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')

    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')
    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')
    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')

    args = parser.parse_args()

    subreddit = args.subreddit
    depth = args.depth
    mpv = "" "".join(args.mpv)

    subreddit_link = ""https://reddit.com/r/"" + subreddit

    # Setup working directory
    work_dir = os.environ['HOME'] + ""/.reddytt""
    sr_dir = work_dir + ""/%s"" % subreddit
    seen_file = sr_dir + ""/seen""
    seen_links = []
    unseen_file = sr_dir + ""/unseen""
    unseen_links = []
    print(""Reddytt: Checking for reddytt working directory (%s)."" % work_dir)

    if not os.path.isdir(work_dir):
        print(""Reddytt: Working directory not found. Creating %s, and files."" % work_dir)
        os.mkdir(work_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    elif not os.path.isdir(sr_dir):
        print(""Reddytt: Working directory found, but no subreddit directory. Creating %s, and files."" % sr_dir)
        os.mkdir(sr_dir)
        os.system(""touch %s"" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system(""touch %s"" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    else:
        print(""Reddytt: Working directory found. Loading variables."")
        with open(seen_file, 'rb') as f:
            seen_links = pickle.load(f)
        with open(unseen_file, 'rb') as f:
            unseen_links = pickle.load(f)

    new_links, links = getytlinks(subreddit_link)

    # Go deeper
    if depth > 0:
        for d in range(depth):
            link = """"
            for l in links:
                if re.search(""after="", l):
                    link = l
            if link == """":
                print(""Reddytt: Could not identify 'after'-variable to progress deeper."")
            else:
                newer_links, links = getytlinks(link)
                new_links += newer_links
                new_links = list(set(new_links))

    # we also want to watch the stored ones
    new_links += unseen_links
    new_links = list(set(new_links))

    # Start watching
    save_links = new_links
    for link in new_links:
        if link in seen_links:
            print(""Reddytt: Link seen. Skipping."")
        else:
            x = os.system(""mpv %(args)s %(link)s"" % {""link"": link, ""args"": mpv})
            print(""Reddytt: That was: %s"" % link)
            if x == 0:
                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.
                # Store the video in seen_links.
                seen_links.append(link)
                save_links.remove(link)
            elif x == 1024:
                # You made a hard exit, and want to stop. (Ctrl+C)
                # Store the links and exit the program.
                print(""Reddytt: Forced exit detected. Saving and exiting."")
                with open(seen_file, 'wb') as f:
                    pickle.dump(seen_links, f)
                with open(unseen_file, 'wb') as f:
                    pickle.dump(save_links, f)
                # Exit program.
                sys.exit()
            else:
                # Something else happened. Bad link perhaps.
                # Store in seen_links to avoid in the future.

                seen_links.append(link)
                save_links.remove(link)

    # The playlist is finished. Save everything.
    with open(seen_file, 'wb') as f:
        pickle.dump(seen_links, f)
    with open(unseen_file, 'wb') as f:
        pickle.dump(save_links, f)
/n/n/n",1
64,64,a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef,"openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/n",0
65,65,a38a05b5c0061840737e1f0ac9c1fc3ad5f4d7ef,"/openqml_pq/projectq.py/n/n# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r""""""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
""""""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """"""ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list([""Simulator"", ""ClassicalSimulator"", ""IBMBackend""])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute(self):
        """""" """"""
        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18
        self._out = self.execute_queued()

    def execute_queued(self):
        """"""Apply the queued operations to the device, and measure the expectation.""""""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError(""{} not supported by device {}"".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError(""expectation() is not yet implemented for this backend"")

    def shutdown(self):
        """"""Shutdown.

        """"""
        pass

    def _deallocate(self):
        """"""Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """"""Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """"""
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """"""Check whether this plugin requires credentials
    #     """"""
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """"""ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """"""

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator(""Z""+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """"""ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """"""

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """"""ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (‘ibmqx4’, or ‘ibmqx5’) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """"""

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the ""user"" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the ""password"" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """"""Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """"""
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print(""IBM probabilities=""+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print(""IBM all probabilities=""+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError(""Estimation of expectation values not yet implemented for the observable {} in backend {}."".format(observable, self.backend))

        return expectation_value#, variance
/n/n/n",1
78,78,5577269b7283a48d0b421dabd44687460955a0c8,"benchmark/cli.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import argparse  # for command line parsing
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import sys
import shutil
from benchmark import core, config, data_service


def get_cli_arguments():
    """""" Returns command line arguments. 

    Returns:
    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), 
    and config argument for where is the config file. """"""

    logging.debug('Getting cli arguments')

    parser = argparse.ArgumentParser(description=""A benchmark for genomics routines in Python."")

    # Enable three exclusive groups of options (using subparsers)
    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525

    subparser = parser.add_subparsers(title=""commands"", dest=""command"")
    subparser.required = True

    config_parser = subparser.add_parser(""config"",
                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')
    config_parser.add_argument(""--output_config"", type=str, required=True,
                               help=""Specify the output path to a configuration file."", metavar=""FILEPATH"")
    config_parser.add_argument(""-f"", action=""store_true"", help=""Overwrite the destination file if it already exists."")

    data_setup_parser = subparser.add_parser(""setup"",
                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')
    data_setup_parser.add_argument(""--config_file"", required=True, help=""Location of the configuration file"",
                                   metavar=""FILEPATH"")

    benchmark_exec_parser = subparser.add_parser(""exec"",
                                                 help='Execution of the benchmark modes. It requires a configuration file.')

    timestamp_current = datetime.datetime.fromtimestamp(time.time())
    benchmark_label_default = ""run_{timestamp}"".format(timestamp=timestamp_current.strftime(""%Y-%m-%d_%H-%M-%S""))
    benchmark_exec_parser.add_argument(""--label"", type=str, default=benchmark_label_default, metavar=""RUN_LABEL"",
                                       help=""Label for the benchmark run."")
    benchmark_exec_parser.add_argument(""--config_file"", type=str, required=True,
                                       help=""Specify the path to a configuration file."", metavar=""FILEPATH"")

    runtime_configuration = vars(parser.parse_args())
    return runtime_configuration


def _main():
    data_dirs = config.DataDirectoriesConfigurationRepresentation()

    cli_arguments = get_cli_arguments()

    command = cli_arguments[""command""]
    if command == ""config"":
        output_config_location = cli_arguments[""output_config""]
        overwrite_mode = cli_arguments[""f""]
        config.generate_default_config_file(output_location=output_config_location,
                                            overwrite=overwrite_mode)
    elif command == ""setup"":
        print(""[Setup] Setting up benchmark data."")

        # Clear out existing files in VCF and Zarr directories
        data_service.remove_directory_tree(data_dirs.vcf_dir)
        data_service.remove_directory_tree(data_dirs.zarr_dir_setup)

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        # Get FTP module settings from runtime config
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        if ftp_config.enabled:
            print(""[Setup][FTP] FTP module enabled. Running FTP download..."")
            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=data_dirs.download_dir)
        else:
            print(""[Setup][FTP] FTP module disabled. Skipping FTP download..."")

        # Process/Organize downloaded files
        data_service.process_data_files(input_dir=data_dirs.input_dir,
                                        temp_dir=data_dirs.temp_dir,
                                        output_dir=data_dirs.vcf_dir)

        # Convert VCF files to Zarr format if the module is enabled
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)
        if vcf_to_zarr_config.enabled:
            data_service.setup_vcf_to_zarr(input_vcf_dir=data_dirs.vcf_dir,
                                           output_zarr_dir=data_dirs.zarr_dir_setup,
                                           conversion_config=vcf_to_zarr_config)
    elif command == ""exec"":
        print(""[Exec] Executing benchmark tool."")

        # Clear out existing files in Zarr benchmark directory
        data_service.remove_directory_tree(data_dirs.zarr_dir_benchmark)

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        benchmark_label = cli_arguments[""label""]

        # Get Benchmark module settings from runtime config
        benchmark_config = config.BenchmarkConfigurationRepresentation(runtime_config)

        # Setup the benchmark runner
        benchmark = core.Benchmark(bench_conf=benchmark_config, data_dirs=data_dirs, benchmark_label=benchmark_label)

        # Run the benchmark
        benchmark.run_benchmark()
    else:
        print(""Error: Unexpected command specified. Exiting..."")
        sys.exit(1)


def main():
    try:
        _main()
    except KeyboardInterrupt:
        print(""Program interrupted. Exiting..."")
        sys.exit(1)
/n/n/nbenchmark/config.py/n/nfrom configparser import ConfigParser
from shutil import copyfile
import os.path
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """"""
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """"""
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = ""./data/input/""
    download_dir = input_dir + ""download/""
    temp_dir = ""./data/temp/""
    vcf_dir = ""./data/vcf/""
    zarr_dir_setup = ""./data/zarr/""
    zarr_dir_benchmark = ""./data/zarr_benchmark/""


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """""" A small utility class for object representation of a standard config. file. """"""

    def __init__(self, file_name):
        """""" Initializes the configuration representation with a supplied file. """"""
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError(""Configuration file {0} not found"".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """""" Utility class for object representation of FTP module configuration. """"""
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = """"  # FTP server to connect to
    username = """"  # Username to login with. Set username and password to blank for anonymous login
    password = """"  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = """"  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, ""ftp""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp[""enabled""])
                if ""server"" in runtime_config.ftp:
                    self.server = runtime_config.ftp[""server""]
                if ""username"" in runtime_config.ftp:
                    self.username = runtime_config.ftp[""username""]
                if ""password"" in runtime_config.ftp:
                    self.password = runtime_config.ftp[""password""]
                if ""use_tls"" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp[""use_tls""])
                if ""directory"" in runtime_config.ftp:
                    self.directory = runtime_config.ftp[""directory""]

                # Convert delimited list of files (string) to Python-style list
                if ""file_delimiter"" in runtime_config.ftp:
                    delimiter = runtime_config.ftp[""file_delimiter""]
                else:
                    delimiter = ""|""

                if ""files"" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp[""files""])
                    if files_str == ""*"":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = [""Blosc""]
vcf_to_zarr_blosc_algorithm_types = [""zstd"", ""blosclz"", ""lz4"", ""lz4hc"", ""zlib"", ""snappy""]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """""" Utility class for object representation of VCF to Zarr conversion module configuration. """"""
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = ""Blosc""  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = ""zstd""
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, ""vcf_to_zarr""):
                # Extract relevant settings from config file
                if ""enabled"" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr[""enabled""])
                if ""alt_number"" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr[""alt_number""]

                    if str(alt_number_str).lower() == ""auto"":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError(""Invalid value provided for alt_number in configuration.\n""
                                        ""Expected: \""auto\"" or integer value"")
                if ""chunk_length"" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr[""chunk_length""]
                    if chunk_length_str == ""default"":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_length in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""chunk_width"" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr[""chunk_width""]
                    if chunk_width_str == ""default"":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError(""Invalid value provided for chunk_width in configuration.\n""
                                        ""Expected: \""default\"" or integer value"")
                if ""compressor"" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr[""compressor""]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if ""blosc_compression_algorithm"" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr[""blosc_compression_algorithm""]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if ""blosc_compression_level"" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr[""blosc_compression_level""]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError(""Invalid value for blosc_compression_level in configuration.\n""
                                             ""blosc_compression_level must be between 0 and 9."")
                    else:
                        raise TypeError(""Invalid value for blosc_compression_level in configuration.\n""
                                        ""blosc_compression_level could not be converted to integer."")
                if ""blosc_shuffle_mode"" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr[""blosc_shuffle_mode""]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                             ""blosc_shuffle_mode must be a valid integer."")
                    else:
                        raise TypeError(""Invalid value for blosc_shuffle_mode in configuration.\n""
                                        ""blosc_shuffle_mode could not be converted to integer."")


benchmark_data_input_types = [""vcf"", ""zarr""]


class BenchmarkConfigurationRepresentation:
    """""" Utility class for object representation of the benchmark module's configuration. """"""
    benchmark_number_runs = 5
    benchmark_data_input = ""vcf""
    benchmark_dataset = """"
    benchmark_allele_count = False
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """"""
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """"""
        if runtime_config is not None:
            if hasattr(runtime_config, ""benchmark""):
                # Extract relevant settings from config file
                if ""benchmark_number_runs"" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark[""benchmark_number_runs""])
                    except ValueError:
                        pass
                if ""benchmark_data_input"" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark[""benchmark_data_input""]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if ""benchmark_dataset"" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark[""benchmark_dataset""]
                if ""benchmark_allele_count"" in runtime_config.benchmark:
                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark[""benchmark_allele_count""])
                if ""benchmark_PCA"" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark[""benchmark_PCA""])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """"""
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """"""
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    default_config_file_location = ""doc/benchmark.conf.default""

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                ""[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled."")
            return

        if os.path.exists(default_config_file_location):
            # Write the default configuration file to specified location
            copyfile(default_config_file_location, output_location)

            # Check whether configuration file now exists and report status
            if os.path.exists(output_location):
                print(""[Config] Configuration file has been generated successfully."")
            else:
                print(""[Config] Configuration file was not generated."")
        else:
            print(""[Config] Default configuration file could not be found. File should be located at:\n\t{}""
                  .format(default_config_file_location))
/n/n/nbenchmark/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import allel
import zarr
import datetime
import time  # for benchmark timer
import csv  # for writing results
import logging
import os
import pandas as pd
from collections import OrderedDict
from benchmark import config, data_service


class BenchmarkResultsData:
    run_number = None
    operation_name = None
    start_time = None
    exec_time = None

    def to_dict(self):
        return OrderedDict([(""Log Timestamp"", datetime.datetime.fromtimestamp(self.start_time)),
                            (""Run Number"", self.run_number),
                            (""Operation"", self.operation_name),
                            (""Execution Time"", self.exec_time)])

    def to_pandas(self):
        data = self.to_dict()
        df = pd.DataFrame(data, index=[1])
        df.index.name = '#'
        return df


class BenchmarkProfiler:
    benchmark_running = False

    def __init__(self, benchmark_label):
        self.results = BenchmarkResultsData()
        self.benchmark_label = benchmark_label

    def set_run_number(self, run_number):
        if not self.benchmark_running:
            self.results.run_number = run_number

    def start_benchmark(self, operation_name):
        if not self.benchmark_running:
            self.results.operation_name = operation_name

            self.benchmark_running = True

            # Start the benchmark timer
            self.results.start_time = time.time()

    def end_benchmark(self):
        if self.benchmark_running:
            end_time = time.time()

            # Calculate the execution time from start and end times
            self.results.exec_time = end_time - self.results.start_time

            # Save benchmark results
            self._record_runtime(self.results, ""{}.psv"".format(self.benchmark_label))

            self.benchmark_running = False

    def get_benchmark_results(self):
        return self.results

    def _record_runtime(self, benchmark_results, output_filename):
        """"""
        Records the benchmark results data entry to the specified PSV file.
        :param benchmark_results: BenchmarkResultsData object containing the benchmark results data
        :param output_filename: Which file to output the benchmark results to
        :type benchmark_results: BenchmarkResultsData
        :type output_filename: str
        """"""
        output_filename = str(output_filename)

        psv_header = not os.path.isfile(output_filename)

        # Open the output file in append mode
        with open(output_filename, ""a"") as psv_file:
            pd_results = benchmark_results.to_pandas()
            pd_results.to_csv(psv_file, sep=""|"", header=psv_header, index=False)


class Benchmark:
    benchmark_zarr_dir = """"  # Directory for which to use data from for benchmark process
    benchmark_zarr_file = """"  # File within benchmark_zarr_dir for which to use for benchmark process

    def __init__(self, bench_conf, data_dirs, benchmark_label):
        """"""
        Sets up a Benchmark object which is used to execute benchmarks.
        :param bench_conf: Benchmark configuration data that controls the benchmark execution
        :param data_dirs: DataDirectoriesConfigurationRepresentation object that contains working data directories
        :param benchmark_label: label to use when saving benchmark results to file
        :type bench_conf: config.BenchmarkConfigurationRepresentation
        :type data_dirs: config.DataDirectoriesConfigurationRepresentation
        :type benchmark_label: str
        """"""
        self.bench_conf = bench_conf
        self.data_dirs = data_dirs
        self.benchmark_label = benchmark_label

        self.benchmark_profiler = BenchmarkProfiler(benchmark_label=self.benchmark_label)

    def run_benchmark(self):
        """"""
        Executes the benchmarking process.
        """"""
        if self.bench_conf is not None and self.data_dirs is not None:
            for run_number in range(1, self.bench_conf.benchmark_number_runs + 1):
                # Clear out existing files in Zarr benchmark directory
                # (Should be done every single run)
                data_service.remove_directory_tree(self.data_dirs.zarr_dir_benchmark)

                # Update run number in benchmark profiler (for results tracking)
                self.benchmark_profiler.set_run_number(run_number)

                # Prepare data directory and file locations for benchmarks
                if self.bench_conf.benchmark_data_input == ""vcf"":
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark

                    # Convert VCF data to Zarr format as part of benchmark
                    self._benchmark_convert_to_zarr()

                elif self.bench_conf.benchmark_data_input == ""zarr"":
                    # Use pre-converted Zarr data which was done ahead of benchmark (i.e. in Setup mode)
                    self.benchmark_zarr_dir = self.data_dirs.zarr_dir_setup
                    self.benchmark_zarr_file = self.bench_conf.benchmark_dataset

                else:
                    print(""[Exec] Error: Invalid option supplied for benchmark data input format."")
                    print(""  - Expected data input formats: vcf, zarr"")
                    print(""  - Provided data input format: {}"".format(self.bench_conf.benchmark_data_input))
                    exit(1)

                # Ensure Zarr dataset exists and can be used for upcoming benchmarks
                benchmark_zarr_path = os.path.join(self.benchmark_zarr_dir, self.benchmark_zarr_file)
                if (benchmark_zarr_path != """") and (os.path.isdir(benchmark_zarr_path)):
                    # TODO: Run remaining benchmarks (e.g. loading into memory, allele counting, PCA, etc.)
                    pass
                else:
                    # Zarr dataset doesn't exist. Print error message and exit
                    print(""[Exec] Error: Zarr dataset could not be found for benchmarking."")
                    print(""  - Zarr dataset location: {}"".format(benchmark_zarr_path))

    def _benchmark_convert_to_zarr(self):
        self.benchmark_zarr_dir = self.data_dirs.zarr_dir_benchmark
        input_vcf_file = self.bench_conf.benchmark_dataset
        input_vcf_path = os.path.join(self.data_dirs.vcf_dir, input_vcf_file)

        if os.path.isfile(input_vcf_path):
            output_zarr_file = input_vcf_file
            output_zarr_file = output_zarr_file[
                               0:len(output_zarr_file) - 4]  # Truncate *.vcf from input filename
            output_zarr_path = os.path.join(self.data_dirs.zarr_dir_benchmark, output_zarr_file)

            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=self.bench_conf.vcf_to_zarr_config,
                                         benchmark_runner=self.benchmark_profiler)

            self.benchmark_zarr_file = output_zarr_file
        else:
            print(""[Exec] Error: Dataset specified in configuration file does not exist. Exiting..."")
            print(""  - Dataset file specified in configuration: {}"".format(input_vcf_file))
            print(""  - Expected file location: {}"".format(input_vcf_path))
            exit(1)
/n/n/nbenchmark/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import urllib.request
from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urllib.request.urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config, benchmark_runner=None):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    If a BenchmarkRunner is provided, the actual VCF to Zarr conversion process will be benchmarked.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :param benchmark_runner: BenchmarkRunner object to be used for benchmarking process
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    :type benchmark_runner: core.BenchmarkProfiler
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get fields to extract (for unit testing only)
        fields = conversion_config.fields

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")
            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)
            numalt = callset['variants/numalt']
            alt_number = np.max(numalt)
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        if benchmark_runner is not None:
            benchmark_runner.start_benchmark(operation_name=""Convert VCF to Zarr"")

        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True, fields=fields,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)

        if benchmark_runner is not None:
            benchmark_runner.end_benchmark()


GENOTYPE_ARRAY_NORMAL = 0
GENOTYPE_ARRAY_DASK = 1
GENOTYPE_ARRAY_CHUNKED = 2


def get_genotype_data(callset, genotype_array_type=GENOTYPE_ARRAY_DASK):
    genotype_ref_name = ''

    # Ensure 'calldata' is within the callset
    if 'calldata' in callset:
        # Try to find either GT or genotype in calldata
        if 'GT' in callset['calldata']:
            genotype_ref_name = 'GT'
        elif 'genotype' in callset['calldata']:
            genotype_ref_name = 'genotype'
        else:
            return None
    else:
        return None

    if genotype_array_type == GENOTYPE_ARRAY_NORMAL:
        return allel.GenotypeArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_DASK:
        return allel.GenotypeDaskArray(callset['calldata'][genotype_ref_name])
    elif genotype_array_type == GENOTYPE_ARRAY_CHUNKED:
        return allel.GenotypeChunkedArray(callset['calldata'][genotype_ref_name])
    else:
        return None
/n/n/ntests/test_cli.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run:  
    python -m unittest tests.test_cli 

""""""
import unittest
import sys
from unittest.mock import patch
from benchmark import cli

class TestCommandLineInterface(unittest.TestCase):


    def run_subparser_test(self,subparser_cmd,parameter,expected, default_key=None, default_value=None):  
        """""" Tests subparsers for missing arguments and default values. """"""
        testargs = [""prog"", subparser_cmd, ""--""+parameter, expected]
        with patch.object(sys, 'argv', testargs):
            args = cli.get_cli_arguments()
            self.assertEqual(args[parameter],expected, 
                subparser_cmd + "" subparser did not parse right config file arg."")
            self.assertEqual(args[""command""],subparser_cmd,  subparser_cmd + "" command was not interpreted properly"")
            if default_key:
               self.assertEqual(args[default_key],default_value, 
                subparser_cmd + "" command parser did not setup the right default key "" + default_key + 
                "" to "" + default_value )


    def test_getting_command_arguments(self):
        """""" Tests for reading args and storing values for running all benchmark options from the command line.""""""
        # Test group 1 -- config
        self.run_subparser_test(""config"",""output_config"",""./benchmark.conf"")
        # Test group 2 -- setup
        self.run_subparser_test(""setup"",""config_file"",""./benhcmark.conf"")  
        # Test group 3 - Tests if it the argparser is setting default values """"""
        self.run_subparser_test(""exec"",""config_file"",""./benchmark.conf"")


    def test_parser_expected_failing(self):
        """""" Test that parsing fails on no command option (a choice of a subparser), or an unrecognized command (""something"") """"""
        testargs = [""prog""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises( SystemExit ) as cm: 
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code, 
                    ""CLI handler was supposed to fail on the missing command line argument."")

        testargs = [""prog"",""something""]
        command_line_error_code = 2
        with patch.object(sys, 'argv', testargs):
            with self.assertRaises( SystemExit ) as cm: 
                cli.get_cli_arguments()
                self.assertEqual(cm.exception.code, command_line_error_code, 
                    ""CLI handler was supposed to fail on the wrong command line argument."")


if __name__ == '__main__':
    unittest.main()/n/n/ntests/test_core.py/n/nimport unittest
from benchmark.core import *
from time import sleep
import os


class TestCoreBenchmark(unittest.TestCase):
    def test_benchmark_profiler_results(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_profiler_results'
        profiler = BenchmarkProfiler(profiler_label)

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = 'Sleep {} seconds'.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            # Grab benchmark results
            results = profiler.get_benchmark_results()
            results_exec_time = int(results.exec_time)  # Convert to int to truncate decimals
            results_operation_name = results.operation_name
            results_run_number = results.run_number

            # Ensure benchmark results match expected values
            self.assertEqual(benchmark_time, results_exec_time, msg='Execution time is incorrect.')
            self.assertEqual(operation_name, results_operation_name, msg='Operation name is incorrect.')
            self.assertEqual(i, results_run_number, msg='Run number is incorrect.')

            i += 1

        # Delete *.psv file created when running benchmark
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

    def test_benchmark_results_psv(self):
        # Setup Benchmark Profiler object
        profiler_label = 'test_benchmark_results_psv'

        # Delete *.psv file created from any previous unit testing
        psv_file = '{}.psv'.format(profiler_label)
        if os.path.exists(psv_file):
            os.remove(psv_file)

        profiler = BenchmarkProfiler(profiler_label)

        operation_name_format = 'Sleep {} seconds'

        # Run a few mock benchmarks
        benchmark_times = [1, 2, 10]
        i = 1
        for benchmark_time in benchmark_times:
            profiler.set_run_number(i)

            operation_name = operation_name_format.format(benchmark_time)

            # Run the mock benchmark, measuring time to run sleep command
            profiler.start_benchmark(operation_name)
            time.sleep(benchmark_time)
            profiler.end_benchmark()

            i += 1

        # Read results psv file
        psv_file = '{}.psv'.format(profiler_label)

        # Ensure psv file was created
        if os.path.exists(psv_file):
            # Read file contents
            with open(psv_file, 'r') as f:
                psv_lines = [line.rstrip('\n') for line in f]

            # Check line count of psv file. Line count should be equal to number of benchmarks run + 1 (for header)
            num_lines = len(psv_lines)
            num_lines_expected = len(benchmark_times) + 1
            self.assertEqual(num_lines_expected, num_lines, msg='Line count in resulting psv file is incorrect.')

            # Ensure header (first line) of psv file is correct
            header_expected = 'Log Timestamp|Run Number|Operation|Execution Time'
            header_actual = psv_lines[0]
            self.assertEqual(header_expected, header_actual)

            # Ensure contents (benchmark data) of psv file is correct
            i = 1
            for line_number in range(1, num_lines):
                content = psv_lines[line_number].split('|')

                # Ensure column count is correct
                num_columns = len(content)
                num_columns_expected = 4
                self.assertEqual(num_columns_expected, num_columns, msg='Column count for psv data is incorrect.')

                # Ensure run number is correct
                run_number_psv = int(content[1])
                run_number_expected = i
                self.assertEqual(run_number_expected, run_number_psv, msg='Run number is incorrect.')

                # Ensure operation name is correct
                operation_name_psv = content[2]
                operation_name_expected = operation_name_format.format(benchmark_times[i - 1])
                self.assertEqual(operation_name_expected, operation_name_psv, msg='Operation name is incorrect.')

                # Ensure execution time is correct
                execution_time_psv = int(float(content[3]))  # Convert to int to truncate decimals
                execution_time_expected = benchmark_times[i - 1]
                self.assertEqual(execution_time_expected, execution_time_psv, msg='Execution time is incorrect')

                i += 1

        else:
            self.fail(msg='Resulting psv file could not be found.')

        # Delete *.psv file created when running benchmark
        if os.path.exists(psv_file):
            os.remove(psv_file)


if __name__ == ""__main__"":
    unittest.main()
/n/n/ntests/test_data_service.py/n/n"""""" Unit test for benchmark CLI functions. 
    To execute on a command line, run from the home directory:  
    python -m unittest tests.test_data_service
""""""
import unittest
import os.path
import shutil
import zarr
import numpy as np
import pathlib

from benchmark import data_service, config


class TestDataServices(unittest.TestCase):
    def test_fetch_data_via_ftp(self):
        local_directory = ""./""

        test_config_location = ""./tests/data/ftp_test_fetch_data.conf""
        runtime_config = config.read_configuration(location=test_config_location)
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        # Attempt to remove local files in case a previous unit test failed to do so (avoid false positive)
        for file in ftp_config.files:
            if os.path.isfile(file):
                os.remove(file)

        data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=local_directory)

        flag = True
        for file in ftp_config.files:
            if not os.path.isfile(file):
                flag = False
                break
        self.assertTrue(flag)

        # Remove the downloaded files
        for file in ftp_config.files:
            if os.path.isfile(file):
                os.remove(file)

    def test_fetch_file_from_url(self):
        """""" Tests fetching a data to be used in benchmarking from a remote URL. """"""
        remote_file_url = ""ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/pilot_data/release/2010_07/trio/snps/trio.2010_06.ychr.sites.vcf.gz""
        local_filename = ""trio.2010_06.ychr.sites.vcf.gz""

        # Attempt to remove local file in case a previous unit test failed to do so (prevents false positive)
        if os.path.isfile(local_filename):
            os.remove(local_filename)

        data_service.fetch_file_from_url(remote_file_url, local_filename)
        self.assertTrue(os.path.isfile(local_filename), ""No local file retrieved"")

        # Remove the downloaded file
        if os.path.isfile(local_filename):
            os.remove(local_filename)

    def test_decompress_gzip(self):
        """""" Tests decompressing the fetched file. """"""
        local_file_gz = ""./tests/data/trio.2010_06.ychr.sites.vcf.gz""
        local_filename = ""trio.2010_06.ychr.sites.vcf""

        # Attempt to remove local file in case a previous unit test failed to do so (prevents false positive)
        if os.path.isfile(local_filename):
            os.remove(local_filename)

        data_service.decompress_gzip(local_file_gz, local_filename)
        self.assertTrue(os.path.isfile(local_filename), ""No local file decompressed."")

        # Remove the downloaded file
        if os.path.isfile(local_filename):
            os.remove(local_filename)

    def test_read_file_contents_existing_file(self):
        local_filepath = ""./tests/data/test_read_file_contents_data.txt""

        if os.path.isfile(local_filepath):
            results = data_service.read_file_contents(local_filepath)
            self.assertEqual(results, ""test data"")
        else:
            self.fail(""Test data file does not exist. Please ensure the file exists and try running test again"")

    def test_read_file_contents_missing_file(self):
        local_filepath = ""./tests/data/test_read_file_contents_data_nonexistent.txt""

        if not os.path.isfile(local_filepath):
            results = data_service.read_file_contents(local_filepath)
            self.assertEqual(results, None)
        else:
            self.fail(""File should not exist on filesystem. Please remove the file and try running test again."")

    def test_process_data_files(self):
        # Define test input files
        test_dir = ""./tests/data/""
        test_files_input = [""trio.2010_06.ychr.sites.vcf.gz""]
        test_files_expected = [""trio.2010_06.ychr.sites.vcf""]

        # Setup test processing directories
        process_data_files_test_dir = ""./data/unittest/""
        input_dir_test = process_data_files_test_dir + ""input/""
        temp_dir_test = process_data_files_test_dir + ""temp/""
        output_dir_test = process_data_files_test_dir + ""vcf/""

        # Remove the test directory created for this unittest (from any previous unit testing)
        if os.path.exists(process_data_files_test_dir):
            shutil.rmtree(process_data_files_test_dir)

        # Create input directory
        pathlib.Path(input_dir_test).mkdir(parents=True, exist_ok=True)

        # Copy test files into input directory to test data processing
        for test_file in test_files_input:
            test_file_expected = test_dir + test_file
            test_file_output = input_dir_test + test_file
            if os.path.exists(test_file_expected):
                shutil.copy(test_file_expected, test_file_output)

        # Process the test files
        data_service.process_data_files(input_dir=input_dir_test, temp_dir=temp_dir_test, output_dir=output_dir_test)

        # Check the results to ensure corresponding vcf files exist in output directory
        error_flag = False
        for test_file_expected in test_files_expected:
            if not os.path.exists(output_dir_test + test_file_expected):
                error_flag = True

        # Remove the test directory created for this unittest
        shutil.rmtree(process_data_files_test_dir)

        # Return an error if the test failed
        if error_flag:
            self.fail(msg=""One or more test files were not processed and placed in output directory."")

    def test_convert_to_zarr(self):
        input_vcf_path = ""./tests/data/trio.2010_06.ychr.sites.vcf""
        output_zarr_path = ""trio.2010_06.ychr.sites.zarr""

        # Attempt to remove local file in case a previous unit test failed to do so (prevents false positive)
        if os.path.isdir(output_zarr_path):
            shutil.rmtree(output_zarr_path)

        if os.path.isfile(input_vcf_path):
            # Setup test settings for Zarr conversion
            vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation()
            vcf_to_zarr_config.fields = 'variants/numalt'
            vcf_to_zarr_config.enabled = True
            vcf_to_zarr_config.compressor = ""Blosc""
            vcf_to_zarr_config.blosc_compression_algorithm = ""zstd""
            vcf_to_zarr_config.blosc_compression_level = 1
            vcf_to_zarr_config.blosc_shuffle_mode = -1

            # Convert VCF file to Zarr
            data_service.convert_to_zarr(input_vcf_path=input_vcf_path,
                                         output_zarr_path=output_zarr_path,
                                         conversion_config=vcf_to_zarr_config)

            # Load the Zarr data from storage for testing
            callset = zarr.open_group(output_zarr_path, mode=""r"")
            numalt = callset['variants/numalt']
            self.assertEqual(np.size(numalt), 959)
            self.assertEqual(np.max(numalt), 1)
        else:
            self.fail(""Test data file does not exist. Please ensure the file exists and try running test again"")

        # Remove the Zarr test data
        if os.path.isdir(output_zarr_path):
            shutil.rmtree(output_zarr_path)


if __name__ == ""__main__"":
    unittest.main()
/n/n/n",0
79,79,5577269b7283a48d0b421dabd44687460955a0c8,"/benchmark/cli.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import argparse  # for command line parsing
import time  # for benchmark timer
import csv  # for writing results
import logging
import sys
import shutil
from benchmark import config, data_service


def get_cli_arguments():
    """""" Returns command line arguments. 

    Returns:
    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), 
    and config argument for where is the config file. """"""

    logging.debug('Getting cli arguments')

    parser = argparse.ArgumentParser(description=""A benchmark for genomics routines in Python."")

    # Enable three exclusive groups of options (using subparsers)
    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525

    subparser = parser.add_subparsers(title=""commands"", dest=""command"")
    subparser.required = True

    config_parser = subparser.add_parser(""config"",
                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')
    config_parser.add_argument(""--output_config"", type=str, required=True,
                               help=""Specify the output path to a configuration file."", metavar=""FILEPATH"")
    config_parser.add_argument(""-f"", action=""store_true"", help=""Overwrite the destination file if it already exists."")

    data_setup_parser = subparser.add_parser(""setup"",
                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')
    data_setup_parser.add_argument(""--config_file"", required=True, help=""Location of the configuration file"",
                                   metavar=""FILEPATH"")

    benchmark_exec_parser = subparser.add_parser(""exec"",
                                                 help='Execution of the benchmark modes. It requires a configuration file.')
    # TODO: use run_(timestamp) as default
    benchmark_exec_parser.add_argument(""--label"", type=str, default=""run"", metavar=""RUN_LABEL"",
                                       help=""Label for the benchmark run."")
    benchmark_exec_parser.add_argument(""--config_file"", type=str, required=True,
                                       help=""Specify the path to a configuration file."", metavar=""FILEPATH"")

    runtime_configuration = vars(parser.parse_args())
    return runtime_configuration


def _main():
    input_directory = ""./data/input/""
    download_directory = input_directory + ""download/""
    temp_directory = ""./data/temp/""
    vcf_directory = ""./data/vcf/""
    zarr_directory_setup = ""./data/zarr/""
    zarr_directory_benchmark = ""./data/zarr_benchmark/""

    cli_arguments = get_cli_arguments()

    command = cli_arguments[""command""]
    if command == ""config"":
        output_config_location = cli_arguments[""output_config""]
        overwrite_mode = cli_arguments[""f""]
        config.generate_default_config_file(output_location=output_config_location,
                                            overwrite=overwrite_mode)
    elif command == ""setup"":
        print(""[Setup] Setting up benchmark data."")

        # Clear out existing files in VCF and Zarr directories
        data_service.remove_directory_tree(vcf_directory)
        data_service.remove_directory_tree(zarr_directory_setup)

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        # Get FTP module settings from runtime config
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        if ftp_config.enabled:
            print(""[Setup][FTP] FTP module enabled. Running FTP download..."")
            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)
        else:
            print(""[Setup][FTP] FTP module disabled. Skipping FTP download..."")

        # Process/Organize downloaded files
        data_service.process_data_files(input_dir=input_directory,
                                        temp_dir=temp_directory,
                                        output_dir=vcf_directory)

        # Convert VCF files to Zarr format if the module is enabled
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)
        if vcf_to_zarr_config.enabled:
            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,
                                           output_zarr_dir=zarr_directory_setup,
                                           conversion_config=vcf_to_zarr_config)
    elif command == ""exec"":
        print(""[Exec] Executing benchmark tool."")

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments[""config_file""])

        # Get VCF to Zarr conversion settings from runtime config
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)

        # TODO: Convert necessary VCF files to Zarr format
        # data_service.convert_to_zarr(""./data/vcf/chr22.1000.vcf"", ""./data/zarr/chr22.1000.zarr"", vcf_to_zarr_config)
    else:
        print(""Error: Unexpected command specified. Exiting..."")
        sys.exit(1)


def main():
    try:
        _main()
    except KeyboardInterrupt:
        print(""Program interrupted. Exiting..."")
        sys.exit(1)
/n/n/n/benchmark/core.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import time  # for benchmark timer
import csv  # for writing results
import logging


def run_benchmark(bench_conf):
    pass


def run_dynamic(ftp_location):
    pass


def run_static():
    pass


def get_remote_files(ftp_server, ftp_directory, files=None):
    pass


def record_runtime(benchmark, timestamp):
    pass


# temporary here
def main():
    pass
/n/n/n/benchmark/data_service.py/n/n"""""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """"""

import urllib.request
from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc, LZ4, LZMA
from benchmark import config

import gzip
import shutil


def create_directory_tree(path):
    """"""
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """"""
    path = str(path)  # Ensure path is in str format
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)


def remove_directory_tree(path):
    """"""
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """"""

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """""" Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """"""
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, ""wb"") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print(""[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}"".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """"""
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """"""

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = ""/"".join(remote_subdirs_list)
        remote_path_absolute = ""/"" + remote_directory + ""/"" + remote_path_relative + ""/""
    else:
        remote_subdirs_list = []
        remote_path_relative = """"
        remote_path_absolute = ""/"" + remote_directory + ""/""

    try:
        local_path = local_directory + ""/"" + remote_path_relative
        os.mkdir(local_path)
        print(""[Setup][FTP] Created local folder: {}"".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print(""[Setup][FTP] Error: Could not change to: {}"".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + ""/"" + remote_path_relative + ""/"" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print(""[Setup][FTP] Switching to directory: {}"".format(remote_path_relative + ""/"" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, ""wb"") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print(""[Setup][FTP] ({}/{}) File downloaded: {}"".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print(""[Setup][FTP] ({}/{}) File already exists. Skipping: {}"".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urllib.request.urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """"""
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """"""

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob(""**/*.gz"")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print(""[Setup][Data] Decompressing file: {}"".format(path_str))
        print(""  - Output: {}"".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """"""
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob(""**/*.vcf"")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print(""[Setup][Data] Converting VCF file to Zarr format: {}"".format(path_str))
        print(""  - Output: {}"".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):
    """""" Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """"""
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get alt number
        if conversion_config.alt_number is None:
            print(""[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file."")
            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)
            numalt = callset['variants/numalt']
            alt_number = np.max(numalt)
        else:
            print(""[VCF-Zarr] Using alt number provided in configuration."")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print(""[VCF-Zarr] Alt number: {}"".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print(""[VCF-Zarr] Chunk length: {}"".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print(""[VCF-Zarr] Chunk width: {}"".format(chunk_width))

        if conversion_config.compressor == ""Blosc"":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError(""Unexpected compressor type specified."")

        print(""[VCF-Zarr] Using {} compressor."".format(conversion_config.compressor))

        print(""[VCF-Zarr] Performing VCF to Zarr conversion..."")
        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)
        print(""[VCF-Zarr] Done."")
/n/n/n",1
20,20,e965e0284789e610c0a50d20a92a82ec5c135064,"python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm import utils
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30
_HMAC_HEADER = 'x-ycm-hmac'

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return BaseRequest.session.post(
            _BuildUri( handler ),
            data = sent_data,
            headers = BaseRequest._ExtraHeaders( sent_data ),
            timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get(
            _BuildUri( handler ),
            headers = BaseRequest._ExtraHeaders(),
            timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        sent_data = ToUtf8Json( data )
        return requests.post( _BuildUri( handler ),
                              data = sent_data,
                              headers = BaseRequest._ExtraHeaders( sent_data ) )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = BaseRequest._ExtraHeaders() )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  @staticmethod
  def _ExtraHeaders( request_body = None ):
    if not request_body:
      request_body = ''
    headers = dict( _HEADERS )
    headers[ _HMAC_HEADER ] = utils.CreateHexHmac( request_body,
                                                   BaseRequest.hmac_secret )
    return headers

  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'
  hmac_secret = ''


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  _ValidateResponseObject( response )
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _ValidateResponseObject( response ):
  if not utils.ContentHexHmacValid( response.content,
                                    response.headers[ _HMAC_HEADER ],
                                    BaseRequest.hmac_secret ):
    raise RuntimeError( 'Received invalid HMAC for response!' )
  return True

def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ),
                             headers = BaseRequest._ExtraHeaders() )
    _ValidateResponseObject( response )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/npython/ycm/server/hmac_plugin.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2014  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import logging
import httplib
from bottle import request, response, abort
from ycm import utils

_HMAC_HEADER = 'x-ycm-hmac'

# This class implements the Bottle plugin API:
# http://bottlepy.org/docs/dev/plugindev.html
#
# We want to ensure that every request coming in has a valid HMAC set in the
# x-ycm-hmac header and that every response coming out sets such a valid header.
# This is to prevent security issues with possible remote code execution.
class HmacPlugin( object ):
  name = 'hmac'
  api = 2


  def __init__( self, hmac_secret ):
    self._hmac_secret = hmac_secret
    self._logger = logging.getLogger( __name__ )


  def __call__( self, callback ):
    def wrapper( *args, **kwargs ):
      body = request.body.read()
      if not utils.ContentHexHmacValid( body,
                                        request.headers[ _HMAC_HEADER ],
                                        self._hmac_secret ):
        self._logger.info( 'Dropping request with bad HMAC.' )
        abort( httplib.UNAUTHORIZED, 'Unauthorized, received bad HMAC.')
        return
      body = callback( *args, **kwargs )
      response.headers[ _HMAC_HEADER ] = utils.CreateHexHmac(
          body, self._hmac_secret )
      return body
    return wrapper

/n/n/npython/ycm/server/ycmd.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

from server_utils import SetUpPythonPath
SetUpPythonPath()

import sys
import logging
import json
import argparse
import waitress
import signal
import os
import base64
from ycm import user_options_store
from ycm import extra_conf_store
from ycm import utils
from ycm.server.watchdog_plugin import WatchdogPlugin
from ycm.server.hmac_plugin import HmacPlugin

def YcmCoreSanityCheck():
  if 'ycm_core' in sys.modules:
    raise RuntimeError( 'ycm_core already imported, ycmd has a bug!' )


# We manually call sys.exit() on SIGTERM and SIGINT so that atexit handlers are
# properly executed.
def SetUpSignalHandler(stdout, stderr, keep_logfiles):
  def SignalHandler( signum, frame ):
    if stderr:
      # Reset stderr, just in case something tries to use it
      tmp = sys.stderr
      sys.stderr = sys.__stderr__
      tmp.close()
    if stdout:
      # Reset stdout, just in case something tries to use it
      tmp = sys.stdout
      sys.stdout = sys.__stdout__
      tmp.close()

    if not keep_logfiles:
      if stderr:
        utils.RemoveIfExists( stderr )
      if stdout:
        utils.RemoveIfExists( stdout )

    sys.exit()

  for sig in [ signal.SIGTERM,
               signal.SIGINT ]:
    signal.signal( sig, SignalHandler )


def Main():
  parser = argparse.ArgumentParser()
  parser.add_argument( '--host', type = str, default = 'localhost',
                       help = 'server hostname')
  # Default of 0 will make the OS pick a free port for us
  parser.add_argument( '--port', type = int, default = 0,
                       help = 'server port')
  parser.add_argument( '--log', type = str, default = 'info',
                       help = 'log level, one of '
                              '[debug|info|warning|error|critical]' )
  parser.add_argument( '--idle_suicide_seconds', type = int, default = 0,
                       help = 'num idle seconds before server shuts down')
  parser.add_argument( '--options_file', type = str, default = '',
                       help = 'file with user options, in JSON format' )
  parser.add_argument( '--stdout', type = str, default = None,
                       help = 'optional file to use for stdout' )
  parser.add_argument( '--stderr', type = str, default = None,
                       help = 'optional file to use for stderr' )
  parser.add_argument( '--keep_logfiles', action = 'store_true', default = None,
                       help = 'retain logfiles after the server exits' )
  args = parser.parse_args()

  if args.stdout is not None:
    sys.stdout = open(args.stdout, ""w"")
  if args.stderr is not None:
    sys.stderr = open(args.stderr, ""w"")

  numeric_level = getattr( logging, args.log.upper(), None )
  if not isinstance( numeric_level, int ):
    raise ValueError( 'Invalid log level: %s' % args.log )

  # Has to be called before any call to logging.getLogger()
  logging.basicConfig( format = '%(asctime)s - %(levelname)s - %(message)s',
                       level = numeric_level )

  options = ( json.load( open( args.options_file, 'r' ) )
              if args.options_file
              else user_options_store.DefaultOptions() )
  utils.RemoveIfExists( args.options_file )
  hmac_secret = base64.b64decode( options[ 'hmac_secret' ] )
  user_options_store.SetAll( options )

  # This ensures that ycm_core is not loaded before extra conf
  # preload was run.
  YcmCoreSanityCheck()
  extra_conf_store.CallGlobalExtraConfYcmCorePreloadIfExists()

  # If not on windows, detach from controlling terminal to prevent
  # SIGINT from killing us.
  if not utils.OnWindows():
    try:
      os.setsid()
    # setsid() can fail if the user started ycmd directly from a shell.
    except OSError:
      pass

  # This can't be a top-level import because it transitively imports
  # ycm_core which we want to be imported ONLY after extra conf
  # preload has executed.
  from ycm.server import handlers
  handlers.UpdateUserOptions( options )
  SetUpSignalHandler(args.stdout, args.stderr, args.keep_logfiles)
  handlers.app.install( WatchdogPlugin( args.idle_suicide_seconds ) )
  handlers.app.install( HmacPlugin( hmac_secret ) )
  waitress.serve( handlers.app,
                  host = args.host,
                  port = args.port,
                  threads = 30 )


if __name__ == ""__main__"":
  Main()

/n/n/npython/ycm/utils.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import tempfile
import os
import sys
import signal
import functools
import socket
import stat
import json
import hmac
import hashlib
from distutils.spawn import find_executable
import subprocess
import collections

WIN_PYTHON27_PATH = 'C:\python27\pythonw.exe'
WIN_PYTHON26_PATH = 'C:\python26\pythonw.exe'


def IsIdentifierChar( char ):
  return char.isalnum() or char == '_'


def SanitizeQuery( query ):
  return query.strip()


# Given an object, returns a str object that's utf-8 encoded.
def ToUtf8IfNeeded( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  return str( value )


# Recurses through the object if it's a dict/iterable and converts all the
# unicode objects to utf-8 strings.
def RecursiveEncodeUnicodeToUtf8( value ):
  if isinstance( value, unicode ):
    return value.encode( 'utf8' )
  if isinstance( value, str ):
    return value
  elif isinstance( value, collections.Mapping ):
    return dict( map( RecursiveEncodeUnicodeToUtf8, value.iteritems() ) )
  elif isinstance( value, collections.Iterable ):
    return type( value )( map( RecursiveEncodeUnicodeToUtf8, value ) )
  else:
    return value


def ToUtf8Json( data ):
  return json.dumps( RecursiveEncodeUnicodeToUtf8( data ),
                     ensure_ascii = False,
                     # This is the encoding of INPUT str data
                     encoding = 'utf-8' )


def PathToTempDir():
  tempdir = os.path.join( tempfile.gettempdir(), 'ycm_temp' )
  if not os.path.exists( tempdir ):
    os.makedirs( tempdir )
    # Needed to support multiple users working on the same machine;
    # see issue 606.
    MakeFolderAccessibleToAll( tempdir )
  return tempdir


def MakeFolderAccessibleToAll( path_to_folder ):
  current_stat = os.stat( path_to_folder )
  # readable, writable and executable by everyone
  flags = ( current_stat.st_mode | stat.S_IROTH | stat.S_IWOTH | stat.S_IXOTH
            | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP )
  os.chmod( path_to_folder, flags )


def RunningInsideVim():
  try:
    import vim  # NOQA
    return True
  except ImportError:
    return False


def GetUnusedLocalhostPort():
  sock = socket.socket()
  # This tells the OS to give us any free port in the range [1024 - 65535]
  sock.bind( ( '', 0 ) )
  port = sock.getsockname()[ 1 ]
  sock.close()
  return port


def RemoveIfExists( filename ):
  try:
    os.remove( filename )
  except OSError:
    pass


def Memoize( obj ):
  cache = obj.cache = {}

  @functools.wraps( obj )
  def memoizer( *args, **kwargs ):
    key = str( args ) + str( kwargs )
    if key not in cache:
      cache[ key ] = obj( *args, **kwargs )
    return cache[ key ]
  return memoizer


@Memoize
def PathToPythonInterpreter():
  if not RunningInsideVim():
    return sys.executable

  import vim  # NOQA
  user_path_to_python = vim.eval( 'g:ycm_path_to_python_interpreter' )
  if user_path_to_python:
    return user_path_to_python

  # We check for 'python2' before 'python' because some OS's (I'm looking at you
  # Arch Linux) have made the... interesting decision to point /usr/bin/python
  # to python3.
  python_names = [ 'python2', 'python' ]
  if OnWindows():
    # On Windows, 'pythonw' doesn't pop-up a console window like running
    # 'python' does.
    python_names.insert( 0, 'pythonw' )

  path_to_python = PathToFirstExistingExecutable( python_names )
  if path_to_python:
    return path_to_python

  # On Windows, Python may not be on the PATH at all, so we check some common
  # install locations.
  if OnWindows():
    if os.path.exists( WIN_PYTHON27_PATH ):
      return WIN_PYTHON27_PATH
    elif os.path.exists( WIN_PYTHON26_PATH ):
      return WIN_PYTHON26_PATH
  raise RuntimeError( 'Python 2.7/2.6 not installed!' )


def PathToFirstExistingExecutable( executable_name_list ):
  for executable_name in executable_name_list:
    path = find_executable( executable_name )
    if path:
      return path
  return None


def OnWindows():
  return sys.platform == 'win32'


def OnCygwin():
  return sys.platform == 'cygwin'


# From here: http://stackoverflow.com/a/8536476/1672783
def TerminateProcess( pid ):
  if OnWindows():
    import ctypes
    PROCESS_TERMINATE = 1
    handle = ctypes.windll.kernel32.OpenProcess( PROCESS_TERMINATE,
                                                 False,
                                                 pid )
    ctypes.windll.kernel32.TerminateProcess( handle, -1 )
    ctypes.windll.kernel32.CloseHandle( handle )
  else:
    os.kill( pid, signal.SIGTERM )


def AddThirdPartyFoldersToSysPath():
  path_to_third_party = os.path.join(
                          os.path.dirname( os.path.abspath( __file__ ) ),
                          '../../third_party' )

  for folder in os.listdir( path_to_third_party ):
    sys.path.insert( 0, os.path.realpath( os.path.join( path_to_third_party,
                                                        folder ) ) )

def ForceSemanticCompletion( request_data ):
  return ( 'force_semantic' in request_data and
           bool( request_data[ 'force_semantic' ] ) )


# A wrapper for subprocess.Popen that works around a Popen bug on Windows.
def SafePopen( *args, **kwargs ):
  if kwargs.get( 'stdin' ) is None:
    # We need this on Windows otherwise bad things happen. See issue #637.
    kwargs[ 'stdin' ] = subprocess.PIPE if OnWindows() else None

  return subprocess.Popen( *args, **kwargs )


def ContentHexHmacValid( content, hmac, hmac_secret ):
  return hmac == CreateHexHmac( content, hmac_secret )


def CreateHexHmac( content, hmac_secret ):
  return hmac.new( hmac_secret,
                   msg = content,
                   digestmod = hashlib.sha256 ).hexdigest()
/n/n/npython/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
import base64
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

HMAC_SECRET_LENGTH = 16
NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    # The temp options file is deleted by ycmd during startup
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      hmac_secret = os.urandom( HMAC_SECRET_LENGTH )
      options_dict = dict( self._user_options )
      options_dict[ 'hmac_secret' ] = base64.b64encode( hmac_secret )
      json.dump( options_dict, options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )
      BaseRequest.hmac_secret = hmac_secret

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",0
21,21,e965e0284789e610c0a50d20a92a82ec5c135064,"/python/ycm/client/base_request.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2013  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import vim
import requests
import urlparse
from retries import retries
from requests_futures.sessions import FuturesSession
from ycm.unsafe_thread_pool_executor import UnsafeThreadPoolExecutor
from ycm import vimsupport
from ycm.utils import ToUtf8Json
from ycm.server.responses import ServerError, UnknownExtraConf

_HEADERS = {'content-type': 'application/json'}
_EXECUTOR = UnsafeThreadPoolExecutor( max_workers = 30 )
# Setting this to None seems to screw up the Requests/urllib3 libs.
_DEFAULT_TIMEOUT_SEC = 30

class BaseRequest( object ):
  def __init__( self ):
    pass


  def Start( self ):
    pass


  def Done( self ):
    return True


  def Response( self ):
    return {}

  # This method blocks
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def GetDataFromHandler( handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest._TalkToHandlerAsync( '',
                                                            handler,
                                                            'GET',
                                                            timeout ) )


  # This is the blocking version of the method. See below for async.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandler( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return JsonFromFuture( BaseRequest.PostDataToHandlerAsync( data,
                                                               handler,
                                                               timeout ) )


  # This returns a future! Use JsonFromFuture to get the value.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def PostDataToHandlerAsync( data, handler, timeout = _DEFAULT_TIMEOUT_SEC ):
    return BaseRequest._TalkToHandlerAsync( data, handler, 'POST', timeout )


  # This returns a future! Use JsonFromFuture to get the value.
  # |method| is either 'POST' or 'GET'.
  # |timeout| is num seconds to tolerate no response from server before giving
  # up; see Requests docs for details (we just pass the param along).
  @staticmethod
  def _TalkToHandlerAsync( data,
                           handler,
                           method,
                           timeout = _DEFAULT_TIMEOUT_SEC ):
    def SendRequest( data, handler, method, timeout ):
      if method == 'POST':
        return BaseRequest.session.post( _BuildUri( handler ),
                                        data = ToUtf8Json( data ),
                                        headers = _HEADERS,
                                        timeout = timeout )
      if method == 'GET':
        return BaseRequest.session.get( _BuildUri( handler ),
                                        headers = _HEADERS,
                                        timeout = timeout )

    @retries( 5, delay = 0.5, backoff = 1.5 )
    def DelayedSendRequest( data, handler, method ):
      if method == 'POST':
        return requests.post( _BuildUri( handler ),
                              data = ToUtf8Json( data ),
                              headers = _HEADERS )
      if method == 'GET':
        return requests.get( _BuildUri( handler ),
                             headers = _HEADERS )

    if not _CheckServerIsHealthyWithCache():
      return _EXECUTOR.submit( DelayedSendRequest, data, handler, method )

    return SendRequest( data, handler, method, timeout )


  session = FuturesSession( executor = _EXECUTOR )
  server_location = 'http://localhost:6666'


def BuildRequestData( start_column = None,
                      query = None,
                      include_buffer_data = True ):
  line, column = vimsupport.CurrentLineAndColumn()
  filepath = vimsupport.GetCurrentBufferFilepath()
  request_data = {
    'filetypes': vimsupport.CurrentFiletypes(),
    'line_num': line,
    'column_num': column,
    'start_column': start_column,
    'line_value': vim.current.line,
    'filepath': filepath
  }

  if include_buffer_data:
    request_data[ 'file_data' ] = vimsupport.GetUnsavedAndCurrentBufferData()
  if query:
    request_data[ 'query' ] = query

  return request_data


def JsonFromFuture( future ):
  response = future.result()
  if response.status_code == requests.codes.server_error:
    _RaiseExceptionForData( response.json() )

  # We let Requests handle the other status types, we only handle the 500
  # error code.
  response.raise_for_status()

  if response.text:
    return response.json()
  return None


def _BuildUri( handler ):
  return urlparse.urljoin( BaseRequest.server_location, handler )


SERVER_HEALTHY = False

def _CheckServerIsHealthyWithCache():
  global SERVER_HEALTHY

  def _ServerIsHealthy():
    response = requests.get( _BuildUri( 'healthy' ) )
    response.raise_for_status()
    return response.json()

  if SERVER_HEALTHY:
    return True

  try:
    SERVER_HEALTHY = _ServerIsHealthy()
    return SERVER_HEALTHY
  except:
    return False


def _RaiseExceptionForData( data ):
  if data[ 'exception' ][ 'TYPE' ] == UnknownExtraConf.__name__:
    raise UnknownExtraConf( data[ 'exception' ][ 'extra_conf_file' ] )

  raise ServerError( '{0}: {1}'.format( data[ 'exception' ][ 'TYPE' ],
                                        data[ 'message' ] ) )
/n/n/n/python/ycm/youcompleteme.py/n/n#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]


/n/n/n",1
12,12,2191fe6c5a850ddcf7a78f7913881cef1677500d,"src/main/python/monitoring_config_generator/yaml_tools/readers.py/n/nimport datetime
import os
import os.path
import urlparse
import socket
from time import localtime, strftime, time

from requests.exceptions import RequestException, ConnectionError, Timeout
import requests
import yaml

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException
from monitoring_config_generator.yaml_tools.merger import merge_yaml_files

def is_file(parsed_uri):
    return parsed_uri.scheme in ['', 'file']


def is_host(parsed_uri):
    return parsed_uri.scheme in ['http', 'https']


def read_config(uri):
    uri_parsed = urlparse.urlparse(uri)
    if is_file(uri_parsed):
        return read_config_from_file(uri_parsed.path)
    elif is_host(uri_parsed):
        return read_config_from_host(uri)
    else:
        raise ValueError('Given url was not acceptable %s' % uri)


def read_config_from_file(path):
    yaml_config = merge_yaml_files(path)
    etag = None
    mtime = os.path.getmtime(path)
    return yaml_config, Header(etag=etag, mtime=mtime)


def read_config_from_host(url):
    try:
        response = requests.get(url)
    except socket.error as e:
        msg = ""Could not open socket for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except ConnectionError as e:
        msg = ""Could not establish connection for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except Timeout as e:
        msg = ""Connect timed out for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except RequestException as e:
        msg = ""Could not get monitoring yaml from '%s', error: %s"" % (url, e)
        raise MonitoringConfigGeneratorException(msg)

    def get_from_header(field):
        return response.headers[field] if field in response.headers else None

    if response.status_code == 200:
        yaml_config = yaml.safe_load(response.content)
        etag = get_from_header('etag')
        mtime = get_from_header('last-modified')
        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())
    else:
        msg = ""Request %s returned with status %s. I don't know how to handle that."" % (url, response.status_code)
        raise MonitoringConfigGeneratorException(msg)

    return yaml_config, Header(etag=etag, mtime=mtime)


class Header(object):
    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'
    ETAG_COMMENT = '# ETag: '
    MTIME_COMMMENT = '# MTime: '

    def __init__(self, etag=None, mtime=0):
        self.etag = etag
        self.mtime = int(mtime)

    def __nonzero__(self):
        return self.etag is None and self.mtime is 0

    def __eq__(self, other):
        return self.etag == other.etag and self.mtime == other.mtime

    def __repr__(self):
        return ""Header(%s, %d)"" % (self.etag, self.mtime)

    def is_newer_than(self, other):
        if self.etag != other.etag or self.etag is None:
            return cmp(self.mtime, other.mtime) > 0
        else:
            return False

    def serialize(self):
        lines = []
        time_string = strftime(""%Y-%m-%d %H:%M:%S"", localtime())
        lines.append(""%s on %s"" % (Header.MON_CONF_GEN_COMMENT, time_string))
        if self.etag:
            lines.append(""%s%s"" % (Header.ETAG_COMMENT, self.etag))
        if self.mtime:
            lines.append(""%s%d"" % (Header.MTIME_COMMMENT, self.mtime))
        return lines

    @staticmethod
    def parse(file_name):
        etag, mtime = None, 0

        def extract(comment, current_value):
            value = None
            if line.startswith(comment):
                value = line.rstrip()[len(comment):]
            return value or current_value

        try:
            with open(file_name, 'r') as config_file:
                for line in config_file.xreadlines():
                    etag = extract(Header.ETAG_COMMENT, etag)
                    mtime = extract(Header.MTIME_COMMMENT, mtime)
                    if etag and mtime:
                        break
        except IOError as e:
            # it is totally fine to not have an etag, in that case there
            # will just be no caching and the server will have to deliver the data again
            pass
        finally:
            return Header(etag=etag, mtime=mtime)
/n/n/n",0
13,13,2191fe6c5a850ddcf7a78f7913881cef1677500d,"/src/main/python/monitoring_config_generator/yaml_tools/readers.py/n/nimport datetime
import os
import os.path
import urlparse
import socket
from time import localtime, strftime, time

from requests.exceptions import RequestException, ConnectionError, Timeout
import requests
import yaml

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException
from monitoring_config_generator.yaml_tools.merger import merge_yaml_files

def is_file(parsed_uri):
    return parsed_uri.scheme in ['', 'file']


def is_host(parsed_uri):
    return parsed_uri.scheme in ['http', 'https']


def read_config(uri):
    uri_parsed = urlparse.urlparse(uri)
    if is_file(uri_parsed):
        return read_config_from_file(uri_parsed.path)
    elif is_host(uri_parsed):
        return read_config_from_host(uri)
    else:
        raise ValueError('Given url was not acceptable %s' % uri)


def read_config_from_file(path):
    yaml_config = merge_yaml_files(path)
    etag = None
    mtime = os.path.getmtime(path)
    return yaml_config, Header(etag=etag, mtime=mtime)


def read_config_from_host(url):
    try:
        response = requests.get(url)
    except socket.error as e:
        msg = ""Could not open socket for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except ConnectionError as e:
        msg = ""Could not establish connection for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except Timeout as e:
        msg = ""Connect timed out for '%s', error: %s"" % (url, e)
        raise HostUnreachableException(msg)
    except RequestException as e:
        msg = ""Could not get monitoring yaml from '%s', error: %s"" % (url, e)
        raise MonitoringConfigGeneratorException(msg)

    def get_from_header(field):
        return response.headers[field] if field in response.headers else None

    if response.status_code == 200:
        yaml_config = yaml.load(response.content)
        etag = get_from_header('etag')
        mtime = get_from_header('last-modified')
        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())
    else:
        msg = ""Request %s returned with status %s. I don't know how to handle that."" % (url, response.status_code)
        raise MonitoringConfigGeneratorException(msg)

    return yaml_config, Header(etag=etag, mtime=mtime)


class Header(object):
    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'
    ETAG_COMMENT = '# ETag: '
    MTIME_COMMMENT = '# MTime: '

    def __init__(self, etag=None, mtime=0):
        self.etag = etag
        self.mtime = int(mtime)

    def __nonzero__(self):
        return self.etag is None and self.mtime is 0

    def __eq__(self, other):
        return self.etag == other.etag and self.mtime == other.mtime

    def __repr__(self):
        return ""Header(%s, %d)"" % (self.etag, self.mtime)

    def is_newer_than(self, other):
        if self.etag != other.etag or self.etag is None:
            return cmp(self.mtime, other.mtime) > 0
        else:
            return False

    def serialize(self):
        lines = []
        time_string = strftime(""%Y-%m-%d %H:%M:%S"", localtime())
        lines.append(""%s on %s"" % (Header.MON_CONF_GEN_COMMENT, time_string))
        if self.etag:
            lines.append(""%s%s"" % (Header.ETAG_COMMENT, self.etag))
        if self.mtime:
            lines.append(""%s%d"" % (Header.MTIME_COMMMENT, self.mtime))
        return lines

    @staticmethod
    def parse(file_name):
        etag, mtime = None, 0

        def extract(comment, current_value):
            value = None
            if line.startswith(comment):
                value = line.rstrip()[len(comment):]
            return value or current_value

        try:
            with open(file_name, 'r') as config_file:
                for line in config_file.xreadlines():
                    etag = extract(Header.ETAG_COMMENT, etag)
                    mtime = extract(Header.MTIME_COMMMENT, mtime)
                    if etag and mtime:
                        break
        except IOError as e:
            # it is totally fine to not have an etag, in that case there
            # will just be no caching and the server will have to deliver the data again
            pass
        finally:
            return Header(etag=etag, mtime=mtime)
/n/n/n",1
74,74,7ff203be36e439b535894764c37a8446351627ec,"lib/Shine/Commands/Base/Command.py/n/n# Command.py -- Base command class
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Support.Debug import Debug

from CommandRCDefs import *

import getopt


#
# Command exceptions are defined in Shine.Command.Exceptions
#

class Command(object):
    """"""
    The base class for command objects that can be added to the commands
    registry.
    """"""
    def __init__(self):
        self.options = {}
        self.getopt_string = """"
        self.params_desc = """"
        self.last_optional = 0
        self.arguments = None

        # All commands have debug support.
        self.debug_support = Debug(self)

    def is_hidden(self):
        """"""Return whether the command should not be displayed to user.""""""
        return False
    
    def get_name(self):
        raise NotImplementedError(""Derived classes must implement."")

    def get_desc(self):
        return ""Undocumented""

    def get_params_desc(self):
        pdesc = self.params_desc.strip()
        if self.has_subcommand():
            return ""%s %s"" % ('|'.join(self.get_subcommands()), pdesc)
        return pdesc

    def has_subcommand(self):
        """"""Return whether the command supports subcommand(s).""""""
        return False

    def get_subcommands(self):
        """"""Return the list of subcommand(s).""""""
        raise NotImplementedError(""Derived classes must implement."")
    
    def add_option(self, flag, arg, attr, cb=None):
        """"""
        Add an option for getopt with optional argument.
        """"""
        assert flag not in self.options

        optional = attr.get('optional', False)
        hidden = attr.get('hidden', False)

        if cb:
            self.options[flag] = cb

        object.__setattr__(self, ""opt_%s"" % flag, None)
            
        self.getopt_string += flag
        if optional:
            leftmark = '['
            rightmark = ']'
        else:
            leftmark = ''
            rightmark = ''

        if arg:
            self.getopt_string += "":""
            if not hidden:
                self.params_desc += ""%s-%s <%s>%s "" % (leftmark,
                    flag, arg, rightmark)
                self.last_optional = 0
        elif not hidden:
            if self.last_optional == 0:
                self.params_desc += ""%s-%s%s "" % (leftmark, flag, rightmark)
            else:
                self.params_desc = self.params_desc[:-2] + ""%s%s "" % (flag,
                    rightmark)
            
            if optional:
                self.last_optional = 1
            else:
                self.last_optional = 2

    def parse(self, args):
        """"""
        Parse command arguments.
        """"""
        options, arguments = getopt.gnu_getopt(args, self.getopt_string)
        self.arguments = arguments

        for opt, arg in options:
            trim_opt = opt[1:]
            callback = self.options.get(trim_opt)
            if callback:
                callback(trim_opt, arg)
            object.__setattr__(self, ""opt_%s"" % trim_opt, arg or True)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation.
        
        Return True when the user confirms the action, False otherwise.
        """"""
        i = raw_input(""%s (y)es/(N)o: "" % prompt)
        return i == 'y' or i == 'Y'


    def filter_rc(self, rc):
        """"""
        Allow derived classes to filter return codes.
        """"""
        # default is to not filter return code
        return rc

/n/n/nlib/Shine/Commands/Base/RemoteCommand.py/n/n# RemoteCommand.py -- Base command with remote capabilities
# Copyright (C) 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *
from Command import Command
from CommandRCDefs import *
from RemoteCallEventHandler import RemoteCallEventHandler
from Support.Nodes import Nodes
from Support.Yes import Yes

import socket


class RemoteCommand(Command):
    
    def __init__(self):
        Command.__init__(self)
        self.remote_call = False
        self.local_flag = False
        attr = { 'optional' : True, 'hidden' : True }
        self.add_option('L', None, attr, cb=self.parse_L)
        self.add_option('R', None, attr, cb=self.parse_R)
        self.nodes_support = Nodes(self)
        self.eventhandler = None

    def parse_L(self, opt, arg):
        self.local_flag = True

    def parse_R(self, opt, arg):
        self.remote_call = True

    def has_local_flag(self):
        return self.local_flag or self.remote_call

    def init_execute(self):
        """"""
        Initialize execution of remote command, if needed. Should be called
        first from derived classes before really executing the command.
        """"""
        # Limit the scope of the command if called with local flag (-L) or
        # called remotely (-R).
        if self.has_local_flag():
            self.opt_n = socket.gethostname().split('.', 1)[0]

    def install_eventhandler(self, local_eventhandler, global_eventhandler):
        """"""
        Select and install the appropriate event handler.
        """"""
        if self.remote_call:
            # When called remotely (-R), install a special event handler
            # that knows how to speak the Shine Proxy Protocol using pickle.
            self.eventhandler = RemoteCallEventHandler()
        elif self.local_flag:
            self.eventhandler = local_eventhandler
        else:
            self.eventhandler = global_eventhandler
        # return handler for convenience
        return self.eventhandler

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation. Overrides Command.ask_confirm to
        avoid confirmation when called remotely (-R).

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.remote_call or Command.ask_confirm(self, prompt)

    def filter_rc(self, rc):
        """"""
        When called remotely, return code are not used to handle shine action
        success or failure, nor for status info. To properly detect ssh or remote
        shine installation failures, we filter the return code here.
        """"""
        if self.remote_call:
            # Only errors of type RUNTIME ERROR are allowed to go up.
            rc &= RC_FLAG_RUNTIME_ERROR

        return Command.filter_rc(self, rc)


class RemoteCriticalCommand(RemoteCommand):

    def __init__(self):
        RemoteCommand.__init__(self)
        self.yes_support = Yes(self)

    def ask_confirm(self, prompt):
        """"""
        Ask user for confirmation if -y not specified.

        Return True when the user confirms the action, False otherwise.
        """"""
        return self.yes_support.has_yes() or RemoteCommand.ask_confirm(self, prompt)

/n/n/nlib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        rc = command.execute()

        # Filter rc
        return command.filter_rc(rc)

/n/n/nlib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.CommandRCDefs import *
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes

from Exceptions import *

class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            raise CommandHelpException(""Lustre model file path (-m <model_file>) argument required."", self)
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return RC_OK

/n/n/nlib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Starting %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if not self.remote_call:
                if rc == RC_OK:
                    if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                        print ""Mount successful on %s"" % m_nodes
                elif rc == RC_RUNTIME_ERROR:
                    for nodes, msg in fs.proxy_errors:
                        print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.CommandRCDefs import *
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError %s"" % ex
            return RC_RUNTIME_ERROR

        return RC_OK
/n/n/nlib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

        return result
/n/n/nlib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)
            if not statusdict:
                continue

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if not self.remote_call and vlevel > 0:
                if view == ""fs"":
                    self.status_view_fs(fs)
                elif view.startswith(""target""):
                    self.status_view_targets(fs)
                elif view.startswith(""disk""):
                    self.status_view_disks(fs)
                else:
                    raise CommandBadParameterError(self.view_support.get_view(),
                            ""fs, targets, disks"")

        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/nlib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount %s: %s"" % (node, client.fs.fs_name, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            if not self.remote_call and vlevel > 0:
                if nodes:
                    m_nodes = nodes.intersection(fs.get_client_servers())
                else:
                    m_nodes = fs.get_client_servers()
                print ""Stopping %s clients on %s..."" % (fs.fs_name, m_nodes)

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                        # m_nodes is defined if not self.remote_call and vlevel > 0
                    print ""Unmount successful on %s"" % m_nodes
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/nlib/Shine/Configuration/FileSystem.py/n/n# FileSystem.py -- Lustre file system configuration
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$


from Globals import Globals
from Model import Model
from Exceptions import *
from TuningModel import TuningModel

from ClusterShell.NodeSet import NodeSet

from NidMap import NidMap
from TargetDevice import TargetDevice

import copy
import os
import sys


class FileSystem(Model):
    """"""
    Lustre File System Configuration class.
    """"""
    def __init__(self, fs_name=None, lmf=None, tuning_file=None):
        """""" Initialize File System config
        """"""
        self.backend = None

        globals = Globals()

        fs_conf_dir = os.path.expandvars(globals.get_conf_dir())
        fs_conf_dir = os.path.normpath(fs_conf_dir)

        # Load the file system from model or extended model
        if not fs_name and lmf:
            Model.__init__(self, lmf)

            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, self.get_one('fs_name'))

            self._setup_target_devices()

            # Reload
            self.set_filename(self.xmf_path)

        elif fs_name:
            self.xmf_path = ""%s/%s.xmf"" % (fs_conf_dir, fs_name)
            Model.__init__(self, self.xmf_path)

        self._setup_nid_map(self.get_one('nid_map'))

        self.fs_name = self.get_one('fs_name')
        
        # Initialize the tuning model to None if no special tuning configuration
        # is provided
        self.tuning_model = None
        
        if tuning_file:
            # It a tuning configuration file is provided load it
            self.tuning_model = TuningModel(tuning_file)
        else:
            self.tuning_model = TuningModel()

        #self._start_backend()

    def _start_backend(self):
        """"""
        Load and start backend subsystem once
        """"""
        if not self.backend:

            from Backend.BackendRegistry import BackendRegistry
            from Backend.Backend import Backend

            # Start the selected config backend system.
            self.backend = BackendRegistry().get_selected()
            if self.backend:
                self.backend.start()

        return self.backend

    def _setup_target_devices(self):
        """""" Generate the eXtended Model File XMF
        """"""
        self._start_backend()

        for target in [ 'mgt', 'mdt', 'ost' ]:

            if self.backend:

                # Returns a list of TargetDevices
                candidates = copy.copy(self.backend.get_target_devices(target))

                try:
                    # Save the model target selection
                    target_models = copy.copy(self.get(target))
                except KeyError, e:
                    raise ConfigException(""No %s target found"" %(target))

                # Delete it (to be replaced... see below)
                self.delete(target)
                 
                # Iterates on ModelDevices
                i = 0
                for target_model in target_models:
                    result = target_model.match_device(candidates)
                    if len(result) == 0 and not target == 'mgt' :
                        raise ConfigDeviceNotFoundError(target_model)
                    for matching in result:
                        candidates.remove(matching)
                        #
                        # target index is now mandatory in XMF files
                        if not matching.has_index():
                            matching.add_index(i)
                            i += 1

                        # `matching' is a TargetDevice, we want to add it to the
                        # underlying Model object. The current way to do this to
                        # create a configuration line string (performed by
                        # TargetDevice.getline()) and then call Model.add(). 
                        # TODO: add methods to Model/ModelDevice to avoid the use
                        #       of temporary configuration string line.
                        self.add(target, matching.getline())
            else:
                # no backend support

                devices = copy.copy(self.get_with_dict(target))

                self.delete(target)

                target_devices = []
                i = 0
                for dict in devices:
                    t = TargetDevice(target, dict)
                    if not t.has_index():
                        t.add_index(i)
                        i += 1
                    target_devices.append(TargetDevice(target, dict))
                    self.add(target, t.getline())

                if len(target_devices) == 0:
                    raise ConfigDeviceNotFoundError(self)




        # Save XMF
        self.save(self.xmf_path, ""Shine Lustre file system config file for %s"" % \
                self.get_one('fs_name'))
            
    def _setup_nid_map(self, maps):
        """"""
        Set self.nid_map using the NidMap helper class
        """"""
        #self.nid_map = NidMap().fromlist(maps)
        self.nid_map = NidMap(maps.get_one('nodes'), maps.get_one('nids'))

    def get_nid(self, node):
        try:
            return self.nid_map[node]
        except KeyError:
            raise ConfigException(""Cannot get NID for %s, aborting. Please verify `nid_map' configuration."" % node)

    def __str__(self):
        return "">> BACKEND:\n%s\n>> MODEL:\n%s"" % (self.backend, Model.__str__(self))

    def close(self):
        if self.backend:
            self.backend.stop()
            self.backend = None
    
    def register_client(self, node):
        """"""
        This function aims to register a new client that will be able to mount the
        file system.
        Parameters:
        @type node: string
        @param node : is the new client node name
        """"""
        if self._start_backend():
            self.backend.register_client(self.fs_name, node)
        
    def unregister_client(self, node):
        """"""
        This function aims to unregister a client of this  file system
        Parameters:
        @type node: string
        @param node : is name of the client node to unregister
        """"""
        if self._start_backend():
            self.backend.unregister_client(self.fs_name, node)
    
    def set_status_client_mount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                    self.backend.MOUNT_COMPLETE, options)

    def set_status_client_mount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_FAILED, options)

    def set_status_client_mount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.MOUNT_WARNING, options)

    def set_status_client_umount_complete(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_COMPLETE, options)

    def set_status_client_umount_failed(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_FAILED, options)

    def set_status_client_umount_warning(self, node, options):
        if self._start_backend():
            self.backend.set_status_client(self.fs_name, node,
                self.backend.UMOUNT_WARNING, options)

    def get_status_clients(self):
        if self._start_backend():
            return self.backend.get_status_clients(self.fs_name)

    def set_status_target_unknown(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNKNOWN
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, node, 
                self.backend.TARGET_UNKNOWN, options)

    def set_status_target_ko(self, target, options):
        """"""
        This function is used to set the specified target status
        to KO
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                backend.TARGET_KO, options)

    def set_status_target_available(self, target, options):
        """"""
        This function is used to set the specified target status
        to AVAILABLE
        """"""
        if self._start_backend():
            # Set the fs_name to Free since these targets are availble
            # which means not used by any file system.
            self.backend.set_status_target(None, target,
                self.backend.TARGET_AVAILABLE, options)

    def set_status_target_formating(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATING, options)

    def set_status_target_format_failed(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMAT_FAILED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMAT_FAILED, options)

    def set_status_target_formated(self, target, options):
        """"""
        This function is used to set the specified target status
        to FORMATED
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_FORMATED, options)

    def set_status_target_offline(self, target, options):
        """"""
        This function is used to set the specified target status
        to OFFLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_OFFLINE, options)

    def set_status_target_starting(self, target, options):
        """"""
        This function is used to set the specified target status
        to STARTING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STARTING, options)

    def set_status_target_online(self, target, options):
        """"""
        This function is used to set the specified target status
        to ONLINE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_ONLINE, options)

    def set_status_target_critical(self, target, options):
        """"""
        This function is used to set the specified target status
        to CRITICAL
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_CRITICAL, options)

    def set_status_target_stopping(self, target, options):
        """"""
        This function is used to set the specified target status
        to STOPPING
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_STOPPING, options)

    def set_status_target_unreachable(self, target, options):
        """"""
        This function is used to set the specified target status
        to UNREACHABLE
        """"""
        if self._start_backend():
            self.backend.set_status_target(self.fs_name, target, 
                self.backend.TARGET_UNREACHABLE, options)

    def get_status_targets(self):
        """"""
        This function returns the status of each targets
        involved in the current file system.
        """"""
        if self._start_backend():
            return self.backend.get_status_targets(self.fs_name)

    def register(self):
        """"""
        This function aims to register the file system configuration
        to the backend.
        """"""
        if self._start_backend():
            return self.backend.register_fs(self)

    def unregister(self):
        """"""
        This function aims to remove a file system configuration from
        the backend.        
        """"""
        result = 0
        if self._start_backend():
            result = self.backend.unregister_fs(self)

        if not result:
            os.unlink(self.xmf_path)

        return result
/n/n/nlib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
        except KeyError:
            raise
        
        return RC_RUNTIME_ERROR


/n/n/nlib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # some common remote errors:
            # rc 127 = command not found
            # rc 126 = found but not executable
            # rc 1 = python failure...
            if rc != 0:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/nlib/Shine/Lustre/FileSystem.py/n/n# FileSystem.py -- Lustre FS
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Lustre FileSystem class.

Represents a Lustre FS.
""""""

import copy
from sets import Set
import socket

from ClusterShell.NodeSet import NodeSet, RangeSet

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

# Action exceptions
from Actions.Action import ActionErrorException
from Actions.Proxies.ProxyAction import *

from Actions.Install import Install
from Actions.Proxies.Preinstall import Preinstall
from Actions.Proxies.FSProxyAction import FSProxyAction
from Actions.Proxies.FSClientProxyAction import FSClientProxyAction

from EventHandler import *
from Client import *
from Server import *
from Target import *


class FSException(Exception):
    def __init__(self, message):
        self.message = message
    def __str__(self):
        return self.message

class FSError(FSException):
    """"""
    Base FileSystem error exception.
    """"""

class FSSyntaxError(FSError):
    def __init__(self, message):
        self.message = ""Syntax error: \""%s\"""" % (message)
    def __str__(self):
        return self.message

class FSBadTargetError(FSSyntaxError):
    def __init__(self, target_name):
        self.message = ""Syntax error: unrecognized target \""%s\"""" % (target_name)

class FSStructureError(FSError):
    """"""
    Lustre file system structure error, raised after an invalid configuration
    is encountered. For example, you will get this error if you try to assign
    two targets `MGT' to a filesystem.
    """"""

class FSRemoteError(FSError):
    """"""
    Remote host(s) not available, or a remote operation failed.
    """"""
    def __init__(self, nodes, rc, message):
        FSError.__init__(self, message)
        self.nodes = nodes
        self.rc = int(rc)

    def __str__(self):
        return ""%s: %s [rc=%d]"" % (self.nodes, self.message, self.rc)


STATUS_SERVERS      = 0x01
STATUS_HASERVERS    = 0x02
STATUS_CLIENTS      = 0x10
STATUS_ANY          = 0xff


class FileSystem:
    """"""
    The Lustre FileSystem abstract class.
    """"""

    def __init__(self, fs_name, event_handler=None):
        self.fs_name = fs_name
        self.debug = False
        self.set_eventhandler(event_handler)
        self.proxy_errors = []

        self.local_hostname = socket.gethostname()
        self.local_hostname_short = self.local_hostname.split('.', 1)[0]

        # file system MGT
        self.mgt = None

        # All FS server targets (MGT, MDT, OST...)
        self.targets = []

        # All FS clients
        self.clients = []

        # filled after successful install
        self.mgt_servers = NodeSet()
        self.mgt_count = 0

        self.mdt_servers = NodeSet()
        self.mdt_count = 0

        self.ost_servers = NodeSet()
        self.ost_count = 0

        self.target_count = 0
        self.target_servers = NodeSet()

    def set_debug(self, debug):
        self.debug = debug

    #
    # file system event handling
    #

    def _invoke_event(self, event, **kwargs):
        if 'target' in kwargs or 'client' in kwargs:
            kwargs.setdefault('node', None)
        getattr(self.event_handler, event)(**kwargs)

    def _invoke_dummy(self, event, **kwargs):
        pass

    def set_eventhandler(self, event_handler):
        self.event_handler = event_handler
        if self.event_handler is None:
            self._invoke = self._invoke_dummy
        else:
            self._invoke = self._invoke_event

    def _handle_shine_event(self, event, node, **params):
        #print ""_handle_shine_event %s %s"" % (event, params)
        target = params.get('target')
        if target:
            found = False
            for t in self.targets:
                if t.match(target):
                    # perform sanity checks here
                    old_nids = t.get_nids()
                    if old_nids != target.get_nids():
                        print ""NIDs mismatch %s -> %s"" % \
                                (','.join(old.nids), ','.join(target.get_nids))
                    # update target from remote one
                    t.update(target)
                    # substitute target parameter by local one
                    params['target'] = t
                    found = True
            if not found:
                print ""Target Update FAILED (%s)"" % target
        
        client = params.get('client')
        if client:
            found = False
            for c in self.clients:
                if c.match(client):
                    # update client from remote one
                    c.update(client)
                    # substitute client parameter
                    params['client'] = c
                    found = True
            if not found:
                print ""Client Update FAILED (%s)"" % client

        self._invoke(event, node=node, **params)

    def _handle_shine_proxy_error(self, nodes, message):
        self.proxy_errors.append((NodeSet(nodes), message))

    #
    # file system construction
    #

    def _attach_target(self, target):
        self.targets.append(target)
        if target.type == 'mgt':
            self.mgt = target
        self._update_structure()

    def _attach_client(self, client):
        self.clients.append(client)
        self._update_structure()

    def new_target(self, server, type, index, dev, jdev=None, group=None,
            tag=None, enabled=True):
        """"""
        Create a new attached target.
        """"""
        #print ""new_target on %s type %s (enabled=%s)"" % (server, type, enabled)

        if type == 'mgt' and self.mgt and len(self.mgt.get_nids()) > 0:
            raise FSStructureError(""A Lustre FS has only one MGT."")

        # Instantiate matching target class (eg. 'ost' -> OST).
        target = getattr(sys.modules[self.__class__.__module__], type.upper())(fs=self,
                server=server, index=index, dev=dev, jdev=jdev, group=group, tag=tag,
                enabled=enabled)
        
        return target

    def new_client(self, server, mount_path, enabled=True):
        """"""
        Create a new attached client.
        """"""
        client = Client(self, server, mount_path, enabled)

        return client

    def get_mgs_nids(self):
        return self.mgt.get_nids()
    
    def get_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients])

    def get_enabled_client_servers(self):
        return NodeSet.fromlist([c.server for c in self.clients if c.action_enabled])

    def get_enabled_target_servers(self):
        return NodeSet.fromlist([t.server for t in self.targets if t.action_enabled])

    def get_client_statecounters(self):
        """"""
        Get (ignored, offline, error, runtime_error, mounted) client state counters tuple.
        """"""
        ignored = 0
        states = {}
        for client in self.clients:
            if client.action_enabled:
                state = states.setdefault(client.state, 0)
                states[client.state] = state + 1
            else:
                ignored += 1
        
        return (ignored,
                states.get(OFFLINE, 0),
                states.get(CLIENT_ERROR, 0),
                states.get(RUNTIME_ERROR, 0),
                states.get(MOUNTED, 0))

    def targets_by_state(self, state):
        for target in self.targets:
            #print target, target.state
            if target.action_enabled and target.state == state:
                yield target

    def target_servers_by_state(self, state):
        servers = NodeSet()
        for target in self.targets_by_state(state):
            #print ""OK %s"" % target
            servers.add(target.servers[0])
        return servers

    def _distant_action_by_server(self, action_class, servers, **kwargs):

        task = task_self()

        # filter local server
        if self.local_hostname in servers:
            distant_servers = servers.difference(self.local_hostname)
        elif self.local_hostname_short in servers:
            distant_servers = servers.difference(self.local_hostname_short)
        else:
            distant_servers = servers

        # perform action on distant servers
        if len(distant_servers) > 0:
            action = action_class(nodes=distant_servers, fs=self, **kwargs)
            action.launch()
            task.resume()

    def install(self, fs_config_file, nodes=None):
        """"""
        Install FS config files.
        """"""
        servers = NodeSet()

        for target in self.targets:
            # install on failover partners too
            for s in target.servers:
                if not nodes or s in nodes:
                    servers.add(s)

        for client in self.clients:
            # install on failover partners too
            if not nodes or client.server in nodes:
                servers.add(client.server)

        assert len(servers) > 0, ""no servers?""

        try:
            self._distant_action_by_server(Preinstall, servers)
            self._distant_action_by_server(Install, servers, config_file=fs_config_file)
        except ProxyActionError, e:
            # switch to public exception
            raise FSRemoteError(e.nodes, e.rc, e.message)
        
    def remove(self):
        """"""
        Remove FS config files.
        """"""

        result = 0

        servers = NodeSet()

        self.action_refcnt = 0
        self.proxy_errors = []

        # iterate over lustre servers
        for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
            if not e_s_targets:
                continue

            if server.is_local():
                # remove local fs configuration file
                conf_dir_path = Globals().get_conf_dir()
                fs_file = os.path.join(Globals().get_conf_dir(), ""%s.xmf"" % self.fs_name)
                rc = os.unlink(fs_file)
                result = max(result, rc)
            else:
                servers.add(server)

        if len(servers) > 0:
            # Perform the remove operations on all targets for these nodes.
            action = FSProxyAction(self, 'remove', servers, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR
        
        return result

    def _update_structure(self):
        # convenience
        for type, targets, servers in self._iter_targets_servers_by_type():
            if type == 'ost':
                self.ost_count = len(targets)
                self.ost_servers = NodeSet(servers)
            elif type == 'mdt':
                self.mdt_count = len(targets)
                self.mdt_servers = NodeSet(servers)
            elif type == 'mgt':
                self.mgt_count = len(targets)
                self.mgt_servers = NodeSet(servers)

        self.target_count = self.mgt_count + self.mdt_count + self.ost_count
        self.target_servers = self.mgt_servers | self.mdt_servers | self.ost_servers

    def _iter_targets_servers_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns a tuple (list of targets,
        list of servers) per target type.
        """"""
        last_target_type = None
        servers = NodeSet()
        targets = Set()

        #self.targets.sort()

        if reverse:
            self.targets.reverse()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(targets) > 0:
                    yield last_target_type, targets, servers
                    servers.clear()     # ClusterShell 1.1+ needed (sorry)
                    targets.clear()

            if target.action_enabled:
                targets.add(target)
                # select server: change master_server for -F node
                servers.add(target.get_selected_server())
            last_target_type = target.type

        if len(targets) > 0:
            yield last_target_type, targets, servers

    def targets_by_type(self, reverse=False):
        """"""
        Per type of target iterator : returns the following tuple:
        (type, (list of all targets of this type, list of enabled targets))
        per target type.
        """"""
        last_target_type = None
        a_targets = Set()
        e_targets = Set()

        for target in self.targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(a_targets) > 0:
                    yield last_target_type, (a_targets, e_targets)
                    a_targets.clear()
                    e_targets.clear()

            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)
            last_target_type = target.type

        if len(a_targets) > 0:
            yield last_target_type, (a_targets, e_targets)

    def _iter_targets_by_server(self):
        """"""
        Per server of target iterator : returns the following tuple:
        (server, (list of all server targets, list of enabled targets))
        per target server.
        """"""
        servers = {}
        for target in self.targets:
            a_targets, e_targets = servers.setdefault(target.get_selected_server(), (Set(), Set()))
            a_targets.add(target)
            if target.action_enabled:
                e_targets.add(target)

        return servers.iteritems()


    def _iter_type_idx_for_targets(self, targets):
        last_target_type = None

        indexes = RangeSet(autostep=3)

        #self.targets.sort()

        for target in targets:
            if last_target_type and last_target_type != target.type:
                # type of target changed, commit actions
                if len(indexes) > 0:
                    yield last_target_type, indexes
                    indexes.clear()     # CS 1.1+
            indexes.add(int(target.index))
            last_target_type = target.type

        if len(indexes) > 0:
            yield last_target_type, indexes

    def format(self, **kwargs):

        # Remember format launched, so we can check their status once
        # all operations are done.
        format_launched = Set()

        servers_formatall = NodeSet()

        self.proxy_errors = []
        self.action_refcnt = 0

        for server, (a_targets, e_targets) in self._iter_targets_by_server():

            if server.is_local():
                # local server
                for target in e_targets:
                    target.format(**kwargs)
                    self.action_refcnt += 1

                format_launched.update(e_targets)

            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action if ""format all targets on this server""
                    # is detected
                    servers_formatall.add(server)
                else:
                    # otherwise, format per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'format',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

                format_launched.update(e_targets)

        if len(servers_formatall) > 0:
            action = FSProxyAction(self, 'format', servers_formatall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check.
        for target in format_launched:
            if target.state != OFFLINE:
                return target.state

        return OFFLINE

    def status(self, flags=STATUS_ANY):
        """"""
        Get status of filesystem.
        """"""

        status_target_launched = Set()
        status_client_launched = Set()
        servers_statusall = NodeSet()
        self.action_refcnt = 0
        self.proxy_errors = []

        # prepare servers status checks
        if flags & STATUS_SERVERS:
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                if len(e_s_targets) == 0:
                    continue

                if server.is_local():
                    for target in e_s_targets:
                        target.status()
                        self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)
                else:
                    # distant server: check if all server targets have been selected
                    if len(a_s_targets) == len(e_s_targets):
                        # ""status on all targets for this server"" detected
                        servers_statusall.add(server)
                    else:
                        # status per selected targets on this server
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(e_s_targets):
                            action = FSProxyAction(self, 'status',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1
                    status_target_launched.update(e_s_targets)

        # prepare clients status checks
        if flags & STATUS_CLIENTS:
            for client in self.clients:
                if client.action_enabled:
                    server = client.server
                    if server.is_local():
                        client.status()
                        self.action_refcnt += 1
                    elif server not in servers_statusall:
                        servers_statusall.add(server)
                    status_client_launched.add(client)

        # launch distant actions
        if len(servers_statusall) > 0:
            action = FSProxyAction(self, 'status', servers_statusall, self.debug)
            action.launch()
            self.action_refcnt += 1

        # run loop
        task_self().resume()
        
        # return a dict of {state : target list}
        rdict = {}

        # all launched targets+clients
        launched = (status_target_launched | status_client_launched)
        if self.proxy_errors:
            # find targets/clients affected by the runtime error(s)
            for target in launched:
                for nodes, msg in self.proxy_errors:
                    if target.server in nodes:
                        target.state = RUNTIME_ERROR

        for target in launched:
            if target.state == None:
                print target, target.server
            assert target.state != None
            targets = rdict.setdefault(target.state, [])
            targets.append(target)
        return rdict

    def status_target(self, target):
        """"""
        Launch a status request for a specific local or remote target.
        """"""

        # Don't call me if the target itself is not enabled.
        assert target.action_enabled

        server = target.get_selected_server()

        if server.is_local():
            # Target is local
            target.status()
        else:
            action = FSProxyAction(self, 'status', NodeSet(server), self.debug,
                    target.type, RangeSet(str(target.index)))
            action.launch()

        self.action_refcnt = 1
        task_self().resume()

    def start(self, **kwargs):
        """"""
        Start Lustre file system servers.
        """"""
        self.proxy_errors = []

        # What starting order to use?
        for target in self.targets:
            if isinstance(target, MDT) and target.action_enabled:
                # Found enabled MDT: perform writeconf check.
                self.status_target(target)
                if target.has_first_time_flag() or target.has_writeconf_flag():
                    # first_time or writeconf flag found, start MDT before OSTs
                    MDT.target_order = 2 # change MDT class variable order

        self.targets.sort()

        # servers_startall is used for optimization, it contains nodes
        # where we have to perform the start operation on all targets
        # found for this FS. This will limit the number of FSProxyAction
        # to spawn.
        servers_startall = NodeSet()

        # Remember targets launched, so we can check their status once
        # all operations are done (here, status are checked after all
        # targets of the same type have completed the start operation -
        # with possible failure).
        targets_launched = Set()

        # Keep number of actions in order to abort task correctly in
        # action's ev_close.
        self.action_refcnt = 0

        result = 0

        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():
            
            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():

                # To summary, we keep targets that are:
                # 1. enabled
                # 2. of according type
                # 3. on this server
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Start targets if we are on the good server.
                    for target in type_e_targets:
                        # Note that target.start() should never block here:
                        # it will perform necessary non-blocking actions and
                        # (when needed) will start local ClusterShell workers.
                        target.start(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""start all FS targets on this server"" detected
                        servers_startall.add(server)
                    else:
                        # Start per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'start',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched targets of this server for late status check.
                targets_launched.update(type_e_targets)

            if len(servers_startall) > 0:
                # Perform the start operations on all targets for these nodes.
                action = FSProxyAction(self, 'start', servers_startall, self.debug)
                action.launch()
                self.action_refcnt += 1

            # Resume current task, ie. start runloop, process workers events
            # and also act as a target-type barrier.
            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_launched:
                if target.state > result:
                    result = target.state
                    if result > RECOVERING:
                        # Avoid broken cascading starts, so we break now if
                        # a target of the previous type failed to start.
                        return result

            # Some needed cleanup before next target type.
            servers_startall.clear()
            targets_launched.clear()

        return result


    def stop(self, **kwargs):
        """"""
        Stop file system.
        """"""
        rc = MOUNTED

        # Stop: reverse order
        self.targets.sort()
        self.targets.reverse()

        # servers_stopall is used for optimization, see the comment in
        # start() for servers_startall.
        servers_stopall = NodeSet()

        # Remember targets when stop was launched.
        targets_stopping = Set()

        self.action_refcnt = 0
        self.proxy_errors = []

        # We use a similar logic than start(): see start() for comments.
        # iterate over targets by type
        for type, (a_targets, e_targets) in self.targets_by_type():

            if not e_targets:
                # no target of this type is enabled
                continue

            # iterate over lustre servers
            for server, (a_s_targets, e_s_targets) in self._iter_targets_by_server():
                type_e_targets = e_targets.intersection(e_s_targets)
                if len(type_e_targets) == 0:
                    # skip as no target of this type is enabled on this server
                    continue

                if server.is_local():
                    # Stop targets if we are on the good server.
                    for target in type_e_targets:
                        target.stop(**kwargs)
                        self.action_refcnt += 1
                else:
                    assert a_s_targets.issuperset(type_e_targets)
                    assert len(type_e_targets) > 0

                    # Distant server: for code and requests optimizations,
                    # we check when all server targets have been selected.
                    if len(type_e_targets) == len(a_s_targets):
                        # ""stop all FS targets on this server"" detected
                        servers_stopall.add(server)
                    else:
                        # Stop per selected targets on this server.
                        for t_type, t_rangeset in \
                                self._iter_type_idx_for_targets(type_e_targets):
                            action = FSProxyAction(self, 'stop',
                                    NodeSet(server), self.debug, t_type, t_rangeset)
                            action.launch()
                            self.action_refcnt += 1

                # Remember launched stopping targets of this server for late status check.
                targets_stopping.update(type_e_targets)

            if len(servers_stopall) > 0:
                # Perform the stop operations on all targets for these nodes.
                action = FSProxyAction(self, 'stop', servers_stopall, self.debug)
                action.launch()
                self.action_refcnt += 1

            task_self().resume()

            if self.proxy_errors:
                return RUNTIME_ERROR

            # Ok, workers have completed, perform late status check...
            for target in targets_stopping:
                if target.state > rc:
                    rc = target.state

            # Some needed cleanup before next target type.
            servers_stopall.clear()
            targets_stopping.clear()

        return rc

    def mount(self, **kwargs):
        """"""
        Mount FS clients.
        """"""
        servers_mountall = NodeSet()
        clients_mounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.start(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_mountall.add(client.server)

            clients_mounting.add(client)

        if len(servers_mountall) > 0:
            action = FSClientProxyAction(self, 'mount', servers_mountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_mounting:
            if client.state != MOUNTED:
                return client.state

        return MOUNTED

    def umount(self, **kwargs):
        """"""
        Unmount FS clients.
        """"""
        servers_umountall = NodeSet()
        clients_umounting = Set()
        self.action_refcnt = 0
        self.proxy_errors = []

        for client in self.clients:

            if not client.action_enabled:
                continue

            if client.server.is_local():
                # local client
                client.stop(**kwargs)
                self.action_refcnt += 1
            else:
                # distant client
                servers_umountall.add(client.server)

            clients_umounting.add(client)

        if len(servers_umountall) > 0:
            action = FSClientProxyAction(self, 'umount', servers_umountall, self.debug)
            action.launch()
            self.action_refcnt += 1

        task_self().resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        # Ok, workers have completed, perform late status check...
        for client in clients_umounting:
            if client.state != OFFLINE:
                return client.state

        return OFFLINE

    def info(self):
        pass

    def tune(self, tuning_model):
        """"""
        Tune server.
        """"""
        task = task_self()
        tune_all = NodeSet()
        type_map = { 'mgt': 'mgs', 'mdt': 'mds', 'ost' : 'oss' }
        self.action_refcnt = 0
        self.proxy_errors = []
        result = 0

        # Install tuning.conf on enabled distant servers
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if e_targets and not server.is_local():
                tune_all.add(server)
        if len(tune_all) > 0:
            self._distant_action_by_server(Install, tune_all, config_file=Globals().get_tuning_file())
            self.action_refcnt += 1
            task.resume()
            tune_all.clear()

        # Apply tunings
        self.action_refcnt = 0
        for server, (a_targets, e_targets) in self._iter_targets_by_server():
            if not e_targets:
                continue
            if server.is_local():
                types = Set()
                for t in e_targets:
                    types.add(type_map[t.type])

                rc = server.tune(tuning_model, types, self.fs_name)
                result = max(result, rc)
            else:
                # distant server
                if len(a_targets) == len(e_targets):
                    # group in one action
                    tune_all.add(server)
                else:
                    # otherwise, tune per selected targets on this server
                    for t_type, t_rangeset in \
                            self._iter_type_idx_for_targets(e_targets):
                        action = FSProxyAction(self, 'tune',
                                NodeSet(server), self.debug, t_type, t_rangeset)
                        action.launch()
                        self.action_refcnt += 1

        if len(tune_all) > 0:
            action = FSProxyAction(self, 'tune', tune_all, self.debug)
            action.launch()
            self.action_refcnt += 1

        task.resume()

        if self.proxy_errors:
            return RUNTIME_ERROR

        return result

/n/n/n",0
75,75,7ff203be36e439b535894764c37a8446351627ec,"/lib/Shine/Commands/CommandRegistry.py/n/n# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """"""Container object to deal with commands.""""""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        ""Return the number of commands.""
        return len(self.cmd_list)

    def __iter__(self):
        ""Iterate over available commands.""
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        ""Register a new command.""
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, ""Incoherency in option arguments""
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """"""
        Execute a shine script command.
        """"""
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException(""Syntax error."", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()

/n/n/n/lib/Shine/Commands/Install.py/n/n# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """"""
    shine install -f /path/to/model.lmf
    """"""
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return ""install""

    def get_desc(self):
        return ""Install a new file system.""

    def execute(self):
        if not self.opt_m:
            print ""Bad argument""
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = "" on %s"" %  install_nodes
            else:
                nodestr = """"

            print ""Configuration files for file system %s have been installed "" \
                    ""successfully%s."" % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print ""Lustre targets summary:""
                print ""\t%d MGT on %s"" % (fs.mgt_count, fs.mgt_servers)
                print ""\t%d MDT on %s"" % (fs.mdt_count, fs.mdt_servers)
                print ""\t%d OST on %s"" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print ""Use `shine format -f %s' to initialize the file system."" % \
                        fs_conf.get_fs_name()

            return 0

/n/n/n/lib/Shine/Commands/Mount.py/n/n# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Mounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Mount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully mounted on %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to mount FS %s on %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """"""
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""mount""

    def get_desc(self):
        return ""Mount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Mount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Commands/Preinstall.py/n/n# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """"""
    shine preinstall -f <filesystem name> -R
    """"""
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return ""preinstall""

    def get_desc(self):
        return ""Preinstall a new file system.""

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print ""OSError""
            raise

/n/n/n/lib/Shine/Commands/Start.py/n/n# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print ""Starting %d targets on %s"" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print ""%s: Starting %s %s (%s)..."" % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print ""%s: Start of %s %s (%s): %s"" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print ""%s: Start of %s %s (%s) succeeded"" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to start %s %s (%s): %s"" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print ""Starting %s %s (%s)..."" % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print ""Start of %s %s (%s): %s"" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print ""Start of %s %s (%s) succeeded"" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""Failed to start %s %s (%s): %s"" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """"""
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return ""start""

    def get_desc(self):
        return ""Start file system servers.""

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Start successful.""
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc
/n/n/n/lib/Shine/Commands/Status.py/n/n# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested ""view"". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
""""""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print ""%s: Failed to status %s %s (%s)"" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print "">> %s"" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print ""%s: Failed to status of FS %s"" % (node, client.fs.fs_name)
        print "">> %s"" % message


class Status(FSLiveCommand):
    """"""
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """"""

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return ""status""

    def get_desc(self):
        return ""Check for file system target status.""


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = ""fs""
            else:
                view = view.lower()

            # disable client checks when not requested
            if view.startswith(""disk"") or view.startswith(""target""):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith(""client""):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == ""fs"":
                self.status_view_fs(fs)
            elif view.startswith(""target""):
                self.status_view_targets(fs)
            elif view.startswith(""disk""):
                self.status_view_disks(fs)
            else:
                raise CommandBadParameterError(self.view_support.get_view(),
                        ""fs, targets, disks"")
        return result

    def status_view_targets(self, fs):
        """"""
        View: lustre targets
        """"""
        print ""FILESYSTEM TARGETS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                else:
                    status = ""UNKNOWN""

                ldic.append(target_dict([[""target"", target.get_id()],
                    [""type"", target.type.upper()],
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""device"", target.dev],
                    [""index"", target.index],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""target"", 0, AsciiTableLayout.LEFT, ""target id"",
                AsciiTableLayout.CENTER)
        layout.set_column(""type"", 1, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        layout.set_column(""index"", 2, AsciiTableLayout.RIGHT, ""idx"",
                AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 3, AsciiTableLayout.LEFT, ""nodes"",
                AsciiTableLayout.CENTER)
        layout.set_column(""device"", 4, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        layout.set_column(""status"", 5, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """"""
        View: lustre FS summary
        """"""
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append(""offline (%d)"" % len(t_offline))
            if len(t_error) > 0:
                status.append(""ERROR (%d)"" % len(t_error))
            if len(t_recovering) > 0:
                status.append(""recovering (%d) for %s"" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append(""online (%d)"" % len(t_online))
            if len(t_runtime) > 0:
                status.append(""CHECK FAILURE (%d)"" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append(""not checked (%d)"" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([[""type"", ""%s"" % type.upper()],
                    [""count"", len(a_targets)], [""nodes"", nodes],
                    [""status"", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append(""not checked (%d)"" % c_ign)
            if c_offline > 0:
                status.append(""offline (%d)"" % c_offline)
            if c_error > 0:
                status.append(""ERROR (%d)"" % c_error)
            if c_runtime > 0:
                status.append(""CHECK FAILURE (%d)"" % c_runtime)
            if c_mounted > 0:
                status.append(""mounted (%d)"" % c_mounted)

            ldic.append(dict([[""type"", ""CLI""], [""count"", len(fs.clients)],
                [""nodes"", ""%s"" % fs.get_client_servers()], [""status"", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column(""type"", 0, AsciiTableLayout.CENTER, ""type"", AsciiTableLayout.CENTER)
        layout.set_column(""count"", 1, AsciiTableLayout.RIGHT, ""#"", AsciiTableLayout.CENTER)
        layout.set_column(""nodes"", 2, AsciiTableLayout.LEFT, ""nodes"", AsciiTableLayout.CENTER)
        layout.set_column(""status"", 3, AsciiTableLayout.LEFT, ""status"", AsciiTableLayout.CENTER)

        print ""FILESYSTEM COMPONENTS STATUS (%s)"" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """"""
        View: lustre disks
        """"""

        print ""FILESYSTEM DISKS (%s)"" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self[""index""] < other[""index""] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = ""offline""
                elif target.state == RECOVERING:
                    status = ""recovering %s"" % target.status_info
                elif target.state == MOUNTED:
                    status = ""online""
                elif target.state == TARGET_ERROR:
                    status = ""ERROR""
                elif target.state == RUNTIME_ERROR:
                    status = ""CHECK FAILURE""
                else:
                    status = ""UNKNOWN""

                if target.dev_size >= TERA:
                    dev_size = ""%.1fT"" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = ""%.1fG"" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = ""%.1fM"" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = ""%.1fK"" % (target.dev_size/KILO)
                else:
                    dev_size = ""%d"" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:
                    jdev = """"

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:
                    tag = """"

                flags = []
                if target.has_need_index_flag():
                    flags.append(""need_index"")
                if target.has_first_time_flag():
                    flags.append(""first_time"")
                if target.has_update_flag():
                    flags.append(""update"")
                if target.has_rewrite_ldd_flag():
                    flags.append(""rewrite_ldd"")
                if target.has_writeconf_flag():
                    flags.append(""writeconf"")
                if target.has_upgrade14_flag():
                    flags.append(""upgrade14"")
                if target.has_param_flag():
                    flags.append(""conf_param"")

                ldic.append(target_dict([\
                    [""nodes"", NodeSet.fromlist(target.servers)],
                    [""dev"", target.dev],
                    [""size"", dev_size],
                    [""jdev"", jdev],
                    [""type"", target.type.upper()],
                    [""index"", target.index],
                    [""tag"", tag],
                    [""label"", target.label],
                    [""flags"", ' '.join(flags)],
                    [""fsname"", target.fs.fs_name],
                    [""status"", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column(""dev"", i, AsciiTableLayout.LEFT, ""device"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""nodes"", i, AsciiTableLayout.LEFT, ""node(s)"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""size"", i, AsciiTableLayout.RIGHT, ""dev size"",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column(""jdev"", i, AsciiTableLayout.RIGHT, ""journal device"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""type"", i, AsciiTableLayout.LEFT, ""type"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""index"", i, AsciiTableLayout.RIGHT, ""index"",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column(""tag"", i, AsciiTableLayout.LEFT, ""tag"",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""label"", i, AsciiTableLayout.LEFT, ""label"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""flags"", i, AsciiTableLayout.LEFT, ""ldd flags"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""fsname"", i, AsciiTableLayout.LEFT, ""fsname"",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column(""status"", i, AsciiTableLayout.LEFT, ""status"",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)

/n/n/n/lib/Shine/Commands/Umount.py/n/n# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

""""""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
""""""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print ""%s: Unmounting %s on %s ..."" % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print ""%s: Umount: %s"" % (node, client.status_info)
            else:
                print ""%s: FS %s succesfully unmounted from %s"" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print ""%s: Failed to unmount FS %s from %s: %s"" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """"""
    shine umount
    """"""

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return ""umount""

    def get_desc(self):
        return ""Unmount file system clients.""

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException(""%s are not client nodes of filesystem '%s'"" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print ""Unmount successful.""
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print ""%s: %s"" % (nodes, msg)

        return result

/n/n/n/lib/Shine/Controller.py/n/n# Controller.py -- Controller class
# Copyright (C) 2007 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Configuration.Globals import Globals
from Commands.CommandRegistry import CommandRegistry

from Configuration.ModelFile import ModelFileException
from Configuration.ModelFile import ModelFileIOError

from Configuration.Exceptions import ConfigException
from Commands.Exceptions import *
from Commands.Base.CommandRCDefs import *

from Lustre.FileSystem import FSRemoteError

from ClusterShell.Task import *
from ClusterShell.NodeSet import *

import getopt
import logging
import re
import sys


def print_csdebug(task, s):
    m = re.search(""(\w+): SHINE:\d:(\w+):"", s)
    if m:
        print ""%s<pickle>"" % m.group(0)
    else:
        print s


class Controller:

    def __init__(self):
        self.logger = logging.getLogger(""shine"")
        #handler = logging.FileHandler(Globals().get_log_file())
        #formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s : %(message)s')
        #handler.setFormatter(formatter)
        #self.logger.addHandler(handler)
        #self.logger.setLevel(Globals().get_log_level())
        self.cmds = CommandRegistry()

        #task_self().set_info(""debug"", True)

        task_self().set_info(""print_debug"", print_csdebug)

    def usage(self):
        cmd_maxlen = 0

        for cmd in self.cmds:
            if not cmd.is_hidden():
                if len(cmd.get_name()) > cmd_maxlen:
                    cmd_maxlen = len(cmd.get_name())
        for cmd in self.cmds:
            if not cmd.is_hidden():
                print ""  %-*s %s"" % (cmd_maxlen, cmd.get_name(),
                    cmd.get_params_desc())

    def print_error(self, errmsg):
        print >>sys.stderr, ""Error:"", errmsg

    def print_help(self, msg, cmd):
        if msg:
            print msg
            print
        print ""Usage: %s %s"" % (cmd.get_name(), cmd.get_params_desc())
        print
        print cmd.get_desc()

    def run_command(self, cmd_args):

        #self.logger.info(""running %s"" % cmd_name)

        try:
            return self.cmds.execute(cmd_args)
        except getopt.GetoptError, e:
            print ""Syntax error: %s"" % e
        except CommandHelpException, e:
            self.print_help(e.message, e.cmd)
        except CommandException, e:
            self.print_error(e.message)
            return RC_USER_ERROR
        except ModelFileIOError, e:
            print ""Error - %s"" % e.message
        except ModelFileException, e:
            print ""ModelFile: %s"" % e
        except ConfigException, e:
            print ""Configuration: %s"" % e
            return RC_RUNTIME_ERROR
        # file system
        except FSRemoteError, e:
            self.print_error(e)
            return e.rc
        except NodeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except RangeSetParseError, e:
            self.print_error(""%s"" % e)
            return RC_USER_ERROR
        except KeyError:
            print ""Error - Unrecognized action""
            print
            raise
        
        return 1


/n/n/n/lib/Shine/Lustre/Actions/Proxies/FSProxyAction.py/n/n# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """"""
    Generic file system command proxy action class.
    """"""

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print ""FSProxyAction %s on %s"" % (action, nodes)

    def launch(self):
        """"""
        Launch FS proxy command.
        """"""
        command = [""%s"" % self.progpath]
        command.append(self.action)
        command.append(""-f %s"" % self.fs.fs_name)
        command.append(""-R"")

        if self.debug:
            command.append(""-d"")

        if self.targets_type:
            command.append(""-t %s"" % self.targets_type)
            if self.targets_indexes:
                command.append(""-i %s"" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """"""
        End of proxy command.
        """"""
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, ""Remote action %s failed: %s"" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()

/n/n/n",1
32,32,f453ed1c417993eab4fc7b3c5288208d97270d13,"ptvsd/__main__.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import argparse
import os.path
import sys

from ptvsd._local import debug_main, run_main
from ptvsd.socket import Address
from ptvsd.version import __version__, __author__  # noqa


##################################
# the script

""""""
For the PyDevd CLI handling see:

  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py
  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)
""""""  # noqa

PYDEVD_OPTS = {
    '--file',
    '--client',
    #'--port',
    '--vm_type',
}

PYDEVD_FLAGS = {
    '--DEBUG',
    '--DEBUG_RECORD_SOCKET_READS',
    '--cmd-line',
    '--module',
    '--multiproc',
    '--multiprocess',
    '--print-in-debugger-startup',
    '--save-signatures',
    '--save-threading',
    '--save-asyncio',
    '--server',
    '--qt-support=auto',
}

USAGE = """"""
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]
""""""  # noqa


def parse_args(argv=None):
    """"""Return the parsed args to use in main().""""""
    if argv is None:
        argv = sys.argv
        prog = argv[0]
        if prog == __file__:
            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))
    else:
        prog = argv[0]
    argv = argv[1:]

    supported, pydevd, script = _group_args(argv)
    args = _parse_args(prog, supported)
    # '--' is used in _run_args to extract pydevd specific args
    extra = pydevd + ['--']
    if script:
        extra += script
    return args, extra


def _group_args(argv):
    supported = []
    pydevd = []
    script = []

    try:
        pos = argv.index('--')
    except ValueError:
        script = []
    else:
        script = argv[pos + 1:]
        argv = argv[:pos]

    for arg in argv:
        if arg == '-h' or arg == '--help':
            return argv, [], script

    gottarget = False
    skip = 0
    for i in range(len(argv)):
        if skip:
            skip -= 1
            continue

        arg = argv[i]
        try:
            nextarg = argv[i + 1]
        except IndexError:
            nextarg = None

        # TODO: Deprecate the PyDevd arg support.
        # PyDevd support
        if gottarget:
            script = argv[i:] + script
            break
        if arg == '--client':
            arg = '--host'
        elif arg == '--file':
            if nextarg is None:  # The filename is missing...
                pydevd.append(arg)
                continue  # This will get handled later.
            if nextarg.endswith(':') and '--module' in pydevd:
                pydevd.remove('--module')
                arg = '-m'
                argv[i + 1] = nextarg = nextarg[:-1]
            else:
                arg = nextarg
                skip += 1

        if arg in PYDEVD_OPTS:
            pydevd.append(arg)
            if nextarg is not None:
                pydevd.append(nextarg)
            skip += 1
        elif arg in PYDEVD_FLAGS:
            pydevd.append(arg)
        elif arg == '--nodebug':
            supported.append(arg)

        # ptvsd support
        elif arg in ('--host', '--server-host', '--port', '-m'):
            if arg == '-m':
                gottarget = True
            supported.append(arg)
            if nextarg is not None:
                supported.append(nextarg)
            skip += 1
        elif arg in ('--single-session', '--wait'):
            supported.append(arg)
        elif not arg.startswith('-'):
            supported.append(arg)
            gottarget = True

        # unsupported arg
        else:
            supported.append(arg)
            break

    return supported, pydevd, script


def _parse_args(prog, argv):
    parser = argparse.ArgumentParser(
        prog=prog,
        usage=USAGE.format(prog),
    )
    parser.add_argument('--nodebug', action='store_true')
    host = parser.add_mutually_exclusive_group()
    host.add_argument('--host')
    host.add_argument('--server-host')
    parser.add_argument('--port', type=int, required=True)

    target = parser.add_mutually_exclusive_group(required=True)
    target.add_argument('-m', dest='module')
    target.add_argument('filename', nargs='?')

    parser.add_argument('--single-session', action='store_true')
    parser.add_argument('--wait', action='store_true')

    parser.add_argument('-V', '--version', action='version')
    parser.version = __version__

    args = parser.parse_args(argv)
    ns = vars(args)

    serverhost = ns.pop('server_host', None)
    clienthost = ns.pop('host', None)
    if serverhost:
        args.address = Address.as_server(serverhost, ns.pop('port'))
    elif not clienthost:
        if args.nodebug:
            args.address = Address.as_client(clienthost, ns.pop('port'))
        else:
            args.address = Address.as_server(clienthost, ns.pop('port'))
    else:
        args.address = Address.as_client(clienthost, ns.pop('port'))

    module = ns.pop('module')
    filename = ns.pop('filename')
    if module is None:
        args.name = filename
        args.kind = 'script'
    else:
        args.name = module
        args.kind = 'module'
    #if argv[-1] != args.name or (module and argv[-1] != '-m'):
    #    parser.error('script/module must be last arg')

    return args


def main(addr, name, kind, extra=(), nodebug=False, **kwargs):
    if nodebug:
        run_main(addr, name, kind, *extra, **kwargs)
    else:
        debug_main(addr, name, kind, *extra, **kwargs)


if __name__ == '__main__':
    args, extra = parse_args()
    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,
         singlesession=args.single_session, wait=args.wait)
/n/n/nptvsd/_local.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import sys
import time

import pydevd
from _pydevd_bundle.pydevd_comm import get_global_debugger

from ptvsd.pydevd_hooks import install
from ptvsd.runner import run as no_debug_runner
from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread


PYDEVD_DEFAULTS = {
    '--qt-support=auto',
}


def _set_pydevd_defaults(pydevd_args):
    args_to_append = []
    for arg in PYDEVD_DEFAULTS:
        if arg not in pydevd_args:
            args_to_append.append(arg)
    return pydevd_args + args_to_append


########################
# high-level functions

def debug_main(address, name, kind, *extra, **kwargs):
    if not kwargs.pop('wait', False) and address.isserver:
        def unblock_debugger():
            debugger = get_global_debugger()
            while debugger is None:
                time.sleep(0.1)
                debugger = get_global_debugger()
            debugger.ready_to_run = True
        new_hidden_thread('ptvsd.unblock_debugger', unblock_debugger).start()
    if kind == 'module':
        run_module(address, name, *extra, **kwargs)
    else:
        run_file(address, name, *extra, **kwargs)


def run_main(address, name, kind, *extra, **kwargs):
    addr = Address.from_raw(address)
    sys.argv[:] = _run_main_argv(name, extra)
    runner = kwargs.pop('_runner', no_debug_runner)
    runner(addr, name, kind == 'module', *extra, **kwargs)


########################
# low-level functions

def run_module(address, modname, *extra, **kwargs):
    """"""Run pydevd for the given module.""""""
    addr = Address.from_raw(address)
    if not addr.isserver:
        kwargs['singlesession'] = True
    run = kwargs.pop('_run', _run)
    prog = kwargs.pop('_prog', sys.argv[0])
    filename = modname + ':'
    argv = _run_argv(addr, filename, extra, _prog=prog)
    argv.insert(argv.index('--file'), '--module')
    run(argv, addr, **kwargs)


def run_file(address, filename, *extra, **kwargs):
    """"""Run pydevd for the given Python file.""""""
    addr = Address.from_raw(address)
    if not addr.isserver:
        kwargs['singlesession'] = True
    run = kwargs.pop('_run', _run)
    prog = kwargs.pop('_prog', sys.argv[0])
    argv = _run_argv(addr, filename, extra, _prog=prog)
    run(argv, addr, **kwargs)


def _run_argv(address, filename, extra, _prog=sys.argv[0]):
    """"""Convert the given values to an argv that pydevd.main() supports.""""""
    if '--' in extra:
        pydevd = list(extra[:extra.index('--')])
        extra = list(extra[len(pydevd) + 1:])
    else:
        pydevd = []
        extra = list(extra)

    pydevd = _set_pydevd_defaults(pydevd)
    host, port = address
    argv = [
        _prog,
        '--port', str(port),
    ]
    if not address.isserver:
        argv.extend([
            '--client', host or 'localhost',
        ])
    return argv + pydevd + [
        '--file', filename,
    ] + extra


def _run_main_argv(filename, extra):
    if '--' in extra:
        pydevd = list(extra[:extra.index('--')])
        extra = list(extra[len(pydevd) + 1:])
    else:
        extra = list(extra)
    return [filename] + extra


def _run(argv, addr, _pydevd=pydevd, _install=install, **kwargs):
    """"""Start pydevd with the given commandline args.""""""
    #print(' '.join(argv))

    # Pydevd assumes that the ""__main__"" module is the ""pydevd"" module
    # and does some tricky stuff under that assumption.  For example,
    # when the debugger starts up it calls save_main_module()
    # (in pydevd_bundle/pydevd_utils.py).  That function explicitly sets
    # sys.modules[""pydevd""] to sys.modules[""__main__""] and then sets
    # the __main__ module to a new one.  This makes some sense since
    # it gives the debugged script a fresh __main__ module.
    #
    # This complicates things for us since we are running a different
    # file (i.e. this one) as the __main__ module.  Consequently,
    # sys.modules[""pydevd""] gets set to ptvsd/__main__.py.  Subsequent
    # imports of the ""pydevd"" module then return the wrong module.  We
    # work around this by avoiding lazy imports of the ""pydevd"" module.
    # We also replace the __main__ module with the ""pydevd"" module here.
    if sys.modules['__main__'].__file__ != _pydevd.__file__:
        sys.modules['__main___orig'] = sys.modules['__main__']
        sys.modules['__main__'] = _pydevd

    daemon = _install(_pydevd, addr, **kwargs)
    sys.argv[:] = argv
    try:
        _pydevd.main()
    except SystemExit as ex:
        daemon.exitcode = int(ex.code)
        raise
/n/n/nptvsd/_remote.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import pydevd
import time

from _pydevd_bundle.pydevd_comm import get_global_debugger

from ptvsd._util import new_hidden_thread
from ptvsd.pydevd_hooks import install
from ptvsd.daemon import session_not_bound, DaemonClosedError


def _pydevd_settrace(redirect_output=None, _pydevd=pydevd, **kwargs):
    if redirect_output is not None:
        kwargs.setdefault('stdoutToServer', redirect_output)
        kwargs.setdefault('stderrToServer', redirect_output)
    # pydevd.settrace() only enables debugging of the current
    # thread and all future threads.  PyDevd is not enabled for
    # existing threads (other than the current one).  Consequently,
    # pydevd.settrace() must be called ASAP in the current thread.
    # See issue #509.
    #
    # This is tricky, however, because settrace() will block until
    # it receives a CMD_RUN message.  You can't just call it in a
    # thread to avoid blocking; doing so would prevent the current
    # thread from being debugged.
    _pydevd.settrace(**kwargs)


# TODO: Split up enable_attach() to align with module organization.
# This should including making better use of Daemon (e,g, the
# start_server() method).
# Then move at least some parts to the appropriate modules.  This module
# is focused on running the debugger.

global_next_session = None


def enable_attach(address, redirect_output=True,
                  _pydevd=pydevd, _install=install,
                  on_attach=lambda: None, **kwargs):
    host, port = address

    def wait_for_connection(daemon, host, port, next_session=None):
        debugger = get_global_debugger()
        while debugger is None:
            time.sleep(0.1)
            debugger = get_global_debugger()

        debugger.ready_to_run = True

        while True:
            session_not_bound.wait()
            try:
                global_next_session()
                on_attach()
            except DaemonClosedError:
                return

    def start_daemon():
        daemon._sock = daemon._start()
        _, next_session = daemon.start_server(addr=(host, port))
        global global_next_session
        global_next_session = next_session
        return daemon._sock

    daemon = _install(_pydevd,
                      address,
                      start_server=None,
                      start_client=(lambda daemon, h, port: start_daemon()),
                      singlesession=False,
                      **kwargs)

    connection_thread = new_hidden_thread('ptvsd.listen_for_connection',
                                          wait_for_connection,
                                          args=(daemon, host, port))
    connection_thread.start()

    _pydevd.settrace(host=host,
                     stdoutToServer=redirect_output,
                     stderrToServer=redirect_output,
                     port=port,
                     suspend=False)
/n/n/nptvsd/_util.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

from __future__ import print_function

import contextlib
import os
import threading
import time
import sys


DEBUG = False
if os.environ.get('PTVSD_DEBUG', ''):
    DEBUG = True


def debug(*msg, **kwargs):
    if not DEBUG:
        return
    tb = kwargs.pop('tb', False)
    assert not kwargs
    if tb:
        import traceback
        traceback.print_exc()
    print(*msg, file=sys.stderr)
    sys.stderr.flush()


@contextlib.contextmanager
def ignore_errors(log=None):
    """"""A context manager that masks any raised exceptions.""""""
    try:
        yield
    except Exception as exc:
        if log is not None:
            log('ignoring error', exc)


def call_all(callables, *args, **kwargs):
    """"""Return the result of calling every given object.""""""
    results = []
    for call in callables:
        try:
            call(*args, **kwargs)
        except Exception as exc:
            results.append((call, exc))
        else:
            results.append((call, None))
    return results


########################
# pydevd stuff

from _pydevd_bundle import pydevd_comm  # noqa


def log_pydevd_msg(cmdid, seq, args, inbound,
                   log=debug, prefix=None, verbose=False):
    """"""Log a representation of the given pydevd msg.""""""
    if log is None or (log is debug and not DEBUG):
        return
    if not verbose and cmdid == pydevd_comm.CMD_WRITE_TO_CONSOLE:
        return

    if prefix is None:
        prefix = '-> ' if inbound else '<- '
    try:
        cmdname = pydevd_comm.ID_TO_MEANING[str(cmdid)]
    except KeyError:
        for cmdname, value in vars(pydevd_comm).items():
            if cmdid == value:
                break
        else:
            cmdname = '???'
    cmd = '{} ({})'.format(cmdid, cmdname)
    args = args.replace('\n', '\\n')
    msg = '{}{:28} [{:>10}]: |{}|'.format(prefix, cmd, seq, args)
    log(msg)


########################
# threading stuff

try:
    ThreadError = threading.ThreadError
except AttributeError:
    ThreadError = RuntimeError


try:
    base = __builtins__.TimeoutError
except AttributeError:
    base = OSError
class TimeoutError(base):  # noqa
    """"""Timeout expired.""""""
    timeout = None
    reason = None

    @classmethod
    def from_timeout(cls, timeout, reason=None):
        """"""Return a TimeoutError with the given timeout.""""""
        msg = 'timed out (after {} seconds)'.format(timeout)
        if reason is not None:
            msg += ' ' + reason
        self = cls(msg)
        self.timeout = timeout
        self.reason = reason
        return self
del base  # noqa


def wait(check, timeout=None, reason=None):
    """"""Wait for the given func to return True.

    If a timeout is given and reached then raise TimeoutError.
    """"""
    if timeout is None or timeout <= 0:
        while not check():
            time.sleep(0.01)
    else:
        if not _wait(check, timeout):
            raise TimeoutError.from_timeout(timeout, reason)


def is_locked(lock):
    """"""Return True if the lock is locked.""""""
    if lock is None:
        return False
    if not lock.acquire(False):
        return True
    lock_release(lock)
    return False


def lock_release(lock):
    """"""Ensure that the lock is released.""""""
    if lock is None:
        return
    try:
        lock.release()
    except ThreadError:  # already unlocked
        pass


def lock_wait(lock, timeout=None, reason='waiting for lock'):
    """"""Wait until the lock is not locked.""""""
    if not _lock_acquire(lock, timeout):
        raise TimeoutError.from_timeout(timeout, reason)
    lock_release(lock)


if sys.version_info >= (3,):
    def _lock_acquire(lock, timeout):
        if timeout is None:
            timeout = -1
        return lock.acquire(timeout=timeout)
else:
    def _lock_acquire(lock, timeout):
        if timeout is None or timeout <= 0:
            return lock.acquire()

        def check():
            return lock.acquire(False)
        return _wait(check, timeout)


def _wait(check, timeout):
    if check():
        return True
    for _ in range(int(timeout * 100)):
        time.sleep(0.01)
        if check():
            return True
    else:
        return False


def new_hidden_thread(name, target, prefix='ptvsd.', daemon=True, **kwargs):
    """"""Return a thread that will be ignored by pydevd.""""""
    if prefix is not None and not name.startswith(prefix):
        name = prefix + name
    t = threading.Thread(
        name=name,
        target=target,
        **kwargs
    )
    t.pydev_do_not_trace = True
    if daemon:
        t.is_pydev_daemon_thread = True
        t.daemon = True
    return t


########################
# closing stuff

class ClosedError(RuntimeError):
    """"""Indicates that the object is closed.""""""


def close_all(closeables):
    """"""Return the result of closing every given object.""""""
    results = []
    for obj in closeables:
        try:
            obj.close()
        except Exception as exc:
            results.append((obj, exc))
        else:
            results.append((obj, None))
    return results


class Closeable(object):
    """"""A base class for types that may be closed.""""""

    NAME = None
    FAIL_ON_ALREADY_CLOSED = True

    def __init__(self):
        super(Closeable, self).__init__()
        self._closed = False
        self._closedlock = threading.Lock()
        self._handlers = []

    def __del__(self):
        try:
            self.close()
        except ClosedError:
            pass

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

    @property
    def closed(self):
        return self._closed

    def add_resource_to_close(self, resource, before=False):
        """"""Add a resource to be closed when closing.""""""
        close = resource.close
        if before:
            def handle_closing(before):
                if not before:
                    return
                close()
        else:
            def handle_closing(before):
                if before:
                    return
                close()
        self.add_close_handler(handle_closing)

    def add_close_handler(self, handle_closing, nodupe=True):
        """"""Add a func to be called when closing.

        The func takes one arg: True if it was called before the main
        close func and False if after.
        """"""
        with self._closedlock:
            if self._closed:
                if self.FAIL_ON_ALREADY_CLOSED:
                    raise ClosedError('already closed')
                return
            if nodupe and handle_closing in self._handlers:
                raise ValueError('close func already added')

            self._handlers.append(handle_closing)

    def check_closed(self):
        """"""Raise ClosedError if closed.""""""
        if self._closed:
            if self.NAME:
                raise ClosedError('{} closed'.format(self.NAME))
            else:
                raise ClosedError('closed')

    @contextlib.contextmanager
    def while_not_closed(self):
        """"""A context manager under which the object will not be closed.""""""
        with self._closedlock:
            self.check_closed()
            yield

    def close(self):
        """"""Release any owned resources and clean up.""""""
        with self._closedlock:
            if self._closed:
                if self.FAIL_ON_ALREADY_CLOSED:
                    raise ClosedError('already closed')
                return
            self._closed = True
            handlers = list(self._handlers)

        results = call_all(handlers, True)
        self._log_results(results)
        self._close()
        results = call_all(handlers, False)
        self._log_results(results)

    # implemented by subclasses

    def _close(self):
        pass

    # internal methods

    def _log_results(self, results, log=None):
        if log is None:
            return
        for obj, exc in results:
            if exc is None:
                continue
            log('failed to close {!r} ({!r})'.format(obj, exc))


########################
# running stuff

class NotRunningError(RuntimeError):
    """"""Something isn't currently running.""""""


class AlreadyStartedError(RuntimeError):
    """"""Something was already started.""""""


class AlreadyRunningError(AlreadyStartedError):
    """"""Something is already running.""""""


class Startable(object):
    """"""A base class for types that may be started.""""""

    RESTARTABLE = False
    FAIL_ON_ALREADY_STOPPED = True

    def __init__(self):
        super(Startable, self).__init__()
        self._is_running = None
        self._startlock = threading.Lock()
        self._numstarts = 0

    def is_running(self, checkclosed=True):
        """"""Return True if currently running.""""""
        if checkclosed and hasattr(self, 'check_closed'):
            self.check_closed()
        is_running = self._is_running
        if is_running is None:
            return False
        return is_running()

    def start(self, *args, **kwargs):
        """"""Begin internal execution.""""""
        with self._startlock:
            if self.is_running():
                raise AlreadyRunningError()
            if not self.RESTARTABLE and self._numstarts > 0:
                raise AlreadyStartedError()

            self._is_running = self._start(*args, **kwargs)
            self._numstarts += 1

    def stop(self, *args, **kwargs):
        """"""Stop execution and wait until done.""""""
        with self._startlock:
            # TODO: Call self.check_closed() here?
            if not self.is_running(checkclosed=False):
                if not self.FAIL_ON_ALREADY_STOPPED:
                    return
                raise NotRunningError()
            self._is_running = None

        self._stop(*args, **kwargs)

    # implemented by subclasses

    def _start(self, *args, **kwargs):
        """"""Return an ""is_running()"" func after starting.""""""
        raise NotImplementedError

    def _stop(self):
        raise NotImplementedError


def is_py34():
    return sys.version_info >= (3, 4,) and sys.version_info < (3, 5,)


def get_line_for_traceback(file_path, line_no):
    try:
        with open(file_path, 'r') as f:
            return f.readlines()[line_no - 1]
    except Exception:
        return None


_enable_debug_break = False


def _allow_debug_break(enabled=True):
    """"""Enable breaking into debugger feature.
    """"""
    global _enable_debug_break
    _enable_debug_break = enabled


def _is_debug_break_allowed():
    return _enable_debug_break
/n/n/nptvsd/daemon.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import contextlib
import sys
import threading

from ptvsd import wrapper
from ptvsd.socket import (
    close_socket, create_server, create_client, connect, Address)
from .exit_handlers import (
    ExitHandlers, UnsupportedSignalError,
    kill_current_proc)
from .session import PyDevdDebugSession
from ._util import (
    ClosedError, NotRunningError, ignore_errors, debug, lock_wait)


session_not_bound = threading.Event()
session_not_bound.set()


def _wait_for_user():
    if sys.__stdout__ is not None:
        try:
            import msvcrt
        except ImportError:
            sys.__stdout__.write('Press Enter to continue . . . ')
            sys.__stdout__.flush()
            sys.__stdin__.read(1)
        else:
            sys.__stdout__.write('Press any key to continue . . . ')
            sys.__stdout__.flush()
            msvcrt.getch()


class DaemonError(RuntimeError):
    """"""Indicates that a Daemon had a problem.""""""
    MSG = 'error'

    def __init__(self, msg=None):
        if msg is None:
            msg = self.MSG
        super(DaemonError, self).__init__(msg)


class DaemonClosedError(DaemonError):
    """"""Indicates that a Daemon was unexpectedly closed.""""""
    MSG = 'closed'


class DaemonStoppedError(DaemonError):
    """"""Indicates that a Daemon was unexpectedly stopped.""""""
    MSG = 'stopped'


# TODO: Inherit from Closeable.
# TODO: Inherit from Startable?

class DaemonBase(object):
    """"""The base class for DAP daemons.""""""

    SESSION = None

    exitcode = None

    def __init__(self, wait_for_user=_wait_for_user,
                 addhandlers=True, killonclose=True,
                 singlesession=False):

        self._lock = threading.Lock()
        self._started = False
        self._stopped = False
        self._closed = False

        # socket-related

        self._sock = None  # set when started
        self._server = None

        # session-related

        self._singlesession = singlesession

        self._session = None
        self._numsessions = 0
        self._sessionlock = None

        # proc-related

        self._wait_for_user = wait_for_user
        self._killonclose = killonclose

        self._exiting_via_atexit_handler = False

        self._exithandlers = ExitHandlers()
        if addhandlers:
            self._install_exit_handlers()

    @property
    def session(self):
        """"""The current session.""""""
        return self._session

    @contextlib.contextmanager
    def started(self):
        """"""A context manager that starts the daemon and stops it for errors.""""""
        self.start()
        try:
            yield self
        except Exception:
            self._stop_quietly()
            raise

    @contextlib.contextmanager
    def running(self):
        """"""A context manager that starts the daemon.

        If there's a failure then the daemon is stopped.  It is also
        stopped at the end of the with block.
        """"""
        self.start()
        try:
            yield self
        finally:
            self._stop_quietly()

    def is_running(self):
        """"""Return True if the daemon is running.""""""
        with self._lock:
            if self._closed:
                return False
            if self._sock is None:
                return False
            return self._started and not self._stopped

    def start(self):
        """"""Return the ""socket"" to use for pydevd after setting it up.""""""
        with self._lock:
            if self._closed:
                raise DaemonClosedError()
            if self._started:
                raise RuntimeError('already started')
            self._started = True

        sock = self._start()
        self._sock = sock
        return sock

    def start_server(self, addr, hidebadsessions=True):
        """"""Return (""socket"", next_session) with a new server socket.""""""
        addr = Address.from_raw(addr)
        with self.started():
            assert self._sessionlock is None
            assert self.session is None
            self._server = create_server(addr.host, addr.port)
            debug('server socket created')
            self._sessionlock = threading.Lock()
        sock = self._sock

        def check_ready(**kwargs):
            self._check_ready_for_session(**kwargs)
            if self._server is None:
                raise DaemonStoppedError()

        def next_session(timeout=None, **kwargs):
            server = self._server
            sessionlock = self._sessionlock
            check_ready(checksession=False)

            debug('getting next session')
            sessionlock.acquire()  # Released in _finish_session().
            debug('session lock acquired')
            # It may have closed or stopped while we waited.
            check_ready()

            timeout = kwargs.pop('timeout', None)
            try:
                debug('getting session socket')
                client = connect(server, None, **kwargs)
                self._bind_session(client)
                debug('starting session')
                self._start_session_safely('ptvsd.Server', timeout=timeout)
                debug('session started')
                return self._session
            except Exception as exc:
                debug('session exc:', exc, tb=True)
                with ignore_errors():
                    self._finish_session()
                if hidebadsessions:
                    debug('hiding bad session')
                    # TODO: Log the error?
                    return None
                self._stop_quietly()
                raise

        return sock, next_session

    def start_client(self, addr):
        """"""Return (""socket"", start_session) with a new client socket.""""""
        addr = Address.from_raw(addr)
        self._singlesession = True
        with self.started():
            assert self.session is None
            client = create_client()
            connect(client, addr)
        sock = self._sock

        def start_session(**kwargs):
            self._check_ready_for_session()
            if self._server is not None:
                raise RuntimeError('running as server')
            if self._numsessions:
                raise RuntimeError('session stopped')

            try:
                self._bind_session(client)
                self._start_session_safely('ptvsd.Client', **kwargs)
                return self._session
            except Exception:
                self._stop_quietly()
                raise

        return sock, start_session

    def start_session(self, session, threadname, **kwargs):
        """"""Start the debug session and remember it.

        If ""session"" is a client socket then a session is created
        from it.
        """"""
        self._check_ready_for_session()
        if self._server is not None:
            raise RuntimeError('running as server')

        self._bind_session(session)
        self._start_session_safely(threadname, **kwargs)
        return self.session

    def close(self):
        """"""Stop all loops and release all resources.""""""
        with self._lock:
            if self._closed:
                raise DaemonClosedError('already closed')
            self._closed = True

        self._close()

    # internal methods

    def _check_ready_for_session(self, checksession=True):
        with self._lock:
            if self._closed:
                raise DaemonClosedError()
            if not self._started:
                raise DaemonStoppedError('never started')
            if self._stopped or self._sock is None:
                raise DaemonStoppedError()
            if checksession and self.session is not None:
                raise RuntimeError('session already started')

    def _close(self):
        self._stop()

        self._sock = None

    def _stop(self):
        with self._lock:
            if self._stopped:
                return
            self._stopped = True

        server = self._server
        self._server = None

        with ignore_errors():
            self._finish_session()

        self._sessionlock = None  # TODO: Call self._clear_sessionlock?

        # TODO: Close the server socket *before* finish the session?
        if server is not None:
            with ignore_errors():
                close_socket(server)

        # TODO: Close self._sock *before* finishing the session?
        if self._sock is not None:
            with ignore_errors():
                close_socket(self._sock)

    def _stop_quietly(self):
        with ignore_errors():
            self._stop()

    def _handle_session_disconnecting(self, session):
        debug('handling disconnecting session')
        if self._singlesession:
            if self._killonclose:
                with self._lock:
                    if not self._exiting_via_atexit_handler:
                        # Ensure the proc is exiting before closing
                        # socket.  Note that we kill the proc instead
                        # of calling sys.exit(0).
                        # Note that this will trigger either the atexit
                        # handler or the signal handler.
                        kill_current_proc()
            else:
                try:
                    self.close()
                except DaemonClosedError:
                    pass

    def _handle_session_closing(self, session):
        debug('handling closing session')

        if self._singlesession:
            if self._killonclose:
                with self._lock:
                    if not self._exiting_via_atexit_handler:
                        # Ensure the proc is exiting before closing
                        # socket.  Note that we kill the proc instead
                        # of calling sys.exit(0).
                        # Note that this will trigger either the atexit
                        # handler or the signal handler.
                        kill_current_proc()
            else:
                try:
                    self.close()
                except DaemonClosedError:
                    pass
        else:
            self._finish_session()

    def _clear_sessionlock(self, done=False):
        sessionlock = self._sessionlock
        if done:
            self._sessionlock = None
        if sessionlock is not None:
            try:
                sessionlock.release()
            except Exception:  # TODO: Make it more specific?
                debug('session lock not released')
            else:
                debug('session lock released')

    # internal session-related methods

    def _bind_session(self, session):
        session_not_bound.clear()
        # TODO: Pass notify_* to session.start() instead.
        session = self.SESSION.from_raw(
            session,
            notify_closing=self._handle_session_closing,
            notify_disconnecting=self._handle_session_disconnecting,
            ownsock=True,
            **self._session_kwargs() or {}
        )
        self._session = session
        self._numsessions += 1

    def _start_session_safely(self, threadname, **kwargs):
        try:
            self._start_session(threadname, **kwargs)
        except Exception:
            with ignore_errors():
                self._finish_session()
            raise

    def _finish_session(self):
        self._numsessions -= 1
        session_not_bound.set()
        try:
            session = self._release_session()
            debug('session stopped')
        finally:
            self._clear_sessionlock()

            if self._singlesession:
                debug('closing daemon after single session')
                try:
                    self.close()
                except DaemonClosedError:
                    pass
        return session

    def _release_session(self):
        session = self.session
        if not self._singlesession:
            # TODO: This shouldn't happen if we are exiting?
            self._session = None

        try:
            session.stop()
        except NotRunningError:
            pass
        try:
            session.close()
        except ClosedError:
            pass

        return session

    # internal proc-related methods

    def _install_exit_handlers(self):
        """"""Set the placeholder handlers.""""""
        self._exithandlers.install()

        try:
            self._exithandlers.add_atexit_handler(self._handle_atexit)
        except ValueError:
            pass
        for signum in self._exithandlers.SIGNALS:
            try:
                self._exithandlers.add_signal_handler(signum,
                                                      self._handle_signal)
            except ValueError:
                # Already added.
                pass
            except UnsupportedSignalError:
                # TODO: This shouldn't happen.
                pass

    def _handle_atexit(self):
        debug('handling atexit')
        with self._lock:
            self._exiting_via_atexit_handler = True
        session = self.session

        if session is not None:
            lock = threading.Lock()
            lock.acquire()

            def wait_debugger(timeout=None):
                lock_wait(lock, timeout)

            def wait_exiting(cfg):
                if cfg:
                    self._wait_for_user()
                lock.release()
            # TODO: Rely on self._stop_debugger().
            session.handle_debugger_stopped(wait_debugger)
            session.handle_exiting(self.exitcode, wait_exiting)

        try:
            self.close()
        except DaemonClosedError:
            pass
        if session is not None:
            session.wait_until_stopped()

    def _handle_signal(self, signum, frame):
        debug('handling signal')
        try:
            self.close()
        except DaemonClosedError:
            pass
        if not self._exiting_via_atexit_handler:
            sys.exit(0)

    # methods for subclasses to override

    def _start(self):
        """"""Return the debugger client socket after starting the daemon.""""""
        raise NotImplementedError

    def _start_session(self, threadname, **kwargs):
        self.session.start(
            threadname,
            **kwargs
        )

    def _session_kwargs(self):
        return None


class Daemon(DaemonBase):
    """"""The process-level manager for the VSC protocol debug adapter.""""""

    SESSION = PyDevdDebugSession

    def __init__(self, wait_for_user=_wait_for_user,
                 notify_session_debugger_ready=None,
                 **kwargs):
        super(Daemon, self).__init__(wait_for_user, **kwargs)

        self._notify_session_debugger_ready = notify_session_debugger_ready

    @property
    def pydevd(self):
        return self._sock

    # internal methods

    def _start(self):
        return wrapper.PydevdSocket(
            self._handle_pydevd_message,
            self._handle_pydevd_close,
            self._getpeername,
            self._getsockname,
        )

    def _start_session(self, threadname, **kwargs):
        super(Daemon, self)._start_session(
            threadname,
            pydevd_notify=self.pydevd.pydevd_notify,
            pydevd_request=self.pydevd.pydevd_request,
            **kwargs
        )

    def _session_kwargs(self):
        def debugger_ready(session):
            if self._notify_session_debugger_ready is not None:
                self._notify_session_debugger_ready(session)

        return dict(
            notify_debugger_ready=debugger_ready,
        )

    # internal methods for PyDevdSocket().

    def _handle_pydevd_message(self, cmdid, seq, text):
        if self.session is None or self.session.closed:
            # TODO: Do more than ignore?
            return
        self.session.handle_pydevd_message(cmdid, seq, text)

    def _handle_pydevd_close(self):
        try:
            self.close()
        except DaemonClosedError:
            pass

    def _getpeername(self):
        if self.session is None or self.session.closed:
            raise NotImplementedError
        return self.session.socket.getpeername()

    def _getsockname(self):
        if self.session is None or self.session.closed:
            raise NotImplementedError
        return self.session.socket.getsockname()
/n/n/nptvsd/exit_handlers.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import atexit
import os
import platform
import signal


class AlreadyInstalledError(RuntimeError):
    """"""Exit handlers were already installed.""""""


class UnsupportedSignalError(RuntimeError):
    """"""A signal is not supported.""""""


def kill_current_proc(signum=signal.SIGTERM):
    """"""Kill the current process.

    Note that this directly kills the process (with SIGTERM, by default)
    rather than using sys.exit().
    """"""
    os.kill(os.getpid(), signum)


class ExitHandlers(object):
    """"""Manages signal and atexit handlers.""""""

    if platform.system() == 'Windows':
        # TODO: Windows *does* support these signals:
        #  SIGABRT, SIGFPE, SIGILL, SIGINT, SIGSEGV, SIGTERM, SIGBREAK
        SIGNALS = []
    else:
        SIGNALS = [
            signal.SIGHUP,
        ]

    def __init__(self):
        self._signal_handlers = {sig: []
                                 for sig in self.SIGNALS}
        self._atexit_handlers = []
        self._installed = False

    @property
    def supported_signals(self):
        return set(self.SIGNALS)

    @property
    def installed(self):
        return self._installed

    def install(self):
        """"""Set the parent handlers.

        This must be called in the main thread.
        """"""
        if self._installed:
            raise AlreadyInstalledError('exit handlers already installed')
        self._installed = True
        self._install_signal_handler()
        self._install_atexit_handler()

    # TODO: Add uninstall()?

    def add_atexit_handler(self, handle_atexit, nodupe=True):
        """"""Add an atexit handler to the list managed here.""""""
        if nodupe and handle_atexit in self._atexit_handlers:
            raise ValueError('atexit handler alraedy added')
        self._atexit_handlers.append(handle_atexit)

    def add_signal_handler(self, signum, handle_signal, nodupe=True,
                           ignoreunsupported=False):
        """"""Add a signal handler to the list managed here.""""""
        # TODO: The initialization of self.SIGNALS should make this
        # special-casing unnecessary.
        if platform.system() == 'Windows':
            return

        try:
            handlers = self._signal_handlers[signum]
        except KeyError:
            if ignoreunsupported:
                return
            raise UnsupportedSignalError(signum)
        if nodupe and handle_signal in handlers:
            raise ValueError('signal handler alraedy added')
        handlers.append(handle_signal)

    # internal methods

    def _install_signal_handler(self):
        # TODO: The initialization of self.SIGNALS should make this
        # special-casing unnecessary.
        if platform.system() == 'Windows':
            return

        orig = {}
        try:
            for sig in self._signal_handlers:
                # TODO: Skip or fail if signal.getsignal() returns None?
                orig[sig] = signal.signal(sig, self._signal_handler)
        except ValueError:
            # Wasn't called in main thread!
            raise

    def _signal_handler(self, signum, frame):
        for handle_signal in self._signal_handlers.get(signum, ()):
            handle_signal(signum, frame)

    def _install_atexit_handler(self):
        self._atexit_handlers = []
        atexit.register(self._atexit_handler)

    def _atexit_handler(self):
        for handle_atexit in self._atexit_handlers:
            handle_atexit()
/n/n/nptvsd/pydevd_hooks.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import sys

from _pydevd_bundle import pydevd_comm

from ptvsd.socket import Address
from ptvsd.daemon import Daemon, DaemonStoppedError, DaemonClosedError
from ptvsd._util import debug, new_hidden_thread


def start_server(daemon, host, port, **kwargs):
    """"""Return a socket to a (new) local pydevd-handling daemon.

    The daemon supports the pydevd client wire protocol, sending
    requests and handling responses (and events).

    This is a replacement for _pydevd_bundle.pydevd_comm.start_server.
    """"""
    sock, next_session = daemon.start_server((host, port))

    def handle_next():
        try:
            session = next_session(**kwargs)
            debug('done waiting')
            return session
        except (DaemonClosedError, DaemonStoppedError):
            # Typically won't happen.
            debug('stopped')
            raise
        except Exception as exc:
            # TODO: log this?
            debug('failed:', exc, tb=True)
            return None

    def serve_forever():
        debug('waiting on initial connection')
        handle_next()
        while True:
            debug('waiting on next connection')
            try:
                handle_next()
            except (DaemonClosedError, DaemonStoppedError):
                break
        debug('done')

    t = new_hidden_thread(
        target=serve_forever,
        name='sessions',
    )
    t.start()
    return sock


def start_client(daemon, host, port, **kwargs):
    """"""Return a socket to an existing ""remote"" pydevd-handling daemon.

    The daemon supports the pydevd client wire protocol, sending
    requests and handling responses (and events).

    This is a replacement for _pydevd_bundle.pydevd_comm.start_client.
    """"""
    sock, start_session = daemon.start_client((host, port))
    start_session(**kwargs)
    return sock


def install(pydevd, address,
            start_server=start_server, start_client=start_client,
            **kwargs):
    """"""Configure pydevd to use our wrapper.

    This is a bit of a hack to allow us to run our VSC debug adapter
    in the same process as pydevd.  Note that, as with most hacks,
    this is somewhat fragile (since the monkeypatching sites may
    change).
    """"""
    addr = Address.from_raw(address)
    daemon = Daemon(**kwargs)

    _start_server = (lambda p: start_server(daemon, addr.host, p))
    _start_server.orig = start_server
    _start_client = (lambda h, p: start_client(daemon, h, p))
    _start_client.orig = start_client

    # These are the functions pydevd invokes to get a socket to the client.
    pydevd_comm.start_server = _start_server
    pydevd_comm.start_client = _start_client

    # Ensure that pydevd is using our functions.
    pydevd.start_server = _start_server
    pydevd.start_client = _start_client
    __main__ = sys.modules['__main__']
    if __main__ is not pydevd:
        if getattr(__main__, '__file__', None) == pydevd.__file__:
            __main__.start_server = _start_server
            __main__.start_client = _start_client
    return daemon
/n/n/nptvsd/session.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

from .socket import is_socket, close_socket
from .wrapper import VSCodeMessageProcessor
from ._util import TimeoutError, ClosedError, Closeable, Startable, debug


class DebugSession(Startable, Closeable):
    """"""A single DAP session for a network client socket.""""""

    MESSAGE_PROCESSOR = None

    NAME = 'debug session'
    FAIL_ON_ALREADY_CLOSED = False
    FAIL_ON_ALREADY_STOPPED = False

    @classmethod
    def from_raw(cls, raw, **kwargs):
        """"""Return a session for the given data.""""""
        if isinstance(raw, cls):
            return raw
        if not is_socket(raw):
            # TODO: Create a new client socket from a remote address?
            #addr = Address.from_raw(raw)
            raise NotImplementedError
        client = raw
        return cls(client, **kwargs)

    @classmethod
    def from_server_socket(cls, server, **kwargs):
        """"""Return a session for the next connection to the given socket.""""""
        client, _ = server.accept()
        return cls(client, ownsock=True, **kwargs)

    def __init__(self, sock,
                 notify_closing=None,
                 notify_disconnecting=None,
                 ownsock=False):
        super(DebugSession, self).__init__()

        if notify_closing is not None:
            def handle_closing(before):
                if before:
                    notify_closing(self)
            self.add_close_handler(handle_closing)

        if notify_disconnecting is None:
            notify_disconnecting = (lambda _: None)
        self._notify_disconnecting = notify_disconnecting

        self._sock = sock
        self._pre_socket_close = None
        if ownsock:
            # Close the socket *after* calling sys.exit() (via notify_closing).
            def handle_closing(before):
                if before:
                    return
                debug('closing session socket')
                proc = self._msgprocessor
                if self._pre_socket_close is not None:
                    self._pre_socket_close()
                if proc is not None:
                    try:
                        proc.wait_while_connected(10)  # seconds
                    except TimeoutError:
                        debug('timed out waiting for disconnect')
                close_socket(self._sock)
            self.add_close_handler(handle_closing)

        self._msgprocessor = None

    @property
    def socket(self):
        return self._sock

    @property
    def msgprocessor(self):
        return self._msgprocessor

    def handle_debugger_stopped(self, wait=None):
        """"""Deal with the debugger exiting.""""""
        proc = self._msgprocessor
        if proc is None:
            return
        proc.handle_debugger_stopped(wait)

    def handle_exiting(self, exitcode=None, wait=None):
        """"""Deal with the debuggee exiting.""""""
        proc = self._msgprocessor
        if proc is None:
            return
        proc.handle_exiting(exitcode, wait)

    def wait_until_stopped(self):
        """"""Block until all resources (e.g. message processor) have stopped.""""""
        proc = self._msgprocessor
        if proc is None:
            return
        # TODO: Do this in VSCodeMessageProcessor.close()?
        proc._wait_for_server_thread()

    # internal methods

    def _new_msg_processor(self, **kwargs):
        return self.MESSAGE_PROCESSOR(
            self._sock,
            notify_disconnecting=self._handle_vsc_disconnect,
            notify_closing=self._handle_vsc_close,
            **kwargs
        )

    def _start(self, threadname, **kwargs):
        """"""Start the message handling for the session.""""""
        self._msgprocessor = self._new_msg_processor(**kwargs)
        self.add_resource_to_close(self._msgprocessor)
        self._msgprocessor.start(threadname)
        return self._msgprocessor_running

    def _stop(self):
        proc = self._msgprocessor
        if proc is None:
            return

        debug('proc stopping')
        # TODO: We should not need to wait if not exiting.
        # The editor will send a ""disconnect"" request at this point.
        proc._wait_for_disconnect()
        proc.close()
        self._msgprocessor = None

    def _close(self):
        debug('session closing')
        pass

    def _msgprocessor_running(self):
        if self._msgprocessor is None:
            return False
        # TODO: Return self._msgprocessor.is_running().
        return True

    # internal methods for VSCodeMessageProcessor

    def _handle_vsc_disconnect(self, pre_socket_close=None):
        debug('disconnecting')
        self._pre_socket_close = pre_socket_close  # TODO: Fail if already set?
        self._notify_disconnecting(self)

    def _handle_vsc_close(self):
        debug('processor closing')
        try:
            self.close()
        except ClosedError:
            pass


class PyDevdDebugSession(DebugSession):
    """"""A single DAP session for a network client socket.""""""

    MESSAGE_PROCESSOR = VSCodeMessageProcessor

    def __init__(self, sock,
                 notify_debugger_ready=None,
                 **kwargs):
        super(PyDevdDebugSession, self).__init__(sock, **kwargs)

        def notify_debugger_ready(session, _notify=notify_debugger_ready):
            if self._notified_debugger_ready:
                return
            self._notified_debugger_ready = True
            if _notify is not None:
                _notify(session)
        self._notified_debugger_ready = False
        self._notify_debugger_ready = notify_debugger_ready

    def handle_pydevd_message(self, cmdid, seq, text):
        if self._msgprocessor is None:
            # TODO: Do more than ignore?
            return
        return self._msgprocessor.on_pydevd_event(cmdid, seq, text)

    # internal methods

    def _new_msg_processor(self, **kwargs):
        return super(PyDevdDebugSession, self)._new_msg_processor(
            notify_debugger_ready=self._handle_vsc_debugger_ready,
            **kwargs
        )

    # internal methods for VSCodeMessageProcessor

    def _handle_vsc_debugger_ready(self):
        debug('ready to debug')
        self._notify_debugger_ready(self)
/n/n/nptvsd/socket.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

from __future__ import absolute_import

from collections import namedtuple
import contextlib
import errno
import socket
try:
    from urllib.parse import urlparse
except ImportError:
    from urlparse import urlparse


try:
    ConnectionError  # noqa
    BrokenPipeError  # noqa
    ConnectionResetError  # noqa
except NameError:
    class BrokenPipeError(Exception):
        # EPIPE and ESHUTDOWN
        pass

    class ConnectionResetError(Exception):
        # ECONNRESET
        pass


NOT_CONNECTED = (
    errno.ENOTCONN,
    errno.EBADF,
)

CLOSED = (
    errno.EPIPE,
    errno.ESHUTDOWN,
    errno.ECONNRESET,
    # Windows
    10038,  # ""An operation was attempted on something that is not a socket""
    10058,
)

EOF = NOT_CONNECTED + CLOSED


@contextlib.contextmanager
def convert_eof():
    """"""A context manager to convert some socket errors into EOFError.""""""
    try:
        yield
    except ConnectionResetError:
        raise EOFError
    except BrokenPipeError:
        raise EOFError
    except OSError as exc:
        if exc.errno in EOF:
            raise EOFError
        raise


class TimeoutError(socket.timeout):
    """"""A socket timeout happened.""""""


def is_socket(sock):
    """"""Return True if the object can be used as a socket.""""""
    return isinstance(sock, socket.socket)


def create_server(host, port):
    """"""Return a local server socket listening on the given port.""""""
    if host is None:
        host = 'localhost'
    server = _new_sock()
    server.bind((host, port))
    server.listen(0)
    return server


def create_client():
    """"""Return a client socket that may be connected to a remote address.""""""
    return _new_sock()


def _new_sock():
    sock = socket.socket(socket.AF_INET,
                         socket.SOCK_STREAM,
                         socket.IPPROTO_TCP)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    return sock


@contextlib.contextmanager
def ignored_errno(*ignored):
    """"""A context manager that ignores the given errnos.""""""
    try:
        yield
    except OSError as exc:
        if exc.errno not in ignored:
            raise


class KeepAlive(namedtuple('KeepAlive', 'interval idle maxfails')):
    """"""TCP keep-alive settings.""""""

    INTERVAL = 3  # seconds
    IDLE = 1  # seconds after idle
    MAX_FAILS = 5

    @classmethod
    def from_raw(cls, raw):
        """"""Return the corresponding KeepAlive.""""""
        if raw is None:
            return None
        elif isinstance(raw, cls):
            return raw
        elif isinstance(raw, (str, int, float)):
            return cls(raw)
        else:
            try:
                raw = dict(raw)
            except TypeError:
                return cls(*raw)
            else:
                return cls(**raw)

    def __new__(cls, interval=None, idle=None, maxfails=None):
        self = super(KeepAlive, cls).__new__(
            cls,
            float(interval) if interval or interval == 0 else cls.INTERVAL,
            float(idle) if idle or idle == 0 else cls.IDLE,
            float(maxfails) if maxfails or maxfails == 0 else cls.MAX_FAILS,
        )
        return self

    def apply(self, sock):
        """"""Set the keepalive values on the socket.""""""
        sock.setsockopt(socket.SOL_SOCKET,
                        socket.SO_KEEPALIVE,
                        1)
        interval = self.interval
        idle = self.idle
        maxfails = self.maxfails
        try:
            if interval > 0:
                sock.setsockopt(socket.IPPROTO_TCP,
                                socket.TCP_KEEPINTVL,
                                interval)
            if idle > 0:
                sock.setsockopt(socket.IPPROTO_TCP,
                                socket.TCP_KEEPIDLE,
                                idle)
            if maxfails >= 0:
                sock.setsockopt(socket.IPPROTO_TCP,
                                socket.TCP_KEEPCNT,
                                maxfails)
        except AttributeError:
            # mostly linux-only
            pass


def connect(sock, addr, keepalive=None):
    """"""Return the client socket for the next connection.""""""
    if addr is None:
        if keepalive is None or keepalive is True:
            keepalive = KeepAlive()
        elif keepalive:
            keepalive = KeepAlive.from_raw(keepalive)
        client, _ = sock.accept()
        if keepalive:
            keepalive.apply(client)
        return client
    else:
        if keepalive:
            raise NotImplementedError
        sock.connect(addr)
        return sock


def shut_down(sock, how=socket.SHUT_RDWR, ignored=NOT_CONNECTED):
    """"""Shut down the given socket.""""""
    with ignored_errno(*ignored or ()):
        sock.shutdown(how)


def close_socket(sock):
    """"""Shutdown and close the socket.""""""
    try:
        shut_down(sock)
    except Exception:
        # TODO: Log errors?
        pass
    sock.close()


class Address(namedtuple('Address', 'host port')):
    """"""An IP address to use for sockets.""""""

    @classmethod
    def from_raw(cls, raw, defaultport=None):
        """"""Return an address corresponding to the given data.""""""
        if isinstance(raw, cls):
            return raw
        elif isinstance(raw, int):
            return cls(None, raw)
        elif isinstance(raw, str):
            if raw == '':
                return cls('', defaultport)
            parsed = urlparse(raw)
            if not parsed.netloc:
                if parsed.scheme:
                    raise ValueError('invalid address {!r}'.format(raw))
                return cls.from_raw('x://' + raw, defaultport=defaultport)
            return cls(
                parsed.hostname or '',
                parsed.port if parsed.port else defaultport,
            )
        elif not raw:
            return cls(None, defaultport)
        else:
            try:
                kwargs = dict(**raw)
            except TypeError:
                return cls(*raw)
            else:
                kwargs.setdefault('host', None)
                kwargs.setdefault('port', defaultport)
                return cls(**kwargs)

    @classmethod
    def as_server(cls, host, port):
        """"""Return an address to use as a server address.""""""
        return cls(host, port, isserver=True)

    @classmethod
    def as_client(cls, host, port):
        """"""Return an address to use as a server address.""""""
        return cls(host, port, isserver=False)

    def __new__(cls, host, port, **kwargs):
        if host == '*':
            host = ''
        isserver = kwargs.pop('isserver', None)
        if isserver is None:
            isserver = (host is None or host == '')
        else:
            isserver = bool(isserver)
        if host is None:
            host = 'localhost'
        self = super(Address, cls).__new__(
            cls,
            str(host),
            int(port) if port is not None else None,
            **kwargs
        )
        self._isserver = isserver
        return self

    def __init__(self, *args, **kwargs):
        if self.port is None:
            raise TypeError('missing port')
        if self.port <= 0 or self.port > 65535:
            raise ValueError('port must be positive int < 65535')

    def __repr__(self):
        orig = super(Address, self).__repr__()
        return '{}, isserver={})'.format(orig[:-1], self._isserver)

    def __eq__(self, other):
        if not super(Address, self).__eq__(other):
            return False
        try:
            other = self.from_raw(other)
        except Exception:
            return False
        return self._isserver == other._isserver

    @property
    def isserver(self):
        return self._isserver
/n/n/ntests/helpers/debugadapter.py/n/nimport os
import os.path
import socket
import time

from ptvsd.socket import Address
from ptvsd._util import Closeable, ClosedError
from .proc import Proc
from .. import PROJECT_ROOT


COPIED_ENV = [
    'PYTHONHASHSEED',

    # Windows
    #'ALLUSERSPROFILE',
    #'APPDATA',
    #'CLIENTNAME',
    #'COMMONPROGRAMFILES',
    #'COMMONPROGRAMFILES(X86)',
    #'COMMONPROGRAMW6432',
    #'COMPUTERNAME',
    #'COMSPEC',
    #'DRIVERDATA',
    #'HOMEDRIVE',
    #'HOMEPATH',
    #'LOCALAPPDATA',
    #'LOGONSERVER',
    #'NUMBER_OF_PROCESSORS',
    #'OS',
    #'PATH',
    #'PATHEXT',
    #'PROCESSOR_ARCHITECTURE',
    #'PROCESSOR_IDENTIFIER',
    #'PROCESSOR_LEVEL',
    #'PROCESSOR_REVISION',
    #'PROGRAMDATA',
    #'PROGRAMFILES',
    #'PROGRAMFILES(X86)',
    #'PROGRAMW6432',
    #'PSMODULEPATH',
    #'PUBLIC',
    #'SESSIONNAME',
    'SYSTEMDRIVE',
    'SYSTEMROOT',
    #'TEMP',
    #'TMP',
    #'USERDOMAIN',
    #'USERDOMAIN_ROAMINGPROFILE',
    #'USERNAME',
    #'USERPROFILE',
    'WINDIR',
]

SERVER_READY_TIMEOUT = 3.0  # seconds

try:
    ConnectionRefusedError
except Exception:
    class ConnectionRefusedError(Exception):
        pass


def _copy_env(verbose=False, env=None):
    variables = {k: v for k, v in os.environ.items() if k in COPIED_ENV}
    # TODO: Be smarter about the seed?
    variables.setdefault('PYTHONHASHSEED', '1234')
    if verbose:
        variables.update({
            'PTVSD_DEBUG': '1',
            'PTVSD_SOCKET_TIMEOUT': '1',
        })
    if env is not None:
        variables.update(env)

    # Ensure Project root is always in current path.
    python_path = variables.get('PYTHONPATH', None)
    if python_path is None:
        variables['PYTHONPATH'] = PROJECT_ROOT
    else:
        variables['PYTHONPATH'] = os.pathsep.join([PROJECT_ROOT, python_path])

    return variables


def wait_for_socket_server(addr, timeout=SERVER_READY_TIMEOUT):
    start_time = time.time()
    while True:
        try:
            sock = socket.create_connection((addr.host, addr.port))
            sock.close()
            time.sleep(0.1)  # wait for daemon to detect to socket close.
            return
        except Exception:
            pass
        time.sleep(0.1)
        if time.time() - start_time > timeout:
            raise ConnectionRefusedError('Timeout waiting for connection')


def wait_for_port_to_free(port, timeout=3.0):
    start_time = time.time()
    while True:
        try:
            time.sleep(0.5)
            sock = socket.create_connection(('localhost', port))
            sock.close()
        except Exception:
            return
        time.sleep(0.1)
        if time.time() - start_time > timeout:
            raise ConnectionRefusedError('Timeout waiting for port to be free')


class DebugAdapter(Closeable):

    VERBOSE = False
    #VERBOSE = True

    PORT = 8888

    # generic factories

    @classmethod
    def start(cls, argv, env=None, cwd=None, **kwargs):
        def new_proc(argv, addr, **kwds):
            env_vars = _copy_env(verbose=cls.VERBOSE, env=env)
            argv = list(argv)
            cls._ensure_addr(argv, addr)
            return Proc.start_python_module(
                'ptvsd', argv, env=env_vars, cwd=cwd, **kwds)

        return cls._start(new_proc, argv, **kwargs)

    @classmethod
    def start_wrapper_script(cls, filename, argv, env=None, cwd=None,
                             **kwargs):  # noqa
        def new_proc(argv, addr, **kwds):
            env_vars = _copy_env(verbose=cls.VERBOSE, env=env)
            return Proc.start_python_script(
                filename, argv, env=env_vars, cwd=cwd, **kwds)

        return cls._start(new_proc, argv, **kwargs)

    @classmethod
    def start_wrapper_module(cls,
                             modulename,
                             argv,
                             env=None,
                             cwd=None,
                             **kwargs):  # noqa
        def new_proc(argv, addr, **kwds):
            env_vars = _copy_env(verbose=cls.VERBOSE, env=env)
            return Proc.start_python_module(
                modulename, argv, env=env_vars, cwd=cwd, **kwds)

        return cls._start(new_proc, argv, **kwargs)

    # specific factory cases

    @classmethod
    def start_nodebug(cls, addr, name, kind='script', **kwargs):
        if kind == 'script':
            argv = ['--nodebug', name]
        elif kind == 'module':
            argv = ['--nodebug', '-m', name]
        else:
            raise NotImplementedError
        return cls.start(argv, addr=addr, **kwargs)

    @classmethod
    def start_as_server(cls, addr, *args, **kwargs):
        addr = Address.as_server(*addr)
        return cls._start_as(addr, *args, server=False, **kwargs)

    @classmethod
    def start_as_client(cls, addr, *args, **kwargs):
        addr = Address.as_client(*addr)
        return cls._start_as(addr, *args, server=False, **kwargs)

    @classmethod
    def start_for_attach(cls, addr, *args, **kwargs):
        srvtimeout = kwargs.pop('srvtimeout', SERVER_READY_TIMEOUT)
        addr = Address.as_server(*addr)
        adapter = cls._start_as(addr, *args, server=True, **kwargs)
        if srvtimeout is not None:
            wait_for_socket_server(addr, timeout=srvtimeout)
        return adapter

    @classmethod
    def _start_as(cls,
                  addr,
                  name,
                  kind='script',
                  extra=None,
                  server=False,
                  **kwargs):
        argv = []
        if server:
            argv += ['--server']
            if kwargs.pop('wait', True):
                argv += ['--wait']
        if kind == 'script':
            argv += [name]
        elif kind == 'module':
            argv += ['-m', name]
        else:
            raise NotImplementedError
        if extra:
            argv += list(extra)
        return cls.start(argv, addr=addr, **kwargs)

    @classmethod
    def start_embedded(cls, addr, filename, argv=[], **kwargs):
        # ptvsd.enable_attach() slows things down, so we must wait longer.
        srvtimeout = kwargs.pop('srvtimeout', SERVER_READY_TIMEOUT + 2)
        addr = Address.as_server(*addr)
        with open(filename, 'r+') as scriptfile:
            content = scriptfile.read()
            # TODO: Handle this case somehow?
            assert 'ptvsd.enable_attach' in content
        adapter = cls.start_wrapper_script(
            filename, argv=argv, addr=addr, **kwargs)
        if srvtimeout is not None:
            wait_for_socket_server(addr, timeout=srvtimeout)
        return adapter

    @classmethod
    def _start(cls, new_proc, argv, addr=None, **kwargs):
        addr = Address.from_raw(addr, defaultport=cls.PORT)
        proc = new_proc(argv, addr, **kwargs)
        return cls(proc, addr, owned=True)

    @classmethod
    def _ensure_addr(cls, argv, addr):
        if '--host' in argv:
            raise ValueError(""unexpected '--host' in argv"")
        if '--server-host' in argv:
            raise ValueError(""unexpected '--server-host' in argv"")
        if '--port' in argv:
            raise ValueError(""unexpected '--port' in argv"")
        host, port = addr

        argv.insert(0, str(port))
        argv.insert(0, '--port')

        argv.insert(0, host)
        if addr.isserver:
            argv.insert(0, '--server-host')
        else:
            argv.insert(0, '--host')

    def __init__(self, proc, addr, owned=False):
        super(DebugAdapter, self).__init__()
        assert isinstance(proc, Proc)
        self._proc = proc
        self._addr = addr

    @property
    def address(self):
        return self._addr

    @property
    def pid(self):
        return self._proc.pid

    @property
    def output(self):
        # TODO: Decode here?
        return self._proc.output

    @property
    def exitcode(self):
        return self._proc.exitcode

    def wait(self, *argv):
        self._proc.wait(*argv)

    # internal methods

    def _close(self):
        if self._proc is not None:
            try:
                self._proc.close()
            except ClosedError:
                pass
        if self.VERBOSE:
            lines = self.output.decode('utf-8').splitlines()
            print(' + ' + '\n + '.join(lines))
/n/n/ntests/helpers/debugclient.py/n/nfrom __future__ import absolute_import

import os
import traceback
import warnings

from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .debugadapter import DebugAdapter, wait_for_socket_server
from .debugsession import DebugSession

# TODO: Add a helper function to start a remote debugger for testing
# remote debugging?


class _LifecycleClient(Closeable):

    SESSION = DebugSession

    def __init__(
            self,
            addr=None,
            port=8888,
            breakpoints=None,
            connecttimeout=1.0,
    ):
        super(_LifecycleClient, self).__init__()
        self._addr = Address.from_raw(addr, defaultport=port)
        self._connecttimeout = connecttimeout
        self._adapter = None
        self._session = None

        self._breakpoints = breakpoints

    @property
    def adapter(self):
        return self._adapter

    @property
    def session(self):
        return self._session

    def start_debugging(self, launchcfg):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        raise NotImplementedError

    def stop_debugging(self):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')

        if self._session is not None:
            self._detach()

        try:
            self._adapter.close()
        except ClosedError:
            pass
        self._adapter = None

    def attach_pid(self, pid, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        raise NotImplementedError

    def attach_socket(self, addr=None, adapter=None, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if adapter is None:
            adapter = self._adapter
        elif self._adapter is not None:
            raise RuntimeError('already using managed adapter')
        if adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        if addr is None:
            addr = adapter.address
        self._attach(addr, **kwargs)
        return self._session

    def detach(self, adapter=None):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._session is None:
            raise RuntimeError('not attached')
        if adapter is None:
            adapter = self._adapter
        assert adapter is not None
        if not self._session.is_client:
            raise RuntimeError('detach not supported')

        self._detach()

    # internal methods

    def _close(self):
        if self._session is not None:
            try:
                self._session.close()
            except ClosedError:
                pass
        if self._adapter is not None:
            try:
                self._adapter.close()
            except ClosedError:
                pass

    def _launch(self,
                argv,
                script=None,
                wait_for_connect=None,
                detachable=True,
                env=None,
                cwd=None,
                **kwargs):
        if script is not None:
            def start(*args, **kwargs):
                return DebugAdapter.start_wrapper_script(
                    script, *args, **kwargs)
        else:
            start = DebugAdapter.start
        new_addr = Address.as_server if detachable else Address.as_client
        addr = new_addr(None, self._addr.port)
        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)

        if wait_for_connect:
            wait_for_connect()
        else:
            try:
                wait_for_socket_server(addr)
            except Exception:
                # If we fail to connect, print out the adapter output.
                self._adapter.VERBOSE = True
                raise
            self._attach(addr, **kwargs)

    def _attach(self, addr, **kwargs):
        if addr is None:
            addr = self._addr
        assert addr.host == 'localhost'
        self._session = self.SESSION.create_client(addr, **kwargs)

    def _detach(self):
        session = self._session
        if session is None:
            return
        self._session = None
        try:
            session.close()
        except ClosedError:
            pass


class DebugClient(_LifecycleClient):
    """"""A high-level abstraction of a debug client (i.e. editor).""""""

    # TODO: Manage breakpoints, etc.
    # TODO: Add debugger methods here (e.g. ""pause"").


class EasyDebugClient(DebugClient):
    def start_detached(self, argv):
        """"""Start an adapter in a background process.""""""
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        # TODO: Launch, handshake and detach?
        self._adapter = DebugAdapter.start(argv, port=self._port)
        return self._adapter

    def host_local_debugger(self,
                            argv,
                            script=None,
                            env=None,
                            cwd=None,
                            **kwargs):  # noqa
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None
        addr = ('localhost', self._addr.port)

        self._run_server_ex = None

        def run():
            try:
                self._session = self.SESSION.create_server(addr, **kwargs)
            except Exception as ex:
                self._run_server_ex = traceback.format_exc()

        t = new_hidden_thread(
            target=run,
            name='test.client',
        )
        t.start()

        def wait():
            t.join(timeout=self._connecttimeout)
            if t.is_alive():
                warnings.warn('timed out waiting for connection')
            if self._session is None:
                message = 'unable to connect after {} secs'.format(  # noqa
                    self._connecttimeout)
                if self._run_server_ex is None:
                    raise Exception(message)
                else:
                    message = message + os.linesep + self._run_server_ex # noqa
                    raise Exception(message)

            # The adapter will close when the connection does.

        self._launch(
            argv,
            script=script,
            wait_for_connect=wait,
            detachable=False,
            env=env,
            cwd=cwd)

        return self._adapter, self._session

    def launch_script(self, filename, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            filename,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        if kwargs.pop('wait', True):
            argv.insert(0, '--wait')
        self._launch(argv, **kwargs)
        return self._adapter, self._session

    def launch_module(self, module, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            '-m',
            module,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session
/n/n/ntests/helpers/debugsession.py/n/nfrom __future__ import absolute_import, print_function

import contextlib
import json
import socket
import sys
import time
import threading
import warnings

from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .message import (
    raw_read_all as read_messages,
    raw_write_one as write_message
)
from .socket import (
    Connection, create_server, create_client, close,
    recv_as_read, send_as_write,
    timeout as socket_timeout)
from .threading import get_locked_and_waiter
from .vsc import parse_message


class DebugSessionConnection(Closeable):

    VERBOSE = False
    #VERBOSE = True

    TIMEOUT = 5.0

    @classmethod
    def create_client(cls, addr, **kwargs):
        def connect(addr, timeout):
            sock = create_client()
            for _ in range(int(timeout * 10)):
                try:
                    sock.connect(addr)
                except (OSError, socket.error):
                    if cls.VERBOSE:
                        print('+', end='')
                        sys.stdout.flush()
                    time.sleep(0.1)
                else:
                    break
            else:
                raise RuntimeError('could not connect')
            return sock
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def create_server(cls, addr, **kwargs):
        def connect(addr, timeout):
            server = create_server(addr)
            with socket_timeout(server, timeout):
                client, _ = server.accept()
            return Connection(client, server)
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def _create(cls, connect, addr, timeout=None):
        if timeout is None:
            timeout = cls.TIMEOUT
        sock = connect(addr, timeout)
        if cls.VERBOSE:
            print('connected')
        self = cls(sock, ownsock=True)
        self._addr = addr
        return self

    def __init__(self, sock, ownsock=False):
        super(DebugSessionConnection, self).__init__()
        self._sock = sock
        self._ownsock = ownsock

    @property
    def is_client(self):
        try:
            return self._sock.server is None
        except AttributeError:
            return True

    def iter_messages(self):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        read = recv_as_read(self._sock)
        for msg, _, _ in read_messages(read, stop=stop):
            if self.VERBOSE:
                print(repr(msg))
            yield parse_message(msg)

    def send(self, req):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        write = send_as_write(self._sock)
        body = json.dumps(req)
        write_message(write, body, stop=stop)

    # internal methods

    def _close(self):
        if self._ownsock:
            close(self._sock)


class DebugSession(Closeable):

    VERBOSE = False
    #VERBOSE = True

    HOST = 'localhost'
    PORT = 8888

    TIMEOUT = None

    @classmethod
    def create_client(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_client(
            addr,
            timeout=kwargs.get('timeout'),
        )
        return cls(conn, owned=True, **kwargs)

    @classmethod
    def create_server(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_server(addr, **kwargs)
        return cls(conn, owned=True, **kwargs)

    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):
        super(DebugSession, self).__init__()
        self._conn = conn
        self._seq = seq
        self._timeout = timeout
        self._owned = owned

        self._handlers = []
        for handler in handlers:
            if callable(handler):
                self._add_handler(handler)
            else:
                self._add_handler(*handler)
        self._received = []
        self._listenerthread = new_hidden_thread(
            target=self._listen,
            name='test.session',
        )
        self._listenerthread.start()

    @property
    def is_client(self):
        return self._conn.is_client

    @property
    def received(self):
        return list(self._received)

    def _create_request(self, command, **args):
        seq = self._seq
        self._seq += 1
        return {
            'type': 'request',
            'seq': seq,
            'command': command,
            'arguments': args,
        }

    def send_request(self, command, **args):
        if self.closed:
            raise RuntimeError('session closed')

        wait = args.pop('wait', False)
        req = self._create_request(command, **args)
        if self.VERBOSE:
            msg = parse_message(req)
            print(' <-', msg)

        if wait:
            with self.wait_for_response(req) as resp:
                self._conn.send(req)
            resp_awaiter = AwaitableResponse(req, lambda: resp[""msg""])
        else:
            resp_awaiter = self._get_awaiter_for_request(req, **args)
            self._conn.send(req)
        return resp_awaiter

    def add_handler(self, handler, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        self._add_handler(handler, **kwargs)

    @contextlib.contextmanager
    def wait_for_event(self, event, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event
        handlername = 'event {!r}'.format(event)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event and condition(msg) # noqa
        handlername = 'event {!r}'.format(event)
        evt = self._get_message_handle(match, handlername)

        return AwaitableEvent(event, lambda: result[""msg""], evt)

    def _get_awaiter_for_request(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        evt = self._get_message_handle(match, handlername)

        return AwaitableResponse(req, lambda: result[""msg""], evt)

    @contextlib.contextmanager
    def wait_for_response(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    # internal methods

    def _close(self):
        if self._owned:
            try:
                self._conn.close()
            except ClosedError:
                pass
        if self._listenerthread != threading.current_thread():
            self._listenerthread.join(timeout=1.0)
            if self._listenerthread.is_alive():
                warnings.warn('session listener still running')
        self._check_handlers()

    def _listen(self):
        eof = None
        try:
            for msg in self._conn.iter_messages():
                if self.VERBOSE:
                    print(' ->', msg)
                self._receive_message(msg)
        except EOFError as ex:
            # Handle EOF outside of except to avoid unnecessary chaining.
            eof = ex
        if eof:
            remainder = getattr(eof, 'remainder', b'')
            if remainder:
                self._receive_message(remainder)
            try:
                self.close()
            except ClosedError:
                pass

    def _receive_message(self, msg):
        for i, handler in enumerate(list(self._handlers)):
            handle_message, _, _ = handler
            handled = handle_message(msg)
            try:
                msg, handled = handled
            except TypeError:
                pass
            if handled:
                self._handlers.remove(handler)
                break
        self._received.append(msg)

    def _add_handler(self, handle_msg, handlername=None, required=True):
        self._handlers.append(
            (handle_msg, handlername, required))

    def _check_handlers(self):
        unhandled = []
        for handle_msg, name, required in self._handlers:
            if not required:
                continue
            unhandled.append(name or repr(handle_msg))
        if unhandled:
            raise RuntimeError('unhandled: {}'.format(unhandled))

    @contextlib.contextmanager
    def _wait_for_message(self, match, handlername, timeout=None):
        if timeout is None:
            timeout = self.TIMEOUT
        lock, wait = get_locked_and_waiter()

        def handler(msg):
            if not match(msg):
                return msg, False
            lock.release()
            return msg, True
        self._add_handler(handler, handlername)
        try:
            yield
        finally:
            wait(timeout or self._timeout, handlername, fail=True)

    def _get_message_handle(self, match, handlername):
        event = threading.Event()

        def handler(msg):
            if not match(msg):
                return msg, False
            event.set()
            return msg, True
        self._add_handler(handler, handlername, False)
        return event


class Awaitable(object):

    @classmethod
    def wait_all(cls, *awaitables):
        timeout = 3.0
        messages = []
        for _ in range(int(timeout * 10)):
            time.sleep(0.1)
            messages = []
            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa
            for awaitable in not_ready:
                if isinstance(awaitable, AwaitableEvent):
                    messages.append('Event {}'.format(awaitable.name))
                else:
                    messages.append('Response {}'.format(awaitable.name))
            if len(messages) == 0:
                return
        else:
            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa

    def __init__(self, name, event=None):
        self._event = event
        self.name = name

    def wait(self, timeout=1.0):
        if self._event is None:
            return
        if not self._event.wait(timeout):
            message = 'Timeout waiting for '
            if isinstance(self, AwaitableEvent):
                message += 'Event {}'.format(self.name)
            else:
                message += 'Response {}'.format(self.name)
            raise TimeoutError(message)


class AwaitableResponse(Awaitable):

    def __init__(self, req, result_getter, event=None):
        super(AwaitableResponse, self).__init__(req[""command""], event)
        self.req = req
        self._result_getter = result_getter

    @property
    def resp(self):
        return self._result_getter()


class AwaitableEvent(Awaitable):

    def __init__(self, name, result_getter, event=None):
        super(AwaitableEvent, self).__init__(name, event)
        self._result_getter = result_getter

    @property
    def event(self):
        return self._result_getter()
/n/n/ntests/ptvsd/test___main__.py/n/nimport unittest

from ptvsd.socket import Address
from ptvsd.__main__ import parse_args
from tests.helpers._io import captured_stdio


class ParseArgsTests(unittest.TestCase):

    EXPECTED_EXTRA = ['--']

    def test_module(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_module_server(self):
        args, extra = parse_args([
            'eggs',
            '--server-host', '10.0.1.1',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_server('10.0.1.1', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_module_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_script(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_script_server(self):
        args, extra = parse_args([
            'eggs',
            '--server-host', '10.0.1.1',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('10.0.1.1', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_script_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote(self):
        args, extra = parse_args([
            'eggs',
            '--host', '1.2.3.4',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_localhost(self):
        args, extra = parse_args([
            'eggs',
            '--host', 'localhost',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('localhost', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--host', '1.2.3.4',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_single_session(self):
        args, extra = parse_args([
            'eggs',
            '--single-session',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('localhost', 8888),
            'nodebug': False,
            'single_session': True,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_local_single_session(self):
        args, extra = parse_args([
            'eggs',
            '--single-session',
            '--server-host', '1.2.3.4',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': True,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_remote_wait(self):
        args, extra = parse_args([
            'eggs',
            '--host', '1.2.3.4',
            '--port', '8888',
            '--wait',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': True,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_extra(self):
        args, extra = parse_args([
            'eggs',
            '--DEBUG',
            '--port', '8888',
            '--vm_type', '???',
            'spam.py',
            '--xyz', '123',
            'abc',
            '--cmd-line',
            '--',
            'foo',
            '--server',
            '--bar'
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, [
            '--DEBUG',
            '--vm_type', '???',
            '--',  # Expected pydevd defaults separator
            '--xyz', '123',
            'abc',
            '--cmd-line',
            'foo',
            '--server',
            '--bar',
        ])

    def test_extra_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--DEBUG',
            '--nodebug',
            '--port', '8888',
            '--vm_type', '???',
            'spam.py',
            '--xyz', '123',
            'abc',
            '--cmd-line',
            '--',
            'foo',
            '--server',
            '--bar'
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, [
            '--DEBUG',
            '--vm_type', '???',
            '--',  # Expected pydevd defaults separator
            '--xyz', '123',
            'abc',
            '--cmd-line',
            'foo',
            '--server',
            '--bar',
        ])

    def test_empty_host(self):
        args, extra = parse_args([
            'eggs',
            '--host', '',
            '--port', '8888',
            'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server('', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_unsupported_arg(self):
        with self.assertRaises(SystemExit):
            with captured_stdio():
                parse_args([
                    'eggs',
                    '--port', '8888',
                    '--xyz', '123',
                    'spam.py',
                ])

    def test_backward_compatibility_host(self):
        args, extra = parse_args([
            'eggs',
            '--client', '1.2.3.4',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_host_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--client', '1.2.3.4',
            '--port', '8888',
            '-m', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client('1.2.3.4', 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_module(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '--module',
            '--file', 'spam:',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_module_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '--module',
            '--file', 'spam:',
        ])

        self.assertEqual(vars(args), {
            'kind': 'module',
            'name': 'spam',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_script(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '--file', 'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_backward_compatibility_script_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '--file', 'spam.py',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam.py',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, self.EXPECTED_EXTRA)

    def test_pseudo_backward_compatibility(self):
        args, extra = parse_args([
            'eggs',
            '--port', '8888',
            '--module',
            '--file', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam',
            'address': Address.as_server(None, 8888),
            'nodebug': False,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, ['--module'] + self.EXPECTED_EXTRA)

    def test_pseudo_backward_compatibility_nodebug(self):
        args, extra = parse_args([
            'eggs',
            '--nodebug',
            '--port', '8888',
            '--module',
            '--file', 'spam',
        ])

        self.assertEqual(vars(args), {
            'kind': 'script',
            'name': 'spam',
            'address': Address.as_client(None, 8888),
            'nodebug': True,
            'single_session': False,
            'wait': False,
        })
        self.assertEqual(extra, ['--module'] + self.EXPECTED_EXTRA)
/n/n/ntests/system_tests/test_connection.py/n/nfrom __future__ import print_function

import contextlib
import os
import time
import sys
import unittest

import ptvsd._util
from ptvsd.socket import create_client, close_socket
from tests.helpers.proc import Proc
from tests.helpers.workspace import Workspace


@contextlib.contextmanager
def _retrier(timeout=1, persec=10, max=None, verbose=False):
    steps = int(timeout * persec) + 1
    delay = 1.0 / persec

    @contextlib.contextmanager
    def attempt(num):
        if verbose:
            print('*', end='')
            sys.stdout.flush()
        yield
        if verbose:
            if num % persec == 0:
                print()
            elif (num * 2) % persec == 0:
                print(' ', end='')

    def attempts():
        # The first attempt always happens.
        num = 1
        with attempt(num):
            yield num
        for num in range(2, steps):
            if max is not None and num > max:
                raise RuntimeError('too many attempts (max {})'.format(max))
            time.sleep(delay)
            with attempt(num):
                yield num
        else:
            raise RuntimeError('timed out')
    yield attempts()
    if verbose:
        print()


class RawConnectionTests(unittest.TestCase):

    VERBOSE = False
    #VERBOSE = True

    def setUp(self):
        super(RawConnectionTests, self).setUp()
        self.workspace = Workspace()
        self.addCleanup(self.workspace.cleanup)

    def _propagate_verbose(self):
        if not self.VERBOSE:
            return

        def unset():
            Proc.VERBOSE = False
            ptvsd._util.DEBUG = False
        self.addCleanup(unset)
        Proc.VERBOSE = True
        ptvsd._util.DEBUG = True

    def _wait_for_ready(self, rpipe):
        if self.VERBOSE:
            print('waiting for ready')
        line = b''
        while True:
            c = os.read(rpipe, 1)
            line += c
            if c == b'\n':
                if self.VERBOSE:
                    print(line.decode('utf-8'), end='')
                if b'getting session socket' in line:
                    break
                line = b''

    @unittest.skip('there is a race here under travis')
    def test_repeated(self):
        def debug(msg):
            if not self.VERBOSE:
                return
            print(msg)

        def connect(addr, wait=None, closeonly=False):
            sock = create_client()
            try:
                sock.settimeout(1)
                sock.connect(addr)
                debug('>connected')
                if wait is not None:
                    debug('>waiting')
                    time.sleep(wait)
            finally:
                debug('>closing')
                if closeonly:
                    sock.close()
                else:
                    close_socket(sock)
        filename = self.workspace.write('spam.py', content=""""""
            raise Exception('should never run')
            """""")
        addr = ('localhost', 5678)
        self._propagate_verbose()
        rpipe, wpipe = os.pipe()
        self.addCleanup(lambda: os.close(rpipe))
        self.addCleanup(lambda: os.close(wpipe))
        proc = Proc.start_python_module('ptvsd', [
            '--server',
            '--wait',
            '--port', '5678',
            '--file', filename,
        ], env={
            'PTVSD_DEBUG': '1',
            'PTVSD_SOCKET_TIMEOUT': '1',
        }, stdout=wpipe)
        with proc:
            # Wait for the server to spin up.
            debug('>a')
            with _retrier(timeout=3, verbose=self.VERBOSE) as attempts:
                for _ in attempts:
                    try:
                        connect(addr)
                        break
                    except Exception:
                        pass
            self._wait_for_ready(rpipe)
            debug('>b')
            connect(addr)
            self._wait_for_ready(rpipe)
            # We should be able to handle more connections.
            debug('>c')
            connect(addr)
            self._wait_for_ready(rpipe)
            # Give ptvsd long enough to try sending something.
            debug('>d')
            connect(addr, wait=0.2)
            self._wait_for_ready(rpipe)
            debug('>e')
            connect(addr)
            self._wait_for_ready(rpipe)
            debug('>f')
            connect(addr, closeonly=True)
            self._wait_for_ready(rpipe)
            debug('>g')
            connect(addr)
            self._wait_for_ready(rpipe)
            debug('>h')
            connect(addr)
            self._wait_for_ready(rpipe)
/n/n/n",0
33,33,f453ed1c417993eab4fc7b3c5288208d97270d13,"/ptvsd/__main__.py/n/n# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import argparse
import os.path
import sys

from ptvsd._local import debug_main, run_main
from ptvsd.socket import Address
from ptvsd.version import __version__, __author__  # noqa


##################################
# the script

""""""
For the PyDevd CLI handling see:

  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py
  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)
""""""  # noqa

PYDEVD_OPTS = {
    '--file',
    '--client',
    #'--port',
    '--vm_type',
}

PYDEVD_FLAGS = {
    '--DEBUG',
    '--DEBUG_RECORD_SOCKET_READS',
    '--cmd-line',
    '--module',
    '--multiproc',
    '--multiprocess',
    '--print-in-debugger-startup',
    '--save-signatures',
    '--save-threading',
    '--save-asyncio',
    '--server',
    '--qt-support=auto',
}

USAGE = """"""
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]
""""""  # noqa


def parse_args(argv=None):
    """"""Return the parsed args to use in main().""""""
    if argv is None:
        argv = sys.argv
        prog = argv[0]
        if prog == __file__:
            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))
    else:
        prog = argv[0]
    argv = argv[1:]

    supported, pydevd, script = _group_args(argv)
    args = _parse_args(prog, supported)
    # '--' is used in _run_args to extract pydevd specific args
    extra = pydevd + ['--']
    if script:
        extra += script
    return args, extra


def _group_args(argv):
    supported = []
    pydevd = []
    script = []

    try:
        pos = argv.index('--')
    except ValueError:
        script = []
    else:
        script = argv[pos + 1:]
        argv = argv[:pos]

    for arg in argv:
        if arg == '-h' or arg == '--help':
            return argv, [], script

    gottarget = False
    skip = 0
    for i in range(len(argv)):
        if skip:
            skip -= 1
            continue

        arg = argv[i]
        try:
            nextarg = argv[i + 1]
        except IndexError:
            nextarg = None

        # TODO: Deprecate the PyDevd arg support.
        # PyDevd support
        if gottarget:
            script = argv[i:] + script
            break
        if arg == '--client':
            arg = '--host'
        elif arg == '--file':
            if nextarg is None:  # The filename is missing...
                pydevd.append(arg)
                continue  # This will get handled later.
            if nextarg.endswith(':') and '--module' in pydevd:
                pydevd.remove('--module')
                arg = '-m'
                argv[i + 1] = nextarg = nextarg[:-1]
            else:
                arg = nextarg
                skip += 1

        if arg in PYDEVD_OPTS:
            pydevd.append(arg)
            if nextarg is not None:
                pydevd.append(nextarg)
            skip += 1
        elif arg in PYDEVD_FLAGS:
            pydevd.append(arg)
        elif arg == '--nodebug':
            supported.append(arg)

        # ptvsd support
        elif arg in ('--host', '--server-host', '--port', '-m'):
            if arg == '-m':
                gottarget = True
            supported.append(arg)
            if nextarg is not None:
                supported.append(nextarg)
            skip += 1
        elif arg in ('--single-session',):
            supported.append(arg)
        elif not arg.startswith('-'):
            supported.append(arg)
            gottarget = True

        # unsupported arg
        else:
            supported.append(arg)
            break

    return supported, pydevd, script


def _parse_args(prog, argv):
    parser = argparse.ArgumentParser(
        prog=prog,
        usage=USAGE.format(prog),
    )
    parser.add_argument('--nodebug', action='store_true')
    host = parser.add_mutually_exclusive_group()
    host.add_argument('--host')
    host.add_argument('--server-host')
    parser.add_argument('--port', type=int, required=True)

    target = parser.add_mutually_exclusive_group(required=True)
    target.add_argument('-m', dest='module')
    target.add_argument('filename', nargs='?')

    parser.add_argument('--single-session', action='store_true')
    parser.add_argument('-V', '--version', action='version')
    parser.version = __version__

    args = parser.parse_args(argv)
    ns = vars(args)

    serverhost = ns.pop('server_host', None)
    clienthost = ns.pop('host', None)
    if serverhost:
        args.address = Address.as_server(serverhost, ns.pop('port'))
    elif not clienthost:
        if args.nodebug:
            args.address = Address.as_client(clienthost, ns.pop('port'))
        else:
            args.address = Address.as_server(clienthost, ns.pop('port'))
    else:
        args.address = Address.as_client(clienthost, ns.pop('port'))

    module = ns.pop('module')
    filename = ns.pop('filename')
    if module is None:
        args.name = filename
        args.kind = 'script'
    else:
        args.name = module
        args.kind = 'module'
    #if argv[-1] != args.name or (module and argv[-1] != '-m'):
    #    parser.error('script/module must be last arg')

    return args


def main(addr, name, kind, extra=(), nodebug=False, **kwargs):
    if nodebug:
        run_main(addr, name, kind, *extra, **kwargs)
    else:
        debug_main(addr, name, kind, *extra, **kwargs)


if __name__ == '__main__':
    args, extra = parse_args()
    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,
         singlesession=args.single_session)
/n/n/n/tests/helpers/debugclient.py/n/nfrom __future__ import absolute_import

import os
import traceback
import warnings

from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .debugadapter import DebugAdapter, wait_for_socket_server
from .debugsession import DebugSession

# TODO: Add a helper function to start a remote debugger for testing
# remote debugging?


class _LifecycleClient(Closeable):

    SESSION = DebugSession

    def __init__(
            self,
            addr=None,
            port=8888,
            breakpoints=None,
            connecttimeout=1.0,
    ):
        super(_LifecycleClient, self).__init__()
        self._addr = Address.from_raw(addr, defaultport=port)
        self._connecttimeout = connecttimeout
        self._adapter = None
        self._session = None

        self._breakpoints = breakpoints

    @property
    def adapter(self):
        return self._adapter

    @property
    def session(self):
        return self._session

    def start_debugging(self, launchcfg):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        raise NotImplementedError

    def stop_debugging(self):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')

        if self._session is not None:
            self._detach()

        try:
            self._adapter.close()
        except ClosedError:
            pass
        self._adapter = None

    def attach_pid(self, pid, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        raise NotImplementedError

    def attach_socket(self, addr=None, adapter=None, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if adapter is None:
            adapter = self._adapter
        elif self._adapter is not None:
            raise RuntimeError('already using managed adapter')
        if adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        if addr is None:
            addr = adapter.address
        self._attach(addr, **kwargs)
        return self._session

    def detach(self, adapter=None):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._session is None:
            raise RuntimeError('not attached')
        if adapter is None:
            adapter = self._adapter
        assert adapter is not None
        if not self._session.is_client:
            raise RuntimeError('detach not supported')

        self._detach()

    # internal methods

    def _close(self):
        if self._session is not None:
            try:
                self._session.close()
            except ClosedError:
                pass
        if self._adapter is not None:
            try:
                self._adapter.close()
            except ClosedError:
                pass

    def _launch(self,
                argv,
                script=None,
                wait_for_connect=None,
                detachable=True,
                env=None,
                cwd=None,
                **kwargs):
        if script is not None:
            def start(*args, **kwargs):
                return DebugAdapter.start_wrapper_script(
                    script, *args, **kwargs)
        else:
            start = DebugAdapter.start
        new_addr = Address.as_server if detachable else Address.as_client
        addr = new_addr(None, self._addr.port)
        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)

        if wait_for_connect:
            wait_for_connect()
        else:
            wait_for_socket_server(addr)
            self._attach(addr, **kwargs)

    def _attach(self, addr, **kwargs):
        if addr is None:
            addr = self._addr
        assert addr.host == 'localhost'
        self._session = self.SESSION.create_client(addr, **kwargs)

    def _detach(self):
        session = self._session
        if session is None:
            return
        self._session = None
        try:
            session.close()
        except ClosedError:
            pass


class DebugClient(_LifecycleClient):
    """"""A high-level abstraction of a debug client (i.e. editor).""""""

    # TODO: Manage breakpoints, etc.
    # TODO: Add debugger methods here (e.g. ""pause"").


class EasyDebugClient(DebugClient):
    def start_detached(self, argv):
        """"""Start an adapter in a background process.""""""
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        # TODO: Launch, handshake and detach?
        self._adapter = DebugAdapter.start(argv, port=self._port)
        return self._adapter

    def host_local_debugger(self,
                            argv,
                            script=None,
                            env=None,
                            cwd=None,
                            **kwargs):  # noqa
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None
        addr = ('localhost', self._addr.port)

        self._run_server_ex = None

        def run():
            try:
                self._session = self.SESSION.create_server(addr, **kwargs)
            except Exception as ex:
                self._run_server_ex = traceback.format_exc()

        t = new_hidden_thread(
            target=run,
            name='test.client',
        )
        t.start()

        def wait():
            t.join(timeout=self._connecttimeout)
            if t.is_alive():
                warnings.warn('timed out waiting for connection')
            if self._session is None:
                message = 'unable to connect after {} secs'.format(  # noqa
                    self._connecttimeout)
                if self._run_server_ex is None:
                    raise Exception(message)
                else:
                    message = message + os.linesep + self._run_server_ex # noqa
                    raise Exception(message)

            # The adapter will close when the connection does.

        self._launch(
            argv,
            script=script,
            wait_for_connect=wait,
            detachable=False,
            env=env,
            cwd=cwd)

        return self._adapter, self._session

    def launch_script(self, filename, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            filename,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session

    def launch_module(self, module, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            '-m',
            module,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session
/n/n/n/tests/helpers/debugsession.py/n/nfrom __future__ import absolute_import, print_function

import contextlib
import json
import socket
import sys
import time
import threading
import warnings

from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .message import (
    raw_read_all as read_messages,
    raw_write_one as write_message
)
from .socket import (
    Connection, create_server, create_client, close,
    recv_as_read, send_as_write,
    timeout as socket_timeout)
from .threading import get_locked_and_waiter
from .vsc import parse_message


class DebugSessionConnection(Closeable):

    VERBOSE = False
    #VERBOSE = True

    TIMEOUT = 5.0

    @classmethod
    def create_client(cls, addr, **kwargs):
        def connect(addr, timeout):
            sock = create_client()
            for _ in range(int(timeout * 10)):
                try:
                    sock.connect(addr)
                except (OSError, socket.error):
                    if cls.VERBOSE:
                        print('+', end='')
                        sys.stdout.flush()
                    time.sleep(0.1)
                else:
                    break
            else:
                raise RuntimeError('could not connect')
            return sock
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def create_server(cls, addr, **kwargs):
        def connect(addr, timeout):
            server = create_server(addr)
            with socket_timeout(server, timeout):
                client, _ = server.accept()
            return Connection(client, server)
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def _create(cls, connect, addr, timeout=None):
        if timeout is None:
            timeout = cls.TIMEOUT
        sock = connect(addr, timeout)
        if cls.VERBOSE:
            print('connected')
        self = cls(sock, ownsock=True)
        self._addr = addr
        return self

    def __init__(self, sock, ownsock=False):
        super(DebugSessionConnection, self).__init__()
        self._sock = sock
        self._ownsock = ownsock

    @property
    def is_client(self):
        try:
            return self._sock.server is None
        except AttributeError:
            return True

    def iter_messages(self):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        read = recv_as_read(self._sock)
        for msg, _, _ in read_messages(read, stop=stop):
            if self.VERBOSE:
                print(repr(msg))
            yield parse_message(msg)

    def send(self, req):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        write = send_as_write(self._sock)
        body = json.dumps(req)
        write_message(write, body, stop=stop)

    # internal methods

    def _close(self):
        if self._ownsock:
            close(self._sock)


class DebugSession(Closeable):

    VERBOSE = False
    #VERBOSE = True

    HOST = 'localhost'
    PORT = 8888

    TIMEOUT = None

    @classmethod
    def create_client(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_client(
            addr,
            timeout=kwargs.get('timeout'),
        )
        return cls(conn, owned=True, **kwargs)

    @classmethod
    def create_server(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_server(addr, **kwargs)
        return cls(conn, owned=True, **kwargs)

    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):
        super(DebugSession, self).__init__()
        self._conn = conn
        self._seq = seq
        self._timeout = timeout
        self._owned = owned

        self._handlers = []
        for handler in handlers:
            if callable(handler):
                self._add_handler(handler)
            else:
                self._add_handler(*handler)
        self._received = []
        self._listenerthread = new_hidden_thread(
            target=self._listen,
            name='test.session',
        )
        self._listenerthread.start()

    @property
    def is_client(self):
        return self._conn.is_client

    @property
    def received(self):
        return list(self._received)

    def _create_request(self, command, **args):
        seq = self._seq
        self._seq += 1
        return {
            'type': 'request',
            'seq': seq,
            'command': command,
            'arguments': args,
        }

    def send_request(self, command, **args):
        if self.closed:
            raise RuntimeError('session closed')

        wait = args.pop('wait', False)
        req = self._create_request(command, **args)
        if self.VERBOSE:
            msg = parse_message(req)
            print(' <-', msg)

        if wait:
            with self.wait_for_response(req) as resp:
                self._conn.send(req)
            resp_awaiter = AwaitableResponse(req, lambda: resp[""msg""])
        else:
            resp_awaiter = self._get_awaiter_for_request(req, **args)
            self._conn.send(req)
        return resp_awaiter

    def add_handler(self, handler, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        self._add_handler(handler, **kwargs)

    @contextlib.contextmanager
    def wait_for_event(self, event, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event
        handlername = 'event {!r}'.format(event)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event and condition(msg) # noqa
        handlername = 'event {!r}'.format(event)
        evt = self._get_message_handle(match, handlername)

        return AwaitableEvent(event, lambda: result[""msg""], evt)

    def _get_awaiter_for_request(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        evt = self._get_message_handle(match, handlername)

        return AwaitableResponse(req, lambda: result[""msg""], evt)

    @contextlib.contextmanager
    def wait_for_response(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    # internal methods

    def _close(self):
        if self._owned:
            try:
                self._conn.close()
            except ClosedError:
                pass
        if self._listenerthread != threading.current_thread():
            self._listenerthread.join(timeout=1.0)
            if self._listenerthread.is_alive():
                warnings.warn('session listener still running')
        self._check_handlers()

    def _listen(self):
        try:
            for msg in self._conn.iter_messages():
                if self.VERBOSE:
                    print(' ->', msg)
                self._receive_message(msg)
        except EOFError:
            try:
                self.close()
            except ClosedError:
                pass

    def _receive_message(self, msg):
        for i, handler in enumerate(list(self._handlers)):
            handle_message, _, _ = handler
            handled = handle_message(msg)
            try:
                msg, handled = handled
            except TypeError:
                pass
            if handled:
                self._handlers.remove(handler)
                break
        self._received.append(msg)

    def _add_handler(self, handle_msg, handlername=None, required=True):
        self._handlers.append(
            (handle_msg, handlername, required))

    def _check_handlers(self):
        unhandled = []
        for handle_msg, name, required in self._handlers:
            if not required:
                continue
            unhandled.append(name or repr(handle_msg))
        if unhandled:
            raise RuntimeError('unhandled: {}'.format(unhandled))

    @contextlib.contextmanager
    def _wait_for_message(self, match, handlername, timeout=None):
        if timeout is None:
            timeout = self.TIMEOUT
        lock, wait = get_locked_and_waiter()

        def handler(msg):
            if not match(msg):
                return msg, False
            lock.release()
            return msg, True
        self._add_handler(handler, handlername)
        try:
            yield
        finally:
            wait(timeout or self._timeout, handlername, fail=True)

    def _get_message_handle(self, match, handlername):
        event = threading.Event()

        def handler(msg):
            if not match(msg):
                return msg, False
            event.set()
            return msg, True
        self._add_handler(handler, handlername, False)
        return event


class Awaitable(object):

    @classmethod
    def wait_all(cls, *awaitables):
        timeout = 3.0
        messages = []
        for _ in range(int(timeout * 10)):
            time.sleep(0.1)
            messages = []
            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa
            for awaitable in not_ready:
                if isinstance(awaitable, AwaitableEvent):
                    messages.append('Event {}'.format(awaitable.name))
                else:
                    messages.append('Response {}'.format(awaitable.name))
            if len(messages) == 0:
                return
        else:
            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa

    def __init__(self, name, event=None):
        self._event = event
        self.name = name

    def wait(self, timeout=1.0):
        if self._event is None:
            return
        if not self._event.wait(timeout):
            message = 'Timeout waiting for '
            if isinstance(self, AwaitableEvent):
                message += 'Event {}'.format(self.name)
            else:
                message += 'Response {}'.format(self.name)
            raise TimeoutError(message)


class AwaitableResponse(Awaitable):

    def __init__(self, req, result_getter, event=None):
        super(AwaitableResponse, self).__init__(req[""command""], event)
        self.req = req
        self._result_getter = result_getter

    @property
    def resp(self):
        return self._result_getter()


class AwaitableEvent(Awaitable):

    def __init__(self, name, result_getter, event=None):
        super(AwaitableEvent, self).__init__(name, event)
        self._result_getter = result_getter

    @property
    def event(self):
        return self._result_getter()
/n/n/n",1
8,8,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
import re
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.text import encode_for_xml
from invenio.utils.filedownload import download_url

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""
    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        line_nb = len(text_lines)
        word_nb = 0
        for line in text_lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))
        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            local_file = download_url(entry)
            text_lines = extractor.text_lines_from_local_file(local_file)
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Output keywords reading a local file.

    Arguments and output are the same as for :see: get_keywords_from_text().
    """"""
    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings.

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""
    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext.

    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/ninvenio/legacy/bibclassify/ontology_reader.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify ontology reader.

The ontology reader reads currently either a RDF/SKOS taxonomy or a
simple controlled vocabulary file (1 word per line). The first role of
this module is to manage the cached version of the ontology file. The
second role is to hold all methods responsible for the creation of
regular expressions. These methods are grammatically related as we take
care of different forms of the same words.  The grammatical rules can be
configured via the configuration file.

The main method from this module is get_regular_expressions.
""""""

from __future__ import print_function

from datetime import datetime, timedelta
from six import iteritems
from six.moves import cPickle

import os
import re
import sys
import tempfile
import time
import urllib2
import traceback
import xml.sax
import thread
import rdflib

from invenio.legacy.bibclassify import config as bconfig
from invenio.modules.classifier.errors import TaxonomyError

log = bconfig.get_logger(""bibclassify.ontology_reader"")
from invenio import config

from invenio.modules.classifier.registry import taxonomies

# only if not running in a stanalone mode
if bconfig.STANDALONE:
    dbquery = None
    from urllib2 import urlopen
else:
    from invenio.legacy import dbquery
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

_contains_digit = re.compile(""\d"")
_starts_with_non = re.compile(""(?i)^non[a-z]"")
_starts_with_anti = re.compile(""(?i)^anti[a-z]"")
_split_by_punctuation = re.compile(""(\W+)"")

_CACHE = {}


def get_cache(taxonomy_id):
    """"""Return thread-safe cache for the given taxonomy id.

    :param taxonomy_id: identifier of the taxonomy
    :type taxonomy_id: str

    :return: dictionary object (empty if no taxonomy_id
        is found), you must not change anything inside it.
        Create a new dictionary and use set_cache if you want
        to update the cache!
    """"""
    # Because of a standalone mode, we don't use the
    # invenio.data_cacher.DataCacher, but it has no effect
    # on proper functionality.

    if taxonomy_id in _CACHE:
        ctime, taxonomy = _CACHE[taxonomy_id]

        # check it is fresh version
        onto_name, onto_path, onto_url = _get_ontology(taxonomy_id)
        cache_path = _get_cache_path(onto_name)

        # if source exists and is newer than the cache hold in memory
        if os.path.isfile(onto_path) and os.path.getmtime(onto_path) > ctime:
            log.info('Forcing taxonomy rebuild as cached'
                     ' version is newer/updated.')
            return {}  # force cache rebuild

        # if cache exists and is newer than the cache hold in memory
        if os.path.isfile(cache_path) and os.path.getmtime(cache_path) > ctime:
            log.info('Forcing taxonomy rebuild as source'
                     ' file is newer/updated.')
            return {}
        log.info('Taxonomy retrieved from cache')
        return taxonomy
    return {}


def set_cache(taxonomy_id, contents):
    """"""Update cache in a thread-safe manner.""""""
    lock = thread.allocate_lock()
    lock.acquire()
    try:
        _CACHE[taxonomy_id] = (time.time(), contents)
    finally:
        lock.release()


def get_regular_expressions(taxonomy_name, rebuild=False, no_cache=False):
    """"""Return a list of patterns compiled from the RDF/SKOS ontology.

    Uses cache if it exists and if the taxonomy hasn't changed.
    """"""
    # Translate the ontology name into a local path. Check if the name
    # relates to an existing ontology.
    onto_name, onto_path, onto_url = _get_ontology(taxonomy_name)
    if not onto_path:
        raise TaxonomyError(""Unable to locate the taxonomy: '%s'.""
                            % taxonomy_name)

    cache_path = _get_cache_path(onto_name)
    log.debug('Taxonomy discovered, now we load it '
              '(from cache: %s, onto_path: %s, cache_path: %s)'
              % (not no_cache, onto_path, cache_path))

    if os.access(cache_path, os.R_OK):
        if os.access(onto_path, os.R_OK):
            if rebuild or no_cache:
                log.debug(""Cache generation was manually forced."")
                return _build_cache(onto_path, skip_cache=no_cache)
        else:
            # ontology file not found. Use the cache instead.
            log.warning(""The ontology couldn't be located. However ""
                        ""a cached version of it is available. Using it as a ""
                        ""reference."")
            return _get_cache(cache_path, source_file=onto_path)

        if (os.path.getmtime(cache_path) >
                os.path.getmtime(onto_path)):
            # Cache is more recent than the ontology: use cache.
            log.debug(""Normal situation, cache is older than ontology,""
                      "" so we load it from cache"")
            return _get_cache(cache_path, source_file=onto_path)
        else:
            # Ontology is more recent than the cache: rebuild cache.
            log.warning(""Cache '%s' is older than '%s'. ""
                        ""We will rebuild the cache"" %
                        (cache_path, onto_path))
            return _build_cache(onto_path, skip_cache=no_cache)

    elif os.access(onto_path, os.R_OK):
        if not no_cache and\
                os.path.exists(cache_path) and\
                not os.access(cache_path, os.W_OK):
            raise TaxonomyError('We cannot read/write into: %s. '
                                'Aborting!' % cache_path)
        elif not no_cache and os.path.exists(cache_path):
            log.warning('Cache %s exists, but is not readable!' % cache_path)
        log.info(""Cache not available. Building it now: %s"" % onto_path)
        return _build_cache(onto_path, skip_cache=no_cache)

    else:
        raise TaxonomyError(""We miss both source and cache""
                            "" of the taxonomy: %s"" % taxonomy_name)


def _get_remote_ontology(onto_url, time_difference=None):
    """"""Check if the online ontology is more recent than the local ontology.

    If yes, try to download and store it in Invenio's cache directory.

    Return a boolean describing the success of the operation.

    :return: path to the downloaded ontology.
    """"""
    if onto_url is None:
        return False

    dl_dir = ((config.CFG_CACHEDIR or tempfile.gettempdir()) + os.sep +
              ""bibclassify"" + os.sep)
    if not os.path.exists(dl_dir):
        os.mkdir(dl_dir)

    local_file = dl_dir + os.path.basename(onto_url)
    remote_modif_time = _get_last_modification_date(onto_url)
    try:
        local_modif_seconds = os.path.getmtime(local_file)
    except OSError:
        # The local file does not exist. Download the ontology.
        download = True
        log.info(""The local ontology could not be found."")
    else:
        local_modif_time = datetime(*time.gmtime(local_modif_seconds)[0:6])
        # Let's set a time delta of 1 hour and 10 minutes.
        time_difference = time_difference or timedelta(hours=1, minutes=10)
        download = remote_modif_time > local_modif_time + time_difference
        if download:
            log.info(""The remote ontology '%s' is more recent ""
                     ""than the local ontology."" % onto_url)

    if download:
        if not _download_ontology(onto_url, local_file):
            log.warning(""Error downloading the ontology from: %s"" % onto_url)

    return local_file


def _get_ontology(ontology):
    """"""Return the (name, path, url) to the short ontology name.

    :param ontology: name of the ontology or path to the file or url.
    """"""
    onto_name = onto_path = onto_url = None

    # first assume we got the path to the file
    if os.path.exists(ontology):
        onto_name = os.path.split(os.path.abspath(ontology))[1]
        onto_path = os.path.abspath(ontology)
        onto_url = """"
    else:
        # if not, try to find it in a known locations
        discovered_file = _discover_ontology(ontology)
        if discovered_file:
            onto_name = os.path.split(discovered_file)[1]
            onto_path = discovered_file
            # i know, this sucks
            x = ontology.lower()
            if ""http:"" in x or ""https:"" in x or ""ftp:"" in x or ""file:"" in x:
                onto_url = ontology
            else:
                onto_url = """"
        else:
            # not found, look into a database
            # (it is last because when bibclassify
            # runs in a standalone mode,
            # it has no database - [rca, old-heritage]
            if not bconfig.STANDALONE:
                result = dbquery.run_sql(""SELECT name, location from clsMETHOD WHERE name LIKE %s"",
                                         ('%' + ontology + '%',))
                for onto_short_name, url in result:
                    onto_name = onto_short_name
                    onto_path = _get_remote_ontology(url)
                    onto_url = url

    return (onto_name, onto_path, onto_url)


def _discover_ontology(ontology_name):
    """"""Look for the file in a known places.

    Inside invenio/etc/bibclassify and a few other places
    like current directory.

    :param ontology: name or path name or url
    :type ontology: str

    :return: absolute path of a file if found, or None
    """"""
    last_part = os.path.split(os.path.abspath(ontology_name))[1]
    if last_part in taxonomies:
        return taxonomies.get(last_part)
    elif last_part + "".rdf"" in taxonomies:
        return taxonomies.get(last_part + "".rdf"")
    else:
        log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)

    # LEGACY
    possible_patterns = [last_part, last_part.lower()]
    if not last_part.endswith('.rdf'):
        possible_patterns.append(last_part + '.rdf')
    places = [config.CFG_CACHEDIR,
              config.CFG_ETCDIR,
              os.path.join(config.CFG_CACHEDIR, ""bibclassify""),
              os.path.join(config.CFG_ETCDIR, ""bibclassify""),
              os.path.abspath('.'),
              os.path.abspath(os.path.join(os.path.dirname(__file__),
                                           ""../../../etc/bibclassify"")),
              os.path.join(os.path.dirname(__file__), ""bibclassify""),
              config.CFG_WEBDIR]

    log.debug(""Searching for taxonomy using string: %s"" % last_part)
    log.debug(""Possible patterns: %s"" % possible_patterns)
    for path in places:

        try:
            if os.path.isdir(path):
                log.debug(""Listing: %s"" % path)
                for filename in os.listdir(path):
                    #log.debug('Testing: %s' % filename)
                    for pattern in possible_patterns:
                        filename_lc = filename.lower()
                        if pattern == filename_lc and\
                                os.path.exists(os.path.join(path, filename)):
                            filepath = os.path.abspath(os.path.join(path,
                                                                    filename))
                            if (os.access(filepath, os.R_OK)):
                                log.debug(""Found taxonomy at: %s"" % filepath)
                                return filepath
                            else:
                                log.warning('Found taxonony at: %s, but it is'
                                            ' not readable. '
                                            'Continue searching...'
                                            % filepath)
        except OSError, os_error_msg:
            log.warning('OS Error when listing path '
                        '""%s"": %s' % (str(path), str(os_error_msg)))
    log.debug(""No taxonomy with pattern '%s' found"" % ontology_name)


class KeywordToken:

    """"""KeywordToken is a class used for the extracted keywords.

    It can be initialized with values from RDF store or from
    simple strings. Specialty of this class is that objects are
    hashable by subject - so in the dictionary two objects with the
    same subject appears as one -- :see: self.__hash__ and self.__cmp__.
    """"""

    def __init__(self, subject, store=None, namespace=None, type='HEP'):
        """"""Initialize KeywordToken with a subject.

        :param subject: string or RDF object
        :param store: RDF graph object
                      (will be used to get info about the subject)
        :param namespace: RDF namespace object, used together with store
        :param type: type of this keyword.
        """"""
        self.id = subject
        self.type = type
        self.short_id = subject
        self.concept = """"
        self.regex = []
        self.nostandalone = False
        self.spires = False
        self.fieldcodes = []
        self.compositeof = []
        self.core = False
        # True means composite keyword
        self._composite = '#Composite' in subject
        self.__hash = None

        # the tokens are coming possibly from a normal text file
        if store is None:
            subject = subject.strip()
            self.concept = subject
            self.regex = _get_searchable_regex(basic=[subject])
            self.nostandalone = False
            self.fieldcodes = []
            self.core = False
            if subject.find(' ') > -1:
                self._composite = True

        # definitions from rdf
        else:
            self.short_id = self.short_id.split('#')[-1]

            # find alternate names for this label
            basic_labels = []

            # turn those patterns into regexes only for simple keywords
            if self._composite is False:
                try:
                    for label in store.objects(subject,
                                               namespace[""prefLabel""]):
                        # XXX shall i make it unicode?
                        basic_labels.append(str(label))
                except TypeError:
                    pass
                self.concept = basic_labels[0]
            else:
                try:
                    self.concept = str(store.value(subject,
                                                   namespace[""prefLabel""],
                                                   any=True))
                except KeyError:
                    log.warning(""Keyword with subject %s has no prefLabel.""
                                "" We use raw name"" %
                                self.short_id)
                    self.concept = self.short_id

            # this is common both to composite and simple keywords
            try:
                for label in store.objects(subject, namespace[""altLabel""]):
                    basic_labels.append(str(label))
            except TypeError:
                pass

            # hidden labels are special (possibly regex) codes
            hidden_labels = []
            try:
                for label in store.objects(subject, namespace[""hiddenLabel""]):
                    hidden_labels.append(unicode(label))
            except TypeError:
                pass

            # compile regular expression that will identify this token
            self.regex = _get_searchable_regex(basic_labels, hidden_labels)

            try:
                for note in map(lambda s: str(s).lower().strip(),
                                store.objects(subject, namespace[""note""])):
                    if note == 'core':
                        self.core = True
                    elif note in (""nostandalone"", ""nonstandalone""):
                        self.nostandalone = True
                    elif 'fc:' in note:
                        self.fieldcodes.append(note[3:].strip())
            except TypeError:
                pass

            # spiresLabel does not have multiple values
            spires_label = store.value(subject, namespace[""spiresLabel""])
            if spires_label:
                self.spires = str(spires_label)

        # important for comparisons
        self.__hash = hash(self.short_id)

        # extract composite parts ids
        if store is not None and self.isComposite():
            small_subject = self.id.split(""#Composite."")[-1]
            component_positions = []
            for label in store.objects(self.id, namespace[""compositeOf""]):
                strlabel = str(label).split(""#"")[-1]
                component_name = label.split(""#"")[-1]
                component_positions.append((small_subject.find(component_name),
                                            strlabel))
            component_positions.sort()
            if not component_positions:
                log.error(""Keyword is marked as composite, ""
                          ""but no composite components refs found: %s""
                          % self.short_id)
            else:
                self.compositeof = map(lambda x: x[1], component_positions)

    def refreshCompositeOf(self, single_keywords, composite_keywords,
                           store=None, namespace=None):
        """"""Re-check sub-parts of this keyword.

        This should be called after the whole RDF was processed, because
        it is using a cache of single keywords and if that
        one is incomplete, you will not identify all parts.
        """"""
        def _get_ckw_components(new_vals, label):
            if label in single_keywords:
                new_vals.append(single_keywords[label])
            elif ('Composite.%s' % label) in composite_keywords:
                for l in composite_keywords['Composite.%s' % label].compositeof:
                    _get_ckw_components(new_vals, l)
            elif label in composite_keywords:
                for l in composite_keywords[label].compositeof:
                    _get_ckw_components(new_vals, l)
            else:
                # One single or composite keyword is missing from the taxonomy.
                # This is due to an error in the taxonomy description.
                message = ""The composite term \""%s\""""\
                          "" should be made of single keywords,""\
                          "" but at least one is missing."" % self.id
                if store is not None:
                    message += ""Needed components: %s""\
                               % list(store.objects(self.id,
                                      namespace[""compositeOf""]))
                message += "" Missing is: %s"" % label
                raise TaxonomyError(message)

        if self.compositeof:
            new_vals = []
            try:
                for label in self.compositeof:
                    _get_ckw_components(new_vals, label)
                self.compositeof = new_vals
            except TaxonomyError as err:
                # the composites will be empty
                # (better than to have confusing, partial matches)
                self.compositeof = []
                log.error(err)

    def isComposite(self):
        """"""Return value of _composite.""""""
        return self._composite

    def getComponents(self):
        """"""Return value of compositeof.""""""
        return self.compositeof

    def getType(self):
        """"""Return value of type.""""""
        return self.type

    def setType(self, value):
        """"""Set value of value.""""""
        self.type = value

    def __hash__(self):
        """"""Return _hash.

        This might change in the future but for the moment we want to
        think that if the concept is the same, then it is the same
        keyword - this sucks, but it is sort of how it is necessary
        to use now.
        """"""
        return self.__hash

    def __cmp__(self, other):
        """"""Compare objects using _hash.""""""
        if self.__hash < other.__hash__():
            return -1
        elif self.__hash == other.__hash__():
            return 0
        else:
            return 1

    def __str__(self, spires=False):
        """"""Return the best output for the keyword.""""""
        if spires:
            if self.spires:
                return self.spires
            elif self._composite:
                return self.concept.replace(':', ',')
            # default action
        return self.concept

    def output(self, spires=False):
        """"""Return string representation with spires value.""""""
        return self.__str__(spires=spires)

    def __repr__(self):
        """"""Class representation.""""""
        return ""<KeywordToken: %s>"" % self.short_id


def _build_cache(source_file, skip_cache=False):
    """"""Build the cached data.

    Either by parsing the RDF taxonomy file or a vocabulary file.

    :param source_file: source file of the taxonomy, RDF file
    :param skip_cache: if True, build cache will not be
        saved (pickled) - it is saved as <source_file.db>
    """"""
    store = rdflib.ConjunctiveGraph()

    if skip_cache:
        log.info(""You requested not to save the cache to disk."")
    else:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        # Make sure we have a cache_dir readable and writable.
        try:
            os.makedirs(cache_dir)
        except:
            pass
        if os.access(cache_dir, os.R_OK):
            if not os.access(cache_dir, os.W_OK):
                raise TaxonomyError(""Cache directory exists but is not""
                                    "" writable. Check your permissions""
                                    "" for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    timer_start = time.clock()

    namespace = None
    single_keywords, composite_keywords = {}, {}

    try:
        log.info(""Building RDFLib's conjunctive graph from: %s"" % source_file)
        try:
            store.parse(source_file)
        except urllib2.URLError:
            if source_file[0] == '/':
                store.parse(""file://"" + source_file)
            else:
                store.parse(""file:///"" + source_file)

    except rdflib.exceptions.Error as e:
        log.error(""Serious error reading RDF file"")
        log.error(e)
        log.error(traceback.format_exc())
        raise rdflib.exceptions.Error(e)

    except (xml.sax.SAXParseException, ImportError) as e:
        # File is not a RDF file. We assume it is a controlled vocabulary.
        log.error(e)
        log.warning(""The ontology file is probably not a valid RDF file. \
            Assuming it is a controlled vocabulary file."")

        filestream = open(source_file, ""r"")
        for line in filestream:
            keyword = line.strip()
            kt = KeywordToken(keyword)
            single_keywords[kt.short_id] = kt
        if not len(single_keywords):
            raise TaxonomyError('The ontology file is not well formated')

    else:  # ok, no exception happened
        log.info(""Now building cache of keywords"")
        # File is a RDF file.
        namespace = rdflib.Namespace(""http://www.w3.org/2004/02/skos/core#"")

        single_count = 0
        composite_count = 0

        subject_objects = store.subject_objects(namespace[""prefLabel""])
        for subject, pref_label in subject_objects:
            kt = KeywordToken(subject, store=store, namespace=namespace)
            if kt.isComposite():
                composite_count += 1
                composite_keywords[kt.short_id] = kt
            else:
                single_keywords[kt.short_id] = kt
                single_count += 1

    cached_data = {}
    cached_data[""single""] = single_keywords
    cached_data[""composite""] = composite_keywords
    cached_data[""creation_time""] = time.gmtime()
    cached_data[""version_info""] = {'rdflib': rdflib.__version__,
                                   'bibclassify': bconfig.VERSION}
    log.debug(""Building taxonomy... %d terms built in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    log.info(""Total count of single keywords: %d ""
             % len(single_keywords))
    log.info(""Total count of composite keywords: %d ""
             % len(composite_keywords))

    if not skip_cache:
        cache_path = _get_cache_path(source_file)
        cache_dir = os.path.dirname(cache_path)
        log.debug(""Writing the cache into: %s"" % cache_path)
        # test again, it could have changed
        if os.access(cache_dir, os.R_OK):
            if os.access(cache_dir, os.W_OK):
                # Serialize.
                filestream = None
                try:
                    filestream = open(cache_path, ""wb"")
                except IOError as msg:
                    # Impossible to write the cache.
                    log.error(""Impossible to write cache to '%s'.""
                              % cache_path)
                    log.error(msg)
                else:
                    log.debug(""Writing cache to file %s"" % cache_path)
                    cPickle.dump(cached_data, filestream, 1)
                if filestream:
                    filestream.close()

            else:
                raise TaxonomyError(""Cache directory exists but is not ""
                                    ""writable. Check your permissions ""
                                    ""for: %s"" % cache_dir)
        else:
            raise TaxonomyError(""Cache directory does not exist""
                                "" (and could not be created): %s"" % cache_dir)

    # now when the whole taxonomy was parsed,
    # find sub-components of the composite kws
    # it is important to keep this call after the taxonomy was saved,
    # because we don't  want to pickle regexes multiple times
    # (as they are must be re-compiled at load time)
    for kt in composite_keywords.values():
        kt.refreshCompositeOf(single_keywords, composite_keywords,
                              store=store, namespace=namespace)

    # house-cleaning
    if store:
        store.close()

    return (single_keywords, composite_keywords)


def _capitalize_first_letter(word):
    """"""Return a regex pattern with the first letter.

    Accepts both lowercase and uppercase.
    """"""
    if word[0].isalpha():
        # These two cases are necessary in order to get a regex pattern
        # starting with '[xX]' and not '[Xx]'. This allows to check for
        # colliding regex afterwards.
        if word[0].isupper():
            return ""["" + word[0].swapcase() + word[0] + ""]"" + word[1:]
        else:
            return ""["" + word[0] + word[0].swapcase() + ""]"" + word[1:]
    return word


def _convert_punctuation(punctuation, conversion_table):
    """"""Return a regular expression for a punctuation string.""""""
    if punctuation in conversion_table:
        return conversion_table[punctuation]
    return re.escape(punctuation)


def _convert_word(word):
    """"""Return the plural form of the word if it exists.

    Otherwise return the word itself.
    """"""
    out = None

    # Acronyms.
    if word.isupper():
        out = word + ""s?""
    # Proper nouns or word with digits.
    elif word.istitle():
        out = word + ""('?s)?""
    elif _contains_digit.search(word):
        out = word

    if out is not None:
        return out

    # Words with non or anti prefixes.
    if _starts_with_non.search(word):
        word = ""non-?"" + _capitalize_first_letter(_convert_word(word[3:]))
    elif _starts_with_anti.search(word):
        word = ""anti-?"" + _capitalize_first_letter(_convert_word(word[4:]))

    if out is not None:
        return _capitalize_first_letter(out)

    # A few invariable words.
    if word in bconfig.CFG_BIBCLASSIFY_INVARIABLE_WORDS:
        return _capitalize_first_letter(word)

    # Some exceptions that would not produce good results with the set of
    # general_regular_expressions.
    regexes = bconfig.CFG_BIBCLASSIFY_EXCEPTIONS
    if word in regexes:
        return _capitalize_first_letter(regexes[word])

    regexes = bconfig.CFG_BIBCLASSIFY_UNCHANGE_REGULAR_EXPRESSIONS
    for regex in regexes:
        if regex.search(word) is not None:
            return _capitalize_first_letter(word)

    regexes = bconfig.CFG_BIBCLASSIFY_GENERAL_REGULAR_EXPRESSIONS
    for regex, replacement in regexes:
        stemmed = regex.sub(replacement, word)
        if stemmed != word:
            return _capitalize_first_letter(stemmed)

    return _capitalize_first_letter(word + ""s?"")


def _get_cache(cache_file, source_file=None):
    """"""Get cached taxonomy using the cPickle module.

    No check is done at that stage.

    :param cache_file: full path to the file holding pickled data
    :param source_file: if we discover the cache is obsolete, we
        will build a new cache, therefore we need the source path
        of the cache
    :return: (single_keywords, composite_keywords).
    """"""
    timer_start = time.clock()

    filestream = open(cache_file, ""rb"")
    try:
        cached_data = cPickle.load(filestream)
        version_info = cached_data['version_info']
        if version_info['rdflib'] != rdflib.__version__\
                or version_info['bibclassify'] != bconfig.VERSION:
            raise KeyError
    except (cPickle.UnpicklingError, ImportError,
            AttributeError, DeprecationWarning, EOFError):
        log.warning(""The existing cache in %s is not readable. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        return _build_cache(source_file)
    except KeyError:
        log.warning(""The existing cache %s is not up-to-date. ""
                    ""Removing and rebuilding it."" % cache_file)
        filestream.close()
        os.remove(cache_file)
        if source_file and os.path.exists(source_file):
            return _build_cache(source_file)
        else:
            log.error(""The cache contains obsolete data (and it was deleted), ""
                      ""however I can't build a new cache, the source does not ""
                      ""exist or is inaccessible! - %s"" % source_file)
    filestream.close()

    single_keywords = cached_data[""single""]
    composite_keywords = cached_data[""composite""]

    # the cache contains only keys of the composite keywords, not the objects
    # so now let's resolve them into objects
    for kw in composite_keywords.values():
        kw.refreshCompositeOf(single_keywords, composite_keywords)

    log.debug(""Retrieved taxonomy from cache %s created on %s"" %
              (cache_file, time.asctime(cached_data[""creation_time""])))

    log.debug(""%d terms read in %.1f sec."" %
              (len(single_keywords) + len(composite_keywords),
               time.clock() - timer_start))

    return (single_keywords, composite_keywords)


def _get_cache_path(source_file):
    """"""Return the path where the cache should be written/located.

    :param onto_name: name of the ontology or the full path
    :return: string, abs path to the cache file in the tmpdir/bibclassify
    """"""
    local_name = os.path.basename(source_file)
    cache_name = local_name + "".db""
    cache_dir = os.path.join(config.CFG_CACHEDIR, ""bibclassify"")

    if not os.path.isdir(cache_dir):
        os.makedirs(cache_dir)

    return os.path.abspath(os.path.join(cache_dir, cache_name))


def _get_last_modification_date(url):
    """"""Get the last modification date of the ontology.""""""
    request = urllib2.Request(url)
    request.get_method = lambda: ""HEAD""
    http_file = urlopen(request)
    date_string = http_file.headers[""last-modified""]
    parsed = time.strptime(date_string, ""%a, %d %b %Y %H:%M:%S %Z"")
    return datetime(*(parsed)[0:6])


def _download_ontology(url, local_file):
    """"""Download the ontology and stores it in CFG_CACHEDIR.""""""
    log.debug(""Copying remote ontology '%s' to file '%s'."" % (url,
                                                              local_file))
    try:
        url_desc = urlopen(url)
        file_desc = open(local_file, 'w')
        file_desc.write(url_desc.read())
        file_desc.close()
    except IOError as e:
        print(e)
        return False
    except:
        log.warning(""Unable to download the ontology. '%s'"" %
                    sys.exc_info()[0])
        return False
    else:
        log.debug(""Done copying."")
        return True


def _get_searchable_regex(basic=None, hidden=None):
    """"""Return the searchable regular expressions for the single keyword.""""""
    # Hidden labels are used to store regular expressions.
    basic = basic or []
    hidden = hidden or []

    hidden_regex_dict = {}
    for hidden_label in hidden:
        if _is_regex(hidden_label):
            hidden_regex_dict[hidden_label] = \
                re.compile(
                    bconfig.CFG_BIBCLASSIFY_WORD_WRAP % hidden_label[1:-1]
                )
        else:
            pattern = _get_regex_pattern(hidden_label)
            hidden_regex_dict[hidden_label] = re.compile(
                bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
            )

    # We check if the basic label (preferred or alternative) is matched
    # by a hidden label regex. If yes, discard it.
    regex_dict = {}
    # Create regex for plural forms and add them to the hidden labels.
    for label in basic:
        pattern = _get_regex_pattern(label)
        regex_dict[label] = re.compile(
            bconfig.CFG_BIBCLASSIFY_WORD_WRAP % pattern
        )

    # Merge both dictionaries.
    regex_dict.update(hidden_regex_dict)

    return regex_dict.values()


def _get_regex_pattern(label):
    """"""Return a regular expression of the label.

    This takes care of plural and different kinds of separators.
    """"""
    parts = _split_by_punctuation.split(label)

    for index, part in enumerate(parts):
        if index % 2 == 0:
            # Word
            if not parts[index].isdigit() and len(parts[index]) > 1:
                parts[index] = _convert_word(parts[index])
        else:
            # Punctuation
            if not parts[index + 1]:
                # The separator is not followed by another word. Treat
                # it as a symbol.
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SYMBOLS
                )
            else:
                parts[index] = _convert_punctuation(
                    parts[index],
                    bconfig.CFG_BIBCLASSIFY_SEPARATORS
                )

    return """".join(parts)


def _is_regex(string):
    """"""Check if a concept is a regular expression.""""""
    return string[0] == ""/"" and string[-1] == ""/""


def check_taxonomy(taxonomy):
    """"""Check the consistency of the taxonomy.

    Outputs a list of errors and warnings.
    """"""
    log.info(""Building graph with Python RDFLib version %s"" %
             rdflib.__version__)

    store = rdflib.ConjunctiveGraph()

    try:
        store.parse(taxonomy)
    except:
        log.error(""The taxonomy is not a valid RDF file. Are you ""
                  ""trying to check a controlled vocabulary?"")
        raise TaxonomyError('Error in RDF file')

    log.info(""Graph was successfully built."")

    prefLabel = ""prefLabel""
    hiddenLabel = ""hiddenLabel""
    altLabel = ""altLabel""
    composite = ""composite""
    compositeOf = ""compositeOf""
    note = ""note""

    both_skw_and_ckw = []

    # Build a dictionary we will reason on later.
    uniq_subjects = {}
    for subject in store.subjects():
        uniq_subjects[subject] = None

    subjects = {}
    for subject in uniq_subjects:
        strsubject = str(subject).split(""#Composite."")[-1]
        strsubject = strsubject.split(""#"")[-1]
        if (strsubject == ""http://cern.ch/thesauri/HEPontology.rdf"" or
           strsubject == ""compositeOf""):
            continue
        components = {}
        for predicate, value in store.predicate_objects(subject):
            strpredicate = str(predicate).split(""#"")[-1]
            strobject = str(value).split(""#Composite."")[-1]
            strobject = strobject.split(""#"")[-1]
            components.setdefault(strpredicate, []).append(strobject)
        if strsubject in subjects:
            both_skw_and_ckw.append(strsubject)
        else:
            subjects[strsubject] = components

    log.info(""Taxonomy contains %s concepts."" % len(subjects))

    no_prefLabel = []
    multiple_prefLabels = []
    bad_notes = []
    # Subjects with no composite or compositeOf predicate
    lonely = []
    both_composites = []
    bad_hidden_labels = {}
    bad_alt_labels = {}
    # Problems with composite keywords
    composite_problem1 = []
    composite_problem2 = []
    composite_problem3 = []
    composite_problem4 = {}
    composite_problem5 = []
    composite_problem6 = []

    stemming_collisions = []
    interconcept_collisions = {}

    for subject, predicates in iteritems(subjects):
        # No prefLabel or multiple prefLabels
        try:
            if len(predicates[prefLabel]) > 1:
                multiple_prefLabels.append(subject)
        except KeyError:
            no_prefLabel.append(subject)

        # Lonely and both composites.
        if composite not in predicates and compositeOf not in predicates:
            lonely.append(subject)
        elif composite in predicates and compositeOf in predicates:
            both_composites.append(subject)

        # Multiple or bad notes
        if note in predicates:
            bad_notes += [(subject, n) for n in predicates[note]
                          if n not in ('nostandalone', 'core')]

        # Bad hidden labels
        if hiddenLabel in predicates:
            for lbl in predicates[hiddenLabel]:
                if lbl.startswith(""/"") ^ lbl.endswith(""/""):
                    bad_hidden_labels.setdefault(subject, []).append(lbl)

        # Bad alt labels
        if altLabel in predicates:
            for lbl in predicates[altLabel]:
                if len(re.findall(""/"", lbl)) >= 2 or "":"" in lbl:
                    bad_alt_labels.setdefault(subject, []).append(lbl)

        # Check composite
        if composite in predicates:
            for ckw in predicates[composite]:
                if ckw in subjects:
                    if compositeOf in subjects[ckw]:
                        if subject not in subjects[ckw][compositeOf]:
                            composite_problem3.append((subject, ckw))
                    else:
                        if ckw not in both_skw_and_ckw:
                            composite_problem2.append((subject, ckw))
                else:
                    composite_problem1.append((subject, ckw))

        # Check compositeOf
        if compositeOf in predicates:
            for skw in predicates[compositeOf]:
                if skw in subjects:
                    if composite in subjects[skw]:
                        if subject not in subjects[skw][composite]:
                            composite_problem6.append((subject, skw))
                    else:
                        if skw not in both_skw_and_ckw:
                            composite_problem5.append((subject, skw))
                else:
                    composite_problem4.setdefault(skw, []).append(subject)

        # Check for stemmed labels
        if compositeOf in predicates:
            labels = (altLabel, hiddenLabel)
        else:
            labels = (prefLabel, altLabel, hiddenLabel)

        patterns = {}
        for label in [lbl for lbl in labels if lbl in predicates]:
            for expression in [expr for expr in predicates[label]
                               if not _is_regex(expr)]:
                pattern = _get_regex_pattern(expression)
                interconcept_collisions.setdefault(pattern, []).\
                    append((subject, label))
                if pattern in patterns:
                    stemming_collisions.append(
                        (subject,
                         patterns[pattern],
                         (label, expression)
                         )
                    )
                else:
                    patterns[pattern] = (label, expression)

    print(""\n==== ERRORS ===="")

    if no_prefLabel:
        print(""\nConcepts with no prefLabel: %d"" % len(no_prefLabel))
        print(""\n"".join([""   %s"" % subj for subj in no_prefLabel]))
    if multiple_prefLabels:
        print((""\nConcepts with multiple prefLabels: %d"" %
               len(multiple_prefLabels)))
        print(""\n"".join([""   %s"" % subj for subj in multiple_prefLabels]))
    if both_composites:
        print((""\nConcepts with both composite properties: %d"" %
               len(both_composites)))
        print(""\n"".join([""   %s"" % subj for subj in both_composites]))
    if bad_hidden_labels:
        print(""\nConcepts with bad hidden labels: %d"" % len(bad_hidden_labels))
        for kw, lbls in iteritems(bad_hidden_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if bad_alt_labels:
        print(""\nConcepts with bad alt labels: %d"" % len(bad_alt_labels))
        for kw, lbls in iteritems(bad_alt_labels):
            print(""   %s:"" % kw)
            print(""\n"".join([""      '%s'"" % lbl for lbl in lbls]))
    if both_skw_and_ckw:
        print((""\nKeywords that are both skw and ckw: %d"" %
               len(both_skw_and_ckw)))
        print(""\n"".join([""   %s"" % subj for subj in both_skw_and_ckw]))

    print()

    if composite_problem1:
        print(""\n"".join([""SKW '%s' references an unexisting CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem1]))
    if composite_problem2:
        print(""\n"".join([""SKW '%s' references a SKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem2]))
    if composite_problem3:
        print(""\n"".join([""SKW '%s' is not composite of CKW '%s'."" %
                         (skw, ckw) for skw, ckw in composite_problem3]))
    if composite_problem4:
        for skw, ckws in iteritems(composite_problem4):
            print(""SKW '%s' does not exist but is "" ""referenced by:"" % skw)
            print(""\n"".join([""    %s"" % ckw for ckw in ckws]))
    if composite_problem5:
        print(""\n"".join([""CKW '%s' references a CKW '%s'."" % kw
                         for kw in composite_problem5]))
    if composite_problem6:
        print(""\n"".join([""CKW '%s' is not composed by SKW '%s'."" % kw
                         for kw in composite_problem6]))

    print(""\n==== WARNINGS ===="")

    if bad_notes:
        print((""\nConcepts with bad notes: %d"" % len(bad_notes)))
        print(""\n"".join([""   '%s': '%s'"" % _note for _note in bad_notes]))
    if stemming_collisions:
        print(""\nFollowing keywords have unnecessary labels that have ""
              ""already been generated by BibClassify."")
        for subj in stemming_collisions:
            print(""   %s:\n     %s\n     and %s"" % subj)

    print(""\nFinished."")
    sys.exit(0)


def test_cache(taxonomy_name='HEP', rebuild_cache=False, no_cache=False):
    """"""Test the cache lookup.""""""
    cache = get_cache(taxonomy_name)
    if not cache:
        set_cache(taxonomy_name, get_regular_expressions(taxonomy_name,
                                                         rebuild=rebuild_cache,
                                                         no_cache=no_cache))
        cache = get_cache(taxonomy_name)
    return (thread.get_ident(), cache)


log.info('Loaded ontology reader')

if __name__ == '__main__':
    test_cache()
/n/n/ninvenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014, 2015 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re

from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Check if a document is a PDF file and returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[-1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    # os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Return the fulltext of the local file.

    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""
    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    # Discard lines that do not contain at least one word.
    return [line for line in lines if _ONE_WORD.search(line) is not None]


def executable_exists(executable):
    """"""Test if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False
/n/n/n",0
9,9,4b56c071c54a0e1f1a86dca49fe455207d4148c7,"/invenio/legacy/bibclassify/engine.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
""""""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
""""""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger(""bibclassify.engine"")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode=""text"",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode=""full"", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """"""Output the keywords for each source in sources.""""""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == ""text"":
            print(""Input file: %s"" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info(""Trying to read input file %s."" % entry)
        text_lines = None
        source = """"
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,
                                                       user_agent=make_user_agent_string(""BibClassify""))
            if text_lines:
                source = entry.split(""/"")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode=""text"",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode=""full"", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """"""Outputs keywords reading a local file. Arguments and output are the same
    as for :see: get_keywords_from_text() """"""

    log.info(""Analyzing keywords for local file %s."" % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode=""text"",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode=""full"", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """"""Extract keywords from the list of strings

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """"""

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext(""\n"".join(text_lines))

    if match_mode == ""partial"":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the ""nonstandalone"" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """"""Find single keywords in the fulltext
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """"""Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """"""
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """"""Finds out human defined keyowrds in a text string. Searches for
    the string ""Keywords:"" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style=""text"", output_limit=0,
                        spires=False, only_core_tags=False):
    """"""Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """"""
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {""text"": _output_text, ""marcxml"": _output_marc, ""html"":
                 _output_html, ""dict"": _output_dict}
    my_styles = {}

    for s in style:
        if s != ""raw"":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles[""raw""] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles[""raw""] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """"""Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """"""
    output = ['<collection><record>\n'
              '<controlfield tag=""001"">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """"""Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC""""""

    kw_template = ('<datafield tag=""%s"" ind1=""%s"" ind2=""%s"">\n'
                   '    <subfield code=""2"">%s</subfield>\n'
                   '    <subfield code=""a"">%s</subfield>\n'
                   '    <subfield code=""n"">%s</subfield>\n'
                   '    <subfield code=""9"">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete[""Single keywords""], output_complete[""Core keywords""]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete[""Author keywords""]),
                            (acro_field, output_complete[""Acronyms""])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return """".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {""Core keywords"": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results[""Author keywords""] = _get_author_keywords(author_keywords, spires=spires)
        results[""Composite keywords""] = _get_compositekws(resized_ckw, spires=spires)
        results[""Single keywords""] = _get_singlekws(resized_skw, spires=spires)
        results[""Field codes""] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results[""Acronyms""] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        ""complete_output"": complete_output,
        ""categories"": categories
    }


def _output_text(complete_output, categories):
    """"""Output the results obtained in text format.


    :return: str, html formatted output
    """"""
    output = """"

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += ""\n\n{0}:\n"".format(result)
            for element in list_result_sorted:
                output += ""\n{0} {1}"".format(list_result[element], element)

    output += ""\n--\n{0}"".format(_signature())

    return output


def _output_html(complete_output, categories):
    """"""Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """"""
    return """"""<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>"""""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """"""
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """"""
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """"""
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {""numbers"": len(info[0]),
                                                    ""details"": info[1]}
    return output


def _get_acronyms(acronyms):
    """"""Return a formatted list of acronyms.""""""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = "", "".join([""%s (%d)"" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """"""Format the output for the author keywords.

    :return: list of formatted author keywors
    """"""
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string""""""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """"""Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """"""
    output = {}
    category = {}

    def _get_value_kw(kw):
        """"""Help to sort the Core keywords.""""""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """"""Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib""""""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """"""Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements""""""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """"""
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """"""
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """"""Turn list of keywords into dictionary.""""""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """"""Return a resized version of keywords to the given length.""""""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """"""
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text.""""""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return ""\n"".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, ""w"")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = ""%s/bibclassify"" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = ""bibclassify_%s.xml"" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """"""Parse marc field and return default indicators if not filled in.""""""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == ""__main__"":
    log.error(""Please use bibclassify_cli from now on."")
/n/n/n/invenio/legacy/bibclassify/text_extractor.py/n/n# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

""""""
BibClassify text extractor.

This module provides method to extract the fulltext from local or remote
documents. Currently 2 formats of documents are supported: PDF and text
documents.

2 methods provide the functionality of the module: text_lines_from_local_file
and text_lines_from_url.

This module also provides the utility 'is_pdf' that uses GNU file in order to
determine if a local file is a PDF file.

This module is STANDALONE safe
""""""

import os
import re
import tempfile
import urllib2
from invenio.legacy.bibclassify import config as bconfig

if bconfig.STANDALONE:
    from urllib2 import urlopen
else:
    from invenio.utils.url import make_invenio_opener

    urlopen = make_invenio_opener('BibClassify').open

log = bconfig.get_logger(""bibclassify.text_extractor"")

_ONE_WORD = re.compile(""[A-Za-z]{2,}"")


def is_pdf(document):
    """"""Checks if a document is a PDF file. Returns True if is is.""""""
    if not executable_exists('pdftotext'):
        log.warning(""GNU file was not found on the system. ""
                    ""Switching to a weak file extension test."")
        if document.lower().endswith("".pdf""):
            return True
        return False
        # Tested with file version >= 4.10. First test is secure and works
    # with file version 4.25. Second condition is tested for file
    # version 4.10.
    file_output = os.popen('file ' + re.escape(document)).read()
    try:
        filetype = file_output.split("":"")[1]
    except IndexError:
        log.error(""Your version of the 'file' utility seems to ""
                  ""be unsupported. Please report this to cds.support@cern.ch."")
        raise Exception('Incompatible pdftotext')

    pdf = filetype.find(""PDF"") > -1
    # This is how it should be done however this is incompatible with
    # file version 4.10.
    #os.popen('file -bi ' + document).read().find(""application/pdf"")
    return pdf


def text_lines_from_local_file(document, remote=False):
    """"""Returns the fulltext of the local file.
    @var document: fullpath to the file that should be read
    @var remote: boolean, if True does not count lines (gosh!)
    @return: list of lines if st was read or an empty list""""""

    try:
        if is_pdf(document):
            if not executable_exists(""pdftotext""):
                log.error(""pdftotext is not available on the system."")
            cmd = ""pdftotext -q -enc UTF-8 %s -"" % re.escape(document)
            filestream = os.popen(cmd)
        else:
            filestream = open(document, ""r"")
    except IOError as ex1:
        log.error(""Unable to read from file %s. (%s)"" % (document, ex1.strerror))
        return []

    # FIXME - we assume it is utf-8 encoded / that is not good
    lines = [line.decode(""utf-8"", 'replace') for line in filestream]
    filestream.close()

    if not _is_english_text('\n'.join(lines)):
        log.warning(""It seems the file '%s' is unvalid and doesn't ""
                    ""contain text. Please communicate this file to the Invenio ""
                    ""team."" % document)

    line_nb = len(lines)
    word_nb = 0
    for line in lines:
        word_nb += len(re.findall(""\S+"", line))

    # Discard lines that do not contain at least one word.
    lines = [line for line in lines if _ONE_WORD.search(line) is not None]

    if not remote:
        log.info(""Local file has %d lines and %d words."" % (line_nb, word_nb))

    return lines


def _is_english_text(text):
    """"""
    Checks if a text is correct english.
    Computes the number of words in the text and compares it to the
    expected number of words (based on an average size of words of 5.1
    letters).

    @param text_lines: the text to analyze
    @type text_lines:  string
    @return:           True if the text is English, False otherwise
    @rtype:            Boolean
    """"""
    # Consider one word and one space.
    avg_word_length = 2.55 + 1
    expected_word_number = float(len(text)) / avg_word_length

    words = [word
             for word in re.split('\W', text)
             if word.isalpha()]

    word_number = len(words)

    return word_number > expected_word_number


def text_lines_from_url(url, user_agent=""""):
    """"""Returns the fulltext of the file found at the URL.""""""
    request = urllib2.Request(url)
    if user_agent:
        request.add_header(""User-Agent"", user_agent)
    try:
        distant_stream = urlopen(request)
        # Write the URL content to a temporary file.
        local_file = tempfile.mkstemp(prefix=""bibclassify."")[1]
        local_stream = open(local_file, ""w"")
        local_stream.write(distant_stream.read())
        local_stream.close()
    except:
        log.error(""Unable to read from URL %s."" % url)
        return None
    else:
        # Read lines from the temporary file.
        lines = text_lines_from_local_file(local_file, remote=True)
        os.remove(local_file)

        line_nb = len(lines)
        word_nb = 0
        for line in lines:
            word_nb += len(re.findall(""\S+"", line))

        log.info(""Remote file has %d lines and %d words."" % (line_nb, word_nb))

        return lines


def executable_exists(executable):
    """"""Tests if an executable is available on the system.""""""
    for directory in os.getenv(""PATH"").split("":""):
        if os.path.exists(os.path.join(directory, executable)):
            return True
    return False


/n/n/n",1
18,18,0cd7d78e4d806852fd75fee03c24cce322f76014,"chippyRuxpin.py/n/n#!/usr/bin/python
# Chippy Ruxpin by Next Thing Co
# Powered by C.H.I.P., the world's first $9 computer!

# apt-get install python-setuptools python-dev build-essential espeak alsa-utils
# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer

# IMPORTANT NOTE ABOUT TWITTER STUFF!
# In order to retrieve tweets, you need to authorize this code to use your twitter account.
# This involves obtaining some special tokens that are specific to you.
# Please visit Twitter's website to obtain this information and put the values in the variables below.
# For more information, visit this URL:
# https://dev.twitter.com/oauth/overview/application-owner-access-tokens

consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'
consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'
accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'
accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'

import sys
import time
import subprocess
import os
from random import randint
from threading import Thread
from chippyRuxpin_audioPlayer import AudioPlayer
from chippyRuxpin_gpio import GPIO
from chippyRuxpin_twitter import ChippyTwitter
from chippyRuxpin_webFramework import WebFramework

fullMsg = """"

MOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0
MOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2
EYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4
EYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6

io = GPIO() #Establish connection to our GPIO pins.
io.setup( MOUTH_OPEN )
io.setup( EYES_OPEN )
io.setup( MOUTH_CLOSE )
io.setup( EYES_CLOSE )

audio = None
isRunning = True

def updateMouth():
    lastMouthEvent = 0
    lastMouthEventTime = 0

    while( audio == None ):
        time.sleep( 0.1 )
        
    while isRunning:
        if( audio.mouthValue != lastMouthEvent ):
            lastMouthEvent = audio.mouthValue
            lastMouthEventTime = time.time()

            if( audio.mouthValue == 1 ):
                io.set( MOUTH_OPEN, 1 )
                io.set( MOUTH_CLOSE, 0 )
            else:
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 1 )
        else:
            if( time.time() - lastMouthEventTime > 0.4 ):
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 0 )

# A routine for blinking the eyes in a semi-random fashion.
def updateEyes():
    while isRunning:
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 1 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 0 )
        time.sleep( randint( 0,7) )
   
def talk(myText):
    if( myText.find( ""twitter"" ) >= 0 ):
        myText += ""0""
        myText = myText[7:-1]
        try:
	    myText = twitter.getTweet( myText )
	except:
	    print( ""!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."")
            return
    
    os.system( ""espeak \"",...\"" 2>/dev/null"" ) # Sometimes the beginning of audio can get cut off. Insert silence.
    time.sleep( 0.5 )
    subprocess.call([""espeak"", ""-w"", ""speech.wav"", myText, ""-s"", ""130""])
    audio.play(""speech.wav"")
    return myText

mouthThread = Thread(target=updateMouth)
mouthThread.start()
eyesThread = Thread(target=updateEyes)
eyesThread.start()     
audio = AudioPlayer()

if( consumerKey.find( 'TWITTER' ) >= 0 ):
    print( ""WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."" )    
else:
    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)

web = WebFramework(talk)
isRunning = False
io.cleanup()
sys.exit(1)
/n/n/n",0
19,19,0cd7d78e4d806852fd75fee03c24cce322f76014,"/chippyRuxpin.py/n/n#!/usr/bin/python
# Chippy Ruxpin by Next Thing Co
# Powered by C.H.I.P., the world's first $9 computer!

# apt-get install python-setuptools python-dev build-essential espeak alsa-utils
# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer

# IMPORTANT NOTE ABOUT TWITTER STUFF!
# In order to retrieve tweets, you need to authorize this code to use your twitter account.
# This involves obtaining some special tokens that are specific to you.
# Please visit Twitter's website to obtain this information and put the values in the variables below.
# For more information, visit this URL:
# https://dev.twitter.com/oauth/overview/application-owner-access-tokens

consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'
consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'
accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'
accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'

import sys
import time
import subprocess
import os
from random import randint
from threading import Thread
from chippyRuxpin_audioPlayer import AudioPlayer
from chippyRuxpin_gpio import GPIO
from chippyRuxpin_twitter import ChippyTwitter
from chippyRuxpin_webFramework import WebFramework

fullMsg = """"

MOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0
MOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2
EYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4
EYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6

io = GPIO() #Establish connection to our GPIO pins.
io.setup( MOUTH_OPEN )
io.setup( EYES_OPEN )
io.setup( MOUTH_CLOSE )
io.setup( EYES_CLOSE )

audio = None
isRunning = True

def updateMouth():
    lastMouthEvent = 0
    lastMouthEventTime = 0

    while( audio == None ):
        time.sleep( 0.1 )
        
    while isRunning:
        if( audio.mouthValue != lastMouthEvent ):
            lastMouthEvent = audio.mouthValue
            lastMouthEventTime = time.time()

            if( audio.mouthValue == 1 ):
                io.set( MOUTH_OPEN, 1 )
                io.set( MOUTH_CLOSE, 0 )
            else:
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 1 )
        else:
            if( time.time() - lastMouthEventTime > 0.4 ):
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 0 )

# A routine for blinking the eyes in a semi-random fashion.
def updateEyes():
    while isRunning:
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 1 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 0 )
        time.sleep( randint( 0,7) )
   
def talk(myText):
    if( myText.find( ""twitter"" ) >= 0 ):
        myText += ""0""
        myText = myText[7:-1]
        try:
	    myText = twitter.getTweet( myText )
	except:
	    print( ""!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."")
            return
    
    os.system( ""espeak \"",...\"" 2>/dev/null"" ) # Sometimes the beginning of audio can get cut off. Insert silence.
    time.sleep( 0.5 )
    os.system( ""espeak -w speech.wav \"""" + myText + ""\"" -s 130"" )
    audio.play(""speech.wav"")
    return myText

mouthThread = Thread(target=updateMouth)
mouthThread.start()
eyesThread = Thread(target=updateEyes)
eyesThread.start()     
audio = AudioPlayer()

if( consumerKey.find( 'TWITTER' ) >= 0 ):
    print( ""WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions."" )    
else:
    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)

web = WebFramework(talk)
isRunning = False
io.cleanup()
sys.exit(1)
/n/n/n",1
36,36,c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f,"Get_the_machine_learning_basics/Classification_Template/classification_template.py/n/n# Classification template

# Importing the libraries
import numpy as np
import matplotlib
matplotlib.use(""tkAgg"")
import matplotlib.pyplot as plt
import pandas as pd
import os

script_dir = os.path.dirname(__file__)
abs_file_path = os.path.join(script_dir, 'Social_Network_Ads.csv')

# Importing the dataset
dataset = pd.read_csv(abs_file_path)
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting classifier to the Training set
# Create your classifier here
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()/n/n/nGet_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py/n/n# Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os

script_dir = os.path.dirname(__file__)
abs_file_path = os.path.join(script_dir, 'Data.csv')

# Importing the dataset
dataset = pd.read_csv(abs_file_path)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values/n/n/nVolume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py/n/n# Artificial Neural Network

# Installing Theano
# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

# Installing Tensorflow
# pip install tensorflow

# Installing Keras
# pip install --upgrade keras

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os

script_dir = os.path.dirname(__file__)
abs_file_path = os.path.join(script_dir, 'Churn_Modelling.csv')

# Importing the dataset
dataset = pd.read_csv(abs_file_path)
X = dataset.iloc[:, 3:13].values
y = dataset.iloc[:, 13].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
onehotencoder = OneHotEncoder(categorical_features=[1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))

# Adding the second hidden layer
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))

# Adding the output layer
classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

# Compiling the ANN
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size=10, epochs=100)

# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)/n/n/n",0
37,37,c5bcd4582baf5e9e2e8460a0b0c3deb306f2a30f,"/Get_the_machine_learning_basics/Classification_Template/classification_template.py/n/n# Classification template

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting classifier to the Training set
# Create your classifier here

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()/n/n/n/Get_the_machine_learning_basics/Data_Preprocessing_Template/data_preprocessing_template.py/n/n# Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Data.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values/n/n/n/Volume_1-Supervised_Deep_Learning/Part_1-Artificial_Neural_Networks-ANN/Section_4-Building_an_ANN/ann.py/n/n# Artificial Neural Network

# Installing Theano
# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

# Installing Tensorflow
# pip install tensorflow

# Installing Keras
# pip install --upgrade keras

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13].values
y = dataset.iloc[:, 13].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
onehotencoder = OneHotEncoder(categorical_features = [1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)

# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)/n/n/n",1
48,48,9ea0c409e6cea69cce632079548165ad5a9f2554,"homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from time import time
import threading

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# This is the NetAtmo data upload interval in seconds
NETATMO_UPDATE_INTERVAL = 600

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', 'º', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', 'º', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None],
    'lastupdated': ['Last Updated', 's', 'mdi:timer', None],
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exists
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensors for monitored properties
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""
        elif self.type == 'lastupdated':
            self._state = int(time() - data['When'])


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station
        self._next_update = time()
        self._update_in_progress = threading.Lock()

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    def update(self):
        """"""Call the Netatmo API to update the data.

        This method is not throttled by the builtin Throttle decorator
        but with a custom logic, which takes into account the time
        of the last update from the cloud.
        """"""
        if time() < self._next_update or \
                not self._update_in_progress.acquire(False):
            return

        try:
            import pyatmo
            self.station_data = pyatmo.WeatherStationData(self.auth)

            if self.station is not None:
                self.data = self.station_data.lastData(
                    station=self.station, exclude=3600)
            else:
                self.data = self.station_data.lastData(exclude=3600)

            newinterval = 0
            for module in self.data:
                if 'When' in self.data[module]:
                    newinterval = self.data[module]['When']
                    break
            if newinterval:
                # Try and estimate when fresh data will be available
                newinterval += NETATMO_UPDATE_INTERVAL - time()
                if newinterval > NETATMO_UPDATE_INTERVAL - 30:
                    newinterval = NETATMO_UPDATE_INTERVAL
                else:
                    if newinterval < NETATMO_UPDATE_INTERVAL / 2:
                        # Never hammer the NetAtmo API more than
                        # twice per update interval
                        newinterval = NETATMO_UPDATE_INTERVAL / 2
                    _LOGGER.warning(
                        ""NetAtmo refresh interval reset to %d seconds"",
                        newinterval)
            else:
                # Last update time not found, fall back to default value
                newinterval = NETATMO_UPDATE_INTERVAL

            self._next_update = time() + newinterval
        finally:
            self._update_in_progress.release()
/n/n/n",0
49,49,9ea0c409e6cea69cce632079548165ad5a9f2554,"/homeassistant/components/sensor/netatmo.py/n/n""""""
Support for the NetAtmo Weather Service.

For more details about this platform, please refer to the documentation at
https://home-assistant.io/components/sensor.netatmo/
""""""
import logging
from datetime import timedelta

import voluptuous as vol

from homeassistant.components.sensor import PLATFORM_SCHEMA
from homeassistant.const import (
    TEMP_CELSIUS, DEVICE_CLASS_HUMIDITY, DEVICE_CLASS_TEMPERATURE,
    STATE_UNKNOWN)
from homeassistant.helpers.entity import Entity
from homeassistant.util import Throttle
import homeassistant.helpers.config_validation as cv

_LOGGER = logging.getLogger(__name__)

CONF_MODULES = 'modules'
CONF_STATION = 'station'

DEPENDENCIES = ['netatmo']

# NetAtmo Data is uploaded to server every 10 minutes
MIN_TIME_BETWEEN_UPDATES = timedelta(seconds=600)

SENSOR_TYPES = {
    'temperature': ['Temperature', TEMP_CELSIUS, None,
                    DEVICE_CLASS_TEMPERATURE],
    'co2': ['CO2', 'ppm', 'mdi:cloud', None],
    'pressure': ['Pressure', 'mbar', 'mdi:gauge', None],
    'noise': ['Noise', 'dB', 'mdi:volume-high', None],
    'humidity': ['Humidity', '%', None, DEVICE_CLASS_HUMIDITY],
    'rain': ['Rain', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_1': ['sum_rain_1', 'mm', 'mdi:weather-rainy', None],
    'sum_rain_24': ['sum_rain_24', 'mm', 'mdi:weather-rainy', None],
    'battery_vp': ['Battery', '', 'mdi:battery', None],
    'battery_lvl': ['Battery_lvl', '', 'mdi:battery', None],
    'min_temp': ['Min Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'max_temp': ['Max Temp.', TEMP_CELSIUS, 'mdi:thermometer', None],
    'windangle': ['Angle', '', 'mdi:compass', None],
    'windangle_value': ['Angle Value', 'º', 'mdi:compass', None],
    'windstrength': ['Strength', 'km/h', 'mdi:weather-windy', None],
    'gustangle': ['Gust Angle', '', 'mdi:compass', None],
    'gustangle_value': ['Gust Angle Value', 'º', 'mdi:compass', None],
    'guststrength': ['Gust Strength', 'km/h', 'mdi:weather-windy', None],
    'rf_status': ['Radio', '', 'mdi:signal', None],
    'rf_status_lvl': ['Radio_lvl', '', 'mdi:signal', None],
    'wifi_status': ['Wifi', '', 'mdi:wifi', None],
    'wifi_status_lvl': ['Wifi_lvl', 'dBm', 'mdi:wifi', None]
}

MODULE_SCHEMA = vol.Schema({
    vol.Required(cv.string):
        vol.All(cv.ensure_list, [vol.In(SENSOR_TYPES)]),
})

PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend({
    vol.Optional(CONF_STATION): cv.string,
    vol.Optional(CONF_MODULES): MODULE_SCHEMA,
})


def setup_platform(hass, config, add_devices, discovery_info=None):
    """"""Set up the available Netatmo weather sensors.""""""
    netatmo = hass.components.netatmo
    data = NetAtmoData(netatmo.NETATMO_AUTH, config.get(CONF_STATION, None))

    dev = []
    import pyatmo
    try:
        if CONF_MODULES in config:
            # Iterate each module
            for module_name, monitored_conditions in\
                    config[CONF_MODULES].items():
                # Test if module exist """"""
                if module_name not in data.get_module_names():
                    _LOGGER.error('Module name: ""%s"" not found', module_name)
                    continue
                # Only create sensor for monitored """"""
                for variable in monitored_conditions:
                    dev.append(NetAtmoSensor(data, module_name, variable))
        else:
            for module_name in data.get_module_names():
                for variable in\
                        data.station_data.monitoredConditions(module_name):
                    if variable in SENSOR_TYPES.keys():
                        dev.append(NetAtmoSensor(data, module_name, variable))
                    else:
                        _LOGGER.warning(""Ignoring unknown var %s for mod %s"",
                                        variable, module_name)
    except pyatmo.NoDevice:
        return None

    add_devices(dev, True)


class NetAtmoSensor(Entity):
    """"""Implementation of a Netatmo sensor.""""""

    def __init__(self, netatmo_data, module_name, sensor_type):
        """"""Initialize the sensor.""""""
        self._name = 'Netatmo {} {}'.format(module_name,
                                            SENSOR_TYPES[sensor_type][0])
        self.netatmo_data = netatmo_data
        self.module_name = module_name
        self.type = sensor_type
        self._state = None
        self._device_class = SENSOR_TYPES[self.type][3]
        self._icon = SENSOR_TYPES[self.type][2]
        self._unit_of_measurement = SENSOR_TYPES[self.type][1]
        module_id = self.netatmo_data.\
            station_data.moduleByName(module=module_name)['_id']
        self.module_id = module_id[1]

    @property
    def name(self):
        """"""Return the name of the sensor.""""""
        return self._name

    @property
    def icon(self):
        """"""Icon to use in the frontend, if any.""""""
        return self._icon

    @property
    def device_class(self):
        """"""Return the device class of the sensor.""""""
        return self._device_class

    @property
    def state(self):
        """"""Return the state of the device.""""""
        return self._state

    @property
    def unit_of_measurement(self):
        """"""Return the unit of measurement of this entity, if any.""""""
        return self._unit_of_measurement

    def update(self):
        """"""Get the latest data from NetAtmo API and updates the states.""""""
        self.netatmo_data.update()
        data = self.netatmo_data.data.get(self.module_name)

        if data is None:
            _LOGGER.warning(""No data found for %s"", self.module_name)
            self._state = STATE_UNKNOWN
            return

        if self.type == 'temperature':
            self._state = round(data['Temperature'], 1)
        elif self.type == 'humidity':
            self._state = data['Humidity']
        elif self.type == 'rain':
            self._state = data['Rain']
        elif self.type == 'sum_rain_1':
            self._state = data['sum_rain_1']
        elif self.type == 'sum_rain_24':
            self._state = data['sum_rain_24']
        elif self.type == 'noise':
            self._state = data['Noise']
        elif self.type == 'co2':
            self._state = data['CO2']
        elif self.type == 'pressure':
            self._state = round(data['Pressure'], 1)
        elif self.type == 'battery_lvl':
            self._state = data['battery_vp']
        elif self.type == 'battery_vp' and self.module_id == '6':
            if data['battery_vp'] >= 5590:
                self._state = ""Full""
            elif data['battery_vp'] >= 5180:
                self._state = ""High""
            elif data['battery_vp'] >= 4770:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4360:
                self._state = ""Low""
            elif data['battery_vp'] < 4360:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '5':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '3':
            if data['battery_vp'] >= 5640:
                self._state = ""Full""
            elif data['battery_vp'] >= 5280:
                self._state = ""High""
            elif data['battery_vp'] >= 4920:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4560:
                self._state = ""Low""
            elif data['battery_vp'] < 4560:
                self._state = ""Very Low""
        elif self.type == 'battery_vp' and self.module_id == '2':
            if data['battery_vp'] >= 5500:
                self._state = ""Full""
            elif data['battery_vp'] >= 5000:
                self._state = ""High""
            elif data['battery_vp'] >= 4500:
                self._state = ""Medium""
            elif data['battery_vp'] >= 4000:
                self._state = ""Low""
            elif data['battery_vp'] < 4000:
                self._state = ""Very Low""
        elif self.type == 'min_temp':
            self._state = data['min_temp']
        elif self.type == 'max_temp':
            self._state = data['max_temp']
        elif self.type == 'windangle_value':
            self._state = data['WindAngle']
        elif self.type == 'windangle':
            if data['WindAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['WindAngle']
            elif data['WindAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['WindAngle']
        elif self.type == 'windstrength':
            self._state = data['WindStrength']
        elif self.type == 'gustangle_value':
            self._state = data['GustAngle']
        elif self.type == 'gustangle':
            if data['GustAngle'] >= 330:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 300:
                self._state = ""NW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 240:
                self._state = ""W (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 210:
                self._state = ""SW (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 150:
                self._state = ""S (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 120:
                self._state = ""SE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 60:
                self._state = ""E (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 30:
                self._state = ""NE (%d\xb0)"" % data['GustAngle']
            elif data['GustAngle'] >= 0:
                self._state = ""N (%d\xb0)"" % data['GustAngle']
        elif self.type == 'guststrength':
            self._state = data['GustStrength']
        elif self.type == 'rf_status_lvl':
            self._state = data['rf_status']
        elif self.type == 'rf_status':
            if data['rf_status'] >= 90:
                self._state = ""Low""
            elif data['rf_status'] >= 76:
                self._state = ""Medium""
            elif data['rf_status'] >= 60:
                self._state = ""High""
            elif data['rf_status'] <= 59:
                self._state = ""Full""
        elif self.type == 'wifi_status_lvl':
            self._state = data['wifi_status']
        elif self.type == 'wifi_status':
            if data['wifi_status'] >= 86:
                self._state = ""Low""
            elif data['wifi_status'] >= 71:
                self._state = ""Medium""
            elif data['wifi_status'] >= 56:
                self._state = ""High""
            elif data['wifi_status'] <= 55:
                self._state = ""Full""


class NetAtmoData(object):
    """"""Get the latest data from NetAtmo.""""""

    def __init__(self, auth, station):
        """"""Initialize the data object.""""""
        self.auth = auth
        self.data = None
        self.station_data = None
        self.station = station

    def get_module_names(self):
        """"""Return all module available on the API as a list.""""""
        self.update()
        return self.data.keys()

    @Throttle(MIN_TIME_BETWEEN_UPDATES)
    def update(self):
        """"""Call the Netatmo API to update the data.""""""
        import pyatmo
        self.station_data = pyatmo.WeatherStationData(self.auth)

        if self.station is not None:
            self.data = self.station_data.lastData(
                station=self.station, exclude=3600)
        else:
            self.data = self.station_data.lastData(exclude=3600)
/n/n/n",1
54,54,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"setup.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""


from setuptools.command.test import test as TestCommand
import sys


if sys.version_info < (3, 3):
    print(""At least Python 3.3 is required.\n"", file=sys.stderr)
    exit(1)


try:
    from setuptools import setup
except ImportError:
    print(""Please install setuptools before installing snakemake."",
          file=sys.stderr)
    exit(1)


# load version info
exec(open(""snakemake/version.py"").read())


class NoseTestCommand(TestCommand):
    def finalize_options(self):
        TestCommand.finalize_options(self)
        self.test_args = []
        self.test_suite = True

    def run_tests(self):
        # Run nose ensuring that argv simulates running nosetests directly
        import nose
        nose.run_exit(argv=['nosetests'])


setup(
    name='snakemake',
    version=__version__,
    author='Johannes Köster',
    author_email='johannes.koester@tu-dortmund.de',
    description=
    'Build systems like GNU Make are frequently used to create complicated '
    'workflows, e.g. in bioinformatics. This project aims to reduce the '
    'complexity of creating workflows by providing a clean and modern domain '
    'specific language (DSL) in python style, together with a fast and '
    'comfortable execution environment.',
    zip_safe=False,
    license='MIT',
    url='https://bitbucket.org/johanneskoester/snakemake',
    packages=['snakemake'],
    entry_points={
        ""console_scripts"":
        [""snakemake = snakemake:main"",
         ""snakemake-bash-completion = snakemake:bash_completion""]
    },
    package_data={'': ['*.css', '*.sh', '*.html']},
    tests_require=['nose>=1.3'],
    install_requires=['boto>=2.38.0','filechunkio>=1.6', 'moto>=0.4.14'],
    cmdclass={'test': NoseTestCommand},
    classifiers=
    [""Development Status :: 5 - Production/Stable"", ""Environment :: Console"",
     ""Intended Audience :: Science/Research"",
     ""License :: OSI Approved :: MIT License"", ""Natural Language :: English"",
     ""Programming Language :: Python :: 3"",
     ""Topic :: Scientific/Engineering :: Bio-Informatics""])
/n/n/nsnakemake/dag.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import textwrap
import time
from collections import defaultdict, Counter
from itertools import chain, combinations, filterfalse, product, groupby
from functools import partial, lru_cache
from operator import itemgetter, attrgetter

from snakemake.io import IOFile, _IOFile, PeriodicityDetector, wait_for_files, is_flagged
from snakemake.jobs import Job, Reason
from snakemake.exceptions import RuleException, MissingInputException
from snakemake.exceptions import MissingRuleException, AmbiguousRuleException
from snakemake.exceptions import CyclicGraphException, MissingOutputException
from snakemake.exceptions import IncompleteFilesException
from snakemake.exceptions import PeriodicWildcardError
from snakemake.exceptions import UnexpectedOutputException, InputFunctionException
from snakemake.logging import logger
from snakemake.output_index import OutputIndex


class DAG:
    def __init__(self, workflow,
                 rules=None,
                 dryrun=False,
                 targetfiles=None,
                 targetrules=None,
                 forceall=False,
                 forcerules=None,
                 forcefiles=None,
                 priorityfiles=None,
                 priorityrules=None,
                 ignore_ambiguity=False,
                 force_incomplete=False,
                 ignore_incomplete=False,
                 notemp=False):

        self.dryrun = dryrun
        self.dependencies = defaultdict(partial(defaultdict, set))
        self.depending = defaultdict(partial(defaultdict, set))
        self._needrun = set()
        self._priority = dict()
        self._downstream_size = dict()
        self._reason = defaultdict(Reason)
        self._finished = set()
        self._dynamic = set()
        self._len = 0
        self.workflow = workflow
        self.rules = set(rules)
        self.ignore_ambiguity = ignore_ambiguity
        self.targetfiles = targetfiles
        self.targetrules = targetrules
        self.priorityfiles = priorityfiles
        self.priorityrules = priorityrules
        self.targetjobs = set()
        self.prioritytargetjobs = set()
        self._ready_jobs = set()
        self.notemp = notemp
        self._jobid = dict()

        self.forcerules = set()
        self.forcefiles = set()
        self.updated_subworkflow_files = set()
        if forceall:
            self.forcerules.update(self.rules)
        elif forcerules:
            self.forcerules.update(forcerules)
        if forcefiles:
            self.forcefiles.update(forcefiles)
        self.omitforce = set()

        self.force_incomplete = force_incomplete
        self.ignore_incomplete = ignore_incomplete

        self.periodic_wildcard_detector = PeriodicityDetector()

        self.update_output_index()

    def init(self):
        """""" Initialise the DAG. """"""
        for job in map(self.rule2job, self.targetrules):
            job = self.update([job])
            self.targetjobs.add(job)

        for file in self.targetfiles:
            job = self.update(self.file2jobs(file), file=file)
            self.targetjobs.add(job)

        self.update_needrun()

    def update_output_index(self):
        self.output_index = OutputIndex(self.rules)

    def check_incomplete(self):
        if not self.ignore_incomplete:
            incomplete = self.incomplete_files
            if incomplete:
                if self.force_incomplete:
                    logger.debug(""Forcing incomplete files:"")
                    logger.debug(""\t"" + ""\n\t"".join(incomplete))
                    self.forcefiles.update(incomplete)
                else:
                    raise IncompleteFilesException(incomplete)

    def check_dynamic(self):
        for job in filter(lambda job: (
            job.dynamic_output and not self.needrun(job)
        ), self.jobs):
            self.update_dynamic(job)

    @property
    def dynamic_output_jobs(self):
        return (job for job in self.jobs if job.dynamic_output)

    @property
    def jobs(self):
        """""" All jobs in the DAG. """"""
        for job in self.bfs(self.dependencies, *self.targetjobs):
            yield job

    @property
    def needrun_jobs(self):
        """""" Jobs that need to be executed. """"""
        for job in filter(self.needrun,
                          self.bfs(self.dependencies, *self.targetjobs,
                                   stop=self.noneedrun_finished)):
            yield job

    @property
    def local_needrun_jobs(self):
        return filter(lambda job: self.workflow.is_local(job.rule),
                      self.needrun_jobs)

    @property
    def finished_jobs(self):
        """""" Jobs that have been executed. """"""
        for job in filter(self.finished, self.bfs(self.dependencies,
                                                  *self.targetjobs)):
            yield job

    @property
    def ready_jobs(self):
        """""" Jobs that are ready to execute. """"""
        return self._ready_jobs

    def ready(self, job):
        """""" Return whether a given job is ready to execute. """"""
        return job in self._ready_jobs

    def needrun(self, job):
        """""" Return whether a given job needs to be executed. """"""
        return job in self._needrun

    def priority(self, job):
        return self._priority[job]

    def downstream_size(self, job):
        return self._downstream_size[job]

    def _job_values(self, jobs, values):
        return [values[job] for job in jobs]

    def priorities(self, jobs):
        return self._job_values(jobs, self._priority)

    def downstream_sizes(self, jobs):
        return self._job_values(jobs, self._downstream_size)

    def noneedrun_finished(self, job):
        """"""
        Return whether a given job is finished or was not
        required to run at all.
        """"""
        return not self.needrun(job) or self.finished(job)

    def reason(self, job):
        """""" Return the reason of the job execution. """"""
        return self._reason[job]

    def finished(self, job):
        """""" Return whether a job is finished. """"""
        return job in self._finished

    def dynamic(self, job):
        """"""
        Return whether a job is dynamic (i.e. it is only a placeholder
        for those that are created after the job with dynamic output has
        finished.
        """"""
        return job in self._dynamic

    def requested_files(self, job):
        """""" Return the files a job requests. """"""
        return set(*self.depending[job].values())

    @property
    def incomplete_files(self):
        return list(chain(*(
            job.output for job in filter(self.workflow.persistence.incomplete,
                                         filterfalse(self.needrun, self.jobs))
        )))

    @property
    def newversion_files(self):
        return list(chain(*(
            job.output
            for job in filter(self.workflow.persistence.newversion, self.jobs)
        )))

    def missing_temp(self, job):
        """"""
        Return whether a temp file that is input of the given job is missing.
        """"""
        for job_, files in self.depending[job].items():
            if self.needrun(job_) and any(not f.exists for f in files):
                return True
        return False

    def check_output(self, job, wait=3):
        """""" Raise exception if output files of job are missing. """"""
        try:
            wait_for_files(job.expanded_output, latency_wait=wait)
        except IOError as e:
            raise MissingOutputException(str(e), rule=job.rule)

        input_maxtime = job.input_maxtime
        if input_maxtime is not None:
            output_mintime = job.output_mintime
            if output_mintime is not None and output_mintime < input_maxtime:
                raise RuleException(
                    ""Output files {} are older than input ""
                    ""files. Did you extract an archive? Make sure that output ""
                    ""files have a more recent modification date than the ""
                    ""archive, e.g. by using 'touch'."".format(
                        "", "".join(job.expanded_output)),
                    rule=job.rule)

    def check_periodic_wildcards(self, job):
        """""" Raise an exception if a wildcard of the given job appears to be periodic,
        indicating a cyclic dependency. """"""
        for wildcard, value in job.wildcards_dict.items():
            periodic_substring = self.periodic_wildcard_detector.is_periodic(
                value)
            if periodic_substring is not None:
                raise PeriodicWildcardError(
                    ""The value {} in wildcard {} is periodically repeated ({}). ""
                    ""This would lead to an infinite recursion. ""
                    ""To avoid this, e.g. restrict the wildcards in this rule to certain values."".format(
                        periodic_substring, wildcard, value),
                    rule=job.rule)

    def handle_protected(self, job):
        """""" Write-protect output files that are marked with protected(). """"""
        for f in job.expanded_output:
            if f in job.protected_output:
                logger.info(""Write-protecting output file {}."".format(f))
                f.protect()

    def handle_touch(self, job):
        """""" Touches those output files that are marked for touching. """"""
        for f in job.expanded_output:
            if f in job.touch_output:
                logger.info(""Touching output file {}."".format(f))
                f.touch_or_create()

    def handle_temp(self, job):
        """""" Remove temp files if they are no longer needed. """"""
        if self.notemp:
            return

        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in job_.temp_output & files:
                    if not needed(job_, f):
                        yield f
            for f in filterfalse(partial(needed, job), job.temp_output):
                if not f in self.targetfiles:
                    yield f

        for f in unneeded_files():
            logger.info(""Removing temporary output file {}."".format(f))
            f.remove()

    def handle_remote(self, job):
        """""" Remove local files if they are no longer needed, and upload to S3. """"""
        
        needed = lambda job_, f: any(
            f in files for j, files in self.depending[job_].items()
            if not self.finished(j) and self.needrun(j) and j != job)

        remote_files = set([f for f in job.expanded_input if f.is_remote]) | set([f for f in job.expanded_output if f.is_remote])
        local_files = set([f for f in job.input if not f.is_remote]) | set([f for f in job.expanded_output if not f.is_remote])
        files_to_keep = set(f for f in remote_files if is_flagged(f, ""keep""))

        # remove local files from list of remote files
        # in case the same file is specified in both places
        remote_files -= local_files
        remote_files -= files_to_keep

        def unneeded_files():
            for job_, files in self.dependencies[job].items():
                for f in (remote_files & files):
                    if not needed(job_, f) and not f.protected:
                        yield f
            for f in filterfalse(partial(needed, job), [f for f in remote_files]):
                if not f in self.targetfiles and not f.protected:
                    yield f

        def expanded_dynamic_depending_input_files():
            for j in self.depending[job]:    
                for f in j.expanded_input:
                    yield f

        unneededFiles = set(unneeded_files())
        unneededFiles -= set(expanded_dynamic_depending_input_files())

        for f in [f for f in job.expanded_output if f.is_remote]:
            if not f.exists_remote:
                logger.info(""Uploading local output file to remote: {}"".format(f))
                f.upload_to_remote()

        for f in set(unneededFiles):
            logger.info(""Removing local output file: {}"".format(f))
            f.remove()

        job.rmdir_empty_remote_dirs()


    def jobid(self, job):
        if job not in self._jobid:
            self._jobid[job] = len(self._jobid)
        return self._jobid[job]

    def update(self, jobs, file=None, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding given jobs and their dependencies. """"""
        if visited is None:
            visited = set()
        producer = None
        exceptions = list()
        jobs = sorted(jobs, reverse=not self.ignore_ambiguity)
        cycles = list()

        for job in jobs:
            if file in job.input:
                cycles.append(job)
                continue
            if job in visited:
                cycles.append(job)
                continue
            try:
                self.check_periodic_wildcards(job)
                self.update_(job,
                             visited=set(visited),
                             skip_until_dynamic=skip_until_dynamic)
                # TODO this might fail if a rule discarded here is needed
                # elsewhere
                if producer:
                    if job < producer or self.ignore_ambiguity:
                        break
                    elif producer is not None:
                        raise AmbiguousRuleException(file, job, producer)
                producer = job
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                exceptions.append(ex)
        if producer is None:
            if cycles:
                job = cycles[0]
                raise CyclicGraphException(job.rule, file, rule=job.rule)
            if exceptions:
                raise exceptions[0]
        return producer

    def update_(self, job, visited=None, skip_until_dynamic=False):
        """""" Update the DAG by adding the given job and its dependencies. """"""
        if job in self.dependencies:
            return
        if visited is None:
            visited = set()
        visited.add(job)
        dependencies = self.dependencies[job]
        potential_dependencies = self.collect_potential_dependencies(
            job).items()

        skip_until_dynamic = skip_until_dynamic and not job.dynamic_output

        missing_input = job.missing_input
        producer = dict()
        exceptions = dict()
        for file, jobs in potential_dependencies:
            try:
                producer[file] = self.update(
                    jobs,
                    file=file,
                    visited=visited,
                    skip_until_dynamic=skip_until_dynamic or file in
                    job.dynamic_input)
            except (MissingInputException, CyclicGraphException,
                    PeriodicWildcardError) as ex:
                if file in missing_input:
                    self.delete_job(job,
                                    recursive=False)  # delete job from tree
                    raise ex

        for file, job_ in producer.items():
            dependencies[job_].add(file)
            self.depending[job_][job].add(file)

        missing_input -= producer.keys()
        if missing_input:
            self.delete_job(job, recursive=False)  # delete job from tree
            raise MissingInputException(job.rule, missing_input)

        if skip_until_dynamic:
            self._dynamic.add(job)

    def update_needrun(self):
        """""" Update the information whether a job needs to be executed. """"""

        def output_mintime(job):
            for job_ in self.bfs(self.depending, job):
                t = job_.output_mintime
                if t:
                    return t

        def needrun(job):
            reason = self.reason(job)
            noinitreason = not reason
            updated_subworkflow_input = self.updated_subworkflow_files.intersection(
                job.input)
            if (job not in self.omitforce and job.rule in self.forcerules or
                not self.forcefiles.isdisjoint(job.output)):
                reason.forced = True
            elif updated_subworkflow_input:
                reason.updated_input.update(updated_subworkflow_input)
            elif job in self.targetjobs:
                # TODO find a way to handle added/removed input files here?
                if not job.output and not job.benchmark:
                    if job.input:
                        if job.rule.norun:
                            reason.updated_input_run.update([f
                                                             for f in job.input
                                                             if not f.exists])
                        else:
                            reason.nooutput = True
                    else:
                        reason.noio = True
                else:
                    if job.rule in self.targetrules:
                        missing_output = job.missing_output()
                    else:
                        missing_output = job.missing_output(
                            requested=set(chain(*self.depending[job].values()))
                            | self.targetfiles)
                    reason.missing_output.update(missing_output)
            if not reason:
                output_mintime_ = output_mintime(job)
                if output_mintime_:
                    updated_input = [
                        f for f in job.input
                        if f.exists and f.is_newer(output_mintime_)
                    ]
                    reason.updated_input.update(updated_input)
            if noinitreason and reason:
                reason.derived = False
            return job

        reason = self.reason
        _needrun = self._needrun
        dependencies = self.dependencies
        depending = self.depending

        _needrun.clear()
        candidates = set(self.jobs)

        queue = list(filter(reason, map(needrun, candidates)))
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            _needrun.add(job)

            for job_, files in dependencies[job].items():
                missing_output = job_.missing_output(requested=files)
                reason(job_).missing_output.update(missing_output)
                if missing_output and not job_ in visited:
                    visited.add(job_)
                    queue.append(job_)

            for job_, files in depending[job].items():
                if job_ in candidates:
                    reason(job_).updated_input_run.update(files)
                    if not job_ in visited:
                        visited.add(job_)
                        queue.append(job_)

        self._len = len(_needrun)

    def update_priority(self):
        """""" Update job priorities. """"""
        prioritized = (lambda job: job.rule in self.priorityrules or
                       not self.priorityfiles.isdisjoint(job.output))
        for job in self.needrun_jobs:
            self._priority[job] = job.rule.priority
        for job in self.bfs(self.dependencies,
                            *filter(prioritized, self.needrun_jobs),
                            stop=self.noneedrun_finished):
            self._priority[job] = Job.HIGHEST_PRIORITY

    def update_ready(self):
        """""" Update information whether a job is ready to execute. """"""
        for job in filter(self.needrun, self.jobs):
            if not self.finished(job) and self._ready(job):
                self._ready_jobs.add(job)

    def update_downstream_size(self):
        for job in self.needrun_jobs:
            self._downstream_size[job] = sum(
                1 for _ in self.bfs(self.depending, job,
                                    stop=self.noneedrun_finished)) - 1

    def postprocess(self):
        self.update_needrun()
        self.update_priority()
        self.update_ready()
        self.update_downstream_size()

    def _ready(self, job):
        return self._finished.issuperset(
            filter(self.needrun, self.dependencies[job]))

    def finish(self, job, update_dynamic=True):
        self._finished.add(job)
        try:
            self._ready_jobs.remove(job)
        except KeyError:
            pass
        # mark depending jobs as ready
        for job_ in self.depending[job]:
            if self.needrun(job_) and self._ready(job_):
                self._ready_jobs.add(job_)

        if update_dynamic and job.dynamic_output:
            logger.info(""Dynamically updating jobs"")
            newjob = self.update_dynamic(job)
            if newjob:
                # simulate that this job ran and was finished before
                self.omitforce.add(newjob)
                self._needrun.add(newjob)
                self._finished.add(newjob)

                self.postprocess()
                self.handle_protected(newjob)
                self.handle_touch(newjob)
                # add finished jobs to len as they are not counted after new postprocess
                self._len += len(self._finished)

    def update_dynamic(self, job):
        dynamic_wildcards = job.dynamic_wildcards
        if not dynamic_wildcards:
            # this happens e.g. in dryrun if output is not yet present
            return

        depending = list(filter(lambda job_: not self.finished(job_),
                                self.bfs(self.depending, job)))
        newrule, non_dynamic_wildcards = job.rule.dynamic_branch(
            dynamic_wildcards,
            input=False)
        self.specialize_rule(job.rule, newrule)

        # no targetfile needed for job
        newjob = Job(newrule, self, format_wildcards=non_dynamic_wildcards)
        self.replace_job(job, newjob)
        for job_ in depending:
            if job_.dynamic_input:
                newrule_ = job_.rule.dynamic_branch(dynamic_wildcards)
                if newrule_ is not None:
                    self.specialize_rule(job_.rule, newrule_)
                    if not self.dynamic(job_):
                        logger.debug(""Updating job {}."".format(job_))
                        newjob_ = Job(newrule_, self,
                                      targetfile=job_.targetfile)

                        unexpected_output = self.reason(
                            job_).missing_output.intersection(
                                newjob.existing_output)
                        if unexpected_output:
                            logger.warning(
                                ""Warning: the following output files of rule {} were not ""
                                ""present when the DAG was created:\n{}"".format(
                                    newjob_.rule, unexpected_output))

                        self.replace_job(job_, newjob_)
        return newjob

    def delete_job(self, job, recursive=True):
        for job_ in self.depending[job]:
            del self.dependencies[job_][job]
        del self.depending[job]
        for job_ in self.dependencies[job]:
            depending = self.depending[job_]
            del depending[job]
            if not depending and recursive:
                self.delete_job(job_)
        del self.dependencies[job]
        if job in self._needrun:
            self._len -= 1
            self._needrun.remove(job)
            del self._reason[job]
        if job in self._finished:
            self._finished.remove(job)
        if job in self._dynamic:
            self._dynamic.remove(job)
        if job in self._ready_jobs:
            self._ready_jobs.remove(job)

    def replace_job(self, job, newjob):
        depending = list(self.depending[job].items())
        if self.finished(job):
            self._finished.add(newjob)

        self.delete_job(job)
        self.update([newjob])

        for job_, files in depending:
            if not job_.dynamic_input:
                self.dependencies[job_][newjob].update(files)
                self.depending[newjob][job_].update(files)
        if job in self.targetjobs:
            self.targetjobs.remove(job)
            self.targetjobs.add(newjob)

    def specialize_rule(self, rule, newrule):
        assert newrule is not None
        self.rules.add(newrule)
        self.update_output_index()

    def collect_potential_dependencies(self, job):
        dependencies = defaultdict(list)
        # use a set to circumvent multiple jobs for the same file
        # if user specified it twice
        file2jobs = self.file2jobs
        for file in set(job.input):
            # omit the file if it comes from a subworkflow
            if file in job.subworkflow_input:
                continue
            try:
                if file in job.dependencies:
                    jobs = [Job(job.dependencies[file], self, targetfile=file)]
                else:
                    jobs = file2jobs(file)
                dependencies[file].extend(jobs)
            except MissingRuleException as ex:
                pass
        return dependencies

    def bfs(self, direction, *jobs, stop=lambda job: False):
        queue = list(jobs)
        visited = set(queue)
        while queue:
            job = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield job
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append(job_)
                    visited.add(job_)

    def level_bfs(self, direction, *jobs, stop=lambda job: False):
        queue = [(job, 0) for job in jobs]
        visited = set(jobs)
        while queue:
            job, level = queue.pop(0)
            if stop(job):
                # stop criterion reached for this node
                continue
            yield level, job
            level += 1
            for job_, _ in direction[job].items():
                if not job_ in visited:
                    queue.append((job_, level))
                    visited.add(job_)

    def dfs(self, direction, *jobs, stop=lambda job: False, post=True):
        visited = set()
        for job in jobs:
            for job_ in self._dfs(direction, job, visited,
                                  stop=stop,
                                  post=post):
                yield job_

    def _dfs(self, direction, job, visited, stop, post):
        if stop(job):
            return
        if not post:
            yield job
        for job_ in direction[job]:
            if not job_ in visited:
                visited.add(job_)
                for j in self._dfs(direction, job_, visited, stop, post):
                    yield j
        if post:
            yield job

    def is_isomorph(self, job1, job2):
        if job1.rule != job2.rule:
            return False
        rule = lambda job: job.rule.name
        queue1, queue2 = [job1], [job2]
        visited1, visited2 = set(queue1), set(queue2)
        while queue1 and queue2:
            job1, job2 = queue1.pop(0), queue2.pop(0)
            deps1 = sorted(self.dependencies[job1], key=rule)
            deps2 = sorted(self.dependencies[job2], key=rule)
            for job1_, job2_ in zip(deps1, deps2):
                if job1_.rule != job2_.rule:
                    return False
                if not job1_ in visited1 and not job2_ in visited2:
                    queue1.append(job1_)
                    visited1.add(job1_)
                    queue2.append(job2_)
                    visited2.add(job2_)
                elif not (job1_ in visited1 and job2_ in visited2):
                    return False
        return True

    def all_longest_paths(self, *jobs):
        paths = defaultdict(list)

        def all_longest_paths(_jobs):
            for job in _jobs:
                if job in paths:
                    continue
                deps = self.dependencies[job]
                if not deps:
                    paths[job].append([job])
                    continue
                all_longest_paths(deps)
                for _job in deps:
                    paths[job].extend(path + [job] for path in paths[_job])

        all_longest_paths(jobs)
        return chain(*(paths[job] for job in jobs))

    def new_wildcards(self, job):
        new_wildcards = set(job.wildcards.items())
        for job_ in self.dependencies[job]:
            if not new_wildcards:
                return set()
            for wildcard in job_.wildcards.items():
                new_wildcards.discard(wildcard)
        return new_wildcards

    def rule2job(self, targetrule):
        return Job(targetrule, self)

    def file2jobs(self, targetfile):
        rules = self.output_index.match(targetfile)
        jobs = []
        exceptions = list()
        for rule in rules:
            if rule.is_producer(targetfile):
                try:
                    jobs.append(Job(rule, self, targetfile=targetfile))
                except InputFunctionException as e:
                    exceptions.append(e)
        if not jobs:
            if exceptions:
                raise exceptions[0]
            raise MissingRuleException(targetfile)
        return jobs

    def rule_dot2(self):
        dag = defaultdict(list)
        visited = set()
        preselect = set()

        def preselect_parents(job):
            for parent in self.depending[job]:
                if parent in preselect:
                    continue
                preselect.add(parent)
                preselect_parents(parent)

        def build_ruledag(job, key=lambda job: job.rule.name):
            if job in visited:
                return
            visited.add(job)
            deps = sorted(self.dependencies[job], key=key)
            deps = [(group[0] if preselect.isdisjoint(group) else
                     preselect.intersection(group).pop())
                    for group in (list(g) for _, g in groupby(deps, key))]
            dag[job].extend(deps)
            preselect_parents(job)
            for dep in deps:
                build_ruledag(dep)

        for job in self.targetjobs:
            build_ruledag(job)

        return self._dot(dag.keys(),
                         print_wildcards=False,
                         print_types=False,
                         dag=dag)

    def rule_dot(self):
        graph = defaultdict(set)
        for job in self.jobs:
            graph[job.rule].update(dep.rule for dep in self.dependencies[job])
        return self._dot(graph)

    def dot(self):
        def node2style(job):
            if not self.needrun(job):
                return ""rounded,dashed""
            if self.dynamic(job) or job.dynamic_input:
                return ""rounded,dotted""
            return ""rounded""

        def format_wildcard(wildcard):
            name, value = wildcard
            if _IOFile.dynamic_fill in value:
                value = ""...""
            return ""{}: {}"".format(name, value)

        node2rule = lambda job: job.rule
        node2label = lambda job: ""\\n"".join(chain([
            job.rule.name
        ], sorted(map(format_wildcard, self.new_wildcards(job)))))

        dag = {job: self.dependencies[job] for job in self.jobs}

        return self._dot(dag,
                         node2rule=node2rule,
                         node2style=node2style,
                         node2label=node2label)

    def _dot(self, graph,
             node2rule=lambda node: node,
             node2style=lambda node: ""rounded"",
             node2label=lambda node: node):

        # color rules
        huefactor = 2 / (3 * len(self.rules))
        rulecolor = {
            rule: ""{:.2f} 0.6 0.85"".format(i * huefactor)
            for i, rule in enumerate(self.rules)
        }

        # markup
        node_markup = '\t{}[label = ""{}"", color = ""{}"", style=""{}""];'.format
        edge_markup = ""\t{} -> {}"".format

        # node ids
        ids = {node: i for i, node in enumerate(graph)}

        # calculate nodes
        nodes = [node_markup(ids[node], node2label(node),
                             rulecolor[node2rule(node)], node2style(node))
                 for node in graph]
        # calculate edges
        edges = [edge_markup(ids[dep], ids[node])
                 for node, deps in graph.items() for dep in deps]

        return textwrap.dedent(""""""\
            digraph snakemake_dag {{
                graph[bgcolor=white, margin=0];
                node[shape=box, style=rounded, fontname=sans, \
                fontsize=10, penwidth=2];
                edge[penwidth=2, color=grey];
            {items}
            }}\
            """""").format(items=""\n"".join(nodes + edges))

    def summary(self, detailed=False):
        if detailed:
            yield ""output_file\tdate\trule\tversion\tinput_file(s)\tshellcmd\tstatus\tplan""
        else:
            yield ""output_file\tdate\trule\tversion\tstatus\tplan""

        for job in self.jobs:
            output = job.rule.output if self.dynamic(
                job) else job.expanded_output
            for f in output:
                rule = self.workflow.persistence.rule(f)
                rule = ""-"" if rule is None else rule

                version = self.workflow.persistence.version(f)
                version = ""-"" if version is None else str(version)

                date = time.ctime(f.mtime) if f.exists else ""-""

                pending = ""update pending"" if self.reason(job) else ""no update""

                input = self.workflow.persistence.input(f)
                input = ""-"" if input is None else "","".join(input)

                shellcmd = self.workflow.persistence.shellcmd(f)
                shellcmd = ""-"" if shellcmd is None else shellcmd
                # remove new line characters, leading and trailing whitespace
                shellcmd = shellcmd.strip().replace(""\n"", ""; "")

                status = ""ok""
                if not f.exists:
                    status = ""missing""
                elif self.reason(job).updated_input:
                    status = ""updated input files""
                elif self.workflow.persistence.version_changed(job, file=f):
                    status = ""version changed to {}"".format(job.rule.version)
                elif self.workflow.persistence.code_changed(job, file=f):
                    status = ""rule implementation changed""
                elif self.workflow.persistence.input_changed(job, file=f):
                    status = ""set of input files changed""
                elif self.workflow.persistence.params_changed(job, file=f):
                    status = ""params changed""
                if detailed:
                    yield ""\t"".join((f, date, rule, version, input, shellcmd,
                                     status, pending))
                else:
                    yield ""\t"".join((f, date, rule, version, status, pending))

    def d3dag(self, max_jobs=10000):
        def node(job):
            jobid = self.jobid(job)
            return {
                ""id"": jobid,
                ""value"": {
                    ""jobid"": jobid,
                    ""label"": job.rule.name,
                    ""rule"": job.rule.name
                }
            }

        def edge(a, b):
            return {""u"": self.jobid(a), ""v"": self.jobid(b)}

        jobs = list(self.jobs)

        if len(jobs) > max_jobs:
            logger.info(
                ""Job-DAG is too large for visualization (>{} jobs)."".format(
                    max_jobs))
        else:
            logger.d3dag(nodes=[node(job) for job in jobs],
                         edges=[edge(dep, job) for job in jobs for dep in
                                self.dependencies[job] if self.needrun(dep)])

    def stats(self):
        rules = Counter()
        rules.update(job.rule for job in self.needrun_jobs)
        rules.update(job.rule for job in self.finished_jobs)
        yield ""Job counts:""
        yield ""\tcount\tjobs""
        for rule, count in sorted(rules.most_common(),
                                  key=lambda item: item[0].name):
            yield ""\t{}\t{}"".format(count, rule)
        yield ""\t{}"".format(len(self))

    def __str__(self):
        return self.dot()

    def __len__(self):
        return self._len
/n/n/nsnakemake/decorators.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import functools
import inspect


def memoize(obj):
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = str(args) + str(kwargs)
        if key not in cache:
            cache[key] = obj(*args, **kwargs)
        return cache[key]

    return memoizer


def decAllMethods(decorator, prefix='test_'):

    def decClass(cls):
        for name, m in inspect.getmembers(cls, inspect.isfunction):
            if prefix == None or name.startswith(prefix):
                setattr(cls, name, decorator(m))
        return cls

    return decClass
/n/n/nsnakemake/exceptions.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import traceback
from tokenize import TokenError

from snakemake.logging import logger


def format_error(ex, lineno,
                 linemaps=None,
                 snakefile=None,
                 show_traceback=False):
    if linemaps is None:
        linemaps = dict()
    msg = str(ex)
    if linemaps and snakefile and snakefile in linemaps:
        lineno = linemaps[snakefile][lineno]
        if isinstance(ex, SyntaxError):
            msg = ex.msg
    location = ("" in line {} of {}"".format(lineno, snakefile) if
                lineno and snakefile else """")
    tb = """"
    if show_traceback:
        tb = ""\n"".join(format_traceback(cut_traceback(ex), linemaps=linemaps))
    return '{}{}{}{}'.format(ex.__class__.__name__, location, "":\n"" + msg
                             if msg else ""."", ""\n{}"".format(tb) if
                             show_traceback and tb else """")


def get_exception_origin(ex, linemaps):
    for file, lineno, _, _ in reversed(traceback.extract_tb(ex.__traceback__)):
        if file in linemaps:
            return lineno, file


def cut_traceback(ex):
    snakemake_path = os.path.dirname(__file__)
    for line in traceback.extract_tb(ex.__traceback__):
        dir = os.path.dirname(line[0])
        if not dir:
            dir = "".""
        if not os.path.isdir(dir) or not os.path.samefile(snakemake_path, dir):
            yield line


def format_traceback(tb, linemaps):
    for file, lineno, function, code in tb:
        if file in linemaps:
            lineno = linemaps[file][lineno]
        if code is not None:
            yield '  File ""{}"", line {}, in {}'.format(file, lineno, function)


def print_exception(ex, linemaps, print_traceback=True):
    """"""
    Print an error message for a given exception.

    Arguments
    ex -- the exception
    linemaps -- a dict of a dict that maps for each snakefile
        the compiled lines to source code lines in the snakefile.
    """"""
    #traceback.print_exception(type(ex), ex, ex.__traceback__)
    if isinstance(ex, SyntaxError) or isinstance(ex, IndentationError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=print_traceback))
        return
    origin = get_exception_origin(ex, linemaps)
    if origin is not None:
        lineno, file = origin
        logger.error(format_error(ex, lineno,
                                  linemaps=linemaps,
                                  snakefile=file,
                                  show_traceback=print_traceback))
        return
    elif isinstance(ex, TokenError):
        logger.error(format_error(ex, None, show_traceback=False))
    elif isinstance(ex, MissingRuleException):
        logger.error(format_error(ex, None,
                                  linemaps=linemaps,
                                  snakefile=ex.filename,
                                  show_traceback=False))
    elif isinstance(ex, RuleException):
        for e in ex._include + [ex]:
            if not e.omit:
                logger.error(format_error(e, e.lineno,
                                          linemaps=linemaps,
                                          snakefile=e.filename,
                                          show_traceback=print_traceback))
    elif isinstance(ex, WorkflowError):
        logger.error(format_error(ex, ex.lineno,
                                  linemaps=linemaps,
                                  snakefile=ex.snakefile,
                                  show_traceback=print_traceback))
    elif isinstance(ex, KeyboardInterrupt):
        logger.info(""Cancelling snakemake on user request."")
    else:
        traceback.print_exception(type(ex), ex, ex.__traceback__)


class WorkflowError(Exception):
    @staticmethod
    def format_args(args):
        for arg in args:
            if isinstance(arg, str):
                yield arg
            else:
                yield ""{}: {}"".format(arg.__class__.__name__, str(arg))

    def __init__(self, *args, lineno=None, snakefile=None, rule=None):
        super().__init__(""\n"".join(self.format_args(args)))
        if rule is not None:
            self.lineno = rule.lineno
            self.snakefile = rule.snakefile
        else:
            self.lineno = lineno
            self.snakefile = snakefile
        self.rule = rule


class WildcardError(WorkflowError):
    pass


class RuleException(Exception):
    """"""
    Base class for exception occuring withing the
    execution or definition of rules.
    """"""

    def __init__(self,
                 message=None,
                 include=None,
                 lineno=None,
                 snakefile=None,
                 rule=None):
        """"""
        Creates a new instance of RuleException.

        Arguments
        message -- the exception message
        include -- iterable of other exceptions to be included
        lineno -- the line the exception originates
        snakefile -- the file the exception originates
        """"""
        super(RuleException, self).__init__(message)
        self._include = set()
        if include:
            for ex in include:
                self._include.add(ex)
                self._include.update(ex._include)
        if rule is not None:
            if lineno is None:
                lineno = rule.lineno
            if snakefile is None:
                snakefile = rule.snakefile

        self._include = list(self._include)
        self.lineno = lineno
        self.filename = snakefile
        self.omit = not message

    @property
    def messages(self):
        return map(str, (ex for ex in self._include + [self] if not ex.omit))


class InputFunctionException(WorkflowError):
    pass


class MissingOutputException(RuleException):
    pass


class IOException(RuleException):
    def __init__(self, prefix, rule, files,
                 include=None,
                 lineno=None,
                 snakefile=None):
        message = (""{} for rule {}:\n{}"".format(prefix, rule, ""\n"".join(files))
                   if files else """")
        super().__init__(message=message,
                         include=include,
                         lineno=lineno,
                         snakefile=snakefile,
                         rule=rule)


class MissingInputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Missing input files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class PeriodicWildcardError(RuleException):
    pass


class ProtectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Write-protected output files"", rule, files, include,
                         lineno=lineno,
                         snakefile=snakefile)


class UnexpectedOutputException(IOException):
    def __init__(self, rule, files, include=None, lineno=None, snakefile=None):
        super().__init__(""Unexpectedly present output files ""
                         ""(accidentally created by other rule?)"", rule, files,
                         include,
                         lineno=lineno,
                         snakefile=snakefile)


class AmbiguousRuleException(RuleException):
    def __init__(self, filename, job_a, job_b, lineno=None, snakefile=None):
        super().__init__(
            ""Rules {job_a} and {job_b} are ambiguous for the file {f}.\n""
            ""Expected input files:\n""
            ""\t{job_a}: {job_a.input}\n""
            ""\t{job_b}: {job_b.input}"".format(job_a=job_a,
                                              job_b=job_b,
                                              f=filename),
            lineno=lineno,
            snakefile=snakefile)
        self.rule1, self.rule2 = job_a.rule, job_b.rule


class CyclicGraphException(RuleException):
    def __init__(self, repeatedrule, file, rule=None):
        super().__init__(""Cyclic dependency on rule {}."".format(repeatedrule),
                         rule=rule)
        self.file = file


class MissingRuleException(RuleException):
    def __init__(self, file, lineno=None, snakefile=None):
        super().__init__(
            ""No rule to produce {} (if you use input functions make sure that they don't raise unexpected exceptions)."".format(
                file),
            lineno=lineno,
            snakefile=snakefile)


class UnknownRuleException(RuleException):
    def __init__(self, name, prefix="""", lineno=None, snakefile=None):
        msg = ""There is no rule named {}."".format(name)
        if prefix:
            msg = ""{} {}"".format(prefix, msg)
        super().__init__(msg, lineno=lineno, snakefile=snakefile)


class NoRulesException(RuleException):
    def __init__(self, lineno=None, snakefile=None):
        super().__init__(""There has to be at least one rule."",
                         lineno=lineno,
                         snakefile=snakefile)


class IncompleteFilesException(RuleException):
    def __init__(self, files):
        super().__init__(
            ""The files below seem to be incomplete. ""
            ""If you are sure that certain files are not incomplete, ""
            ""mark them as complete with\n\n""
            ""    snakemake --cleanup-metadata <filenames>\n\n""
            ""To re-generate the files rerun your command with the ""
            ""--rerun-incomplete flag.\nIncomplete files:\n{}"".format(
                ""\n"".join(files)))


class IOFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class RemoteFileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class S3FileException(RuleException):
    def __init__(self, msg, lineno=None, snakefile=None):
        super().__init__(msg, lineno=lineno, snakefile=snakefile)

class ClusterJobException(RuleException):
    def __init__(self, job, jobid, jobscript):
        super().__init__(
            ""Error executing rule {} on cluster (jobid: {}, jobscript: {}). ""
            ""For detailed error see the cluster log."".format(job.rule.name,
                                                             jobid, jobscript),
            lineno=job.rule.lineno,
            snakefile=job.rule.snakefile)


class CreateRuleException(RuleException):
    pass


class TerminatedException(Exception):
    pass
/n/n/nsnakemake/executors.py/n/n__author__ = ""Johannes Köster""
__contributors__ = [""David Alexander""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import time
import datetime
import json
import textwrap
import stat
import shutil
import random
import string
import threading
import concurrent.futures
import subprocess
import signal
from functools import partial
from itertools import chain
from collections import namedtuple

from snakemake.jobs import Job
from snakemake.shell import shell
from snakemake.logging import logger
from snakemake.stats import Stats
from snakemake.utils import format, Unformattable
from snakemake.io import get_wildcard_names, Wildcards
from snakemake.exceptions import print_exception, get_exception_origin
from snakemake.exceptions import format_error, RuleException
from snakemake.exceptions import ClusterJobException, ProtectedOutputException, WorkflowError
from snakemake.futures import ProcessPoolExecutor


class AbstractExecutor:
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 printthreads=True,
                 latency_wait=3,
                 benchmark_repeats=1):
        self.workflow = workflow
        self.dag = dag
        self.quiet = quiet
        self.printreason = printreason
        self.printshellcmds = printshellcmds
        self.printthreads = printthreads
        self.latency_wait = latency_wait
        self.benchmark_repeats = benchmark_repeats

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.check_protected_output()
        self._run(job)
        callback(job)

    def shutdown(self):
        pass

    def _run(self, job):
        self.printjob(job)

    def rule_prefix(self, job):
        return ""local "" if self.workflow.is_local(job.rule) else """"

    def printjob(self, job):
        # skip dynamic jobs that will be ""executed"" only in dryrun mode
        if self.dag.dynamic(job):
            return

        def format_files(job, io, ruleio, dynamicio):
            for f in io:
                f_ = ruleio[f]
                if f in dynamicio:
                    yield ""{} (dynamic)"".format(f.format_dynamic())
                else:
                    yield f

        priority = self.dag.priority(job)
        logger.job_info(jobid=self.dag.jobid(job),
                        msg=job.message,
                        name=job.rule.name,
                        local=self.workflow.is_local(job.rule),
                        input=list(format_files(job, job.input, job.ruleio,
                                                job.dynamic_input)),
                        output=list(format_files(job, job.output, job.ruleio,
                                                 job.dynamic_output)),
                        log=list(job.log),
                        benchmark=job.benchmark,
                        reason=str(self.dag.reason(job)),
                        resources=job.resources_dict,
                        priority=""highest""
                        if priority == Job.HIGHEST_PRIORITY else priority,
                        threads=job.threads)

        if job.dynamic_output:
            logger.info(""Subsequent jobs will be added dynamically ""
                        ""depending on the output of this rule"")

    def print_job_error(self, job):
        logger.error(""Error in job {} while creating output file{} {}."".format(
            job, ""s"" if len(job.output) > 1 else """", "", "".join(job.output)))

    def finish_job(self, job):
        self.dag.handle_touch(job)
        self.dag.check_output(job, wait=self.latency_wait)
        self.dag.handle_remote(job)
        self.dag.handle_protected(job)
        self.dag.handle_temp(job)


class DryrunExecutor(AbstractExecutor):
    def _run(self, job):
        super()._run(job)
        logger.shellcmd(job.shellcmd)


class RealExecutor(AbstractExecutor):
    def __init__(self, workflow, dag,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        self.stats = Stats()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job)
        self.stats.report_job_start(job)
        try:
            self.workflow.persistence.started(job)
        except IOError as e:
            logger.info(
                ""Failed to set marker file for job started ({}). ""
                ""Snakemake will work, but cannot ensure that output files ""
                ""are complete in case of a kill signal or power loss. ""
                ""Please ensure write permissions for the ""
                ""directory {}"".format(e, self.workflow.persistence.path))

    def finish_job(self, job):
        super().finish_job(job)
        self.stats.report_job_end(job)
        try:
            self.workflow.persistence.finished(job)
        except IOError as e:
            logger.info(""Failed to remove marker file for job started ""
                        ""({}). Please ensure write permissions for the ""
                        ""directory {}"".format(e,
                                              self.workflow.persistence.path))


class TouchExecutor(RealExecutor):
    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        try:
            for f in job.expanded_output:
                f.touch()
            if job.benchmark:
                job.benchmark.touch()
            time.sleep(0.1)
            self.finish_job(job)
            callback(job)
        except OSError as ex:
            print_exception(ex, self.workflow.linemaps)
            error_callback(job)


_ProcessPoolExceptions = (KeyboardInterrupt, )
try:
    from concurrent.futures.process import BrokenProcessPool
    _ProcessPoolExceptions = (KeyboardInterrupt, BrokenProcessPool)
except ImportError:
    pass


class CPUExecutor(RealExecutor):
    def __init__(self, workflow, dag, workers,
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 threads=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)

        self.pool = (concurrent.futures.ThreadPoolExecutor(max_workers=workers)
                     if threads else ProcessPoolExecutor(max_workers=workers))

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        job.prepare()
        super()._run(job)

        benchmark = None
        if job.benchmark is not None:
            benchmark = str(job.benchmark)

        future = self.pool.submit(
            run_wrapper, job.rule.run_func, job.input.plainstrings(),
            job.output.plainstrings(), job.params, job.wildcards, job.threads,
            job.resources, job.log.plainstrings(), job.rule.version, benchmark,
            self.benchmark_repeats, self.workflow.linemaps, self.workflow.debug)
        future.add_done_callback(partial(self._callback, job, callback,
                                         error_callback))

    def shutdown(self):
        self.pool.shutdown()

    def cancel(self):
        self.pool.shutdown()

    def _callback(self, job, callback, error_callback, future):
        try:
            ex = future.exception()
            if ex:
                raise ex
            self.finish_job(job)
            callback(job)
        except _ProcessPoolExceptions:
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            # no error callback, just silently ignore the interrupt as the main scheduler is also killed
        except (Exception, BaseException) as ex:
            self.print_job_error(job)
            print_exception(ex, self.workflow.linemaps)
            job.cleanup()
            self.workflow.persistence.cleanup(job)
            error_callback(job)


class ClusterExecutor(RealExecutor):

    default_jobscript = ""jobscript.sh""

    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None):
        super().__init__(workflow, dag,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats)
        if workflow.snakemakepath is None:
            raise ValueError(""Cluster executor needs to know the path ""
                             ""to the snakemake binary."")

        jobscript = workflow.jobscript
        if jobscript is None:
            jobscript = os.path.join(os.path.dirname(__file__),
                                     self.default_jobscript)
        try:
            with open(jobscript) as f:
                self.jobscript = f.read()
        except IOError as e:
            raise WorkflowError(e)

        if not ""jobid"" in get_wildcard_names(jobname):
            raise WorkflowError(
                ""Defined jobname (\""{}\"") has to contain the wildcard {jobid}."")

        self.exec_job = (
            'cd {workflow.workdir_init} && '
            '{workflow.snakemakepath} --snakefile {workflow.snakefile} '
            '--force -j{cores} --keep-target-files '
            '--wait-for-files {job.input} --latency-wait {latency_wait} '
            '--benchmark-repeats {benchmark_repeats} '
            '{overwrite_workdir} {overwrite_config} --nocolor '
            '--notemp --quiet --no-hooks --nolock {target}')

        if printshellcmds:
            self.exec_job += "" --printshellcmds ""

        if not any(dag.dynamic_output_jobs):
            # disable restiction to target rule in case of dynamic rules!
            self.exec_job += "" --allowed-rules {job.rule.name} ""
        self.jobname = jobname
        self._tmpdir = None
        self.cores = cores if cores else """"
        self.cluster_config = cluster_config if cluster_config else dict()

        self.active_jobs = list()
        self.lock = threading.Lock()
        self.wait = True
        self.wait_thread = threading.Thread(target=self._wait_for_jobs)
        self.wait_thread.daemon = True
        self.wait_thread.start()

    def shutdown(self):
        with self.lock:
            self.wait = False
        self.wait_thread.join()
        shutil.rmtree(self.tmpdir)

    def cancel(self):
        self.shutdown()

    def _run(self, job, callback=None, error_callback=None):
        super()._run(job, callback=callback, error_callback=error_callback)
        logger.shellcmd(job.shellcmd)

    @property
    def tmpdir(self):
        if self._tmpdir is None:
            while True:
                self._tmpdir = "".snakemake/tmp."" + """".join(
                    random.sample(string.ascii_uppercase + string.digits, 6))
                if not os.path.exists(self._tmpdir):
                    os.mkdir(self._tmpdir)
                    break
        return os.path.abspath(self._tmpdir)

    def get_jobscript(self, job):
        return os.path.join(
            self.tmpdir,
            job.format_wildcards(self.jobname,
                                 rulename=job.rule.name,
                                 jobid=self.dag.jobid(job),
                                 cluster=self.cluster_wildcards(job)))

    def spawn_jobscript(self, job, jobscript, **kwargs):
        overwrite_workdir = """"
        if self.workflow.overwrite_workdir:
            overwrite_workdir = ""--directory {} "".format(
                self.workflow.overwrite_workdir)
        overwrite_config = """"
        if self.workflow.overwrite_configfile:
            overwrite_config = ""--configfile {} "".format(
                self.workflow.overwrite_configfile)
        if self.workflow.config_args:
            overwrite_config += ""--config {} "".format(
                "" "".join(self.workflow.config_args))

        target = job.output if job.output else job.rule.name
        format = partial(str.format,
                         job=job,
                         overwrite_workdir=overwrite_workdir,
                         overwrite_config=overwrite_config,
                         workflow=self.workflow,
                         cores=self.cores,
                         properties=job.json(),
                         latency_wait=self.latency_wait,
                         benchmark_repeats=self.benchmark_repeats,
                         target=target, **kwargs)
        try:
            exec_job = format(self.exec_job)
            with open(jobscript, ""w"") as f:
                print(format(self.jobscript, exec_job=exec_job), file=f)
        except KeyError as e:
            raise WorkflowError(
                ""Error formatting jobscript: {} not found\n""
                ""Make sure that your custom jobscript it up to date."".format(e))
        os.chmod(jobscript, os.stat(jobscript).st_mode | stat.S_IXUSR)

    def cluster_wildcards(self, job):
        cluster = self.cluster_config.get(""__default__"", dict()).copy()
        cluster.update(self.cluster_config.get(job.rule.name, dict()))
        return Wildcards(fromdict=cluster)


GenericClusterJob = namedtuple(""GenericClusterJob"", ""job callback error_callback jobscript jobfinished jobfailed"")


class GenericClusterExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config)
        self.submitcmd = submitcmd
        self.external_jobid = dict()
        self.exec_job += ' && touch ""{jobfinished}"" || touch ""{jobfailed}""'

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        jobfinished = os.path.join(self.tmpdir, ""{}.jobfinished"".format(jobid))
        jobfailed = os.path.join(self.tmpdir, ""{}.jobfailed"".format(jobid))
        self.spawn_jobscript(job, jobscript,
                             jobfinished=jobfinished,
                             jobfailed=jobfailed)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)
        try:
            ext_jobid = subprocess.check_output(
                '{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                                   jobscript=jobscript),
                shell=True).decode().split(""\n"")
        except subprocess.CalledProcessError as ex:
            raise WorkflowError(
                ""Error executing jobscript (exit code {}):\n{}"".format(
                    ex.returncode, ex.output.decode()),
                rule=job.rule)
        if ext_jobid and ext_jobid[0]:
            ext_jobid = ext_jobid[0]
            self.external_jobid.update((f, ext_jobid) for f in job.output)
            logger.debug(""Submitted job {} with external jobid {}."".format(
                jobid, ext_jobid))

        submit_callback(job)
        with self.lock:
            self.active_jobs.append(GenericClusterJob(job, callback, error_callback, jobscript, jobfinished, jobfailed))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    if os.path.exists(active_job.jobfinished):
                        os.remove(active_job.jobfinished)
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    elif os.path.exists(active_job.jobfailed):
                        os.remove(active_job.jobfailed)
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            active_job.jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
                    else:
                        self.active_jobs.append(active_job)
            time.sleep(1)


SynchronousClusterJob = namedtuple(""SynchronousClusterJob"", ""job callback error_callback jobscript process"")


class SynchronousClusterExecutor(ClusterExecutor):
    """"""
    invocations like ""qsub -sync y"" (SGE) or ""bsub -K"" (LSF) are
    synchronous, blocking the foreground thread and returning the
    remote exit code at remote exit.
    """"""

    def __init__(self, workflow, dag, cores,
                 submitcmd=""qsub"",
                 cluster_config=None,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 latency_wait=3,
                 benchmark_repeats=1):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        self.submitcmd = submitcmd
        self.external_jobid = dict()

    def cancel(self):
        logger.info(""Will exit after finishing currently running jobs."")
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        workdir = os.getcwd()
        jobid = self.dag.jobid(job)

        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        deps = "" "".join(self.external_jobid[f] for f in job.input
                        if f in self.external_jobid)
        try:
            submitcmd = job.format_wildcards(
                self.submitcmd,
                dependencies=deps,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        process = subprocess.Popen('{submitcmd} ""{jobscript}""'.format(submitcmd=submitcmd,
                                           jobscript=jobscript), shell=True)
        submit_callback(job)

        with self.lock:
            self.active_jobs.append(SynchronousClusterJob(job, callback, error_callback, jobscript, process))

    def _wait_for_jobs(self):
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    exitcode = active_job.process.poll()
                    if exitcode is None:
                        # job not yet finished
                        self.active_jobs.append(active_job)
                    elif exitcode == 0:
                        # job finished successfully
                        os.remove(active_job.jobscript)
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        # job failed
                        os.remove(active_job.jobscript)
                        self.print_job_error(active_job.job)
                        print_exception(ClusterJobException(active_job.job, self.dag.jobid(active_job.job),
                                                            jobscript),
                                        self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


DRMAAClusterJob = namedtuple(""DRMAAClusterJob"", ""job jobid callback error_callback jobscript"")


class DRMAAExecutor(ClusterExecutor):
    def __init__(self, workflow, dag, cores,
                 jobname=""snakejob.{rulename}.{jobid}.sh"",
                 printreason=False,
                 quiet=False,
                 printshellcmds=False,
                 drmaa_args="""",
                 latency_wait=3,
                 benchmark_repeats=1,
                 cluster_config=None, ):
        super().__init__(workflow, dag, cores,
                         jobname=jobname,
                         printreason=printreason,
                         quiet=quiet,
                         printshellcmds=printshellcmds,
                         latency_wait=latency_wait,
                         benchmark_repeats=benchmark_repeats,
                         cluster_config=cluster_config, )
        try:
            import drmaa
        except ImportError:
            raise WorkflowError(
                ""Python support for DRMAA is not installed. ""
                ""Please install it, e.g. with easy_install3 --user drmaa"")
        except RuntimeError as e:
            raise WorkflowError(""Error loading drmaa support:\n{}"".format(e))
        self.session = drmaa.Session()
        self.drmaa_args = drmaa_args
        self.session.initialize()
        self.submitted = list()

    def cancel(self):
        from drmaa.const import JobControlAction
        for jobid in self.submitted:
            self.session.control(jobid, JobControlAction.TERMINATE)
        self.shutdown()

    def run(self, job,
            callback=None,
            submit_callback=None,
            error_callback=None):
        super()._run(job)
        jobscript = self.get_jobscript(job)
        self.spawn_jobscript(job, jobscript)

        try:
            drmaa_args = job.format_wildcards(
                self.drmaa_args,
                cluster=self.cluster_wildcards(job))
        except AttributeError as e:
            raise WorkflowError(str(e), rule=job.rule)

        import drmaa
        try:
            jt = self.session.createJobTemplate()
            jt.remoteCommand = jobscript
            jt.nativeSpecification = drmaa_args

            jobid = self.session.runJob(jt)
        except (drmaa.errors.InternalException,
                drmaa.errors.InvalidAttributeValueException) as e:
            print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                            self.workflow.linemaps)
            error_callback(job)
            return
        logger.info(""Submitted DRMAA job (jobid {})"".format(jobid))
        self.submitted.append(jobid)
        self.session.deleteJobTemplate(jt)

        submit_callback(job)

        with self.lock:
            self.active_jobs.append(DRMAAClusterJob(job, jobid, callback, error_callback, jobscript))

    def shutdown(self):
        super().shutdown()
        self.session.exit()

    def _wait_for_jobs(self):
        import drmaa
        while True:
            with self.lock:
                if not self.wait:
                    return
                active_jobs = self.active_jobs
                self.active_jobs = list()
                for active_job in active_jobs:
                    try:
                        retval = self.session.wait(active_job.jobid,
                                                   drmaa.Session.TIMEOUT_NO_WAIT)
                    except drmaa.errors.InternalException as e:
                        print_exception(WorkflowError(""DRMAA Error: {}"".format(e)),
                                        self.workflow.linemaps)
                        os.remove(active_job.jobscript)
                        active_job.error_callback(active_job.job)
                        break
                    except drmaa.errors.ExitTimeoutException as e:
                        # job still active
                        self.active_jobs.append(active_job)
                        break
                    # job exited
                    os.remove(active_job.jobscript)
                    if retval.hasExited and retval.exitStatus == 0:
                        self.finish_job(active_job.job)
                        active_job.callback(active_job.job)
                    else:
                        self.print_job_error(active_job.job)
                        print_exception(
                            ClusterJobException(active_job.job, self.dag.jobid(active_job.job), active_job.jobscript),
                            self.workflow.linemaps)
                        active_job.error_callback(active_job.job)
            time.sleep(1)


def run_wrapper(run, input, output, params, wildcards, threads, resources, log,
                version, benchmark, benchmark_repeats, linemaps, debug=False):
    """"""
    Wrapper around the run method that handles directory creation and
    output file deletion on error.

    Arguments
    run       -- the run method
    input     -- list of input files
    output    -- list of output files
    wildcards -- so far processed wildcards
    threads   -- usable threads
    log       -- list of log files
    """"""
    if os.name == ""posix"" and debug:
        sys.stdin = open('/dev/stdin')

    try:
        runs = 1 if benchmark is None else benchmark_repeats
        wallclock = []
        for i in range(runs):
            w = time.time()
            # execute the actual run method.
            run(input, output, params, wildcards, threads, resources, log,
                version)
            w = time.time() - w
            wallclock.append(w)

    except (KeyboardInterrupt, SystemExit) as e:
        # re-raise the keyboard interrupt in order to record an error in the scheduler but ignore it
        raise e
    except (Exception, BaseException) as ex:
        # this ensures that exception can be re-raised in the parent thread
        lineno, file = get_exception_origin(ex, linemaps)
        raise RuleException(format_error(ex, lineno,
                                         linemaps=linemaps,
                                         snakefile=file,
                                         show_traceback=True))

    if benchmark is not None:
        try:
            with open(benchmark, ""w"") as f:
                json.dump({
                    name: {
                        ""s"": times,
                        ""h:m:s"": [str(datetime.timedelta(seconds=t))
                                  for t in times]
                    }
                    for name, times in zip(""wall_clock_times"".split(),
                                           [wallclock])
                }, f,
                          indent=4)
        except (Exception, BaseException) as ex:
            raise WorkflowError(ex)
/n/n/nsnakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
import functools
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.logging import logger
import snakemake.remote_providers.S3 as S3

def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None

        return obj

    def __init__(self, file):
        self._remote_object = None
        if self.is_remote:
            additional_args = get_flag_value(self._file, ""additional_remote_args"") if get_flag_value(self._file, ""additional_remote_args"") else []
            additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
            self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, *additional_args, **additional_kwargs)
        pass

    def _referToRemote(func):
        """""" 
            A decorator so that if the file is remote and has a version 
            of the same file-related function, call that version instead. 
        """"""
        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            if self.is_remote:
                if self.remote_object:
                    if hasattr( self.remote_object, func.__name__):
                        return getattr( self.remote_object, func.__name__)(*args, **kwargs)
            return func(self, *args, **kwargs)
        return wrapper

    @property
    def is_remote(self):
        return is_flagged(self._file, ""remote"")
    
    @property
    def remote_object(self):
        if not self._remote_object:
            if self.is_remote:
               additional_kwargs = get_flag_value(self._file, ""additional_remote_kwargs"") if get_flag_value(self._file, ""additional_remote_kwargs"") else {}
               self._remote_object = get_flag_value(self._file, ""remote_provider"").RemoteObject(self, **additional_kwargs)
        return self._remote_object
    

    @property
    @_referToRemote
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    @_referToRemote
    def exists(self):
        return os.path.exists(self.file)

    @property
    def exists_local(self):
        return os.path.exists(self.file)

    @property
    def exists_remote(self):
        return (self.is_remote and self.remote_object.exists())
    

    @property
    def protected(self):
        return self.exists_local and not os.access(self.file, os.W_OK)
    
    @property
    @_referToRemote
    def mtime(self):
        return lstat(self.file).st_mtime

    @property
    def flags(self):
        return getattr(self._file, ""flags"", {})

    @property
    def mtime_local(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    @_referToRemote
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    @property
    def size_local(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists_local and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def download_from_remote(self):
        logger.info(""Downloading from remote: {}"".format(self.file))

        if self.is_remote and self.remote_object.exists():
            self.remote_object.download()
        else:
            raise RemoteFileException(""The file to be downloaded does not seem to exist remotely."")
 
    def upload_to_remote(self):
        logger.info(""Uploading to remote: {}"".format(self.file))

        if self.is_remote and not self.remote_object.exists():
            self.remote_object.upload()
        else:
            raise RemoteFileException(""The file to be uploaded does not seem to exist remotely."")

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self, times=None):
        """""" times must be 2-tuple: (atime, mtime) """"""
        try:
            lutime(self.file, times)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        # this bit ensures flags are transferred over to files after
        # wildcards are applied

        flagsBeforeWildcardResolution = getattr(f, ""flags"", {})


        fileWithWildcardsApplied = IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                                      rule=self.rule)

        fileWithWildcardsApplied.set_flags(getattr(f, ""flags"", {}))

        return fileWithWildcardsApplied

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def clone_flags(self, other):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        if isinstance(other._file, AnnotatedString):
            self._file.flags = getattr(other._file, ""flags"", {})

    def set_flags(self, flags):
        if isinstance(self._file, str):
            self._file = AnnotatedString(self._file)
        self._file.flags = flags

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags and value.flags[flag]
    if isinstance(value, _IOFile):
        return flag in value.flags and value.flags[flag]
    return False

def get_flag_value(value, flag_type):
    if isinstance(value, AnnotatedString):
        if flag_type in value.flags:
            return value.flags[flag_type]
        else:
            return None

def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    if is_flagged(value, ""remote""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"", True)
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")

def remote(value, provider=S3, keep=False, additional_args=None, additional_kwargs=None):

    additional_args = [] if not additional_args else additional_args
    additional_kwargs = {} if not additional_kwargs else additional_kwargs

    if not provider:
        raise RemoteFileException(""Provider (S3, etc.) must be specified for remote file as kwarg."")
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Remote and temporary flags are mutually exclusive."")
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Remote and protected flags are mutually exclusive."")
    return flag(
                flag(
                    flag( 
                        flag( 
                            flag(value, ""remote""), 
                            ""remote_provider"", 
                            provider
                        ), 
                        ""additional_remote_kwargs"", 
                        additional_kwargs
                    ),
                    ""additional_remote_args"",
                    additional_args
                ),
                ""keep"",
                keep
            )

def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards

def glob_wildcards_remote(pattern, provider=S3, additional_kwargs=None):
    additional_kwargs = additional_kwargs if additional_kwargs else {}
    referenceObj = IOFile(remote(pattern, provider=provider, **additional_kwargs))
    key_list = [k.name for k in referenceObj._remote_object.list] 

    pattern = ""./""+ referenceObj._remote_object.name
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for f in key_list:
        match = re.match(pattern, f)
        if match:
            for name, value in match.groupdict().items():
                getattr(wildcards, name).append(value)
    return wildcards

# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/nsnakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile, is_flagged, contains_wildcard
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    fileToYield = IOFile(f, self.rule)

                    fileToYield.clone_flags(f_)

                    yield fileToYield
            else:
                yield f

    @property
    def expanded_input(self):
        """""" Iterate over input files while dynamic output is expanded. """"""

        for f, f_ in zip(self.input, self.rule.input):
            if not type(f_).__name__ == ""function"":
                if type(f_.file).__name__ not in [""str"", ""function""]:
                    if contains_wildcard(f_):

                        expansion = self.expand_dynamic(
                            f_,
                            restriction=self.wildcards,
                            omit_value=_IOFile.dynamic_fill)
                        if not expansion:
                            yield f_
                        for f, _ in expansion:

                            fileToYield = IOFile(f, self.rule)

                            fileToYield.clone_flags(f_)

                            yield fileToYield
                    else:
                        yield f
                else:
                    yield f
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)


    @property
    def present_remote_input(self):
        files = set()

        for f in self.input:
            if f.is_remote:
                if f.exists_remote:
                    files.add(f)
        return files
    
    @property
    def present_remote_output(self):
        files = set()

        for f in self.remote_output:
            if f.exists_remote:
                files.add(f)
        return files

    @property
    def missing_remote_input(self):
        return self.remote_input - self.present_remote_input

    @property
    def missing_remote_output(self):
        return self.remote_output - self.present_remote_output

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files


    @property
    def remote_input(self):
        for f in self.input:
            if f.is_remote:
                yield f

    @property
    def remote_output(self):
        for f in self.output:
            if f.is_remote:
                yield f

    @property
    def remote_input_newer_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_input_older_than_local(self):
        files = set()
        for f in self.remote_input:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_newer_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime > f.mtime_local):
                files.add(f)
        return files

    @property
    def remote_output_older_than_local(self):
        files = set()
        for f in self.remote_output:
            if (f.exists_remote and f.exists_local) and (f.mtime < f.mtime_local):
                files.add(f)
        return files

    def transfer_updated_files(self):
        for f in self.remote_output_older_than_local | self.remote_input_older_than_local:
            f.upload_to_remote()

        for f in self.remote_output_newer_than_local | self.remote_input_newer_than_local:
            f.download_from_remote()
    
    @property
    def files_to_download(self):
        toDownload = set()

        for f in self.input:
            if f.is_remote:
                if not f.exists_local and f.exists_remote:
                    toDownload.add(f)

        toDownload = toDownload | self.remote_input_newer_than_local
        return toDownload

    @property
    def files_to_upload(self):
        return self.missing_remote_input & self.remote_input_older_than_local

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()

        for f in self.files_to_download:
            f.download_from_remote()

        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]

        to_remove.extend([f for f in self.remote_input if f.exists])
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

            self.rmdir_empty_remote_dirs()

    @property
    def empty_remote_dirs(self):
        remote_files = [f for f in (set(self.output) | set(self.input)) if f.is_remote]
        emptyDirsToRemove = set(os.path.dirname(f) for f in remote_files if not len(os.listdir(os.path.dirname(f))))
        return emptyDirsToRemove

    def rmdir_empty_remote_dirs(self):
        for d in self.empty_remote_dirs:
            pathToDel = d
            while len(pathToDel) > 0 and len(os.listdir(pathToDel)) == 0:
                logger.info(""rmdir empty dir: {}"".format(pathToDel))
                os.rmdir(pathToDel)
                pathToDel = os.path.dirname(pathToDel)


    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/nsnakemake/remote_providers/RemoteObjectProvider.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

from abc import ABCMeta, abstractmethod


class RemoteObject:
    """""" This is an abstract class to be used to derive remote object classes for 
        different cloud storage providers. For example, there could be classes for interacting with 
        Amazon AWS S3 and Google Cloud Storage, both derived from this common base class.
    """"""
    __metaclass__ = ABCMeta

    def __init__(self, ioFile):
        self._iofile = ioFile
        self._file = ioFile._file

    @abstractmethod
    def file(self):
        pass

    @abstractmethod
    def exists(self):
        pass

    @abstractmethod
    def mtime(self):
        pass

    @abstractmethod
    def size(self):
        pass

    @abstractmethod
    def download(self, *args, **kwargs):
        pass

    @abstractmethod
    def upload(self, *args, **kwargs):
        pass

    @abstractmethod
    def list(self, *args, **kwargs):
        pass

    @abstractmethod
    def name(self, *args, **kwargs):
        pass
/n/n/nsnakemake/remote_providers/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

import re

from snakemake.remote_providers.RemoteObjectProvider import RemoteObject
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError, RemoteFileException, S3FileException
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import memoize

import boto


class RemoteObject(RemoteObject):
    """""" This is a class to interact with the AWS S3 object store.
    """"""

    def __init__(self, *args, **kwargs):
        super(RemoteObject, self).__init__(*args, **kwargs)

        # pass all args but the first, which is the ioFile
        self._s3c = S3Helper(*args[1:], **kwargs)

    # === Implementations of abstract class members ===

    def file(self):
        return self._file

    def exists(self):
        if self._matched_s3_path:
            return self._s3c.exists_in_bucket(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file cannot be parsed as an s3 path in form 'bucket/key': %s"" % self.file())

    def mtime(self):
        if self.exists():
            return self._s3c.key_last_modified(self.s3_bucket, self.s3_key)
        else:
            raise S3FileException(""The file does not seem to exist remotely: %s"" % self.file())

    def size(self):
        if self.exists():
            return self._s3c.key_size(self.s3_bucket, self.s3_key)
        else:
            return self._iofile.size_local

    def download(self):
        self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file())

    def upload(self):
        conn = boto.connect_s3()
        if self.size() > 5000:
            self._s3c.upload_to_s3_multipart(self.s3_bucket, self.file(), self.s3_key)
        else:
            self._s3c.upload_to_s3(self.s3_bucket, self.file(), self.s3_key)

    @property
    def list(self):
        return self._s3c.list_keys(self.s3_bucket)

    # === Related methods ===

    @property
    def _matched_s3_path(self):
        return re.search(""(?P<bucket>[^/]*)/(?P<key>.*)"", self.file())

    @property
    def s3_bucket(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""bucket"")
        return None

    @property
    def name(self):
        return self.s3_key

    @property
    def s3_key(self):
        if len(self._matched_s3_path.groups()) == 2:
            return self._matched_s3_path.group(""key"")

    def s3_create_stub(self):
        if self._matched_s3_path:
            if not self.exists:
                self._s3c.download_from_s3(self.s3_bucket, self.s3_key, self.file, createStubOnly=True)
        else:
            raise S3FileException(""The file to be downloaded cannot be parsed as an s3 path in form 'bucket/key': %s"" %
                                  self.file())
/n/n/nsnakemake/remote_providers/__init__.py/n/n
/n/n/nsnakemake/remote_providers/implementations/S3.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os
import math
import time
import email.utils
from time import mktime
import datetime
from multiprocessing import Pool

# third-party modules
import boto
from boto.s3.key import Key
from filechunkio import FileChunkIO


class S3Helper(object):

    def __init__(self, *args, **kwargs):
        # as per boto, expects the environment variables to be set:
        # AWS_ACCESS_KEY_ID
        # AWS_SECRET_ACCESS_KEY
        # Otherwise these values need to be passed in as kwargs
        self.conn = boto.connect_s3(*args, **kwargs)

    def upload_to_s3(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        k = Key(b)

        if key:
            k.key = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)
            k.key = pathKey
        try:
            bytesWritten = k.set_contents_from_filename(
                filePath,
                replace=replace,
                reduced_redundancy=reduced_redundancy,
                headers=headers)
            if bytesWritten:
                return k.key
            else:
                return None
        except:
            return None

    def download_from_s3(
            self,
            bucketName,
            key,
            destinationPath=None,
            expandKeyIntoDirs=True,
            makeDestDirs=True,
            headers=None, createStubOnly=False):
        """""" Download a file from s3

            This function downloads an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                destinationPath: If specified, the file will be saved to this path, otherwise cwd.
                expandKeyIntoDirs: Since S3 keys can include slashes, if this is True (defult)
                    then S3 keys with slashes are expanded into directories on the receiving end.
                    If it is False, the key is passed to os.path.basename() to get the substring
                    following the last slash.
                makeDestDirs: If this is True (default) and the destination path includes directories
                    that do not exist, they will be created.
                headers: Additional headers to pass to AWS

            Returns:
                The destination path of the downloaded file on the receiving end, or None if the filePath
                could not be downloaded
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)

        if destinationPath:
            destinationPath = os.path.realpath(os.path.expanduser(destinationPath))
        else:
            if expandKeyIntoDirs:
                destinationPath = os.path.join(os.getcwd(), key)
            else:
                destinationPath = os.path.join(os.getcwd(), os.path.basename(key))

        # if the destination path does not exist
        if not os.path.exists(os.path.dirname(destinationPath)) and makeDestDirs:
            os.makedirs(os.path.dirname(destinationPath))

        k.key = key if key else os.path.basename(filePath)

        try:
            if not createStubOnly:
                k.get_contents_to_filename(destinationPath, headers=headers)
            else:
                # just create an empty file with the right timestamps
                with open(destinationPath, 'wb') as fp:
                    modified_tuple = email.utils.parsedate_tz(k.last_modified)
                    modified_stamp = int(email.utils.mktime_tz(modified_tuple))
                    os.utime(fp.name, (modified_stamp, modified_stamp))
            return destinationPath
        except:
            return None

    def _upload_part(self, bucketName, multipart_id, part_num, source_path, offset, bytesToWrite, numberOfRetries=5):

        def _upload(retriesRemaining=numberOfRetries):
            try:
                b = self.conn.get_bucket(bucketName)
                for mp in b.get_all_multipart_uploads():
                    if mp.id == multipart_id:
                        with FileChunkIO(source_path, 'r', offset=offset, bytes=bytesToWrite) as fp:
                            mp.upload_part_from_file(fp=fp, part_num=part_num)
                        break
            except Exception() as e:
                if retriesRemaining:
                    _upload(retriesRemaining=retriesRemaining - 1)
                else:
                    raise e

        _upload()

    def upload_to_s3_multipart(
            self,
            bucketName,
            filePath,
            key=None,
            useRelativePathForKey=True,
            relativeStartDir=None,
            replace=False,
            reduced_redundancy=False,
            headers=None,
            parallel_processes=4):
        """""" Upload a file to S3

            This function uploads a file to an AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                filePath: The path to the file to upload.
                key: The key to set for the file on S3. If not specified, this will default to the
                    name of the file.
                useRelativePathForKey: If set to True (default), and key is None, the S3 key will include slashes
                    representing the path of the file relative to the CWD. If False only the
                    file basename will be used for the key.
                relativeStartDir: The start dir to use for useRelativePathForKey. No effect if key is set.
                replace: If True a file with the same key will be replaced with the one being written
                reduced_redundancy: Sets the file to AWS reduced redundancy storage.
                headers: additional heads to pass to AWS
                parallel_processes: Number of concurrent uploads

            Returns: The key of the file on S3 if written, None otherwise
        """"""
        filePath = os.path.realpath(os.path.expanduser(filePath))

        assert bucketName, ""bucketName must be specified""
        assert os.path.exists(filePath), ""The file path specified does not exist: %s"" % filePath
        assert os.path.isfile(filePath), ""The file path specified does not appear to be a file: %s"" % filePath

        try:
            b = self.conn.get_bucket(bucketName)
        except:
            b = self.conn.create_bucket(bucketName)

        pathKey = None
        if key:
            pathKey = key
        else:
            if useRelativePathForKey:
                if relativeStartDir:
                    pathKey = os.path.relpath(filePath, relativeStartDir)
                else:
                    pathKey = os.path.relpath(filePath)
            else:
                pathKey = os.path.basename(filePath)

        mp = b.initiate_multipart_upload(pathKey, headers=headers)

        sourceSize = os.stat(filePath).st_size

        bytesPerChunk = 52428800  # 50MB = 50 * 1024 * 1024
        chunkCount = int(math.ceil(sourceSize / float(bytesPerChunk)))

        pool = Pool(processes=parallel_processes)
        for i in range(chunkCount):
            offset = i * bytesPerChunk
            remainingBytes = sourceSize - offset
            bytesToWrite = min([bytesPerChunk, remainingBytes])
            partNum = i + 1
            pool.apply_async(self._upload_part, [bucketName, mp.id, partNum, filePath, offset, bytesToWrite])
        pool.close()
        pool.join()

        if len(mp.get_all_parts()) == chunkCount:
            mp.complete_upload()
            try:
                key = b.get_key(pathKey)
                return key.key
            except:
                return None
        else:
            mp.cancel_upload()
            return None

    def delete_from_bucket(self, bucketName, key, headers=None):
        """""" Delete a file from s3

            This function deletes an object from a specified AWS S3 bucket.

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                The name of the object deleted
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        ret = k.delete(headers=headers)
        return ret.name

    def exists_in_bucket(self, bucketName, key, headers=None):
        """""" Returns whether the key exists in the bucket

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                True | False
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = Key(b)
        k.key = key
        return k.exists(headers=headers)

    def key_size(self, bucketName, key, headers=None):
        """""" Returns the size of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                Size in kb
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        return k.size

    def key_last_modified(self, bucketName, key, headers=None):
        """""" Returns a timestamp of a key based on a HEAD request

            Args:
                bucketName: the name of the S3 bucket to use (bucket name only, not ARN)
                key: the key of the object to delete from the bucket
                headers: Additional headers to pass to AWS

            Returns:
                timestamp
        """"""
        assert bucketName, ""bucketName must be specified""
        assert key, ""Key must be specified""

        b = self.conn.get_bucket(bucketName)
        k = b.lookup(key)

        # email.utils parsing of timestamp mirrors boto whereas
        # time.strptime() can have TZ issues due to DST
        modified_tuple = email.utils.parsedate_tz(k.last_modified)
        epochTime = int(email.utils.mktime_tz(modified_tuple))

        return epochTime

    def list_keys(self, bucketName):
        return self.conn.get_bucket(bucketName).list()
/n/n/nsnakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        # need to clone the flags so intermediate
                        # dynamic remote file paths are expanded and 
                        # removed appropriately
                        ioFile = IOFile(e, rule=branch)
                        ioFile.clone_flags(f)
                        expansion[i].append(ioFile)
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/nsnakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, remote, glob_wildcards, glob_wildcards_remote, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/ntests/test_remote/S3Mocked.py/n/n__author__ = ""Christopher Tomkins-Tinch""
__copyright__ = ""Copyright 2015, Christopher Tomkins-Tinch""
__email__ = ""tomkinsc@broadinstitute.org""
__license__ = ""MIT""

# built-ins
import os, sys
from contextlib import contextmanager
import pickle
import time
import threading

# third-party
import boto
from moto import mock_s3

# intra-module
from snakemake.remote_providers.S3 import RemoteObject as S3RemoteObject
from snakemake.remote_providers.implementations.S3 import S3Helper
from snakemake.decorators import decAllMethods

def noop():
    pass

def pickledMotoWrapper(func):
    """"""
        This is a class decorator that in turn decorates all methods within
        a class to mock out boto calls with moto-simulated ones.
        Since the moto backends are not presistent across calls by default, 
        the wrapper also pickles the bucket state after each function call,
        and restores it before execution. This way uploaded files are available
        for follow-on tasks. Since snakemake may execute with multiple threads
        it also waits for the pickled bucket state file to be available before
        loading it in. This is a hackey alternative to using proper locks,
        but works ok in practice.
    """"""
    def wrapper_func(self, *args, **kwargs):
        motoContextFile = ""motoState.p""

        motoContext = mock_s3()

        # load moto buckets from pickle
        if os.path.isfile(motoContextFile) and os.path.getsize(motoContextFile) > 0:
            with file_lock(motoContextFile):
                with open( motoContextFile, ""rb"" ) as f:
                    motoContext.backends[""global""].buckets = pickle.load( f )

        motoContext.backends[""global""].reset = noop

        mockedFunction = motoContext(func)

        retval = mockedFunction(self, *args, **kwargs)

        with file_lock(motoContextFile):
            with open( motoContextFile, ""wb"" ) as f:
                pickle.dump(motoContext.backends[""global""].buckets, f)

        return retval
    return wrapper_func

@decAllMethods(pickledMotoWrapper, prefix=None)
class RemoteObject(S3RemoteObject):
    """""" 
        This is a derivative of the S3 remote provider that mocks
        out boto-based S3 calls using the ""moto"" Python package.
        Only the initializer is different; it ""uploads"" the input 
        test file to the moto-simulated bucket at the start.
    """"""

    def __init__(self, *args, **kwargs):
        bucketName = 'test-remote-bucket'
        testFile = ""test.txt""

        conn = boto.connect_s3()
        if bucketName not in [b.name for b in conn.get_all_buckets()]:
            conn.create_bucket(bucketName)

        # ""Upload"" files that should be in S3 before tests...
        s3c = S3Helper()
        if not s3c.exists_in_bucket(bucketName, testFile):
            s3c.upload_to_s3(bucketName, testFile)

        return super(RemoteObject, self).__init__(*args, **kwargs)


# ====== Helpers =====

@contextmanager
def file_lock(filepath):
    lock_file = filepath + "".lock""

    while os.path.isfile(lock_file):
        time.sleep(0.1)

    with open(lock_file, 'w') as f:
        f.write(""1"")

    try:
        yield
    finally:
        if os.path.isfile(lock_file):
            os.remove(lock_file)

/n/n/ntests/test_remote/__init__.py/n/n/n/n/ntests/tests.py/n/n__authors__ = [""Tobias Marschall"", ""Marcel Martin"", ""Johannes Köster""]
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import sys
import os
from os.path import join
from subprocess import call
from tempfile import mkdtemp
import hashlib
import urllib
from shutil import rmtree

from snakemake import snakemake


def dpath(path):
    """"""get path to a data file (relative to the directory this
	test lives in)""""""
    return os.path.realpath(join(os.path.dirname(__file__), path))


SCRIPTPATH = dpath(""../bin/snakemake"")


def md5sum(filename):
    data = open(filename, 'rb').read()
    return hashlib.md5(data).hexdigest()


def is_connected():
    try:
        urllib.request.urlopen(""http://www.google.com"", timeout=1)
        return True
    except urllib.request.URLError:
        return False


def run(path,
        shouldfail=False,
        needs_connection=False,
        snakefile=""Snakefile"",
        subpath=None,
        check_md5=True, **params):
    """"""
    Test the Snakefile in path.
    There must be a Snakefile in the path and a subdirectory named
    expected-results.
    """"""
    if needs_connection and not is_connected():
        print(""Skipping test because of missing internet connection"",
              file=sys.stderr)
        return False

    results_dir = join(path, 'expected-results')
    snakefile = join(path, snakefile)
    assert os.path.exists(snakefile)
    assert os.path.exists(results_dir) and os.path.isdir(
        results_dir), '{} does not exist'.format(results_dir)
    tmpdir = mkdtemp()
    try:
        config = {}
        if subpath is not None:
            # set up a working directory for the subworkflow and pass it in `config`
            # for now, only one subworkflow is supported
            assert os.path.exists(subpath) and os.path.isdir(
                subpath), '{} does not exist'.format(subpath)
            subworkdir = os.path.join(tmpdir, ""subworkdir"")
            os.mkdir(subworkdir)
            call('cp `find {} -maxdepth 1 -type f` {}'.format(subpath,
                                                              subworkdir),
                 shell=True)
            config['subworkdir'] = subworkdir

        call('cp `find {} -maxdepth 1 -type f` {}'.format(path, tmpdir),
             shell=True)
        success = snakemake(snakefile,
                            cores=3,
                            workdir=tmpdir,
                            stats=""stats.txt"",
                            snakemakepath=SCRIPTPATH,
                            config=config, **params)
        if shouldfail:
            assert not success, ""expected error on execution""
        else:
            assert success, ""expected successful execution""
            for resultfile in os.listdir(results_dir):
                if resultfile == "".gitignore"" or not os.path.isfile(
                    os.path.join(results_dir, resultfile)):
                    # this means tests cannot use directories as output files
                    continue
                targetfile = join(tmpdir, resultfile)
                expectedfile = join(results_dir, resultfile)
                assert os.path.exists(
                    targetfile), 'expected file ""{}"" not produced'.format(
                        resultfile)
                if check_md5:
                    assert md5sum(targetfile) == md5sum(
                        expectedfile), 'wrong result produced for file ""{}""'.format(
                            resultfile)
    finally:
        rmtree(tmpdir)


def test01():
    run(dpath(""test01""))


def test02():
    run(dpath(""test02""))


def test03():
    run(dpath(""test03""), targets=['test.out'])


def test04():
    run(dpath(""test04""), targets=['test.out'])


def test05():
    run(dpath(""test05""))


def test06():
    run(dpath(""test06""), targets=['test.bla.out'])


def test07():
    run(dpath(""test07""), targets=['test.out', 'test2.out'])


def test08():
    run(dpath(""test08""), targets=['test.out', 'test2.out'])


def test09():
    run(dpath(""test09""), shouldfail=True)


def test10():
    run(dpath(""test10""))


def test11():
    run(dpath(""test11""))


def test12():
    run(dpath(""test12""))


def test13():
    run(dpath(""test13""))


def test14():
    run(dpath(""test14""), snakefile=""Snakefile.nonstandard"", cluster=""./qsub"")


def test15():
    run(dpath(""test15""))


def test_report():
    run(dpath(""test_report""), check_md5=False)


def test_dynamic():
    run(dpath(""test_dynamic""))


def test_params():
    run(dpath(""test_params""))


def test_same_wildcard():
    run(dpath(""test_same_wildcard""))


def test_conditional():
    run(dpath(""test_conditional""),
        targets=""test.out test.0.out test.1.out test.2.out"".split())


def test_shell():
    run(dpath(""test_shell""))


def test_temp():
    run(dpath(""test_temp""),
        cluster=""./qsub"",
        targets=""test.realigned.bam"".split())


def test_keyword_list():
    run(dpath(""test_keyword_list""))


def test_subworkflows():
    run(dpath(""test_subworkflows""), subpath=dpath(""test02""))


def test_globwildcards():
    run(dpath(""test_globwildcards""))


def test_local_import():
    run(dpath(""test_local_import""))


def test_ruledeps():
    run(dpath(""test_ruledeps""))


def test_persistent_dict():
    run(dpath(""test_persistent_dict""))


def test_url_include():
    run(dpath(""test_url_include""), needs_connection=True)


def test_touch():
    run(dpath(""test_touch""))


def test_config():
    run(dpath(""test_config""))


def test_update_config():
    run(dpath(""test_update_config""))


def test_benchmark():
    run(dpath(""test_benchmark""), check_md5=False)


def test_temp_expand():
    run(dpath(""test_temp_expand""))


def test_wildcard_count_ambiguity():
    run(dpath(""test_wildcard_count_ambiguity""))


def test_cluster_dynamic():
    run(dpath(""test_cluster_dynamic""), cluster=""./qsub"")


def test_dynamic_complex():
    run(dpath(""test_dynamic_complex""))


def test_srcdir():
    run(dpath(""test_srcdir""))


def test_multiple_includes():
    run(dpath(""test_multiple_includes""))


def test_yaml_config():
    run(dpath(""test_yaml_config""))

def test_remote():
   run(dpath(""test_remote""))


def test_cluster_sync():
    run(dpath(""test14""),
        snakefile=""Snakefile.nonstandard"",
        cluster_sync=""./qsub"")

def test_symlink_temp():
    run(dpath(""test_symlink_temp""), shouldfail=True)


if __name__ == '__main__':
    import nose
    nose.run(defaultTest=__name__)
/n/n/n",0
55,55,7ddb8ae8e900d19aa609ca8b97ba5f44b7844e4d,"/snakemake/io.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """"""
    A file that is either input or output of a rule.
    """"""

    dynamic_fill = ""__snakemake_dynamic__""

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == ""function""
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError(""This IOFile is specified as a function and ""
                             ""may not be used directly."")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):
        return self.exists and not os.access(self.file, os.W_OK)

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """""" Raise WorkflowError if file is a broken symlink. """"""
        if not self.exists and lstat(self.file):
            raise WorkflowError(""File {} seems to be a broken symlink."".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 ""File exists"" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):
        try:
            lutime(self.file, None)
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    ""Output file {} of rule {} shall be touched but ""
                    ""does not exist."".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, ""w"") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, ""{*}"")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}"")

#    ""\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}"")


def wait_for_files(files, latency_wait=3):
    """"""Wait for given files to be present in filesystem.""""""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info(""Waiting at most {} seconds for missing files."".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError(""Missing files after {} seconds:\n{}"".format(
            latency_wait, ""\n"".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group(""name"")
        if wildcard in wildcards:
            if match.group(""constraint""):
                raise ValueError(
                    ""If multiple wildcards of the same name ""
                    ""appear in a string, eventual constraints have to be defined ""
                    ""at the first occurence and will be inherited by the others."")
            f.append(""(?P={})"".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append(""(?P<{}>{})"".format(wildcard, match.group(""constraint"") if
                                         match.group(""constraint"") else "".+""))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append(""$"")  # ensure that the match spans the whole file
    return """".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group(""name"")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return ""{{{}}}"".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags
    return False


def temp(value):
    """"""
    A flag for an input or output file that shall be removed after usage.
    """"""
    if is_flagged(value, ""protected""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""temp"")


def temporary(value):
    """""" An alias for temp. """"""
    return temp(value)


def protected(value):
    """""" A flag for a file that shall be write protected after creation. """"""
    if is_flagged(value, ""temp""):
        raise SyntaxError(
            ""Protected and temporary flags are mutually exclusive."")
    return flag(value, ""protected"")


def dynamic(value):
    """"""
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """"""
    annotated = flag(value, ""dynamic"")
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError(""Dynamic files need exactly one wildcard."")
        for match in matches:
            if match.group(""constraint""):
                raise SyntaxError(
                    ""The wildcards in dynamic files cannot be constrained."")
    return annotated


def touch(value):
    return flag(value, ""touch"")


def expand(*args, **wildcards):
    """"""
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """"""
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError(""No values given for wildcard {}."".format(e))


def limit(pattern, **wildcards):
    """"""
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """"""
    return pattern.format(**{
        wildcard: ""{{{},{}}}"".format(wildcard, ""|"".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """"""
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """"""
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search(""{[^{]"", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "".""

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple(""Wildcards"", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ""."":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """"""
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """"""

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """"""
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """"""
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """"""
        Add a name to the last item.

        Arguments
        name -- a name
        """"""
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """"""
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """"""
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """"""
        Get the defined names as (name, index) pairs.
        """"""
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """"""
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """"""
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return "" "".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    ""Tries to load a configfile first as JSON, then as YAML, into a dict.""
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError(""Config file is not valid JSON and PyYAML ""
                                    ""has not been installed. Please install ""
                                    ""PyYAML to use YAML config files."")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError(""Config file is not valid JSON or YAML."")
    except FileNotFoundError:
        raise WorkflowError(""Config file {} not found."".format(configpath))


def load_configfile(configpath):
    ""Loads a JSON or YAML configfile as a dict, then checks that it's a dict.""
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError(""Config file must be given as JSON or YAML ""
                            ""with keys at top level."")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """"""
        Args:
            max_len (int): The maximum length of the periodic substring.
        """"""
        self.regex = re.compile(
            ""((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$"".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """"""Returns the periodic substring or None if not periodic.""""""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group(""value"")
/n/n/n/snakemake/jobs.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict[""_cores""]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + """".join(self.output)
                                 ).encode(""utf-8"")).decode(""utf-8"")

    @property
    def inputsize(self):
        """"""
        Return the size of the input files.
        Input files need to be present.
        """"""
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """""" Return the message for this job. """"""
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable in message ""
                                ""of shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """""" Return the shell command. """"""
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException(""Unknown variable when printing ""
                                ""shell command: {}"".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """""" Iterate over output files while dynamic output is expanded. """"""
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """""" Return all wildcard values determined from dynamic output. """"""
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """""" Return missing input files. """"""
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """""" Return oldest output file. """"""
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """""" Return newest input file. """"""
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """""" Return missing output files. """"""
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add(""{} (dynamic)"".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """"""
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """"""

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                ""Warning: the following output files of rule {} were not ""
                ""present when the DAG was created:\n{}"".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """""" Cleanup output files. """"""
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info(""Removing output files of failed job {}""
                        "" since they might be corrupted:\n{}"".format(
                            self, "", "".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """""" Format a string with variables from the job. """"""
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException(""NameError: "" + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException(""IndexError: "" + str(ex), rule=self.rule)

    def properties(self, omit_resources=""_cores _nodes"".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            ""rule"": self.rule.name,
            ""local"": self.dag.workflow.is_local(self.rule),
            ""input"": self.input,
            ""output"": self.output,
            ""params"": params,
            ""threads"": self.threads,
            ""resources"": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """""" Expand dynamic files. """"""
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append(""Forced execution"")
        else:
            if self.noio:
                s.append(""Rules with neither input nor ""
                         ""output files are always executed."")
            elif self.nooutput:
                s.append(""Rules with a run or shell declaration but no output ""
                         ""are always executed."")
            else:
                if self.missing_output:
                    s.append(""Missing output files: {}"".format(
                        "", "".join(self.missing_output)))
                if self.incomplete_output:
                    s.append(""Incomplete output files: {}"".format(
                        "", "".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append(""Updated input files: {}"".format(
                        "", "".join(updated_input)))
                if self.updated_input_run:
                    s.append(""Input files updated by another job: {}"".format(
                        "", "".join(self.updated_input_run)))
        s = ""; "".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)
/n/n/n/snakemake/rules.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """"""
        Create a rule

        Arguments
        name -- the name of the rule
        """"""
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """"""
        Return True if rule contains wildcards.
        """"""
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """"""
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """"""
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """"""
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """"""
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    ""A rule with dynamic output may not define any ""
                    ""non-dynamic output files."")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        ""Not all output files of rule {} ""
                        ""contain the same wildcards."".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """"""
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """"""
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, ""temp""):
                if not output:
                    raise SyntaxError(""Only output files may be temporary"")
                self.temp_output.add(_item)
            if is_flagged(item, ""protected""):
                if not output:
                    raise SyntaxError(""Only output files may be protected"")
                self.protected_output.add(_item)
            if is_flagged(item, ""touch""):
                if not output:
                    raise SyntaxError(
                        ""Only output files may be marked for touching."")
                self.touch_output.add(_item)
            if is_flagged(item, ""dynamic""):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, ""subworkflow""):
                if output:
                    raise SyntaxError(
                        ""Only input files may refer to a subworkflow"")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags[""subworkflow""]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    ""Only input files can be specified as functions"")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    ""Input and output files have to be specified as strings or lists of strings."")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError(""Params have to be specified as strings."")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError(""Log files have to be specified as strings."")

    def expand_wildcards(self, wildcards=None):
        """"""
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """"""

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                ""Input function did not return str or list of str."",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                ""Could not resolve wildcards in rule {}:\n{}"".format(
                    self.name, ""\n"".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                ""Wildcards in input, params, log or benchmark file of rule {} cannot be ""
                ""determined from output files:\n{}"".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """"""
        Returns True if this rule is a producer of the requested output.
        """"""
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException(""{} in wildcard statement"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException(""{}"".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """"""
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """"""
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """"""
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """"""
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """"""
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """"""
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """"""
        Return whether rule2 has a higher priority than rule1.
        """"""
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()
/n/n/n/snakemake/workflow.py/n/n__author__ = ""Johannes Köster""
__copyright__ = ""Copyright 2015, Johannes Köster""
__email__ = ""koester@jimmy.harvard.edu""
__license__ = ""MIT""

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """"""
        Create the controller.
        """"""
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix=""Error in ruleorder definition."")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """"""
        Add a rule.
        """"""
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                ""The name {} is already used by another rule"".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """"""
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """"""
        return name in self._rules

    def get_rule(self, name):
        """"""
        Get rule by name.

        Arguments
        name -- the name of the rule
        """"""
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in ""_cores _nodes"".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources[""_cores""] = cores
        self.global_resources[""_nodes""] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info(""Unlocking working directory."")
                return True
            except IOError:
                logger.error(""Error: Unlocking the directory {} failed. Maybe ""
                             ""you don't have the permissions?"")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                ""Error: Directory cannot be locked. Please make ""
                ""sure that no other Snakemake process is trying to create ""
                ""the same files in the following directory:\n{}\n""
                ""If you are sure that no other ""
                ""instances of snakemake are running on this directory, ""
                ""the remaining lock was likely caused by a kill signal or ""
                ""a power loss. It can be removed with ""
                ""the --unlock argument."".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        ""Executing subworkflow {}."".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info(""Subworkflow {}: Nothing to be done."".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info(""Executing main workflow."")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    ""Dependency resolution disabled (--nodeps) ""
                    ""but missing input ""
                    ""files detected. If this happens on a cluster, please make sure ""
                    ""that you handle the dependencies yourself or turn of ""
                    ""--immediate-submit. Missing input files:\n{}"".format(
                        ""\n"".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print(""\n"".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print(""\n"".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep=""\n"")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        ""Provided cluster nodes: {}"".format(nodes))
                else:
                    logger.resources_info(""Provided cores: {}"".format(cores))
                    logger.resources_info(""Rules claiming more threads will be scaled down."")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        ""Provided resources: "" + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        ""Ignored resources: "" + ignored_resources)
                logger.run_info(""\n"".join(dag.stats()))
            else:
                logger.info(""Nothing to be done."")
        if dryrun and not len(dag):
            logger.info(""Nothing to be done."")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info(""\n"".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """"""
        Include a snakefile.
        """"""
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info(""Multiple include of {} ignored"".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, ""exec""), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """""" Update the global config with the given dictionary. """"""
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException(""Threads value has to be an integer."",
                                        rule=rule)
                rule.resources[""_cores""] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException(""Resources have to be named."")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        ""Resources values have to be integers."",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException(""Priority values have to be numeric."",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = ""__{}"".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, ""Snakefile""))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = ""."" if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), ""subworkflow"", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """""" A namespace for rules so that they can be accessed via dot notation. """"""
    pass


def srcdir(path):
    """"""Return the absolute path, relative to the source directory of the current Snakefile.""""""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)
/n/n/n",1
66,66,9f02e2167b84f34a8c4a47702854304e5940dba6,"api/tests/python/end_points/test_sounds_like.py/n/nfrom namex.models import User
import requests
import json
import pytest
from tests.python import integration_solr, integration_synonym_api
import urllib
from hamcrest import *


token_header = {
                ""alg"": ""RS256"",
                ""typ"": ""JWT"",
                ""kid"": ""flask-jwt-oidc-test-client""
               }
claims = {
            ""iss"": ""https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc"",
            ""sub"": ""43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc"",
            ""aud"": ""NameX-Dev"",
            ""exp"": 31531718745,
            ""iat"": 1531718745,
            ""jti"": ""flask-jwt-oidc-test-support"",
            ""typ"": ""Bearer"",
            ""username"": ""test-user"",
            ""realm_access"": {
                ""roles"": [
                    ""{}"".format(User.EDITOR),
                    ""{}"".format(User.APPROVER),
                    ""viewer"",
                    ""user""
                ]
            }
         }


@pytest.fixture(scope=""session"", autouse=True)
def reload_schema(solr):
    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'
    r = requests.get(url)

    assert r.status_code == 200


@integration_solr
def test_solr_available(solr, app, client, jwt):
    url = solr + '/solr/possible.conflicts/admin/ping'
    r = requests.get(url)

    assert r.status_code == 200


def clean_database(solr):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'text/xml'}
    data = '<delete><query>id:*</query></delete>'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def seed_database_with(solr, name, id='1', source='CORP'):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'application/json'}
    data = '[{""source"":""' + source + '"", ""name"":""' + name + '"", ""id"":""'+ id +'""}]'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def verify(data, expected):

    print(""Expected: "", expected)

    # remove the search divider(s): ----<query term>
    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]

    print(""Actual: "", actual)

    assert_that(len(actual), equal_to(len(expected)))
    for i in range(len(actual)):
        assert_that(actual[i]['name'], equal_to(expected[i]['name']))


def verify_results(client, jwt, query, expected):
    data = search(client, jwt, query)
    verify(data, expected)


def search(client, jwt, query):
    token = jwt.create_jwt(claims, token_header)
    headers = {'Authorization': 'Bearer ' + token}
    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'
    print(url)
    rv = client.get(url, headers=headers)

    assert rv.status_code == 200
    return json.loads(rv.data)


@integration_synonym_api
@integration_solr
def test_all_good(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'GOLDSTREAM ELECTRICAL LTD'}
       ]
    )


@pytest.mark.skip(reason=""Rhyming not implemented yet"")
@integration_synonym_api
@integration_solr
def test_sounds_like(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')
    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')
    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')
    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')
    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')
    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')
    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')
    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')
    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')
    seed_database_with(solr, 'BLABLA ANYTHING', id='11')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'COLDSTREAM VENTURES INC.'},
           {'name': 'GOLDSPRING PROPERTIES LTD'},
           {'name': 'GOLDSTEIN HOLDINGS INC.'},
           {'name': 'GOLDSTREAM ELECTRICAL CORP'},
           {'name': 'GOLDSTRIPES AVIATION INC'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_liberti(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LIBERTI', id='1')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_deeper(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LABORATORY', id='1')
    seed_database_with(solr, 'LAPORTE', id='2')
    seed_database_with(solr, 'LIBERTI', id='3')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_jasmine(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'JASMINE', id='1')
    verify_results(client, jwt,
       query='OSMOND',
       expected=[
           {'name': '----OSMOND'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_fey(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEY', id='1')
    verify_results(client, jwt,
       query='FAY',
       expected=[
           {'name': '----FAY'},
           {'name': 'FEY'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_venizia(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'VENIZIA', id='1')
    seed_database_with(solr, 'VENEZIA', id='2')
    seed_database_with(solr, 'VANSEA', id='3')
    seed_database_with(solr, 'WENSO', id='4')
    verify_results(client, jwt,
       query='VENIZIA',
       expected=[
           {'name': '----VENIZIA'},
           {'name': 'VENEZIA'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_ys_and_is(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'CRYSTAL'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOLDSMITHS', id='1')
    verify_results(client, jwt,
       query='COLDSTREAM',
       expected=[
           {'name': '----COLDSTREAM'},
           {'name': 'KOLDSMITHS'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks_again(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRAZY', id='1')
    seed_database_with(solr, 'KAIZEN', id='2')
    verify_results(client, jwt,
       query='CAYZEN',
       expected=[
           {'name': '----CAYZEN'},
           {'name': 'KAIZEN'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_short_word(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FE', id='1')
    verify_results(client, jwt,
       query='FA',
       expected=[
           {'name': '----FA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_single_vowel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEDS', id='1')
    verify_results(client, jwt,
       query='FADS',
       expected=[
           {'name': '----FADS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_feel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEEL', id='1')
    verify_results(client, jwt,
       query='FILL',
       expected=[
           {'name': '----FILL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_bear(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEAR', id='1')
    verify_results(client, jwt,
       query='BARE',
       expected=[
           {'name': '----BARE'},
           {'name': 'BEAR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_corp(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_designation_in_query_is_ignored(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FINGER LIMATED', id='1')
    verify_results(client, jwt,
       query='SUN LIMITED',
       expected=[
           {'name': '----SUN'}
       ]
    )


@integration_synonym_api
@integration_solr
def leak(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LEAK', id='1')
    verify_results(client, jwt,
       query='LEEK',
       expected=[
           {'name': 'LEAK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_plank(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PLANCK', id='1')
    verify_results(client, jwt,
       query='PLANK',
       expected=[
           {'name': '----PLANK'},
           {'name': 'PLANCK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_krystal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_christal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CHRISTAL',
       expected=[
           {'name': '----CHRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kl(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KLASS', id='1')
    verify_results(client, jwt,
       query='CLASS',
       expected=[
           {'name': '----CLASS'},
           {'name': 'KLASS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pheel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PHEEL', id='1')
    verify_results(client, jwt,
       query='FEEL',
       expected=[
           {'name': '----FEEL'},
           {'name': 'PHEEL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ghable(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GHABLE', id='1')
    verify_results(client, jwt,
       query='GABLE',
       expected=[
           {'name': '----GABLE'},
           {'name': 'GHABLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_gnat(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'GNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'KNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PNEU', id='1')
    verify_results(client, jwt,
       query='NEU',
       expected=[
           {'name': '----NEU'},
           {'name': 'PNEU'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wr(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WREN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'WREN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_rh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'RHEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_soft_c_is_not_k(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KIRK', id='1')
    verify_results(client, jwt,
       query='CIRCLE',
       expected=[
           {'name': '----CIRCLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_oi_oy(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'OYSTER', id='1')
    verify_results(client, jwt,
       query='OISTER',
       expected=[
           {'name': '----OISTER'},
           {'name': 'OYSTER'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_dont_add_match_twice(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN GNAT', id='1')
    verify_results(client, jwt,
       query='REN NAT',
       expected=[
           {'name': '----REN NAT'},
           {'name': 'RHEN GNAT'},
           {'name': '----REN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_neighbour(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'NEIGHBOUR', id='1')
    verify_results(client, jwt,
       query='NAYBOR',
       expected=[
           {'name': '----NAYBOR'},
           {'name': 'NEIGHBOUR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_mac_mc(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'MCGREGOR', id='1')
    verify_results(client, jwt,
       query='MACGREGOR',
       expected=[
           {'name': '----MACGREGOR'},
           {'name': 'MCGREGOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ex_x(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'EXTREME', id='1')
    verify_results(client, jwt,
       query='XTREME',
       expected=[
           {'name': '----XTREME'},
           {'name': 'EXTREME'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WHITE', id='1')
    verify_results(client, jwt,
       query='WITE',
       expected=[
           {'name': '----WITE'},
           {'name': 'WHITE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_qu(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KWIK', id='1')
    verify_results(client, jwt,
       query='QUICK',
       expected=[
           {'name': '----QUICK'},
           {'name': 'KWIK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ps(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PSYCHO', id='1')
    verify_results(client, jwt,
       query='SYCHO',
       expected=[
           {'name': '----SYCHO'},
           {'name': 'PSYCHO'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_terra(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TERRA', id='1')
    verify_results(client, jwt,
       query='TARA',
       expected=[
           {'name': 'TERRA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ayaan(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AYAAN', id='1')
    verify_results(client, jwt,
       query='AYAN',
       expected=[
           {'name': '----AYAN'},
           {'name': 'AYAAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_aggri(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AGGRI', id='1')
    verify_results(client, jwt,
       query='AGRI',
       expected=[
           {'name': '----AGRI'},
           {'name': 'AGGRI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kofi(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOFI', id='1')
    verify_results(client, jwt,
       query='COFFI',
       expected=[
           {'name': '----COFFI'},
           {'name': 'KOFI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_tru(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TRU', id='1')
    verify_results(client, jwt,
       query='TRUE',
       expected=[
           {'name': '----TRUE'},
           {'name': 'TRU'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_dymond(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DYMOND', id='1')
    verify_results(client, jwt,
       query='DIAMOND',
       expected=[
           {'name': 'DYMOND'}
       ]
    )


@pytest.mark.skip(reason=""compound words not handled yet"")
@integration_synonym_api
@integration_solr
def test_bee_kleen(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEE KLEEN', id='1')
    verify_results(client, jwt,
       query='BE-CLEAN',
       expected=[
           {'name': 'BEE KLEEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')
    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')
    verify_results(client, jwt,
       query='BLUEPRINT BEAUTY',
       expected=[
           {'name': '----BLUEPRINT BEAUTY'},
           {'name': 'BLUEPRINT BEAUTEE'},
           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_both_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')
    verify_results(client, jwt,
       query='INTERVENTION BEHAVIOUR',
       expected=[
           {'name': '----INTERVENTION BEHAVIOUR'},
           {'name': '----INTERVENTION'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_at_right_level(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resists_qword_matching_several_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_a(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')
    verify_results(client, jwt,
       query='ALAN HARGREAVES CORPORATION',
       expected=[
           {'name': '----ALAN HARGREAVES'},
           {'name': '----ALAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_e(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ACME', id='1')
    verify_results(client, jwt,
       query='EQUIOM',
       expected=[
           {'name': '----EQUIOM'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_not_match_consonant(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')
    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')
    verify_results(client, jwt,
       query='EH',
       expected=[
           {'name': '----EH'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_unusual_result(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')
    verify_results(client, jwt,
       query='TABLE',
       expected=[
           {'name': '----TABLE'}
       ]
    )

@integration_synonym_api
@integration_solr
def test_stack_ignores_wildcards(client, jwt, app):
    verify_results(client, jwt,
        query=""TESTING* @WILDCARDS"",
        expected=[
            {'name': '----TESTING WILDCARDS'},
            {'name': '----TESTING'}
        ]
    )

@integration_synonym_api
@integration_solr
@pytest.mark.parametrize(""query"", [
    ('T.H.E.'),
    ('COMPANY'),
    ('ASSN'),
    ('THAT'),
    ('LIMITED CORP.'),
])
def test_query_stripped_to_empty_string(solr,client, jwt, query):
    clean_database(solr)
    seed_database_with(solr, 'JM Van Damme inc', id='1')
    seed_database_with(solr, 'SOME RANDOM NAME', id='2')
    verify_results(client, jwt,
        query=query,
        expected=[{'name':'----*'}]
    )/n/n/n",0
67,67,9f02e2167b84f34a8c4a47702854304e5940dba6,"/api/tests/python/end_points/test_sounds_like.py/n/nfrom namex.models import User
import requests
import json
import pytest
from tests.python import integration_solr, integration_synonym_api
import urllib
from hamcrest import *


token_header = {
                ""alg"": ""RS256"",
                ""typ"": ""JWT"",
                ""kid"": ""flask-jwt-oidc-test-client""
               }
claims = {
            ""iss"": ""https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc"",
            ""sub"": ""43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc"",
            ""aud"": ""NameX-Dev"",
            ""exp"": 31531718745,
            ""iat"": 1531718745,
            ""jti"": ""flask-jwt-oidc-test-support"",
            ""typ"": ""Bearer"",
            ""username"": ""test-user"",
            ""realm_access"": {
                ""roles"": [
                    ""{}"".format(User.EDITOR),
                    ""{}"".format(User.APPROVER),
                    ""viewer"",
                    ""user""
                ]
            }
         }


@pytest.fixture(scope=""session"", autouse=True)
def reload_schema(solr):
    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'
    r = requests.get(url)

    assert r.status_code == 200


@integration_solr
def test_solr_available(solr, app, client, jwt):
    url = solr + '/solr/possible.conflicts/admin/ping'
    r = requests.get(url)

    assert r.status_code == 200


def clean_database(solr):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'text/xml'}
    data = '<delete><query>id:*</query></delete>'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def seed_database_with(solr, name, id='1', source='CORP'):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'application/json'}
    data = '[{""source"":""' + source + '"", ""name"":""' + name + '"", ""id"":""'+ id +'""}]'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def verify(data, expected):

    print(""Expected: "", expected)

    # remove the search divider(s): ----<query term>
    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]

    print(""Actual: "", actual)

    assert_that(len(actual), equal_to(len(expected)))
    for i in range(len(actual)):
        assert_that(actual[i]['name'], equal_to(expected[i]['name']))


def verify_results(client, jwt, query, expected):
    data = search(client, jwt, query)
    verify(data, expected)


def search(client, jwt, query):
    token = jwt.create_jwt(claims, token_header)
    headers = {'Authorization': 'Bearer ' + token}
    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'
    print(url)
    rv = client.get(url, headers=headers)

    assert rv.status_code == 200
    return json.loads(rv.data)


@integration_synonym_api
@integration_solr
def test_all_good(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'GOLDSTREAM ELECTRICAL LTD'}
       ]
    )


@pytest.mark.skip(reason=""Rhyming not implemented yet"")
@integration_synonym_api
@integration_solr
def test_sounds_like(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')
    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')
    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')
    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')
    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')
    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')
    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')
    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')
    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')
    seed_database_with(solr, 'BLABLA ANYTHING', id='11')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'COLDSTREAM VENTURES INC.'},
           {'name': 'GOLDSPRING PROPERTIES LTD'},
           {'name': 'GOLDSTEIN HOLDINGS INC.'},
           {'name': 'GOLDSTREAM ELECTRICAL CORP'},
           {'name': 'GOLDSTRIPES AVIATION INC'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_liberti(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LIBERTI', id='1')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_deeper(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LABORATORY', id='1')
    seed_database_with(solr, 'LAPORTE', id='2')
    seed_database_with(solr, 'LIBERTI', id='3')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_jasmine(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'JASMINE', id='1')
    verify_results(client, jwt,
       query='OSMOND',
       expected=[
           {'name': '----OSMOND'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_fey(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEY', id='1')
    verify_results(client, jwt,
       query='FAY',
       expected=[
           {'name': '----FAY'},
           {'name': 'FEY'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_venizia(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'VENIZIA', id='1')
    seed_database_with(solr, 'VENEZIA', id='2')
    seed_database_with(solr, 'VANSEA', id='3')
    seed_database_with(solr, 'WENSO', id='4')
    verify_results(client, jwt,
       query='VENIZIA',
       expected=[
           {'name': '----VENIZIA'},
           {'name': 'VENEZIA'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_ys_and_is(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'CRYSTAL'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOLDSMITHS', id='1')
    verify_results(client, jwt,
       query='COLDSTREAM',
       expected=[
           {'name': '----COLDSTREAM'},
           {'name': 'KOLDSMITHS'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks_again(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRAZY', id='1')
    seed_database_with(solr, 'KAIZEN', id='2')
    verify_results(client, jwt,
       query='CAYZEN',
       expected=[
           {'name': '----CAYZEN'},
           {'name': 'KAIZEN'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_short_word(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FE', id='1')
    verify_results(client, jwt,
       query='FA',
       expected=[
           {'name': '----FA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_single_vowel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEDS', id='1')
    verify_results(client, jwt,
       query='FADS',
       expected=[
           {'name': '----FADS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_feel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEEL', id='1')
    verify_results(client, jwt,
       query='FILL',
       expected=[
           {'name': '----FILL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_bear(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEAR', id='1')
    verify_results(client, jwt,
       query='BARE',
       expected=[
           {'name': '----BARE'},
           {'name': 'BEAR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_corp(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_designation_in_query_is_ignored(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FINGER LIMATED', id='1')
    verify_results(client, jwt,
       query='SUN LIMITED',
       expected=[
           {'name': '----SUN'}
       ]
    )


@integration_synonym_api
@integration_solr
def leak(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LEAK', id='1')
    verify_results(client, jwt,
       query='LEEK',
       expected=[
           {'name': 'LEAK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_plank(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PLANCK', id='1')
    verify_results(client, jwt,
       query='PLANK',
       expected=[
           {'name': '----PLANK'},
           {'name': 'PLANCK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_krystal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_christal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CHRISTAL',
       expected=[
           {'name': '----CHRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kl(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KLASS', id='1')
    verify_results(client, jwt,
       query='CLASS',
       expected=[
           {'name': '----CLASS'},
           {'name': 'KLASS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pheel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PHEEL', id='1')
    verify_results(client, jwt,
       query='FEEL',
       expected=[
           {'name': '----FEEL'},
           {'name': 'PHEEL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ghable(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GHABLE', id='1')
    verify_results(client, jwt,
       query='GABLE',
       expected=[
           {'name': '----GABLE'},
           {'name': 'GHABLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_gnat(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'GNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'KNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PNEU', id='1')
    verify_results(client, jwt,
       query='NEU',
       expected=[
           {'name': '----NEU'},
           {'name': 'PNEU'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wr(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WREN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'WREN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_rh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'RHEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_soft_c_is_not_k(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KIRK', id='1')
    verify_results(client, jwt,
       query='CIRCLE',
       expected=[
           {'name': '----CIRCLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_oi_oy(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'OYSTER', id='1')
    verify_results(client, jwt,
       query='OISTER',
       expected=[
           {'name': '----OISTER'},
           {'name': 'OYSTER'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_dont_add_match_twice(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN GNAT', id='1')
    verify_results(client, jwt,
       query='REN NAT',
       expected=[
           {'name': '----REN NAT'},
           {'name': 'RHEN GNAT'},
           {'name': '----REN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_neighbour(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'NEIGHBOUR', id='1')
    verify_results(client, jwt,
       query='NAYBOR',
       expected=[
           {'name': '----NAYBOR'},
           {'name': 'NEIGHBOUR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_mac_mc(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'MCGREGOR', id='1')
    verify_results(client, jwt,
       query='MACGREGOR',
       expected=[
           {'name': '----MACGREGOR'},
           {'name': 'MCGREGOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ex_x(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'EXTREME', id='1')
    verify_results(client, jwt,
       query='XTREME',
       expected=[
           {'name': '----XTREME'},
           {'name': 'EXTREME'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WHITE', id='1')
    verify_results(client, jwt,
       query='WITE',
       expected=[
           {'name': '----WITE'},
           {'name': 'WHITE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_qu(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KWIK', id='1')
    verify_results(client, jwt,
       query='QUICK',
       expected=[
           {'name': '----QUICK'},
           {'name': 'KWIK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ps(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PSYCHO', id='1')
    verify_results(client, jwt,
       query='SYCHO',
       expected=[
           {'name': '----SYCHO'},
           {'name': 'PSYCHO'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_terra(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TERRA', id='1')
    verify_results(client, jwt,
       query='TARA',
       expected=[
           {'name': 'TERRA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ayaan(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AYAAN', id='1')
    verify_results(client, jwt,
       query='AYAN',
       expected=[
           {'name': '----AYAN'},
           {'name': 'AYAAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_aggri(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AGGRI', id='1')
    verify_results(client, jwt,
       query='AGRI',
       expected=[
           {'name': '----AGRI'},
           {'name': 'AGGRI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kofi(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOFI', id='1')
    verify_results(client, jwt,
       query='COFFI',
       expected=[
           {'name': '----COFFI'},
           {'name': 'KOFI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_tru(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TRU', id='1')
    verify_results(client, jwt,
       query='TRUE',
       expected=[
           {'name': '----TRUE'},
           {'name': 'TRU'}
       ]
    )


@pytest.mark.skip(reason=""not handled yet"")
@integration_synonym_api
@integration_solr
def test_dymond(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DYMOND', id='1')
    verify_results(client, jwt,
       query='DIAMOND',
       expected=[
           {'name': 'DYMOND'}
       ]
    )


@pytest.mark.skip(reason=""compound words not handled yet"")
@integration_synonym_api
@integration_solr
def test_bee_kleen(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEE KLEEN', id='1')
    verify_results(client, jwt,
       query='BE-CLEAN',
       expected=[
           {'name': 'BEE KLEEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')
    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')
    verify_results(client, jwt,
       query='BLUEPRINT BEAUTY',
       expected=[
           {'name': '----BLUEPRINT BEAUTY'},
           {'name': 'BLUEPRINT BEAUTEE'},
           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_both_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')
    verify_results(client, jwt,
       query='INTERVENTION BEHAVIOUR',
       expected=[
           {'name': '----INTERVENTION BEHAVIOUR'},
           {'name': '----INTERVENTION'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_at_right_level(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resists_qword_matching_several_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_a(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')
    verify_results(client, jwt,
       query='ALAN HARGREAVES CORPORATION',
       expected=[
           {'name': '----ALAN HARGREAVES'},
           {'name': '----ALAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_e(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ACME', id='1')
    verify_results(client, jwt,
       query='EQUIOM',
       expected=[
           {'name': '----EQUIOM'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_not_match_consonant(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')
    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')
    verify_results(client, jwt,
       query='EH',
       expected=[
           {'name': '----EH'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_unusual_result(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')
    verify_results(client, jwt,
       query='TABLE',
       expected=[
           {'name': '----TABLE'}
       ]
    )

@integration_synonym_api
@integration_solr
def test_stack_ignores_wildcards(client, jwt, app):
    verify_results(client, jwt,
        query=""TESTING* @WILDCARDS"",
        expected=[
            {'name': '----TESTING WILDCARDS'},
            {'name': '----TESTING'}
        ]
    )

@integration_synonym_api
@integration_solr
@pytest.mark.parametrize(""query"", [
    ('T.H.E.'),
    ('COMPANY'),
    ('ASSN'),
    ('THAT'),
    ('LIMITED CORP.'),
])
def test_query_stripped_to_empty_string(solr,client, jwt, query):
    clean_database(solr)
    seed_database_with(solr, 'JM Van Damme inc', id='1')
    seed_database_with(solr, 'SOME RANDOM NAME', id='2')
    verify_results(client, jwt,
        query=query,
        expected=[{'name':'----*'}]
    )
/n/n/n",1
