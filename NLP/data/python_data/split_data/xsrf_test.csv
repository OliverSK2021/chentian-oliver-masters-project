,Unnamed: 0,id,code,label
16,16,4164d239f0f59b9ef04e3d168e68f958991fe88f,"titlebot.py/n/n#!/usr/bin/env python2
# coding: utf-8

import os
import sys
import socket
import string
import time
import urllib2
import HTMLParser
import zlib

import libirc

HOST=""irc.freenode.net""
PORT=6667
NICK=""titlebot""
IDENT=""titlebot""
REALNAME=""titlebot""
CHANS=[""##Orz""]

def ParseURL(s):
    http_idx=s.find('http:')
    https_idx=s.find('https:')
    if https_idx==-1:
        if http_idx==-1:
            return None
        else:
            return s[http_idx:]
    else:
        if http_idx==-1:
            return s[https_idx:]
        else:
            return s[min(http_idx, https_idx):]

try:
    c=libirc.IRCConnection()
    c.connect((HOST, PORT))
    c.setnick(NICK)
    c.setuser(IDENT, REALNAME)
    for CHAN in CHANS:
        c.join(CHAN)
except:
    time.sleep(10)
    sys.stderr.write(""Restarting...\n"")
    os.execlp(""python2"", ""python2"", __file__)
    raise
CHAN=CHANS[0]
socket.setdefaulttimeout(10)

html_parser=HTMLParser.HTMLParser()

quiting=False
while not quiting:
    if not c.sock:
        quiting=True
        time.sleep(10)
        sys.stderr.write(""Restarting...\n"")
        os.execlp(""python2"", ""python2"", __file__)
        break
    try:
        line=c.recvline(block=True)
        if not line:
            continue
        sys.stderr.write(""%s\n"" % line.encode('utf-8', 'replace'))
        line=c.parse(line=line)
        if line and line[""cmd""]==""PRIVMSG"":
            if line[""dest""]==NICK:
                if line[""msg""]==u""Get out of this channel!"": # A small hack
                    c.quit(u""%s asked to leave."" % line[""nick""])
                    quiting=True
            else:
                CHAN=line[""dest""]
                for w in line[""msg""].split():
                    w=ParseURL(w)
                    if w:
                        w=w.split("">"", 1)[0].split('""', 1)[0]
                        if re.match(""https?:/*git.io(/|$)"", w): # Fix buggy git.io
                            continue
                        opener=urllib2.build_opener()
                        opener.addheaders = [(""Accept-Charset"", ""utf-8, iso-8859-1""), (""Accept-Language"", ""zh-cn, zh-hans, zh-tw, zh-hant, zh, en-us, en-gb, en""), (""Range"", ""bytes=0-16383""), (""User-Agent"", ""Mozilla/5.0 (compatible; Titlebot; like IRCbot; +https://github.com/m13253/titlebot)""), (""X-Forwarded-For"", ""10.2.0.101""), (""X-moz"", ""prefetch""), (""X-Prefetch"", ""yes""), (""X-Requested-With"", ""Titlebot"")]
                        h=opener.open(w.encode(""utf-8"", ""replace""))
                        if h.code==200 or h.code==206:
                            if not ""Content-Type"" in h.info() or h.info()[""Content-Type""].split("";"")[0]==""text/html"":
                                wbuf=h.read(16384)
                                read_times=1
                                while len(wbuf)<16384 and read_times<4:
                                    read_times+=1
                                    wbuf_=h.read(16384)
                                    if wbuf_:
                                        wbuf+=wbuf_
                                    else:
                                        break
                                if ""Content-Encoding"" in h.info() and h.info()[""Content-Encoding""]==""gzip"": # Fix buggy www.bilibili.tv
                                    try:
                                        gunzip_obj=zlib.decompressobj(16+zlib.MAX_WBITS)
                                        wbuf=gunzip_obj.decompress(wbuf)
                                    except:
                                        pass
                                if wbuf.find(""<title>"")!=-1:
                                    titleenc=wbuf.split(""<title>"")[1].split(""</title>"")[0]
                                    title=None
                                    for enc in (""utf-8"", ""gbk"", ""gb18030"", ""iso-8859-1""):
                                        try:
                                            title=titleenc.decode(enc)
                                            break
                                        except UnicodeDecodeError:
                                            pass
                                    if title==None:
                                        title=title.decode(""utf-8"", ""replace"")
                                    title=html_parser.unescape(title).replace(""\r"", """").replace(""\n"", "" "").strip()
                                    c.say(CHAN, u""⇪标题: %s"" % title)
                                else:
                                    c.say(CHAN, u""⇪无标题网页"")
                            else:
                                if ""Content-Range"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Range""].split(""/"")[1]))
                                elif ""Content-Length"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Length""]))
                                else:
                                    c.say(CHAN, u""⇪文件类型: %s\r\n"" % h.info()[""Content-Type""])
                        else:
                            c.say(CHAN, u""⇪HTTP %d 错误\r\n"" % h.code)
    except Exception as e:
        try:
            c.say(CHAN, u""哎呀，%s 好像出了点问题: %s"" % (NICK, e))
        except:
            pass
    except socket.error as e:
        sys.stderr.write(""Error: %s\n"", e)
        c.quit(""Network error."")

# vim: et ft=python sts=4 sw=4 ts=4
/n/n/n",0
17,17,4164d239f0f59b9ef04e3d168e68f958991fe88f,"/titlebot.py/n/n#!/usr/bin/env python2
# coding: utf-8

import os
import sys
import socket
import string
import time
import urllib2
import HTMLParser
import zlib

import libirc

HOST=""irc.freenode.net""
PORT=6667
NICK=""titlebot""
IDENT=""titlebot""
REALNAME=""titlebot""
CHANS=[""##Orz""]

def ParseURL(s):
    http_idx=s.find('http:')
    https_idx=s.find('https:')
    if https_idx==-1:
        if http_idx==-1:
            return None
        else:
            return s[http_idx:]
    else:
        if http_idx==-1:
            return s[https_idx:]
        else:
            return s[min(http_idx, https_idx):]

try:
    c=libirc.IRCConnection()
    c.connect((HOST, PORT))
    c.setnick(NICK)
    c.setuser(IDENT, REALNAME)
    for CHAN in CHANS:
        c.join(CHAN)
except:
    time.sleep(10)
    sys.stderr.write(""Restarting...\n"")
    os.execlp(""python2"", ""python2"", __file__)
    raise
CHAN=CHANS[0]
socket.setdefaulttimeout(10)

html_parser=HTMLParser.HTMLParser()

quiting=False
while not quiting:
    if not c.sock:
        quiting=True
        time.sleep(10)
        sys.stderr.write(""Restarting...\n"")
        os.execlp(""python2"", ""python2"", __file__)
        break
    try:
        line=c.recvline(block=True)
        if not line:
            continue
        sys.stderr.write(""%s\n"" % line.encode('utf-8', 'replace'))
        line=c.parse(line=line)
        if line and line[""cmd""]==""PRIVMSG"":
            if line[""dest""]==NICK:
                if line[""msg""]==u""Get out of this channel!"": # A small hack
                    c.quit(u""%s asked to leave."" % line[""nick""])
                    quiting=True
            else:
                CHAN=line[""dest""]
                for w in line[""msg""].split():
                    w=ParseURL(w)
                    if w:
                        w=w.split("">"", 1)[0].split('""', 1)[0]
                        if re.match(""https?:/*git.io(/|$)"", w): # Fix buggy git.io
                            continue
                        opener=urllib2.build_opener()
                        opener.addheaders = [(""Accept-Charset"", ""utf-8, iso-8859-1""), (""Accept-Language"", ""zh-cn, zh-hans, zh-tw, zh-hant, zh, en-us, en-gb, en""), (""Range"", ""bytes=0-16383""), (""User-Agent"", ""Mozilla/5.0 (compatible; Titlebot; like IRCbot; +https://github.com/m13253/titlebot)""), (""X-Forwarded-For"", ""10.2.0.101""), (""X-moz"", ""prefetch""), (""X-Prefetch"", ""yes"")]
                        h=opener.open(w.encode(""utf-8"", ""replace""))
                        if h.code==200 or h.code==206:
                            if not ""Content-Type"" in h.info() or h.info()[""Content-Type""].split("";"")[0]==""text/html"":
                                wbuf=h.read(16384)
                                read_times=1
                                while len(wbuf)<16384 and read_times<4:
                                    read_times+=1
                                    wbuf_=h.read(16384)
                                    if wbuf_:
                                        wbuf+=wbuf_
                                    else:
                                        break
                                if ""Content-Encoding"" in h.info() and h.info()[""Content-Encoding""]==""gzip"": # Fix buggy www.bilibili.tv
                                    try:
                                        gunzip_obj=zlib.decompressobj(16+zlib.MAX_WBITS)
                                        wbuf=gunzip_obj.decompress(wbuf)
                                    except:
                                        pass
                                if wbuf.find(""<title>"")!=-1:
                                    titleenc=wbuf.split(""<title>"")[1].split(""</title>"")[0]
                                    title=None
                                    for enc in (""utf-8"", ""gbk"", ""gb18030"", ""iso-8859-1""):
                                        try:
                                            title=titleenc.decode(enc)
                                            break
                                        except UnicodeDecodeError:
                                            pass
                                    if title==None:
                                        title=title.decode(""utf-8"", ""replace"")
                                    title=html_parser.unescape(title).replace(""\r"", """").replace(""\n"", "" "").strip()
                                    c.say(CHAN, u""⇪标题: %s"" % title)
                                else:
                                    c.say(CHAN, u""⇪无标题网页"")
                            else:
                                if ""Content-Range"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Range""].split(""/"")[1]))
                                elif ""Content-Length"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Length""]))
                                else:
                                    c.say(CHAN, u""⇪文件类型: %s\r\n"" % h.info()[""Content-Type""])
                        else:
                            c.say(CHAN, u""⇪HTTP %d 错误\r\n"" % h.code)
    except Exception as e:
        try:
            c.say(CHAN, u""哎呀，%s 好像出了点问题: %s"" % (NICK, e))
        except:
            pass
    except socket.error as e:
        sys.stderr.write(""Error: %s\n"", e)
        c.quit(""Network error."")

# vim: et ft=python sts=4 sw=4 ts=4
/n/n/n",1
38,38,e64a478b09842d55be64a7cf7badb83ac3eb6493,"kijiji_repost_headless/__main__.py/n/nimport argparse
import os
import sys
from time import sleep

import kijiji_api
import generate_inf_file as generator

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


def main():
    parser = argparse.ArgumentParser(description=""Post ads on Kijiji"")
    parser.add_argument('-u', '--username', help='username of your kijiji account')
    parser.add_argument('-p', '--password', help='password of your kijiji account')

    subparsers = parser.add_subparsers(help='sub-command help')

    post_parser = subparsers.add_parser('post', help='post a new ad')
    post_parser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    post_parser.set_defaults(function=post_ad)

    folder_parser = subparsers.add_parser('folder', help='post ad from folder')
    folder_parser.add_argument('folder_name', type=str, help='folder containing ad details')
    folder_parser.set_defaults(function=post_folder)

    repost_folder_parser = subparsers.add_parser('repost_folder', help='post ad from folder')
    repost_folder_parser.add_argument('folder_name', type=str, help='folder containing ad details')
    repost_folder_parser.set_defaults(function=repost_folder)

    show_parser = subparsers.add_parser('show', help='show currently listed ads')
    show_parser.set_defaults(function=show_ads)

    delete_parser = subparsers.add_parser('delete', help='delete a listed ad')
    delete_parser.add_argument('id', type=str, help='id of the ad you wish to delete')
    delete_parser.set_defaults(function=delete_ad)

    nuke_parser = subparsers.add_parser('nuke', help='delete all ads')
    nuke_parser.set_defaults(function=nuke)

    check_parser = subparsers.add_parser('check_ad', help='check if ad is active')
    check_parser.add_argument('folder_name', type=str, help='folder containing ad details')
    check_parser.set_defaults(function=check_ad)

    repost_parser = subparsers.add_parser('repost', help='repost an existing ad')
    repost_parser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    repost_parser.set_defaults(function=repost_ad)

    build_parser = subparsers.add_parser('build_ad', help='Generates the item.inf file for a new ad')
    build_parser.set_defaults(function=generate_inf_file)

    args = parser.parse_args()
    try:
        args.function(args)
    except AttributeError:
        parser.print_help()


def get_folder_data(args):
    """"""
    Set ad data inf file and extract login credentials from inf files
    """"""
    args.inf_file = ""item.inf""
    cred_file = args.folder_name + ""/login.inf""
    creds = [line.strip() for line in open(cred_file, 'r')]
    args.username = creds[0]
    args.password = creds[1]


def get_inf_details(inf_file):
    """"""
    Extract ad data from inf file
    """"""
    with open(inf_file, 'rt') as infFileLines:
        data = {key: val for line in infFileLines for (key, val) in (line.strip().split(""=""),)}
    files = [open(picture, 'rb').read() for picture in data['imageCsv'].split("","")]
    return [data, files]


def post_folder(args):
    """"""
    Post new ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folder_name)
    post_ad(args)


def post_ad(args):
    """"""
    Post new ad
    """"""
    [data, image_files] = get_inf_details(args.inf_file)
    attempts = 1
    while not check_ad(args) and attempts < 5:
        if attempts > 1:
            print(""Failed Attempt #{}, trying again."".format(attempts))
        attempts += 1
        api = kijiji_api.KijijiApi()
        api.login(args.username, args.password)
        api.post_ad_using_data(data, image_files)
    if not check_ad(args):
        print(""Failed Attempt #{}, giving up."".format(attempts))


def show_ads(args):
    """"""
    Print list of all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    [print(""{} '{}'"".format(ad_id, ad_name)) for ad_name, ad_id in api.get_all_ads()]


def delete_ad(args):
    """"""
    Delete ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    api.delete_ad(args.id)


def delete_ad_using_title(name):
    """"""
    Delete ad based on ad title
    """"""
    api = kijiji_api.KijijiApi()
    api.delete_ad_using_title(name)


def repost_ad(args):
    """"""
    Repost ad

    Try to delete ad with same title if possible before reposting new ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    del_ad_name = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            del_ad_name = val
    try:
        api.delete_ad_using_title(del_ad_name)
        print(""Existing ad deleted before reposting"")
    except kijiji_api.DeleteAdException:
        print(""Did not find an existing ad with matching title, skipping ad deletion"")
        pass
    # Must wait a bit before posting the same ad even after deleting it, otherwise Kijiji will automatically remove it
    sleep(180)
    post_ad(args)


def repost_folder(args):
    """"""
    Repost ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folder_name)
    repost_ad(args)


def check_ad(args):
    """"""
    Check if ad is live
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    ad_name = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            ad_name = val
    all_ads = api.get_all_ads()
    return [t for t, i in all_ads if t == ad_name]


def nuke(args):
    """"""
    Delete all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    all_ads = api.get_all_ads()
    [api.delete_ad(ad_id) for ad_name, ad_id in all_ads]


def generate_inf_file():
    generator.run_program()


if __name__ == ""__main__"":
    main()
/n/n/nkijiji_repost_headless/kijiji_api.py/n/nimport json
import re
import sys
from time import strftime

import bs4
import requests
import yaml

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


class KijijiApiException(Exception):
    """"""
    Custom KijijiApi exception class
    """"""
    def __init__(self, msg=""KijijiApi exception encountered."", dump=None):
        self.msg = msg
        self.dumpfilepath = """"
        if dump:
            self.dumpfilepath = ""kijijiapi_dump_{}.txt"".format(strftime(""%Y%m%dT%H%M%S""))
            with open(self.dumpfilepath, 'a') as f:
                f.write(dump)

    def __str__(self):
        if self.dumpfilepath:
            return ""{}\nSee {} in current directory for latest dumpfile."".format(self.msg, self.dumpfilepath)
        else:
            return self.msg


def get_token(html, attrib_name):
    """"""
    Return value of first match for element with name attribute
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    res = soup.select(""[name={}]"".format(attrib_name))
    if not res:
        raise KijijiApiException(""Element with name attribute '{}' not found in html text."".format(attrib_name), html)
    return res[0]['value']


def get_kj_data(html):
    """"""
    Return dict of Kijiji page data
    The 'window.__data' JSON object contains many useful key/values
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    p = re.compile('window.__data=(.*);')
    script_list = soup.find_all(""script"", {""src"": False})
    for script in script_list:
        if script:
            m = p.search(script.string)
            if m:
                return json.loads(m.group(1))
    raise KijijiApiException(""'__data' JSON object not found in html text."", html)


def get_xsrf_token(html):
    """"""
    Return XSRF token
    This function is only necessary for the 'm-my-ads.html' page, as this particular page
    does not contain the usual 'ca.kijiji.xsrf.token' hidden HTML form input element, which is easier to scrape
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    p = re.compile('Zoop\.init\(.*config: ({.+?}).*\);')
    for script in soup.find_all(""script"", {""src"": False}):
        if script:
            m = p.search(script.string.replace(""\n"", """"))
            if m:
                # Using yaml to load since this is not valid JSON
                return yaml.load(m.group(1))['token']
    raise KijijiApiException(""XSRF token not found in html text."", html)


class KijijiApi:
    """"""
    All functions require to be logged in to Kijiji first in order to function correctly
    """"""
    def __init__(self):
        config = {}
        self.session = requests.Session()

    def login(self, username, password):
        """"""
        Login to Kijiji for the current session
        """"""
        login_url = 'https://www.kijiji.ca/t-login.html'
        resp = self.session.get(login_url)
        payload = {
            'emailOrNickname': username,
            'password': password,
            'rememberMe': 'true',
            '_rememberMe': 'on',
            'ca.kijiji.xsrf.token': get_token(resp.text, 'ca.kijiji.xsrf.token'),
            'targetUrl': get_kj_data(resp.text)['config']['targetUrl'],
        }
        resp = self.session.post(login_url, data=payload)
        if not self.is_logged_in():
            raise KijijiApiException(""Could not log in."", resp.text)

    def is_logged_in(self):
        """"""
        Return true if logged into Kijiji for the current session
        """"""
        return ""Sign Out"" in self.session.get('https://www.kijiji.ca/m-my-ads.html/').text

    def logout(self):
        """"""
        Logout of Kijiji for the current session
        """"""
        self.session.get('https://www.kijiji.ca/m-logout.html')

    def delete_ad(self, ad_id):
        """"""
        Delete ad based on ad ID
        """"""
        my_ads_page = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        params = {
            'Action': 'DELETE_ADS',
            'Mode': 'ACTIVE',
            'needsRedirect': 'false',
            'ads': '[{{""adId"":""{}"",""reason"":""PREFER_NOT_TO_SAY"",""otherReason"":""""}}]'.format(ad_id),
            'ca.kijiji.xsrf.token': get_xsrf_token(my_ads_page.text),
        }
        resp = self.session.post('https://www.kijiji.ca/j-delete-ad.json', data=params)
        if ""OK"" not in resp.text:
            raise KijijiApiException(""Could not delete ad."", resp.text)

    def delete_ad_using_title(self, title):
        """"""
        Delete ad based on ad title
        """"""
        all_ads = self.get_all_ads()
        [self.delete_ad(i) for t, i in all_ads if t.strip() == title.strip()]

    def upload_image(self, token, image_files=[]):
        """"""
        Upload one or more photos to Kijiji

        'image_files' is a list of binary objects corresponding to images
        """"""
        image_urls = []
        image_upload_url = 'https://www.kijiji.ca/p-upload-image.html'
        for img_file in image_files:
            for i in range(0, 3):
                r = self.session.post(image_upload_url, files={'file': img_file}, headers={""X-Ebay-Box-Token"": token})
                r.raise_for_status()
                try:
                    image_tree = json.loads(r.text)
                    img_url = image_tree['thumbnailUrl']
                    print(""Image Upload success on try #{}"".format(i+1))
                    image_urls.append(img_url)
                    break
                except (KeyError, ValueError):
                    print(""Image Upload failed on try #{}"".format(i+1))
        return [image for image in image_urls if image is not None]

    def post_ad_using_data(self, data, image_files=[]):
        """"""
        Post new ad

        'data' is a dictionary of ad data that to be posted
        'image_files' is a list of binary objects corresponding to images to upload
        """"""
        # Load ad posting page (arbitrary category)
        resp = self.session.get('https://www.kijiji.ca/p-admarkt-post-ad.html?categoryId=15')

        # Get token required for upload
        m = re.search(r""initialXsrfToken: '(\S+)'"", resp.text)
        if m:
            image_upload_token = m.group(1)
        else:
            raise KijijiApiException(""'initialXsrfToken' not found in html text."", resp.text)

        # Upload the images
        image_list = self.upload_image(image_upload_token, image_files)
        data['images'] = "","".join(image_list)

        # Retrieve XSRF tokens
        data['ca.kijiji.xsrf.token'] = get_token(resp.text, 'ca.kijiji.xsrf.token')
        data['postAdForm.fraudToken'] = get_token(resp.text, 'postAdForm.fraudToken')

        # Format ad data and check constraints
        data['postAdForm.description'] = data['postAdForm.description'].replace(""\\n"", ""\n"")
        title_len = len(data.get(""postAdForm.title"", """"))
        if not title_len >= 10:
            raise KijijiApiException(""Your ad title is too short! (min 10 chars)"")
        if title_len > 64:
            raise KijijiApiException(""Your ad title is too long! (max 64 chars)"")

        # Upload the ad itself
        new_ad_url = ""https://www.kijiji.ca/p-submit-ad.html""
        resp = self.session.post(new_ad_url, data=data)
        resp.raise_for_status()
        if ""Delete Ad?"" not in resp.text:
            if ""There was an issue posting your ad, please contact Customer Service."" in resp.text:
                raise KijijiApiException(""Could not post ad; this user is banned."", resp.text)
            else:
                raise KijijiApiException(""Could not post ad."", resp.text)

        # Extract ad ID from response set-cookie
        ad_id = re.search('kjrva=(\d+)', resp.headers['Set-Cookie']).group(1)

        return ad_id

    def get_all_ads(self):
        """"""
        Return an iterator of tuples containing the ad title and ad ID for every ad
        """"""
        resp = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        user_id = get_kj_data(resp.text)['config']['userId']
        my_ads_url = 'https://www.kijiji.ca/j-get-my-ads.json?currentOffset=0&show=ACTIVE&user={}'.format(user_id)
        my_ads_page = self.session.get(my_ads_url)
        my_ads_tree = json.loads(my_ads_page.text)
        ad_ids = [entry['id'] for entry in my_ads_tree['myAdEntries']]
        ad_names = [entry['title'] for entry in my_ads_tree['myAdEntries']]
        return zip(ad_names, ad_ids)
/n/n/n",0
39,39,e64a478b09842d55be64a7cf7badb83ac3eb6493,"/kijiji_repost_headless/__main__.py/n/nimport argparse
import os
import sys
from time import sleep

import kijiji_api
import generate_inf_file as generator

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


def main():
    parser = argparse.ArgumentParser(description=""Post ads on Kijiji"")
    parser.add_argument('-u', '--username', help='username of your kijiji account')
    parser.add_argument('-p', '--password', help='password of your kijiji account')

    subparsers = parser.add_subparsers(help='sub-command help')

    postParser = subparsers.add_parser('post', help='post a new ad')
    postParser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    postParser.set_defaults(function=post_ad)

    folderParser = subparsers.add_parser('folder', help='post ad from folder')
    folderParser.add_argument('folderName', type=str, help='folder containing ad details')
    folderParser.set_defaults(function=post_folder)

    repostFolderParser = subparsers.add_parser('repost_folder', help='post ad from folder')
    repostFolderParser.add_argument('folderName', type=str, help='folder containing ad details')
    repostFolderParser.set_defaults(function=repost_folder)

    showParser = subparsers.add_parser('show', help='show currently listed ads')
    showParser.set_defaults(function=show_ads)

    deleteParser = subparsers.add_parser('delete', help='delete a listed ad')
    deleteParser.add_argument('id', type=str, help='id of the ad you wish to delete')
    deleteParser.set_defaults(function=delete_ad)

    nukeParser = subparsers.add_parser('nuke', help='delete all ads')
    nukeParser.set_defaults(function=nuke)

    checkParser = subparsers.add_parser('check_ad', help='check if ad is active')
    checkParser.add_argument('folderName', type=str, help='folder containing ad details')
    checkParser.set_defaults(function=check_ad)

    repostParser = subparsers.add_parser('repost', help='repost an existing ad')
    repostParser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    repostParser.set_defaults(function=repost_ad)

    buildParser = subparsers.add_parser('build_ad', help='Generates the item.inf file for a new ad')
    buildParser.set_defaults(function=generate_inf_file)

    args = parser.parse_args()
    try:
        args.function(args)
    except AttributeError:
        parser.print_help()


def get_folder_data(args):
    """"""
    Set ad data inf file and extract login credentials from inf files
    """"""
    args.inf_file = ""item.inf""
    cred_file = args.folderName + ""/login.inf""
    creds = [line.strip() for line in open(cred_file, 'r')]
    args.username = creds[0]
    args.password = creds[1]


def get_inf_details(inf_file):
    """"""
    Extract ad data from inf file
    """"""
    with open(inf_file, 'rt') as infFileLines:
        data = {key: val for line in infFileLines for (key, val) in (line.strip().split(""=""),)}
    files = [open(picture, 'rb').read() for picture in data['imageCsv'].split("","")]
    return [data, files]


def post_folder(args):
    """"""
    Post new ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folderName)
    post_ad(args)


def post_ad(args):
    """"""
    Post new ad
    """"""
    [data, imageFiles] = get_inf_details(args.inf_file)
    attempts = 1
    while not check_ad(args) and attempts < 5:
        if attempts > 1:
            print(""Failed Attempt #"" + str(attempts) + "", trying again."")
        attempts += 1
        api = kijiji_api.KijijiApi()
        api.login(args.username, args.password)
        api.post_ad_using_data(data, imageFiles)
        sleep(180)
    if not check_ad(args):
        print(""Failed Attempt #"" + str(attempts) + "", giving up."")


def show_ads(args):
    """"""
    Print list of all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    [print(""{} '{}'"".format(adId, adName)) for adName, adId in api.get_all_ads()]


def delete_ad(args):
    """"""
    Delete ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    api.delete_ad(args.id)


def delete_ad_using_title(name):
    """"""
    Delete ad based on ad title
    """"""
    api = kijiji_api.KijijiApi()
    api.delete_ad_using_title(name)


def repost_ad(args):
    """"""
    Repost ad

    Try to delete ad with same title if possible before reposting new ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    delAdName = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            delAdName = val
    try:
        api.delete_ad_using_title(delAdName)
        print(""Existing ad deleted before reposting"")
    except kijiji_api.DeleteAdException:
        print(""Did not find an existing ad with matching title, skipping ad deletion"")
        pass
    # Must wait a bit before posting the same ad even after deleting it, otherwise Kijiji will automatically remove it
    sleep(180)
    post_ad(args)


def repost_folder(args):
    """"""
    Repost ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folderName)
    repost_ad(args)


def check_ad(args):
    """"""
    Check if ad is live
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    AdName = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            AdName = val
    allAds = api.get_all_ads()
    return [t for t, i in allAds if t == AdName]


def nuke(args):
    """"""
    Delete all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    allAds = api.get_all_ads()
    [api.delete_ad(adId) for adName, adId in allAds]

def generate_inf_file(args):
    generator.run_program()

if __name__ == ""__main__"":
    main()
/n/n/n/kijiji_repost_headless/kijiji_api.py/n/nimport requests
import json
import bs4
import re
import sys
from multiprocessing import Pool
from time import strftime

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


class KijijiApiException(Exception):
    """"""
    Custom KijijiApi exception class
    """"""
    def __init__(self, dump=None):
        self.dumpfilepath = """"
        if dump:
            self.dumpfilepath = ""kijiji_dump_{}.txt"".format(strftime(""%Y%m%dT%H%M%S""))
            with open(self.dumpfilepath, 'a') as f:
                f.write(dump)
    def __str__(self):
        if self.dumpfilepath:
            return ""See {} in current directory for latest dumpfile."".format(self.dumpfilepath)
        else:
            return """"

class SignInException(KijijiApiException):
    def __str__(self):
        return ""Could not sign in.\n""+super().__str__()

class PostAdException(KijijiApiException):
    def __str__(self):
        return ""Could not post ad.\n""+super().__str__()

class BannedException(KijijiApiException):
    def __str__(self):
        return ""Could not post ad, this user is banned.\n""+super().__str__()

class DeleteAdException(KijijiApiException):
    def __str__(self):
        return ""Could not delete ad.\n""+super().__str__()


def get_token(html, token_name):
    """"""
    Retrive CSRF token from webpage
    Tokens are different every time a page is visitied
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    res = soup.select(""[name={}]"".format(token_name))
    if not res:
        print(""Token '{}' not found in html text."".format(token_name))
        return """"
    return res[0]['value']


class KijijiApi:
    """"""
    All functions require to be logged in to Kijiji first in order to function correctly
    """"""
    def __init__(self):
        config = {}
        self.session = requests.Session()

    def login(self, username, password):
        """"""
        Login to Kijiji for the current session
        """"""
        login_url = 'https://www.kijiji.ca/t-login.html'
        resp = self.session.get(login_url)
        payload = {
            'emailOrNickname': username,
            'password': password,
            'rememberMe': 'true',
            '_rememberMe': 'on',
            'ca.kijiji.xsrf.token': get_token(resp.text, 'ca.kijiji.xsrf.token'),
            'targetUrl': 'L3QtbG9naW4uaHRtbD90YXJnZXRVcmw9TDNRdGJHOW5hVzR1YUhSdGJEOTBZWEpuWlhSVmNtdzlUREpuZEZwWFVuUmlNalV3WWpJMGRGbFlTbXhaVXpoNFRucEJkMDFxUVhsWWJVMTZZbFZLU1dGVmJHdGtiVTVzVlcxa1VWSkZPV0ZVUmtWNlUyMWpPVkJSTFMxZVRITTBVMk5wVW5wbVRHRlFRVUZwTDNKSGNtVk9kejA5XnpvMnFzNmc2NWZlOWF1T1BKMmRybEE9PQ--'
            }
        resp = self.session.post(login_url, data=payload)
        if not self.is_logged_in():
            raise SignInException(resp.text)

    def is_logged_in(self):
        """"""
        Return true if logged into Kijiji for the current session
        """"""
        index_page_text = self.session.get('https://www.kijiji.ca/m-my-ads.html/').text
        return ""Sign Out"" in index_page_text

    def logout(self):
        """"""
        Logout of Kijiji for the current session
        """"""
        self.session.get('https://www.kijiji.ca/m-logout.html')

    def delete_ad(self, ad_id):
        """"""
        Delete ad based on ad ID
        """"""
        my_ads_page = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        params = {
            'Action': 'DELETE_ADS',
            'Mode': 'ACTIVE',
            'needsRedirect': 'false',
            'ads': '[{{""adId"":""{}"",""reason"":""PREFER_NOT_TO_SAY"",""otherReason"":""""}}]'.format(ad_id),
            'ca.kijiji.xsrf.token': get_token(my_ads_page.text, 'ca.kijiji.xsrf.token')
            }
        resp = self.session.post('https://www.kijiji.ca/j-delete-ad.json', data=params)
        if (""OK"" not in resp.text):
            raise DeleteAdException(resp.text)

    def delete_ad_using_title(self, title):
        """"""
        Delete ad based on ad title
        """"""
        allAds = self.get_all_ads()
        [self.delete_ad(i) for t, i in allAds if t.strip() == title.strip()]

    def upload_image(self, token, image_files=[]):
        """"""
        Upload one or more photos to Kijiji concurrently using Pool

        'image_files' is a list of binary objects corresponding to images
        """"""
        image_urls = []
        image_upload_url = 'https://www.kijiji.ca/p-upload-image.html'
        for img_file in image_files:
            for i in range(0, 3):
                files = {'file': img_file}
                r = self.session.post(image_upload_url, files=files, headers={""x-ebay-box-token"": token})
                if (r.status_code != 200):
                    print(r.status_code)
                try:
                    image_tree = json.loads(r.text)
                    img_url = image_tree['thumbnailUrl']
                    print(""Image Upload success on try #{}"".format(i+1))
                    image_urls.append(img_url)
                    break
                except (KeyError, ValueError) as e:
                    print(""Image Upload failed on try #{}"".format(i+1))
        return [image for image in image_urls if image is not None]

    def post_ad_using_data(self, data, image_files=[]):
        """"""
        Post new ad

        'data' is a dictionary of ad data that to be posted
        'image_files' is a list of binary objects corresponding to images to upload
        """"""
        # Load ad posting page
        resp = self.session.get('https://www.kijiji.ca/p-admarkt-post-ad.html?categoryId=773')

        #Get tokens required for upload
        token_regex = r""initialXsrfToken: '\S+'""
        image_upload_token = re.findall(token_regex, resp.text)[0].strip(""initialXsrfToken: '"").strip(""'"")

        # Upload the images
        imageList = self.upload_image(image_upload_token, image_files)
        data['images'] = "","".join(imageList)

        # Retrive tokens for website
        data['ca.kijiji.xsrf.token'] = get_token(resp.text, 'ca.kijiji.xsrf.token')
        data['postAdForm.fraudToken'] = get_token(resp.text, 'postAdForm.fraudToken')
        data['postAdForm.description'] = data['postAdForm.description'].replace(""\\n"", ""\n"")

        # Upload the ad itself
        new_ad_url = ""https://www.kijiji.ca/p-submit-ad.html""
        resp = self.session.post(new_ad_url, data=data)
        if not len(data.get(""postAdForm.title"", """")) >= 10:
            raise AssertionError(""Your title is too short!"")
        if (int(resp.status_code) != 200 or \
                ""Delete Ad?"" not in resp.text):
            if ""There was an issue posting your ad, please contact Customer Service."" in resp.text:
                raise BannedException(resp.text)
            else:
                raise PostAdException(resp.text)

        # Get adId and return it
        new_cookie_with_ad_id = resp.headers['Set-Cookie']
        ad_id = re.search('\d+', new_cookie_with_ad_id).group()
        return ad_id

    def get_all_ads(self):
        """"""
        Return an iterator of tuples containing the ad title and ad ID for every ad
        """"""
        resp = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        user_id=get_token(resp.text, 'userId')
        my_ads_url = 'https://www.kijiji.ca/j-get-my-ads.json?_=1&currentOffset=0&isPromoting=false&show=ACTIVE&user={}'.format(user_id)
        my_ads_page = self.session.get(my_ads_url)
        my_ads_tree = json.loads(my_ads_page.text)
        ad_ids = [entry['id'] for entry in my_ads_tree['myAdEntries']]
        ad_names = [entry['title'] for entry in my_ads_tree['myAdEntries']]
        return zip(ad_names, ad_ids)
/n/n/n",1
72,72,d6f091c4439c174c7700776c0cee03053403f600,"notebook/base/handlers.py/n/n""""""Base Tornado handlers for the notebook server.""""""

# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import functools
import json
import os
import re
import sys
import traceback
try:
    # py3
    from http.client import responses
except ImportError:
    from httplib import responses
try:
    from urllib.parse import urlparse # Py 3
except ImportError:
    from urlparse import urlparse # Py 2

from jinja2 import TemplateNotFound
from tornado import web, gen, escape
from tornado.log import app_log

from notebook._sysinfo import get_sys_info

from traitlets.config import Application
from ipython_genutils.path import filefind
from ipython_genutils.py3compat import string_types

import notebook
from notebook.utils import is_hidden, url_path_join, url_is_absolute, url_escape
from notebook.services.security import csp_report_uri

#-----------------------------------------------------------------------------
# Top-level handlers
#-----------------------------------------------------------------------------
non_alphanum = re.compile(r'[^A-Za-z0-9]')

sys_info = json.dumps(get_sys_info())

def log():
    if Application.initialized():
        return Application.instance().log
    else:
        return app_log

class AuthenticatedHandler(web.RequestHandler):
    """"""A RequestHandler with an authenticated user.""""""

    @property
    def content_security_policy(self):
        """"""The default Content-Security-Policy header
        
        Can be overridden by defining Content-Security-Policy in settings['headers']
        """"""
        return '; '.join([
            ""frame-ancestors 'self'"",
            # Make sure the report-uri is relative to the base_url
            ""report-uri "" + url_path_join(self.base_url, csp_report_uri),
        ])

    def set_default_headers(self):
        headers = self.settings.get('headers', {})

        if ""Content-Security-Policy"" not in headers:
            headers[""Content-Security-Policy""] = self.content_security_policy

        # Allow for overriding headers
        for header_name,value in headers.items() :
            try:
                self.set_header(header_name, value)
            except Exception as e:
                # tornado raise Exception (not a subclass)
                # if method is unsupported (websocket and Access-Control-Allow-Origin
                # for example, so just ignore)
                self.log.debug(e)
    
    def clear_login_cookie(self):
        self.clear_cookie(self.cookie_name)
    
    def get_current_user(self):
        if self.login_handler is None:
            return 'anonymous'
        return self.login_handler.get_user(self)

    def skip_check_origin(self):
        """"""Ask my login_handler if I should skip the origin_check
        
        For example: in the default LoginHandler, if a request is token-authenticated,
        origin checking should be skipped.
        """"""
        if self.login_handler is None or not hasattr(self.login_handler, 'should_check_origin'):
            return False
        return not self.login_handler.should_check_origin(self)

    @property
    def token_authenticated(self):
        """"""Have I been authenticated with a token?""""""
        if self.login_handler is None or not hasattr(self.login_handler, 'is_token_authenticated'):
            return False
        return self.login_handler.is_token_authenticated(self)

    @property
    def cookie_name(self):
        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(
            self.request.host
        ))
        return self.settings.get('cookie_name', default_cookie_name)
    
    @property
    def logged_in(self):
        """"""Is a user currently logged in?""""""
        user = self.get_current_user()
        return (user and not user == 'anonymous')

    @property
    def login_handler(self):
        """"""Return the login handler for this application, if any.""""""
        return self.settings.get('login_handler_class', None)

    @property
    def token(self):
        """"""Return the login token for this application, if any.""""""
        return self.settings.get('token', None)

    @property
    def one_time_token(self):
        """"""Return the one-time-use token for this application, if any.""""""
        return self.settings.get('one_time_token', None)

    @property
    def login_available(self):
        """"""May a user proceed to log in?

        This returns True if login capability is available, irrespective of
        whether the user is already logged in or not.

        """"""
        if self.login_handler is None:
            return False
        return bool(self.login_handler.get_login_available(self.settings))


class IPythonHandler(AuthenticatedHandler):
    """"""IPython-specific extensions to authenticated handling
    
    Mostly property shortcuts to IPython-specific settings.
    """"""

    @property
    def ignore_minified_js(self):
        """"""Wether to user bundle in template. (*.min files)
        
        Mainly use for development and avoid file recompilation
        """"""
        return self.settings.get('ignore_minified_js', False)

    @property
    def config(self):
        return self.settings.get('config', None)
    
    @property
    def log(self):
        """"""use the IPython log by default, falling back on tornado's logger""""""
        return log()

    @property
    def jinja_template_vars(self):
        """"""User-supplied values to supply to jinja templates.""""""
        return self.settings.get('jinja_template_vars', {})
    
    #---------------------------------------------------------------
    # URLs
    #---------------------------------------------------------------
    
    @property
    def version_hash(self):
        """"""The version hash to use for cache hints for static files""""""
        return self.settings.get('version_hash', '')
    
    @property
    def mathjax_url(self):
        url = self.settings.get('mathjax_url', '')
        if not url or url_is_absolute(url):
            return url
        return url_path_join(self.base_url, url)
    
    @property
    def mathjax_config(self):
        return self.settings.get('mathjax_config', 'TeX-AMS-MML_HTMLorMML-full,Safe')

    @property
    def base_url(self):
        return self.settings.get('base_url', '/')

    @property
    def default_url(self):
        return self.settings.get('default_url', '')

    @property
    def ws_url(self):
        return self.settings.get('websocket_url', '')

    @property
    def contents_js_source(self):
        self.log.debug(""Using contents: %s"", self.settings.get('contents_js_source',
            'services/built/contents'))
        return self.settings.get('contents_js_source', 'services/built/contents')
    
    #---------------------------------------------------------------
    # Manager objects
    #---------------------------------------------------------------
    
    @property
    def kernel_manager(self):
        return self.settings['kernel_manager']

    @property
    def contents_manager(self):
        return self.settings['contents_manager']
    
    @property
    def session_manager(self):
        return self.settings['session_manager']
    
    @property
    def terminal_manager(self):
        return self.settings['terminal_manager']
    
    @property
    def kernel_spec_manager(self):
        return self.settings['kernel_spec_manager']

    @property
    def config_manager(self):
        return self.settings['config_manager']

    #---------------------------------------------------------------
    # CORS
    #---------------------------------------------------------------
    
    @property
    def allow_origin(self):
        """"""Normal Access-Control-Allow-Origin""""""
        return self.settings.get('allow_origin', '')
    
    @property
    def allow_origin_pat(self):
        """"""Regular expression version of allow_origin""""""
        return self.settings.get('allow_origin_pat', None)
    
    @property
    def allow_credentials(self):
        """"""Whether to set Access-Control-Allow-Credentials""""""
        return self.settings.get('allow_credentials', False)
    
    def set_default_headers(self):
        """"""Add CORS headers, if defined""""""
        super(IPythonHandler, self).set_default_headers()
        if self.allow_origin:
            self.set_header(""Access-Control-Allow-Origin"", self.allow_origin)
        elif self.allow_origin_pat:
            origin = self.get_origin()
            if origin and self.allow_origin_pat.match(origin):
                self.set_header(""Access-Control-Allow-Origin"", origin)
        if self.allow_credentials:
            self.set_header(""Access-Control-Allow-Credentials"", 'true')
    
    def get_origin(self):
        # Handle WebSocket Origin naming convention differences
        # The difference between version 8 and 13 is that in 8 the
        # client sends a ""Sec-Websocket-Origin"" header and in 13 it's
        # simply ""Origin"".
        if ""Origin"" in self.request.headers:
            origin = self.request.headers.get(""Origin"")
        else:
            origin = self.request.headers.get(""Sec-Websocket-Origin"", None)
        return origin

    # origin_to_satisfy_tornado is present because tornado requires
    # check_origin to take an origin argument, but we don't use it
    def check_origin(self, origin_to_satisfy_tornado=""""):
        """"""Check Origin for cross-site API requests, including websockets

        Copied from WebSocket with changes:

        - allow unspecified host/origin (e.g. scripts)
        - allow token-authenticated requests
        """"""
        if self.allow_origin == '*' or self.skip_check_origin():
            return True

        host = self.request.headers.get(""Host"")
        origin = self.request.headers.get(""Origin"")

        # If no header is provided, allow it.
        # Origin can be None for:
        # - same-origin (IE, Firefox)
        # - Cross-site POST form (IE, Firefox)
        # - Scripts
        # The cross-site POST (XSRF) case is handled by tornado's xsrf_token
        if origin is None or host is None:
            return True

        origin = origin.lower()
        origin_host = urlparse(origin).netloc

        # OK if origin matches host
        if origin_host == host:
            return True

        # Check CORS headers
        if self.allow_origin:
            allow = self.allow_origin == origin
        elif self.allow_origin_pat:
            allow = bool(self.allow_origin_pat.match(origin))
        else:
            # No CORS headers deny the request
            allow = False
        if not allow:
            self.log.warning(""Blocking Cross Origin API request for %s.  Origin: %s, Host: %s"",
                self.request.path, origin, host,
            )
        return allow

    def check_xsrf_cookie(self):
        """"""Bypass xsrf checks when token-authenticated""""""
        if self.token_authenticated or self.settings.get('disable_check_xsrf', False):
            # Token-authenticated requests do not need additional XSRF-check
            # Servers without authentication are vulnerable to XSRF
            return
        return super(IPythonHandler, self).check_xsrf_cookie()

    #---------------------------------------------------------------
    # template rendering
    #---------------------------------------------------------------
    
    def get_template(self, name):
        """"""Return the jinja template object for a given name""""""
        return self.settings['jinja2_env'].get_template(name)
    
    def render_template(self, name, **ns):
        ns.update(self.template_namespace)
        template = self.get_template(name)
        return template.render(**ns)
    
    @property
    def template_namespace(self):
        return dict(
            base_url=self.base_url,
            default_url=self.default_url,
            ws_url=self.ws_url,
            logged_in=self.logged_in,
            login_available=self.login_available,
            token_available=bool(self.token or self.one_time_token),
            static_url=self.static_url,
            sys_info=sys_info,
            contents_js_source=self.contents_js_source,
            version_hash=self.version_hash,
            ignore_minified_js=self.ignore_minified_js,
            xsrf_form_html=self.xsrf_form_html,
            token=self.token,
            xsrf_token=self.xsrf_token.decode('utf8'),
            **self.jinja_template_vars
        )
    
    def get_json_body(self):
        """"""Return the body of the request as JSON data.""""""
        if not self.request.body:
            return None
        # Do we need to call body.decode('utf-8') here?
        body = self.request.body.strip().decode(u'utf-8')
        try:
            model = json.loads(body)
        except Exception:
            self.log.debug(""Bad JSON: %r"", body)
            self.log.error(""Couldn't parse JSON"", exc_info=True)
            raise web.HTTPError(400, u'Invalid JSON in body of request')
        return model

    def write_error(self, status_code, **kwargs):
        """"""render custom error pages""""""
        exc_info = kwargs.get('exc_info')
        message = ''
        status_message = responses.get(status_code, 'Unknown HTTP Error')
        exception = '(unknown)'
        if exc_info:
            exception = exc_info[1]
            # get the custom message, if defined
            try:
                message = exception.log_message % exception.args
            except Exception:
                pass
            
            # construct the custom reason, if defined
            reason = getattr(exception, 'reason', '')
            if reason:
                status_message = reason
        
        # build template namespace
        ns = dict(
            status_code=status_code,
            status_message=status_message,
            message=message,
            exception=exception,
        )
        
        self.set_header('Content-Type', 'text/html')
        # render the template
        try:
            html = self.render_template('%s.html' % status_code, **ns)
        except TemplateNotFound:
            self.log.debug(""No template for %d"", status_code)
            html = self.render_template('error.html', **ns)
        
        self.write(html)


class APIHandler(IPythonHandler):
    """"""Base class for API handlers""""""

    def prepare(self):
        if not self.check_origin():
            raise web.HTTPError(404)
        return super(APIHandler, self).prepare()

    @property
    def content_security_policy(self):
        csp = '; '.join([
                super(APIHandler, self).content_security_policy,
                ""default-src 'none'"",
            ])
        return csp
    
    def finish(self, *args, **kwargs):
        self.set_header('Content-Type', 'application/json')
        return super(APIHandler, self).finish(*args, **kwargs)

    def options(self, *args, **kwargs):
        self.set_header('Access-Control-Allow-Headers', 'accept, content-type, authorization')
        self.set_header('Access-Control-Allow-Methods',
                        'GET, PUT, POST, PATCH, DELETE, OPTIONS')
        self.finish()


class Template404(IPythonHandler):
    """"""Render our 404 template""""""
    def prepare(self):
        raise web.HTTPError(404)


class AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):
    """"""static files should only be accessible when logged in""""""

    @web.authenticated
    def get(self, path):
        if os.path.splitext(path)[1] == '.ipynb':
            name = path.rsplit('/', 1)[-1]
            self.set_header('Content-Type', 'application/json')
            self.set_header('Content-Disposition','attachment; filename=""%s""' % escape.url_escape(name))
        
        return web.StaticFileHandler.get(self, path)
    
    def set_headers(self):
        super(AuthenticatedFileHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments:
            self.add_header(""Cache-Control"", ""no-cache"")
    
    def compute_etag(self):
        return None
    
    def validate_absolute_path(self, root, absolute_path):
        """"""Validate and return the absolute path.
        
        Requires tornado 3.1
        
        Adding to tornado's own handling, forbids the serving of hidden files.
        """"""
        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)
        abs_root = os.path.abspath(root)
        if is_hidden(abs_path, abs_root):
            self.log.info(""Refusing to serve hidden file, via 404 Error"")
            raise web.HTTPError(404)
        return abs_path


def json_errors(method):
    """"""Decorate methods with this to return GitHub style JSON errors.
    
    This should be used on any JSON API on any handler method that can raise HTTPErrors.
    
    This will grab the latest HTTPError exception using sys.exc_info
    and then:
    
    1. Set the HTTP status code based on the HTTPError
    2. Create and return a JSON body with a message field describing
       the error in a human readable form.
    """"""
    @functools.wraps(method)
    @gen.coroutine
    def wrapper(self, *args, **kwargs):
        try:
            result = yield gen.maybe_future(method(self, *args, **kwargs))
        except web.HTTPError as e:
            self.set_header('Content-Type', 'application/json')
            status = e.status_code
            message = e.log_message
            self.log.warning(message)
            self.set_status(e.status_code)
            reply = dict(message=message, reason=e.reason)
            self.finish(json.dumps(reply))
        except Exception:
            self.set_header('Content-Type', 'application/json')
            self.log.error(""Unhandled error in API request"", exc_info=True)
            status = 500
            message = ""Unknown server error""
            t, value, tb = sys.exc_info()
            self.set_status(status)
            tb_text = ''.join(traceback.format_exception(t, value, tb))
            reply = dict(message=message, reason=None, traceback=tb_text)
            self.finish(json.dumps(reply))
        else:
            # FIXME: can use regular return in generators in py3
            raise gen.Return(result)
    return wrapper



#-----------------------------------------------------------------------------
# File handler
#-----------------------------------------------------------------------------

# to minimize subclass changes:
HTTPError = web.HTTPError

class FileFindHandler(IPythonHandler, web.StaticFileHandler):
    """"""subclass of StaticFileHandler for serving files from a search path""""""
    
    # cache search results, don't search for files more than once
    _static_paths = {}
    
    def set_headers(self):
        super(FileFindHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments or \
                any(self.request.path.startswith(path) for path in self.no_cache_paths):
            self.set_header(""Cache-Control"", ""no-cache"")
    
    def initialize(self, path, default_filename=None, no_cache_paths=None):
        self.no_cache_paths = no_cache_paths or []
        
        if isinstance(path, string_types):
            path = [path]
        
        self.root = tuple(
            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path
        )
        self.default_filename = default_filename
    
    def compute_etag(self):
        return None
    
    @classmethod
    def get_absolute_path(cls, roots, path):
        """"""locate a file to serve on our static file search path""""""
        with cls._lock:
            if path in cls._static_paths:
                return cls._static_paths[path]
            try:
                abspath = os.path.abspath(filefind(path, roots))
            except IOError:
                # IOError means not found
                return ''
            
            cls._static_paths[path] = abspath
            

            log().debug(""Path %s served from %s""%(path, abspath))
            return abspath
    
    def validate_absolute_path(self, root, absolute_path):
        """"""check if the file should be served (raises 404, 403, etc.)""""""
        if absolute_path == '':
            raise web.HTTPError(404)
        
        for root in self.root:
            if (absolute_path + os.sep).startswith(root):
                break
        
        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)


class APIVersionHandler(APIHandler):

    @json_errors
    def get(self):
        # not authenticated, so give as few info as possible
        self.finish(json.dumps({""version"":notebook.__version__}))


class TrailingSlashHandler(web.RequestHandler):
    """"""Simple redirect handler that strips trailing slashes
    
    This should be the first, highest priority handler.
    """"""
    
    def get(self):
        self.redirect(self.request.uri.rstrip('/'))
    
    post = put = get


class FilesRedirectHandler(IPythonHandler):
    """"""Handler for redirecting relative URLs to the /files/ handler""""""
    
    @staticmethod
    def redirect_to_files(self, path):
        """"""make redirect logic a reusable static method
        
        so it can be called from other handlers.
        """"""
        cm = self.contents_manager
        if cm.dir_exists(path):
            # it's a *directory*, redirect to /tree
            url = url_path_join(self.base_url, 'tree', url_escape(path))
        else:
            orig_path = path
            # otherwise, redirect to /files
            parts = path.split('/')

            if not cm.file_exists(path=path) and 'files' in parts:
                # redirect without files/ iff it would 404
                # this preserves pre-2.0-style 'files/' links
                self.log.warning(""Deprecated files/ URL: %s"", orig_path)
                parts.remove('files')
                path = '/'.join(parts)

            if not cm.file_exists(path=path):
                raise web.HTTPError(404)

            url = url_path_join(self.base_url, 'files', url_escape(path))
        self.log.debug(""Redirecting %s to %s"", self.request.path, url)
        self.redirect(url)
    
    def get(self, path=''):
        return self.redirect_to_files(self, path)


class RedirectWithParams(web.RequestHandler):
    """"""Sam as web.RedirectHandler, but preserves URL parameters""""""
    def initialize(self, url, permanent=True):
        self._url = url
        self._permanent = permanent

    def get(self):
        sep = '&' if '?' in self._url else '?'
        url = sep.join([self._url, self.request.query])
        self.redirect(url, permanent=self._permanent)

#-----------------------------------------------------------------------------
# URL pattern fragments for re-use
#-----------------------------------------------------------------------------

# path matches any number of `/foo[/bar...]` or just `/` or ''
path_regex = r""(?P<path>(?:(?:/[^/]+)+|/?))""

#-----------------------------------------------------------------------------
# URL to handler mappings
#-----------------------------------------------------------------------------


default_handlers = [
    (r"".*/"", TrailingSlashHandler),
    (r""api"", APIVersionHandler)
]
/n/n/nnotebook/notebookapp.py/n/n# coding: utf-8
""""""A tornado based Jupyter notebook server.""""""

# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

from __future__ import absolute_import, print_function

import binascii
import datetime
import errno
import importlib
import io
import json
import logging
import mimetypes
import os
import random
import re
import select
import signal
import socket
import sys
import threading
import warnings
import webbrowser

try: #PY3
    from base64 import encodebytes
except ImportError: #PY2
    from base64 import encodestring as encodebytes


from jinja2 import Environment, FileSystemLoader

# Install the pyzmq ioloop. This has to be done before anything else from
# tornado is imported.
from zmq.eventloop import ioloop
ioloop.install()

# check for tornado 3.1.0
msg = ""The Jupyter Notebook requires tornado >= 4.0""
try:
    import tornado
except ImportError:
    raise ImportError(msg)
try:
    version_info = tornado.version_info
except AttributeError:
    raise ImportError(msg + "", but you have < 1.1.0"")
if version_info < (4,0):
    raise ImportError(msg + "", but you have %s"" % tornado.version)

from tornado import httpserver
from tornado import web
from tornado.httputil import url_concat
from tornado.log import LogFormatter, app_log, access_log, gen_log

from notebook import (
    DEFAULT_STATIC_FILES_PATH,
    DEFAULT_TEMPLATE_PATH_LIST,
    __version__,
)

# py23 compatibility
try:
    raw_input = raw_input
except NameError:
    raw_input = input

from .base.handlers import Template404, RedirectWithParams
from .log import log_request
from .services.kernels.kernelmanager import MappingKernelManager
from .services.config import ConfigManager
from .services.contents.manager import ContentsManager
from .services.contents.filemanager import FileContentsManager
from .services.sessions.sessionmanager import SessionManager

from .auth.login import LoginHandler
from .auth.logout import LogoutHandler
from .base.handlers import FileFindHandler

from traitlets.config import Config
from traitlets.config.application import catch_config_error, boolean_flag
from jupyter_core.application import (
    JupyterApp, base_flags, base_aliases,
)
from jupyter_client import KernelManager
from jupyter_client.kernelspec import KernelSpecManager, NoSuchKernel, NATIVE_KERNEL_NAME
from jupyter_client.session import Session
from nbformat.sign import NotebookNotary
from traitlets import (
    Dict, Unicode, Integer, List, Bool, Bytes, Instance,
    TraitError, Type, Float, observe, default, validate
)
from ipython_genutils import py3compat
from jupyter_core.paths import jupyter_runtime_dir, jupyter_path
from notebook._sysinfo import get_sys_info

from .utils import url_path_join, check_pid, url_escape

#-----------------------------------------------------------------------------
# Module globals
#-----------------------------------------------------------------------------

_examples = """"""
jupyter notebook                       # start the notebook
jupyter notebook --certfile=mycert.pem # use SSL/TLS certificate
""""""

DEV_NOTE_NPM = """"""It looks like you're running the notebook from source.
If you're working on the Javascript of the notebook, try running

    npm run build:watch

in another terminal window to have the system incrementally
watch and build the notebook's JavaScript for you, as you make changes.
""""""

#-----------------------------------------------------------------------------
# Helper functions
#-----------------------------------------------------------------------------

def random_ports(port, n):
    """"""Generate a list of n random ports near the given port.

    The first 5 ports will be sequential, and the remaining n-5 will be
    randomly selected in the range [port-2*n, port+2*n].
    """"""
    for i in range(min(5, n)):
        yield port + i
    for i in range(n-5):
        yield max(1, port + random.randint(-2*n, 2*n))

def load_handlers(name):
    """"""Load the (URL pattern, handler) tuples for each component.""""""
    name = 'notebook.' + name
    mod = __import__(name, fromlist=['default_handlers'])
    return mod.default_handlers

#-----------------------------------------------------------------------------
# The Tornado web application
#-----------------------------------------------------------------------------

class NotebookWebApplication(web.Application):

    def __init__(self, jupyter_app, kernel_manager, contents_manager,
                 session_manager, kernel_spec_manager,
                 config_manager, log,
                 base_url, default_url, settings_overrides, jinja_env_options):

        # If the user is running the notebook in a git directory, make the assumption
        # that this is a dev install and suggest to the developer `npm run build:watch`.
        base_dir = os.path.realpath(os.path.join(__file__, '..', '..'))
        dev_mode = os.path.exists(os.path.join(base_dir, '.git'))
        if dev_mode:
            log.info(DEV_NOTE_NPM)

        settings = self.init_settings(
            jupyter_app, kernel_manager, contents_manager,
            session_manager, kernel_spec_manager, config_manager, log, base_url,
            default_url, settings_overrides, jinja_env_options)
        handlers = self.init_handlers(settings)

        super(NotebookWebApplication, self).__init__(handlers, **settings)

    def init_settings(self, jupyter_app, kernel_manager, contents_manager,
                      session_manager, kernel_spec_manager,
                      config_manager,
                      log, base_url, default_url, settings_overrides,
                      jinja_env_options=None):

        _template_path = settings_overrides.get(
            ""template_path"",
            jupyter_app.template_file_path,
        )
        if isinstance(_template_path, py3compat.string_types):
            _template_path = (_template_path,)
        template_path = [os.path.expanduser(path) for path in _template_path]

        jenv_opt = {""autoescape"": True}
        jenv_opt.update(jinja_env_options if jinja_env_options else {})

        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)
        
        sys_info = get_sys_info()
        if sys_info['commit_source'] == 'repository':
            # don't cache (rely on 304) when working from master
            version_hash = ''
        else:
            # reset the cache on server restart
            version_hash = datetime.datetime.now().strftime(""%Y%m%d%H%M%S"")

        if jupyter_app.ignore_minified_js:
            log.warning(""""""The `ignore_minified_js` flag is deprecated and no 
                longer works.  Alternatively use `npm run build:watch` when
                working on the notebook's Javascript and LESS"""""")
            warnings.warn(""The `ignore_minified_js` flag is deprecated and will be removed in Notebook 6.0"", DeprecationWarning)

        settings = dict(
            # basics
            log_function=log_request,
            base_url=base_url,
            default_url=default_url,
            template_path=template_path,
            static_path=jupyter_app.static_file_path,
            static_custom_path=jupyter_app.static_custom_path,
            static_handler_class = FileFindHandler,
            static_url_prefix = url_path_join(base_url,'/static/'),
            static_handler_args = {
                # don't cache custom.js
                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],
            },
            version_hash=version_hash,
            ignore_minified_js=jupyter_app.ignore_minified_js,
            
            # rate limits
            iopub_msg_rate_limit=jupyter_app.iopub_msg_rate_limit,
            iopub_data_rate_limit=jupyter_app.iopub_data_rate_limit,
            rate_limit_window=jupyter_app.rate_limit_window,
            
            # authentication
            cookie_secret=jupyter_app.cookie_secret,
            login_url=url_path_join(base_url,'/login'),
            login_handler_class=jupyter_app.login_handler_class,
            logout_handler_class=jupyter_app.logout_handler_class,
            password=jupyter_app.password,
            xsrf_cookies=True,
            disable_check_xsrf=ipython_app.disable_check_xsrf,

            # managers
            kernel_manager=kernel_manager,
            contents_manager=contents_manager,
            session_manager=session_manager,
            kernel_spec_manager=kernel_spec_manager,
            config_manager=config_manager,

            # IPython stuff
            jinja_template_vars=jupyter_app.jinja_template_vars,
            nbextensions_path=jupyter_app.nbextensions_path,
            websocket_url=jupyter_app.websocket_url,
            mathjax_url=jupyter_app.mathjax_url,
            mathjax_config=jupyter_app.mathjax_config,
            config=jupyter_app.config,
            config_dir=jupyter_app.config_dir,
            jinja2_env=env,
            terminals_available=False,  # Set later if terminals are available
        )

        # allow custom overrides for the tornado web app.
        settings.update(settings_overrides)
        return settings

    def init_handlers(self, settings):
        """"""Load the (URL pattern, handler) tuples for each component.""""""
        
        # Order matters. The first handler to match the URL will handle the request.
        handlers = []
        handlers.extend(load_handlers('tree.handlers'))
        handlers.extend([(r""/login"", settings['login_handler_class'])])
        handlers.extend([(r""/logout"", settings['logout_handler_class'])])
        handlers.extend(load_handlers('files.handlers'))
        handlers.extend(load_handlers('notebook.handlers'))
        handlers.extend(load_handlers('nbconvert.handlers'))
        handlers.extend(load_handlers('bundler.handlers'))
        handlers.extend(load_handlers('kernelspecs.handlers'))
        handlers.extend(load_handlers('edit.handlers'))
        handlers.extend(load_handlers('services.api.handlers'))
        handlers.extend(load_handlers('services.config.handlers'))
        handlers.extend(load_handlers('services.kernels.handlers'))
        handlers.extend(load_handlers('services.contents.handlers'))
        handlers.extend(load_handlers('services.sessions.handlers'))
        handlers.extend(load_handlers('services.nbconvert.handlers'))
        handlers.extend(load_handlers('services.kernelspecs.handlers'))
        handlers.extend(load_handlers('services.security.handlers'))
        
        # BEGIN HARDCODED WIDGETS HACK
        # TODO: Remove on notebook 5.0
        widgets = None
        try:
            import widgetsnbextension
        except:
            try:
                import ipywidgets as widgets
                handlers.append(
                    (r""/nbextensions/widgets/(.*)"", FileFindHandler, {
                        'path': widgets.find_static_assets(),
                        'no_cache_paths': ['/'], # don't cache anything in nbextensions
                    }),
                )
            except:
                app_log.warning('Widgets are unavailable. Please install widgetsnbextension or ipywidgets 4.0')
        # END HARDCODED WIDGETS HACK
        
        handlers.append(
            (r""/nbextensions/(.*)"", FileFindHandler, {
                'path': settings['nbextensions_path'],
                'no_cache_paths': ['/'], # don't cache anything in nbextensions
            }),
        )
        handlers.append(
            (r""/custom/(.*)"", FileFindHandler, {
                'path': settings['static_custom_path'],
                'no_cache_paths': ['/'], # don't cache anything in custom
            })
        )
        # register base handlers last
        handlers.extend(load_handlers('base.handlers'))
        # set the URL that will be redirected from `/`
        handlers.append(
            (r'/?', RedirectWithParams, {
                'url' : settings['default_url'],
                'permanent': False, # want 302, not 301
            })
        )

        # prepend base_url onto the patterns that we match
        new_handlers = []
        for handler in handlers:
            pattern = url_path_join(settings['base_url'], handler[0])
            new_handler = tuple([pattern] + list(handler[1:]))
            new_handlers.append(new_handler)
        # add 404 on the end, which will catch everything that falls through
        new_handlers.append((r'(.*)', Template404))
        return new_handlers


class NbserverListApp(JupyterApp):
    version = __version__
    description=""List currently running notebook servers.""
    
    flags = dict(
        json=({'NbserverListApp': {'json': True}},
              ""Produce machine-readable JSON output.""),
    )
    
    json = Bool(False, config=True,
          help=""If True, each line of output will be a JSON object with the ""
                  ""details from the server info file."")

    def start(self):
        if not self.json:
            print(""Currently running servers:"")
        for serverinfo in list_running_servers(self.runtime_dir):
            if self.json:
                print(json.dumps(serverinfo))
            else:
                url = serverinfo['url']
                if serverinfo.get('token'):
                    url = url + '?token=%s' % serverinfo['token']
                print(url, ""::"", serverinfo['notebook_dir'])

#-----------------------------------------------------------------------------
# Aliases and Flags
#-----------------------------------------------------------------------------

flags = dict(base_flags)
flags['no-browser']=(
    {'NotebookApp' : {'open_browser' : False}},
    ""Don't open the notebook in a browser after startup.""
)
flags['pylab']=(
    {'NotebookApp' : {'pylab' : 'warn'}},
    ""DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.""
)
flags['no-mathjax']=(
    {'NotebookApp' : {'enable_mathjax' : False}},
    """"""Disable MathJax
    
    MathJax is the javascript library Jupyter uses to render math/LaTeX. It is
    very large, so you may want to disable it if you have a slow internet
    connection, or for offline use of the notebook.
    
    When disabled, equations etc. will appear as their untransformed TeX source.
    """"""
)

flags['allow-root']=(
    {'NotebookApp' : {'allow_root' : True}},
    ""Allow the notebook to be run from root user.""
)

# Add notebook manager flags
flags.update(boolean_flag('script', 'FileContentsManager.save_script',
               'DEPRECATED, IGNORED',
               'DEPRECATED, IGNORED'))

aliases = dict(base_aliases)

aliases.update({
    'ip': 'NotebookApp.ip',
    'port': 'NotebookApp.port',
    'port-retries': 'NotebookApp.port_retries',
    'transport': 'KernelManager.transport',
    'keyfile': 'NotebookApp.keyfile',
    'certfile': 'NotebookApp.certfile',
    'client-ca': 'NotebookApp.client_ca',
    'notebook-dir': 'NotebookApp.notebook_dir',
    'browser': 'NotebookApp.browser',
    'pylab': 'NotebookApp.pylab',
})

#-----------------------------------------------------------------------------
# NotebookApp
#-----------------------------------------------------------------------------

class NotebookApp(JupyterApp):

    name = 'jupyter-notebook'
    version = __version__
    description = """"""
        The Jupyter HTML Notebook.
        
        This launches a Tornado based HTML Notebook Server that serves up an
        HTML5/Javascript Notebook client.
    """"""
    examples = _examples
    aliases = aliases
    flags = flags
    
    classes = [
        KernelManager, Session, MappingKernelManager,
        ContentsManager, FileContentsManager, NotebookNotary,
        KernelSpecManager,
    ]
    flags = Dict(flags)
    aliases = Dict(aliases)
    
    subcommands = dict(
        list=(NbserverListApp, NbserverListApp.description.splitlines()[0]),
    )

    _log_formatter_cls = LogFormatter

    @default('log_level')
    def _default_log_level(self):
        return logging.INFO

    @default('log_datefmt')
    def _default_log_datefmt(self):
        """"""Exclude date from default date format""""""
        return ""%H:%M:%S""
    
    @default('log_format')
    def _default_log_format(self):
        """"""override default log format to include time""""""
        return u""%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d %(name)s]%(end_color)s %(message)s""

    ignore_minified_js = Bool(False,
            config=True,
            help='Deprecated: Use minified JS file or not, mainly use during dev to avoid JS recompilation', 
            )

    # file to be opened in the notebook server
    file_to_run = Unicode('', config=True)

    # Network related information
    
    allow_origin = Unicode('', config=True,
        help=""""""Set the Access-Control-Allow-Origin header
        
        Use '*' to allow any origin to access your server.
        
        Takes precedence over allow_origin_pat.
        """"""
    )
    
    allow_origin_pat = Unicode('', config=True,
        help=""""""Use a regular expression for the Access-Control-Allow-Origin header
        
        Requests from an origin matching the expression will get replies with:
        
            Access-Control-Allow-Origin: origin
        
        where `origin` is the origin of the request.
        
        Ignored if allow_origin is set.
        """"""
    )
    
    allow_credentials = Bool(False, config=True,
        help=""Set the Access-Control-Allow-Credentials: true header""
    )
    
    allow_root = Bool(False, config=True, 
        help=""Whether to allow the user to run the notebook as root.""
    )

    default_url = Unicode('/tree', config=True,
        help=""The default URL to redirect to from `/`""
    )
    
    ip = Unicode('localhost', config=True,
        help=""The IP address the notebook server will listen on.""
    )

    @default('ip')
    def _default_ip(self):
        """"""Return localhost if available, 127.0.0.1 otherwise.
        
        On some (horribly broken) systems, localhost cannot be bound.
        """"""
        s = socket.socket()
        try:
            s.bind(('localhost', 0))
        except socket.error as e:
            self.log.warning(""Cannot bind to localhost, using 127.0.0.1 as default ip\n%s"", e)
            return '127.0.0.1'
        else:
            s.close()
            return 'localhost'

    @validate('ip')
    def _valdate_ip(self, proposal):
        value = proposal['value']
        if value == u'*':
            value = u''
        return value

    port = Integer(8888, config=True,
        help=""The port the notebook server will listen on.""
    )

    port_retries = Integer(50, config=True,
        help=""The number of additional ports to try if the specified port is not available.""
    )

    certfile = Unicode(u'', config=True, 
        help=""""""The full path to an SSL/TLS certificate file.""""""
    )
    
    keyfile = Unicode(u'', config=True, 
        help=""""""The full path to a private key file for usage with SSL/TLS.""""""
    )
    
    client_ca = Unicode(u'', config=True,
        help=""""""The full path to a certificate authority certificate for SSL/TLS client authentication.""""""
    )
    
    cookie_secret_file = Unicode(config=True,
        help=""""""The file where the cookie secret is stored.""""""
    )

    @default('cookie_secret_file')
    def _default_cookie_secret_file(self):
        return os.path.join(self.runtime_dir, 'notebook_cookie_secret')
    
    cookie_secret = Bytes(b'', config=True,
        help=""""""The random bytes used to secure cookies.
        By default this is a new random number every time you start the Notebook.
        Set it to a value in a config file to enable logins to persist across server sessions.
        
        Note: Cookie secrets should be kept private, do not share config files with
        cookie_secret stored in plaintext (you can read the value from a file).
        """"""
    )
    
    @default('cookie_secret')
    def _default_cookie_secret(self):
        if os.path.exists(self.cookie_secret_file):
            with io.open(self.cookie_secret_file, 'rb') as f:
                return f.read()
        else:
            secret = encodebytes(os.urandom(1024))
            self._write_cookie_secret_file(secret)
            return secret
    
    def _write_cookie_secret_file(self, secret):
        """"""write my secret to my secret_file""""""
        self.log.info(""Writing notebook server cookie secret to %s"", self.cookie_secret_file)
        with io.open(self.cookie_secret_file, 'wb') as f:
            f.write(secret)
        try:
            os.chmod(self.cookie_secret_file, 0o600)
        except OSError:
            self.log.warning(
                ""Could not set permissions on %s"",
                self.cookie_secret_file
            )

    token = Unicode('<generated>',
        help=""""""Token used for authenticating first-time connections to the server.

        When no password is enabled,
        the default is to generate a new, random token.

        Setting to an empty string disables authentication altogether, which is NOT RECOMMENDED.
        """"""
    ).tag(config=True)

    one_time_token = Unicode(
        help=""""""One-time token used for opening a browser.

        Once used, this token cannot be used again.
        """"""
    )

    _token_generated = True

    @default('token')
    def _token_default(self):
        if self.password:
            # no token if password is enabled
            self._token_generated = False
            return u''
        else:
            self._token_generated = True
            return binascii.hexlify(os.urandom(24)).decode('ascii')

    @observe('token')
    def _token_changed(self, change):
        self._token_generated = False

    password = Unicode(u'', config=True,
                      help=""""""Hashed password to use for web authentication.

                      To generate, type in a python/IPython shell:

                        from notebook.auth import passwd; passwd()

                      The string should be of the form type:salt:hashed-password.
                      """"""
    )

    password_required = Bool(False, config=True,
                      help=""""""Forces users to use a password for the Notebook server.
                      This is useful in a multi user environment, for instance when
                      everybody in the LAN can access each other's machine though ssh.

                      In such a case, server the notebook server on localhost is not secure
                      since any user can connect to the notebook server via ssh.

                      """"""

    disable_check_xsrf = Bool(False, config=True,
        help=""""""Disable cross-site-request-forgery protection

        Jupyter notebook 4.3.1 introduces protection from cross-site request forgeries,
        requiring API requests to either:

        - originate from the (validated with XSRF cookie and token), or
        - authenticate with a token

        Some anonymous compute resources still desire the ability to run code,
        completely without authentication.
        These services can disable all authentication and security checks,
        with the full knowledge of what that implies.
        """"""
    )

    open_browser = Bool(True, config=True,
                        help=""""""Whether to open in a browser after starting.
                        The specific browser used is platform dependent and
                        determined by the python standard library `webbrowser`
                        module, unless it is overridden using the --browser
                        (NotebookApp.browser) configuration option.
                        """""")

    browser = Unicode(u'', config=True,
                      help=""""""Specify what command to use to invoke a web
                      browser when opening the notebook. If not specified, the
                      default browser will be determined by the `webbrowser`
                      standard library module, which allows setting of the
                      BROWSER environment variable to override it.
                      """""")
    
    webapp_settings = Dict(config=True,
        help=""DEPRECATED, use tornado_settings""
    )

    @observe('webapp_settings') 
    def _update_webapp_settings(self, change):
        self.log.warning(""\n    webapp_settings is deprecated, use tornado_settings.\n"")
        self.tornado_settings = change['new']
    
    tornado_settings = Dict(config=True,
            help=""Supply overrides for the tornado.web.Application that the ""
                 ""Jupyter notebook uses."")
    
    terminado_settings = Dict(config=True,
            help='Supply overrides for terminado. Currently only supports ""shell_command"".')

    cookie_options = Dict(config=True,
        help=""Extra keyword arguments to pass to `set_secure_cookie`.""
             "" See tornado's set_secure_cookie docs for details.""
    )
    ssl_options = Dict(config=True,
            help=""""""Supply SSL options for the tornado HTTPServer.
            See the tornado docs for details."""""")
    
    jinja_environment_options = Dict(config=True, 
            help=""Supply extra arguments that will be passed to Jinja environment."")

    jinja_template_vars = Dict(
        config=True,
        help=""Extra variables to supply to jinja templates when rendering."",
    )
    
    enable_mathjax = Bool(True, config=True,
        help=""""""Whether to enable MathJax for typesetting math/TeX

        MathJax is the javascript library Jupyter uses to render math/LaTeX. It is
        very large, so you may want to disable it if you have a slow internet
        connection, or for offline use of the notebook.

        When disabled, equations etc. will appear as their untransformed TeX source.
        """"""
    )

    @observe('enable_mathjax')
    def _update_enable_mathjax(self, change):
        """"""set mathjax url to empty if mathjax is disabled""""""
        if not change['new']:
            self.mathjax_url = u''

    base_url = Unicode('/', config=True,
                               help='''The base URL for the notebook server.

                               Leading and trailing slashes can be omitted,
                               and will automatically be added.
                               ''')

    @validate('base_url')
    def _update_base_url(self, proposal):
        value = proposal['value']
        if not value.startswith('/'):
            value = '/' + value
        elif not value.endswith('/'):
            value = value + '/'
        return value
    
    base_project_url = Unicode('/', config=True, help=""""""DEPRECATED use base_url"""""")

    @observe('base_project_url')
    def _update_base_project_url(self, change):
        self.log.warning(""base_project_url is deprecated, use base_url"")
        self.base_url = change['new']

    extra_static_paths = List(Unicode(), config=True,
        help=""""""Extra paths to search for serving static files.
        
        This allows adding javascript/css to be available from the notebook server machine,
        or overriding individual files in the IPython""""""
    )
    
    @property
    def static_file_path(self):
        """"""return extra paths + the default location""""""
        return self.extra_static_paths + [DEFAULT_STATIC_FILES_PATH]
    
    static_custom_path = List(Unicode(),
        help=""""""Path to search for custom.js, css""""""
    )

    @default('static_custom_path')
    def _default_static_custom_path(self):
        return [
            os.path.join(d, 'custom') for d in (
                self.config_dir,
                DEFAULT_STATIC_FILES_PATH)
        ]

    extra_template_paths = List(Unicode(), config=True,
        help=""""""Extra paths to search for serving jinja templates.

        Can be used to override templates from notebook.templates.""""""
    )

    @property
    def template_file_path(self):
        """"""return extra paths + the default locations""""""
        return self.extra_template_paths + DEFAULT_TEMPLATE_PATH_LIST

    extra_nbextensions_path = List(Unicode(), config=True,
        help=""""""extra paths to look for Javascript notebook extensions""""""
    )
    
    @property
    def nbextensions_path(self):
        """"""The path to look for Javascript notebook extensions""""""
        path = self.extra_nbextensions_path + jupyter_path('nbextensions')
        # FIXME: remove IPython nbextensions path after a migration period
        try:
            from IPython.paths import get_ipython_dir
        except ImportError:
            pass
        else:
            path.append(os.path.join(get_ipython_dir(), 'nbextensions'))
        return path

    websocket_url = Unicode("""", config=True,
        help=""""""The base URL for websockets,
        if it differs from the HTTP server (hint: it almost certainly doesn't).
        
        Should be in the form of an HTTP origin: ws[s]://hostname[:port]
        """"""
    )

    mathjax_url = Unicode("""", config=True,
        help=""""""A custom url for MathJax.js.
        Should be in the form of a case-sensitive url to MathJax,
        for example:  /static/components/MathJax/MathJax.js
        """"""
    )

    @default('mathjax_url')
    def _default_mathjax_url(self):
        if not self.enable_mathjax:
            return u''
        static_url_prefix = self.tornado_settings.get(""static_url_prefix"", ""static"")
        return url_path_join(static_url_prefix, 'components', 'MathJax', 'MathJax.js')
    
    @observe('mathjax_url')
    def _update_mathjax_url(self, change):
        new = change['new']
        if new and not self.enable_mathjax:
            # enable_mathjax=False overrides mathjax_url
            self.mathjax_url = u''
        else:
            self.log.info(""Using MathJax: %s"", new)

    mathjax_config = Unicode(""TeX-AMS-MML_HTMLorMML-full,Safe"", config=True,
        help=""""""The MathJax.js configuration file that is to be used.""""""
    )

    @observe('mathjax_config')
    def _update_mathjax_config(self, change):
        self.log.info(""Using MathJax configuration file: %s"", change['new'])

    contents_manager_class = Type(
        default_value=FileContentsManager,
        klass=ContentsManager,
        config=True,
        help='The notebook manager class to use.'
    )

    kernel_manager_class = Type(
        default_value=MappingKernelManager,
        config=True,
        help='The kernel manager class to use.'
    )

    session_manager_class = Type(
        default_value=SessionManager,
        config=True,
        help='The session manager class to use.'
    )

    config_manager_class = Type(
        default_value=ConfigManager,
        config = True,
        help='The config manager class to use'
    )

    kernel_spec_manager = Instance(KernelSpecManager, allow_none=True)

    kernel_spec_manager_class = Type(
        default_value=KernelSpecManager,
        config=True,
        help=""""""
        The kernel spec manager class to use. Should be a subclass
        of `jupyter_client.kernelspec.KernelSpecManager`.

        The Api of KernelSpecManager is provisional and might change
        without warning between this version of Jupyter and the next stable one.
        """"""
    )

    login_handler_class = Type(
        default_value=LoginHandler,
        klass=web.RequestHandler,
        config=True,
        help='The login handler class to use.',
    )

    logout_handler_class = Type(
        default_value=LogoutHandler,
        klass=web.RequestHandler,
        config=True,
        help='The logout handler class to use.',
    )

    trust_xheaders = Bool(False, config=True,
        help=(""Whether to trust or not X-Scheme/X-Forwarded-Proto and X-Real-Ip/X-Forwarded-For headers""
              ""sent by the upstream reverse proxy. Necessary if the proxy handles SSL"")
    )

    info_file = Unicode()

    @default('info_file')
    def _default_info_file(self):
        info_file = ""nbserver-%s.json"" % os.getpid()
        return os.path.join(self.runtime_dir, info_file)
    
    pylab = Unicode('disabled', config=True,
        help=""""""
        DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.
        """"""
    )

    @observe('pylab')
    def _update_pylab(self, change):
        """"""when --pylab is specified, display a warning and exit""""""
        if change['new'] != 'warn':
            backend = ' %s' % change['new']
        else:
            backend = ''
        self.log.error(""Support for specifying --pylab on the command line has been removed."")
        self.log.error(
            ""Please use `%pylab{0}` or `%matplotlib{0}` in the notebook itself."".format(backend)
        )
        self.exit(1)

    notebook_dir = Unicode(config=True,
        help=""The directory to use for notebooks and kernels.""
    )

    @default('notebook_dir')
    def _default_notebook_dir(self):
        if self.file_to_run:
            return os.path.dirname(os.path.abspath(self.file_to_run))
        else:
            return py3compat.getcwd()

    @validate('notebook_dir')
    def _notebook_dir_validate(self, proposal):
        value = proposal['value']
        # Strip any trailing slashes
        # *except* if it's root
        _, path = os.path.splitdrive(value)
        if path == os.sep:
            return value
        value = value.rstrip(os.sep)
        if not os.path.isabs(value):
            # If we receive a non-absolute path, make it absolute.
            value = os.path.abspath(value)
        if not os.path.isdir(value):
            raise TraitError(""No such notebook dir: %r"" % value)
        return value

    @observe('notebook_dir')
    def _update_notebook_dir(self, change):
        """"""Do a bit of validation of the notebook dir.""""""
        # setting App.notebook_dir implies setting notebook and kernel dirs as well
        new = change['new']
        self.config.FileContentsManager.root_dir = new
        self.config.MappingKernelManager.root_dir = new

    # TODO: Remove me in notebook 5.0
    server_extensions = List(Unicode(), config=True,
        help=(""DEPRECATED use the nbserver_extensions dict instead"")
    )
    
    @observe('server_extensions')
    def _update_server_extensions(self, change):
        self.log.warning(""server_extensions is deprecated, use nbserver_extensions"")
        self.server_extensions = change['new']
        
    nbserver_extensions = Dict({}, config=True,
        help=(""Dict of Python modules to load as notebook server extensions.""
              ""Entry values can be used to enable and disable the loading of""
              ""the extensions. The extensions will be loaded in alphabetical ""
              ""order."")
    )

    reraise_server_extension_failures = Bool(
        False,
        config=True,
        help=""Reraise exceptions encountered loading server extensions?"",
    )

    iopub_msg_rate_limit = Float(1000, config=True, help=""""""(msgs/sec)
        Maximum rate at which messages can be sent on iopub before they are
        limited."""""")

    iopub_data_rate_limit = Float(1000000, config=True, help=""""""(bytes/sec)
        Maximum rate at which messages can be sent on iopub before they are
        limited."""""")

    rate_limit_window = Float(3, config=True, help=""""""(sec) Time window used to 
        check the message and data rate limits."""""")

    def parse_command_line(self, argv=None):
        super(NotebookApp, self).parse_command_line(argv)

        if self.extra_args:
            arg0 = self.extra_args[0]
            f = os.path.abspath(arg0)
            self.argv.remove(arg0)
            if not os.path.exists(f):
                self.log.critical(""No such file or directory: %s"", f)
                self.exit(1)
            
            # Use config here, to ensure that it takes higher priority than
            # anything that comes from the config dirs.
            c = Config()
            if os.path.isdir(f):
                c.NotebookApp.notebook_dir = f
            elif os.path.isfile(f):
                c.NotebookApp.file_to_run = f
            self.update_config(c)

    def init_configurables(self):
        self.kernel_spec_manager = self.kernel_spec_manager_class(
            parent=self,
        )
        self.kernel_manager = self.kernel_manager_class(
            parent=self,
            log=self.log,
            connection_dir=self.runtime_dir,
            kernel_spec_manager=self.kernel_spec_manager,
        )
        self.contents_manager = self.contents_manager_class(
            parent=self,
            log=self.log,
        )
        self.session_manager = self.session_manager_class(
            parent=self,
            log=self.log,
            kernel_manager=self.kernel_manager,
            contents_manager=self.contents_manager,
        )
        self.config_manager = self.config_manager_class(
            parent=self,
            log=self.log,
            config_dir=os.path.join(self.config_dir, 'nbconfig'),
        )

    def init_logging(self):
        # This prevents double log messages because tornado use a root logger that
        # self.log is a child of. The logging module dipatches log messages to a log
        # and all of its ancenstors until propagate is set to False.
        self.log.propagate = False
        
        for log in app_log, access_log, gen_log:
            # consistent log output name (NotebookApp instead of tornado.access, etc.)
            log.name = self.log.name
        # hook up tornado 3's loggers to our app handlers
        logger = logging.getLogger('tornado')
        logger.propagate = True
        logger.parent = self.log
        logger.setLevel(self.log.level)
    
    def init_webapp(self):
        """"""initialize tornado webapp and httpserver""""""
        self.tornado_settings['allow_origin'] = self.allow_origin
        if self.allow_origin_pat:
            self.tornado_settings['allow_origin_pat'] = re.compile(self.allow_origin_pat)
        self.tornado_settings['allow_credentials'] = self.allow_credentials
        self.tornado_settings['cookie_options'] = self.cookie_options
        self.tornado_settings['token'] = self.token
        if (self.open_browser or self.file_to_run) and not self.password:
            self.one_time_token = binascii.hexlify(os.urandom(24)).decode('ascii')
            self.tornado_settings['one_time_token'] = self.one_time_token

        # ensure default_url starts with base_url
        if not self.default_url.startswith(self.base_url):
            self.default_url = url_path_join(self.base_url, self.default_url)

        if self.password_required and (not self.password):
            self.log.critical(""Notebook servers are configured to only be run with a password."")
            self.log.critical(""Hint: run the following command to set a password"")
            self.log.critical(""\t$ python -m notebook.auth password"")
            sys.exit(1)

        self.web_app = NotebookWebApplication(
            self, self.kernel_manager, self.contents_manager,
            self.session_manager, self.kernel_spec_manager,
            self.config_manager,
            self.log, self.base_url, self.default_url, self.tornado_settings,
            self.jinja_environment_options
        )
        ssl_options = self.ssl_options
        if self.certfile:
            ssl_options['certfile'] = self.certfile
        if self.keyfile:
            ssl_options['keyfile'] = self.keyfile
        if self.client_ca:
            ssl_options['ca_certs'] = self.client_ca
        if not ssl_options:
            # None indicates no SSL config
            ssl_options = None
        else:
            # SSL may be missing, so only import it if it's to be used
            import ssl
            # Disable SSLv3 by default, since its use is discouraged.
            ssl_options.setdefault('ssl_version', ssl.PROTOCOL_TLSv1)
            if ssl_options.get('ca_certs', False):
                ssl_options.setdefault('cert_reqs', ssl.CERT_REQUIRED)
        
        self.login_handler_class.validate_security(self, ssl_options=ssl_options)
        self.http_server = httpserver.HTTPServer(self.web_app, ssl_options=ssl_options,
                                                 xheaders=self.trust_xheaders)

        success = None
        for port in random_ports(self.port, self.port_retries+1):
            try:
                self.http_server.listen(port, self.ip)
            except socket.error as e:
                if e.errno == errno.EADDRINUSE:
                    self.log.info('The port %i is already in use, trying another port.' % port)
                    continue
                elif e.errno in (errno.EACCES, getattr(errno, 'WSAEACCES', errno.EACCES)):
                    self.log.warning(""Permission to listen on port %i denied"" % port)
                    continue
                else:
                    raise
            else:
                self.port = port
                success = True
                break
        if not success:
            self.log.critical('ERROR: the notebook server could not be started because '
                              'no available port could be found.')
            self.exit(1)
    
    @property
    def display_url(self):
        ip = self.ip if self.ip else '[all ip addresses on your system]'
        url = self._url(ip)
        if self.token:
            # Don't log full token if it came from config
            token = self.token if self._token_generated else '...'
            url = url_concat(url, {'token': token})
        return url

    @property
    def connection_url(self):
        ip = self.ip if self.ip else 'localhost'
        return self._url(ip)

    def _url(self, ip):
        proto = 'https' if self.certfile else 'http'
        return ""%s://%s:%i%s"" % (proto, ip, self.port, self.base_url)

    def init_terminals(self):
        try:
            from .terminal import initialize
            initialize(self.web_app, self.notebook_dir, self.connection_url, self.terminado_settings)
            self.web_app.settings['terminals_available'] = True
        except ImportError as e:
            log = self.log.debug if sys.platform == 'win32' else self.log.warning
            log(""Terminals not available (error was %s)"", e)

    def init_signal(self):
        if not sys.platform.startswith('win') and sys.stdin.isatty():
            signal.signal(signal.SIGINT, self._handle_sigint)
        signal.signal(signal.SIGTERM, self._signal_stop)
        if hasattr(signal, 'SIGUSR1'):
            # Windows doesn't support SIGUSR1
            signal.signal(signal.SIGUSR1, self._signal_info)
        if hasattr(signal, 'SIGINFO'):
            # only on BSD-based systems
            signal.signal(signal.SIGINFO, self._signal_info)
    
    def _handle_sigint(self, sig, frame):
        """"""SIGINT handler spawns confirmation dialog""""""
        # register more forceful signal handler for ^C^C case
        signal.signal(signal.SIGINT, self._signal_stop)
        # request confirmation dialog in bg thread, to avoid
        # blocking the App
        thread = threading.Thread(target=self._confirm_exit)
        thread.daemon = True
        thread.start()
    
    def _restore_sigint_handler(self):
        """"""callback for restoring original SIGINT handler""""""
        signal.signal(signal.SIGINT, self._handle_sigint)
    
    def _confirm_exit(self):
        """"""confirm shutdown on ^C
        
        A second ^C, or answering 'y' within 5s will cause shutdown,
        otherwise original SIGINT handler will be restored.
        
        This doesn't work on Windows.
        """"""
        info = self.log.info
        info('interrupted')
        print(self.notebook_info())
        sys.stdout.write(""Shutdown this notebook server (y/[n])? "")
        sys.stdout.flush()
        r,w,x = select.select([sys.stdin], [], [], 5)
        if r:
            line = sys.stdin.readline()
            if line.lower().startswith('y') and 'n' not in line.lower():
                self.log.critical(""Shutdown confirmed"")
                ioloop.IOLoop.current().stop()
                return
        else:
            print(""No answer for 5s:"", end=' ')
        print(""resuming operation..."")
        # no answer, or answer is no:
        # set it back to original SIGINT handler
        # use IOLoop.add_callback because signal.signal must be called
        # from main thread
        ioloop.IOLoop.current().add_callback(self._restore_sigint_handler)
    
    def _signal_stop(self, sig, frame):
        self.log.critical(""received signal %s, stopping"", sig)
        ioloop.IOLoop.current().stop()

    def _signal_info(self, sig, frame):
        print(self.notebook_info())
    
    def init_components(self):
        """"""Check the components submodule, and warn if it's unclean""""""
        # TODO: this should still check, but now we use bower, not git submodule
        pass

    def init_server_extensions(self):
        """"""Load any extensions specified by config.

        Import the module, then call the load_jupyter_server_extension function,
        if one exists.
        
        The extension API is experimental, and may change in future releases.
        """"""
        
        # TODO: Remove me in notebook 5.0
        for modulename in self.server_extensions:
            # Don't override disable state of the extension if it already exist
            # in the new traitlet
            if not modulename in self.nbserver_extensions:
                self.nbserver_extensions[modulename] = True
        
        for modulename in sorted(self.nbserver_extensions):
            if self.nbserver_extensions[modulename]:
                try:
                    mod = importlib.import_module(modulename)
                    func = getattr(mod, 'load_jupyter_server_extension', None)
                    if func is not None:
                        func(self)
                except Exception:
                    if self.reraise_server_extension_failures:
                        raise
                    self.log.warning(""Error loading server extension %s"", modulename,
                                  exc_info=True)

    def init_mime_overrides(self):
        # On some Windows machines, an application has registered an incorrect
        # mimetype for CSS in the registry. Tornado uses this when serving
        # .css files, causing browsers to reject the stylesheet. We know the
        # mimetype always needs to be text/css, so we override it here.
        mimetypes.add_type('text/css', '.css')

    @catch_config_error
    def initialize(self, argv=None):
        super(NotebookApp, self).initialize(argv)
        self.init_logging()
        if self._dispatching:
            return
        self.init_configurables()
        self.init_components()
        self.init_webapp()
        self.init_terminals()
        self.init_signal()
        self.init_server_extensions()
        self.init_mime_overrides()

    def cleanup_kernels(self):
        """"""Shutdown all kernels.
        
        The kernels will shutdown themselves when this process no longer exists,
        but explicit shutdown allows the KernelManagers to cleanup the connection files.
        """"""
        self.log.info('Shutting down kernels')
        self.kernel_manager.shutdown_all()

    def notebook_info(self):
        ""Return the current working directory and the server url information""
        info = self.contents_manager.info_string() + ""\n""
        info += ""%d active kernels \n"" % len(self.kernel_manager._kernels)
        return info + ""The Jupyter Notebook is running at: %s"" % self.display_url

    def server_info(self):
        """"""Return a JSONable dict of information about this server.""""""
        return {'url': self.connection_url,
                'hostname': self.ip if self.ip else 'localhost',
                'port': self.port,
                'secure': bool(self.certfile),
                'base_url': self.base_url,
                'token': self.token,
                'notebook_dir': os.path.abspath(self.notebook_dir),
                'password': bool(self.password),
                'pid': os.getpid(),
               }

    def write_server_info_file(self):
        """"""Write the result of server_info() to the JSON file info_file.""""""
        with open(self.info_file, 'w') as f:
            json.dump(self.server_info(), f, indent=2, sort_keys=True)

    def remove_server_info_file(self):
        """"""Remove the nbserver-<pid>.json file created for this server.
        
        Ignores the error raised when the file has already been removed.
        """"""
        try:
            os.unlink(self.info_file)
        except OSError as e:
            if e.errno != errno.ENOENT:
                raise

    def start(self):
        """""" Start the Notebook server app, after initialization
        
        This method takes no arguments so all configuration and initialization
        must be done prior to calling this method.""""""

        if not self.allow_root:
            # check if we are running as root, and abort if it's not allowed
            try:
                uid = os.geteuid()
            except AttributeError:
                uid = -1 # anything nonzero here, since we can't check UID assume non-root
            if uid == 0:
                self.log.critical(""Running as root is not recommended. Use --allow-root to bypass."")
                self.exit(1)

        super(NotebookApp, self).start()

        info = self.log.info
        for line in self.notebook_info().split(""\n""):
            info(line)
        info(""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."")

        self.write_server_info_file()

        if self.open_browser or self.file_to_run:
            try:
                browser = webbrowser.get(self.browser or None)
            except webbrowser.Error as e:
                self.log.warning('No web browser found: %s.' % e)
                browser = None
            
            if self.file_to_run:
                if not os.path.exists(self.file_to_run):
                    self.log.critical(""%s does not exist"" % self.file_to_run)
                    self.exit(1)

                relpath = os.path.relpath(self.file_to_run, self.notebook_dir)
                uri = url_escape(url_path_join('notebooks', *relpath.split(os.sep)))
            else:
                # default_url contains base_url, but so does connection_url
                uri = self.default_url[len(self.base_url):]
            if self.one_time_token:
                uri = url_concat(uri, {'token': self.one_time_token})
            if browser:
                b = lambda : browser.open(url_path_join(self.connection_url, uri),
                                          new=2)
                threading.Thread(target=b).start()

        if self.token and self._token_generated:
            # log full URL with generated token, so there's a copy/pasteable link
            # with auth info.
            self.log.critical('\n'.join([
                '\n',
                'Copy/paste this URL into your browser when you connect for the first time,',
                'to login with a token:',
                '    %s' % url_concat(self.connection_url, {'token': self.token}),
            ]))

        self.io_loop = ioloop.IOLoop.current()
        if sys.platform.startswith('win'):
            # add no-op to wake every 5s
            # to handle signals that may be ignored by the inner loop
            pc = ioloop.PeriodicCallback(lambda : None, 5000)
            pc.start()
        try:
            self.io_loop.start()
        except KeyboardInterrupt:
            info(""Interrupted..."")
        finally:
            self.remove_server_info_file()
            self.cleanup_kernels()

    def stop(self):
        def _stop():
            self.http_server.stop()
            self.io_loop.stop()
        self.io_loop.add_callback(_stop)


def list_running_servers(runtime_dir=None):
    """"""Iterate over the server info files of running notebook servers.
    
    Given a runtime directory, find nbserver-* files in the security directory,
    and yield dicts of their information, each one pertaining to
    a currently running notebook server instance.
    """"""
    if runtime_dir is None:
        runtime_dir = jupyter_runtime_dir()

    # The runtime dir might not exist
    if not os.path.isdir(runtime_dir):
        return

    for file in os.listdir(runtime_dir):
        if file.startswith('nbserver-'):
            with io.open(os.path.join(runtime_dir, file), encoding='utf-8') as f:
                info = json.load(f)

            # Simple check whether that process is really still running
            # Also remove leftover files from IPython 2.x without a pid field
            if ('pid' in info) and check_pid(info['pid']):
                yield info
            else:
                # If the process has died, try to delete its info file
                try:
                    os.unlink(os.path.join(runtime_dir, file))
                except OSError:
                    pass  # TODO: This should warn or log or something
#-----------------------------------------------------------------------------
# Main entry point
#-----------------------------------------------------------------------------

main = launch_new_instance = NotebookApp.launch_instance
/n/n/n",0
73,73,d6f091c4439c174c7700776c0cee03053403f600,"/notebook/base/handlers.py/n/n""""""Base Tornado handlers for the notebook server.""""""

# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import functools
import json
import os
import re
import sys
import traceback
try:
    # py3
    from http.client import responses
except ImportError:
    from httplib import responses
try:
    from urllib.parse import urlparse # Py 3
except ImportError:
    from urlparse import urlparse # Py 2

from jinja2 import TemplateNotFound
from tornado import web, gen, escape
from tornado.log import app_log

from notebook._sysinfo import get_sys_info

from traitlets.config import Application
from ipython_genutils.path import filefind
from ipython_genutils.py3compat import string_types

import notebook
from notebook.utils import is_hidden, url_path_join, url_is_absolute, url_escape
from notebook.services.security import csp_report_uri

#-----------------------------------------------------------------------------
# Top-level handlers
#-----------------------------------------------------------------------------
non_alphanum = re.compile(r'[^A-Za-z0-9]')

sys_info = json.dumps(get_sys_info())

def log():
    if Application.initialized():
        return Application.instance().log
    else:
        return app_log

class AuthenticatedHandler(web.RequestHandler):
    """"""A RequestHandler with an authenticated user.""""""

    @property
    def content_security_policy(self):
        """"""The default Content-Security-Policy header
        
        Can be overridden by defining Content-Security-Policy in settings['headers']
        """"""
        return '; '.join([
            ""frame-ancestors 'self'"",
            # Make sure the report-uri is relative to the base_url
            ""report-uri "" + url_path_join(self.base_url, csp_report_uri),
        ])

    def set_default_headers(self):
        headers = self.settings.get('headers', {})

        if ""Content-Security-Policy"" not in headers:
            headers[""Content-Security-Policy""] = self.content_security_policy

        # Allow for overriding headers
        for header_name,value in headers.items() :
            try:
                self.set_header(header_name, value)
            except Exception as e:
                # tornado raise Exception (not a subclass)
                # if method is unsupported (websocket and Access-Control-Allow-Origin
                # for example, so just ignore)
                self.log.debug(e)
    
    def clear_login_cookie(self):
        self.clear_cookie(self.cookie_name)
    
    def get_current_user(self):
        if self.login_handler is None:
            return 'anonymous'
        return self.login_handler.get_user(self)

    def skip_check_origin(self):
        """"""Ask my login_handler if I should skip the origin_check
        
        For example: in the default LoginHandler, if a request is token-authenticated,
        origin checking should be skipped.
        """"""
        if self.login_handler is None or not hasattr(self.login_handler, 'should_check_origin'):
            return False
        return not self.login_handler.should_check_origin(self)

    @property
    def token_authenticated(self):
        """"""Have I been authenticated with a token?""""""
        if self.login_handler is None or not hasattr(self.login_handler, 'is_token_authenticated'):
            return False
        return self.login_handler.is_token_authenticated(self)

    @property
    def cookie_name(self):
        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(
            self.request.host
        ))
        return self.settings.get('cookie_name', default_cookie_name)
    
    @property
    def logged_in(self):
        """"""Is a user currently logged in?""""""
        user = self.get_current_user()
        return (user and not user == 'anonymous')

    @property
    def login_handler(self):
        """"""Return the login handler for this application, if any.""""""
        return self.settings.get('login_handler_class', None)

    @property
    def token(self):
        """"""Return the login token for this application, if any.""""""
        return self.settings.get('token', None)

    @property
    def one_time_token(self):
        """"""Return the one-time-use token for this application, if any.""""""
        return self.settings.get('one_time_token', None)

    @property
    def login_available(self):
        """"""May a user proceed to log in?

        This returns True if login capability is available, irrespective of
        whether the user is already logged in or not.

        """"""
        if self.login_handler is None:
            return False
        return bool(self.login_handler.get_login_available(self.settings))


class IPythonHandler(AuthenticatedHandler):
    """"""IPython-specific extensions to authenticated handling
    
    Mostly property shortcuts to IPython-specific settings.
    """"""

    @property
    def ignore_minified_js(self):
        """"""Wether to user bundle in template. (*.min files)
        
        Mainly use for development and avoid file recompilation
        """"""
        return self.settings.get('ignore_minified_js', False)

    @property
    def config(self):
        return self.settings.get('config', None)
    
    @property
    def log(self):
        """"""use the IPython log by default, falling back on tornado's logger""""""
        return log()

    @property
    def jinja_template_vars(self):
        """"""User-supplied values to supply to jinja templates.""""""
        return self.settings.get('jinja_template_vars', {})
    
    #---------------------------------------------------------------
    # URLs
    #---------------------------------------------------------------
    
    @property
    def version_hash(self):
        """"""The version hash to use for cache hints for static files""""""
        return self.settings.get('version_hash', '')
    
    @property
    def mathjax_url(self):
        url = self.settings.get('mathjax_url', '')
        if not url or url_is_absolute(url):
            return url
        return url_path_join(self.base_url, url)
    
    @property
    def mathjax_config(self):
        return self.settings.get('mathjax_config', 'TeX-AMS-MML_HTMLorMML-full,Safe')

    @property
    def base_url(self):
        return self.settings.get('base_url', '/')

    @property
    def default_url(self):
        return self.settings.get('default_url', '')

    @property
    def ws_url(self):
        return self.settings.get('websocket_url', '')

    @property
    def contents_js_source(self):
        self.log.debug(""Using contents: %s"", self.settings.get('contents_js_source',
            'services/built/contents'))
        return self.settings.get('contents_js_source', 'services/built/contents')
    
    #---------------------------------------------------------------
    # Manager objects
    #---------------------------------------------------------------
    
    @property
    def kernel_manager(self):
        return self.settings['kernel_manager']

    @property
    def contents_manager(self):
        return self.settings['contents_manager']
    
    @property
    def session_manager(self):
        return self.settings['session_manager']
    
    @property
    def terminal_manager(self):
        return self.settings['terminal_manager']
    
    @property
    def kernel_spec_manager(self):
        return self.settings['kernel_spec_manager']

    @property
    def config_manager(self):
        return self.settings['config_manager']

    #---------------------------------------------------------------
    # CORS
    #---------------------------------------------------------------
    
    @property
    def allow_origin(self):
        """"""Normal Access-Control-Allow-Origin""""""
        return self.settings.get('allow_origin', '')
    
    @property
    def allow_origin_pat(self):
        """"""Regular expression version of allow_origin""""""
        return self.settings.get('allow_origin_pat', None)
    
    @property
    def allow_credentials(self):
        """"""Whether to set Access-Control-Allow-Credentials""""""
        return self.settings.get('allow_credentials', False)
    
    def set_default_headers(self):
        """"""Add CORS headers, if defined""""""
        super(IPythonHandler, self).set_default_headers()
        if self.allow_origin:
            self.set_header(""Access-Control-Allow-Origin"", self.allow_origin)
        elif self.allow_origin_pat:
            origin = self.get_origin()
            if origin and self.allow_origin_pat.match(origin):
                self.set_header(""Access-Control-Allow-Origin"", origin)
        if self.allow_credentials:
            self.set_header(""Access-Control-Allow-Credentials"", 'true')
    
    def get_origin(self):
        # Handle WebSocket Origin naming convention differences
        # The difference between version 8 and 13 is that in 8 the
        # client sends a ""Sec-Websocket-Origin"" header and in 13 it's
        # simply ""Origin"".
        if ""Origin"" in self.request.headers:
            origin = self.request.headers.get(""Origin"")
        else:
            origin = self.request.headers.get(""Sec-Websocket-Origin"", None)
        return origin

    # origin_to_satisfy_tornado is present because tornado requires
    # check_origin to take an origin argument, but we don't use it
    def check_origin(self, origin_to_satisfy_tornado=""""):
        """"""Check Origin for cross-site API requests, including websockets

        Copied from WebSocket with changes:

        - allow unspecified host/origin (e.g. scripts)
        - allow token-authenticated requests
        """"""
        if self.allow_origin == '*' or self.skip_check_origin():
            return True

        host = self.request.headers.get(""Host"")
        origin = self.request.headers.get(""Origin"")

        # If no header is provided, allow it.
        # Origin can be None for:
        # - same-origin (IE, Firefox)
        # - Cross-site POST form (IE, Firefox)
        # - Scripts
        # The cross-site POST (XSRF) case is handled by tornado's xsrf_token
        if origin is None or host is None:
            return True

        origin = origin.lower()
        origin_host = urlparse(origin).netloc

        # OK if origin matches host
        if origin_host == host:
            return True

        # Check CORS headers
        if self.allow_origin:
            allow = self.allow_origin == origin
        elif self.allow_origin_pat:
            allow = bool(self.allow_origin_pat.match(origin))
        else:
            # No CORS headers deny the request
            allow = False
        if not allow:
            self.log.warning(""Blocking Cross Origin API request for %s.  Origin: %s, Host: %s"",
                self.request.path, origin, host,
            )
        return allow

    def check_xsrf_cookie(self):
        """"""Bypass xsrf checks when token-authenticated""""""
        if self.token_authenticated:
            # Token-authenticated requests do not need additional XSRF-check
            # Servers without authentication are vulnerable to XSRF
            return
        return super(IPythonHandler, self).check_xsrf_cookie()

    #---------------------------------------------------------------
    # template rendering
    #---------------------------------------------------------------
    
    def get_template(self, name):
        """"""Return the jinja template object for a given name""""""
        return self.settings['jinja2_env'].get_template(name)
    
    def render_template(self, name, **ns):
        ns.update(self.template_namespace)
        template = self.get_template(name)
        return template.render(**ns)
    
    @property
    def template_namespace(self):
        return dict(
            base_url=self.base_url,
            default_url=self.default_url,
            ws_url=self.ws_url,
            logged_in=self.logged_in,
            login_available=self.login_available,
            token_available=bool(self.token or self.one_time_token),
            static_url=self.static_url,
            sys_info=sys_info,
            contents_js_source=self.contents_js_source,
            version_hash=self.version_hash,
            ignore_minified_js=self.ignore_minified_js,
            xsrf_form_html=self.xsrf_form_html,
            token=self.token,
            xsrf_token=self.xsrf_token.decode('utf8'),
            **self.jinja_template_vars
        )
    
    def get_json_body(self):
        """"""Return the body of the request as JSON data.""""""
        if not self.request.body:
            return None
        # Do we need to call body.decode('utf-8') here?
        body = self.request.body.strip().decode(u'utf-8')
        try:
            model = json.loads(body)
        except Exception:
            self.log.debug(""Bad JSON: %r"", body)
            self.log.error(""Couldn't parse JSON"", exc_info=True)
            raise web.HTTPError(400, u'Invalid JSON in body of request')
        return model

    def write_error(self, status_code, **kwargs):
        """"""render custom error pages""""""
        exc_info = kwargs.get('exc_info')
        message = ''
        status_message = responses.get(status_code, 'Unknown HTTP Error')
        exception = '(unknown)'
        if exc_info:
            exception = exc_info[1]
            # get the custom message, if defined
            try:
                message = exception.log_message % exception.args
            except Exception:
                pass
            
            # construct the custom reason, if defined
            reason = getattr(exception, 'reason', '')
            if reason:
                status_message = reason
        
        # build template namespace
        ns = dict(
            status_code=status_code,
            status_message=status_message,
            message=message,
            exception=exception,
        )
        
        self.set_header('Content-Type', 'text/html')
        # render the template
        try:
            html = self.render_template('%s.html' % status_code, **ns)
        except TemplateNotFound:
            self.log.debug(""No template for %d"", status_code)
            html = self.render_template('error.html', **ns)
        
        self.write(html)


class APIHandler(IPythonHandler):
    """"""Base class for API handlers""""""

    def prepare(self):
        if not self.check_origin():
            raise web.HTTPError(404)
        return super(APIHandler, self).prepare()

    @property
    def content_security_policy(self):
        csp = '; '.join([
                super(APIHandler, self).content_security_policy,
                ""default-src 'none'"",
            ])
        return csp
    
    def finish(self, *args, **kwargs):
        self.set_header('Content-Type', 'application/json')
        return super(APIHandler, self).finish(*args, **kwargs)

    def options(self, *args, **kwargs):
        self.set_header('Access-Control-Allow-Headers', 'accept, content-type, authorization')
        self.set_header('Access-Control-Allow-Methods',
                        'GET, PUT, POST, PATCH, DELETE, OPTIONS')
        self.finish()


class Template404(IPythonHandler):
    """"""Render our 404 template""""""
    def prepare(self):
        raise web.HTTPError(404)


class AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):
    """"""static files should only be accessible when logged in""""""

    @web.authenticated
    def get(self, path):
        if os.path.splitext(path)[1] == '.ipynb':
            name = path.rsplit('/', 1)[-1]
            self.set_header('Content-Type', 'application/json')
            self.set_header('Content-Disposition','attachment; filename=""%s""' % escape.url_escape(name))
        
        return web.StaticFileHandler.get(self, path)
    
    def set_headers(self):
        super(AuthenticatedFileHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments:
            self.add_header(""Cache-Control"", ""no-cache"")
    
    def compute_etag(self):
        return None
    
    def validate_absolute_path(self, root, absolute_path):
        """"""Validate and return the absolute path.
        
        Requires tornado 3.1
        
        Adding to tornado's own handling, forbids the serving of hidden files.
        """"""
        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)
        abs_root = os.path.abspath(root)
        if is_hidden(abs_path, abs_root):
            self.log.info(""Refusing to serve hidden file, via 404 Error"")
            raise web.HTTPError(404)
        return abs_path


def json_errors(method):
    """"""Decorate methods with this to return GitHub style JSON errors.
    
    This should be used on any JSON API on any handler method that can raise HTTPErrors.
    
    This will grab the latest HTTPError exception using sys.exc_info
    and then:
    
    1. Set the HTTP status code based on the HTTPError
    2. Create and return a JSON body with a message field describing
       the error in a human readable form.
    """"""
    @functools.wraps(method)
    @gen.coroutine
    def wrapper(self, *args, **kwargs):
        try:
            result = yield gen.maybe_future(method(self, *args, **kwargs))
        except web.HTTPError as e:
            self.set_header('Content-Type', 'application/json')
            status = e.status_code
            message = e.log_message
            self.log.warning(message)
            self.set_status(e.status_code)
            reply = dict(message=message, reason=e.reason)
            self.finish(json.dumps(reply))
        except Exception:
            self.set_header('Content-Type', 'application/json')
            self.log.error(""Unhandled error in API request"", exc_info=True)
            status = 500
            message = ""Unknown server error""
            t, value, tb = sys.exc_info()
            self.set_status(status)
            tb_text = ''.join(traceback.format_exception(t, value, tb))
            reply = dict(message=message, reason=None, traceback=tb_text)
            self.finish(json.dumps(reply))
        else:
            # FIXME: can use regular return in generators in py3
            raise gen.Return(result)
    return wrapper



#-----------------------------------------------------------------------------
# File handler
#-----------------------------------------------------------------------------

# to minimize subclass changes:
HTTPError = web.HTTPError

class FileFindHandler(IPythonHandler, web.StaticFileHandler):
    """"""subclass of StaticFileHandler for serving files from a search path""""""
    
    # cache search results, don't search for files more than once
    _static_paths = {}
    
    def set_headers(self):
        super(FileFindHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments or \
                any(self.request.path.startswith(path) for path in self.no_cache_paths):
            self.set_header(""Cache-Control"", ""no-cache"")
    
    def initialize(self, path, default_filename=None, no_cache_paths=None):
        self.no_cache_paths = no_cache_paths or []
        
        if isinstance(path, string_types):
            path = [path]
        
        self.root = tuple(
            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path
        )
        self.default_filename = default_filename
    
    def compute_etag(self):
        return None
    
    @classmethod
    def get_absolute_path(cls, roots, path):
        """"""locate a file to serve on our static file search path""""""
        with cls._lock:
            if path in cls._static_paths:
                return cls._static_paths[path]
            try:
                abspath = os.path.abspath(filefind(path, roots))
            except IOError:
                # IOError means not found
                return ''
            
            cls._static_paths[path] = abspath
            

            log().debug(""Path %s served from %s""%(path, abspath))
            return abspath
    
    def validate_absolute_path(self, root, absolute_path):
        """"""check if the file should be served (raises 404, 403, etc.)""""""
        if absolute_path == '':
            raise web.HTTPError(404)
        
        for root in self.root:
            if (absolute_path + os.sep).startswith(root):
                break
        
        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)


class APIVersionHandler(APIHandler):

    @json_errors
    def get(self):
        # not authenticated, so give as few info as possible
        self.finish(json.dumps({""version"":notebook.__version__}))


class TrailingSlashHandler(web.RequestHandler):
    """"""Simple redirect handler that strips trailing slashes
    
    This should be the first, highest priority handler.
    """"""
    
    def get(self):
        self.redirect(self.request.uri.rstrip('/'))
    
    post = put = get


class FilesRedirectHandler(IPythonHandler):
    """"""Handler for redirecting relative URLs to the /files/ handler""""""
    
    @staticmethod
    def redirect_to_files(self, path):
        """"""make redirect logic a reusable static method
        
        so it can be called from other handlers.
        """"""
        cm = self.contents_manager
        if cm.dir_exists(path):
            # it's a *directory*, redirect to /tree
            url = url_path_join(self.base_url, 'tree', url_escape(path))
        else:
            orig_path = path
            # otherwise, redirect to /files
            parts = path.split('/')

            if not cm.file_exists(path=path) and 'files' in parts:
                # redirect without files/ iff it would 404
                # this preserves pre-2.0-style 'files/' links
                self.log.warning(""Deprecated files/ URL: %s"", orig_path)
                parts.remove('files')
                path = '/'.join(parts)

            if not cm.file_exists(path=path):
                raise web.HTTPError(404)

            url = url_path_join(self.base_url, 'files', url_escape(path))
        self.log.debug(""Redirecting %s to %s"", self.request.path, url)
        self.redirect(url)
    
    def get(self, path=''):
        return self.redirect_to_files(self, path)


class RedirectWithParams(web.RequestHandler):
    """"""Sam as web.RedirectHandler, but preserves URL parameters""""""
    def initialize(self, url, permanent=True):
        self._url = url
        self._permanent = permanent

    def get(self):
        sep = '&' if '?' in self._url else '?'
        url = sep.join([self._url, self.request.query])
        self.redirect(url, permanent=self._permanent)

#-----------------------------------------------------------------------------
# URL pattern fragments for re-use
#-----------------------------------------------------------------------------

# path matches any number of `/foo[/bar...]` or just `/` or ''
path_regex = r""(?P<path>(?:(?:/[^/]+)+|/?))""

#-----------------------------------------------------------------------------
# URL to handler mappings
#-----------------------------------------------------------------------------


default_handlers = [
    (r"".*/"", TrailingSlashHandler),
    (r""api"", APIVersionHandler)
]
/n/n/n",1
2,2,398ed11584313a371763240392c4dda1cf986deb,"core/logger.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

import os
from core.colors import *
from files.config import *
from core.verbout import verbout
from files.discovered import INTERNAL_URLS, FILES_EXEC, SCAN_ERRORS
from files.discovered import VULN_LIST, FORMS_TESTED, REQUEST_TOKENS, STRENGTH_LIST

def logger(filename, content):
    '''
    This module is for logging all the stuff we found
            while crawling and scanning.
    '''
    output_file = OUTPUT_DIR + filename + '.log'
    with open(output_file, 'w+', encoding='utf8') as f:
        if type(content) is tuple or type(content) is list:
            for m in content:  # if it is list or tuple, it is iterable
                f.write(m+'\n')
        else:
            f.write(content)  # else we write out as it is... ;)
        f.write('\n')

def pheaders(tup):
    '''
    This module prints out the headers as received in the
                    requests normally.
    '''
    verbout(GR, 'Receiving headers...\n')
    verbout(color.GREY,'  '+color.UNDERLINE+'HEADERS'+color.END+color.GREY+':'+'\n')
    for key, val in tup.items():
        verbout('  ',color.CYAN+key+': '+color.ORANGE+val)
    verbout('','')

def GetLogger():
    if INTERNAL_URLS:
        logger('internal-links', INTERNAL_URLS)
    if SCAN_ERRORS:
        logger('errored', SCAN_ERRORS)
    if FILES_EXEC:
        logger('files-found', FILES_EXEC)
    if REQUEST_TOKENS:
        logger('anti-csrf-tokens', REQUEST_TOKENS)
    if FORMS_TESTED:
        logger('forms-tested', FORMS_TESTED)
    if VULN_LIST:
        logger('vulnerabilities', VULN_LIST)
    if STRENGTH_LIST:
        logger('strengths', STRENGTH_LIST)

def ErrorLogger(url, error):
    con = '(i) '+url+' -> '+error.__str__()
    SCAN_ERRORS.append(con)

def VulnLogger(url, vuln):
    tent = '[!] '+url+' -> '+vuln
    VULN_LIST.append(tent)

def NovulLogger(url, strength):
    tent = '[+] '+url+' -> '+strength
    STRENGTH_LIST.append(tent)
/n/n/ncore/main.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-:-:-:#
#    XSRFProbe     #
#-:-:-:-:-:-:-:-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

# Standard Package imports
import os
import re
import time
import warnings
import difflib
import http.cookiejar
from bs4 import BeautifulSoup
try:
    from urllib.parse import urlencode
    from urllib.error import HTTPError, URLError
    from urllib.request import build_opener, HTTPCookieProcessor
except ImportError:  # Throws exception in Case of Python2
    print(""\033[1;91m [-] \033[1;93mXSRFProbe\033[0m isn't compatible with Python 2.x versions.\n\033[1;91m [-] \033[0mUse Python 3.x to run \033[1;93mXSRFProbe."")
    quit()
try:
    import requests, stringdist, lxml, bs4
except ImportError:
    print(' [-] Required dependencies are not installed.\n [-] Run \033[1;93mpip3 install -r requirements.txt\033[0m to fix it.')

# Imports from core
from core.options import *
from core.colors import *
from core.inputin import inputin
from core.request import Get, Post
from core.verbout import verbout
from core.forms import form10, form20
from core.banner import banner, banabout
from core.logger import ErrorLogger, GetLogger
from core.logger import VulnLogger, NovulLogger

# Imports from files
from files.config import *
from files.discovered import FORMS_TESTED

# Imports from modules
from modules import Debugger
from modules import Parser
from modules import Crawler
from modules.Origin import Origin
from modules.Cookie import Cookie
from modules.Tamper import Tamper
from modules.Entropy import Entropy
from modules.Referer import Referer
from modules.Encoding import Encoding
from modules.Analysis import Analysis
from modules.Checkpost import PostBased
# Import Ends

# First rule, remove the warnings!
warnings.filterwarnings('ignore')

def Engine():  # lets begin it!

    os.system('clear')  # Clear shit from terminal :p
    banner()  # Print the banner
    banabout()  # The second banner
    web, fld = inputin()  # Take the input
    form1 = form10()  # Get the form 1 ready
    form2 = form20()  # Get the form 2 ready

    # For the cookies that we encounter during requests...
    Cookie0 = http.cookiejar.CookieJar()  # First as User1
    Cookie1 = http.cookiejar.CookieJar()  # Then as User2
    resp1 = build_opener(HTTPCookieProcessor(Cookie0))  # Process cookies
    resp2 = build_opener(HTTPCookieProcessor(Cookie1))  # Process cookies

    actionDone = []  # init to the done stuff

    csrf = ''  # no token initialise / invalid token
    ref_detect = 0x00  # Null Char Flag
    ori_detect = 0x00  # Null Char Flags
    form = Debugger.Form_Debugger()  # init to the form parser+token generator

    bs1 = BeautifulSoup(form1).findAll('form',action=True)[0]  # make sure the stuff works properly
    bs2 = BeautifulSoup(form2).findAll('form',action=True)[0]  # same as above

    init1 = web  # First init
    resp1.open(init1)  # Makes request as User2
    resp2.open(init1)  # Make request as User1

    # Now there are 2 different modes of scanning and crawling here.
    # 1st -> Testing a single endpoint without the --crawl flag.
    # 2nd -> Testing all endpoints with the --crawl flag.
    try:
        # Implementing the first mode. [NO CRAWL]
        if not CRAWL_SITE:
            url = web
            response = Get(url).text
            try:
                verbout(O,'Trying to parse response...')
                soup = BeautifulSoup(response)  # Parser init
            except HTMLParser.HTMLParseError:
                verbout(R,'BeautifulSoup Error: '+url)
            i = 0 # Init user number
            if REFERER_ORIGIN_CHECKS:
                # Referer Based Checks if True...
                verbout(O, 'Checking endpoint request validation via '+color.GREY+'Referer'+color.END+' Checks...')
                if Referer(url):
                    ref_detect = 0x01
                verbout(O, 'Confirming the vulnerability...')

                # We have finished with Referer Based Checks, lets go for Origin Based Ones...
                verbout(O, 'Confirming endpoint request validation via '+color.GREY+'Origin'+color.END+' Checks...')
                if Origin(url):
                    ori_detect = 0x01
            if COOKIE_BASED:
                Cookie(url)
            # Now lets get the forms...
            verbout(O, 'Retrieving all forms on ' +color.GREY+url+color.END+'...')
            for m in Debugger.getAllForms(soup):  # iterating over all forms extracted
                verbout(O,'Testing form:\n\n'+color.CYAN+' %s' % (m.prettify()))
                FORMS_TESTED.append('(i) '+url+':\n\n'+m.prettify()+'\n')
                try:
                    if m['action']:
                        pass
                except KeyError:
                    m['action'] = '/' + url.rsplit('/', 1)[1]
                    ErrorLogger(url, 'No standard form ""action"".')
                action = Parser.buildAction(url, m['action'])  # get all forms which have 'action' attribute
                if not action in actionDone and action!='':  # if url returned is not a null value nor duplicate...
                    # If form submission is kept to True
                    if FORM_SUBMISSION:
                        try:
                            # NOTE: Slow connections may cause read timeouts which may result in AttributeError
                            result, genpoc = form.prepareFormInputs(m)  # prepare inputs
                            r1 = Post(url, action, result).text  # make request with token values generated as user1
                            result, genpoc = form.prepareFormInputs(m)  # prepare the input types
                            r2 = Post(url, action, result).text  # again make request with token values generated as user2
                            # Go for token based entropy checks...
                            try:
                                if m['name']:
                                    query, token = Entropy(result, url, m['action'], m['name'])
                            except KeyError:
                                query, token = Entropy(result, url, m['action'])
                            # Now its time to detect the encoding type (if any) of the Anti-CSRF token.
                            fnd = Encoding(token)
                            if fnd == 0x01:
                                VulnLogger(url, 'Token is a string encoded value which can be probably decrypted.')
                            else:
                                NovulLogger(url, 'Anti-CSRF token is not a string encoded value.')
                            # Go for token parameter tamper checks.
                            if (query and token):
                                Tamper(url, action, result, r2, query, token)
                            o2 = resp2.open(url).read()  # make request as user2
                            try:
                                form2 = Debugger.getAllForms(BeautifulSoup(o2))[i]  # user2 gets his form
                            except IndexError:
                                verbout(R, 'Form Error')
                                ErrorLogger(url, 'Form Index Error.')
                                continue  # making sure program won't end here (dirty fix :( )
                            verbout(GR, 'Preparing form inputs...')
                            contents2, genpoc = form.prepareFormInputs(form2)  # prepare for form 2 as user2
                            r3 = Post(url,action,contents2).text  # make request as user3 with user2's form
                            if POST_BASED and not query and not token:
                                try:
                                    if m['name']:
                                        PostBased(url, r1, r2, r3, m['action'], result, genpoc, m['name'])
                                except KeyError:
                                    PostBased(url, r1, r2, r3, m['action'], result, genpoc)
                            else:
                                print(color.GREEN+' [+] The form was requested with a Anti-CSRF token.')
                                print(color.GREEN+' [+] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.GREEN+' to POST-Based CSRF Attacks!')
                                NovulLogger(url, 'Not vulnerable to POST-Based CSRF Attacks.')
                        except HTTPError as msg:  # if runtime exception...
                            verbout(R, 'Exception : '+msg.__str__())  # again exception :(
                            ErrorLogger(url, msg)

                actionDone.append(action)  # add the stuff done
                i+=1  # Increase user iteration

        else:
            # Implementing the 2nd mode [CRAWLING AND SCANNING].
            verbout(GR, ""Initializing crawling and scanning..."")
            crawler = Crawler.Handler(init1, resp1)  # Init to the Crawler handler

            while crawler.noinit():  # Until 0 urls left
                url = next(crawler)  # Go for next!

                print(C+'Testing :> '+color.CYAN+url)  # Display what url its crawling

                try:
                    soup = crawler.process(fld)  # Start the parser
                    if not soup:
                        continue  # Making sure not to end the program yet...
                    i = 0  # Set count = 0 (user number 0, which will be subsequently incremented)
                    if REFERER_ORIGIN_CHECKS:
                        # Referer Based Checks if True...
                        verbout(O, 'Checking endpoint request validation via '+color.GREY+'Referer'+color.END+' Checks...')
                        if Referer(url):
                            ref_detect = 0x01
                        verbout(O, 'Confirming the vulnerability...')

                        # We have finished with Referer Based Checks, lets go for Origin Based Ones...
                        verbout(O, 'Confirming endpoint request validation via '+color.GREY+'Origin'+color.END+' Checks...')
                        if Origin(url):
                            ori_detect = 0x01

                    if COOKIE_BASED:
                        Cookie(url)

                    # Now lets get the forms...
                    verbout(O, 'Retrieving all forms on ' +color.GREY+url+color.END+'...')
                    for m in Debugger.getAllForms(soup):  # iterating over all forms extracted
                        FORMS_TESTED.append('(i) '+url+':\n\n'+m.prettify()+'\n')
                        try:
                            if m['action']:
                                pass
                        except KeyError:
                            m['action'] = '/' + url.rsplit('/', 1)[1]
                            ErrorLogger(url, 'No standard ""action"" attribute.')
                        action = Parser.buildAction(url, m['action'])  # get all forms which have 'action' attribute
                        if not action in actionDone and action != '':  # if url returned is not a null value nor duplicate...
                            # If form submission is kept to True
                            if FORM_SUBMISSION:
                                try:
                                    result, genpoc = form.prepareFormInputs(m)  # prepare inputs
                                    r1 = Post(url, action, result).text  # make request with token values generated as user1
                                    result, genpoc = form.prepareFormInputs(m)  # prepare the input types
                                    r2 = Post(url, action, result).text  # again make request with token values generated as user2
                                    # Go for token based entropy checks...
                                    try:
                                        if m['name']:
                                            query, token = Entropy(result, url, m['action'], m['name'])
                                    except KeyError:
                                        query, token = Entropy(result, url, m['action'])
                                        ErrorLogger(url, 'No standard form ""name"".')
                                    # Now its time to detect the encoding type (if any) of the Anti-CSRF token.
                                    fnd = Encoding(token)
                                    if fnd == 0x01:
                                        VulnLogger(url, 'String encoded token value. Token might be decrypted.')
                                    else:
                                        NovulLogger(url, 'Anti-CSRF token is not a string encoded value.')
                                    # Go for token parameter tamper checks.
                                    if (query and token):
                                        Tamper(url, action, result, r2, query, token)
                                    o2 = resp2.open(url).read()  # make request as user2
                                    try:
                                        form2 = Debugger.getAllForms(BeautifulSoup(o2))[i]  # user2 gets his form
                                    except IndexError:
                                        verbout(R, 'Form Error')
                                        ErrorLogger(url, 'Form Index Error.')
                                        continue  # making sure program won't end here (dirty fix :( )
                                    verbout(GR, 'Preparing form inputs...')
                                    contents2, genpoc = form.prepareFormInputs(form2)  # prepare for form 2 as user2
                                    r3 = Post(url,action,contents2).text  # make request as user3 with user2's form
                                    if POST_BASED and not query and not token:
                                        try:
                                            if m['name']:
                                                PostBased(url, r1, r2, r3, m['action'], result, genpoc, m['name'])
                                        except KeyError:
                                            PostBased(url, r1, r2, r3, m['action'], result, genpoc)
                                    else:
                                        print(color.GREEN+' [+] The form was requested with a Anti-CSRF token.')
                                        print(color.GREEN+' [+] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.GREEN+' to P0ST-Based CSRF Attacks!')
                                        NovulLogger(url, 'Not vulnerable to POST-Based CSRF Attacks.')
                                except HTTPError as msg:  # if runtime exception...
                                    verbout(color.RED, ' [-] Exception : '+color.END+msg.__str__())  # again exception :(
                                    ErrorLogger(url, msg)
                        actionDone.append(action)  # add the stuff done
                        i+=1  # Increase user iteration
                except URLError as e:  # if again...
                    verbout(R, 'Exception at : '+url)  # again exception -_-
                    time.sleep(0.4)
                    verbout(O, 'Moving on...')
                    ErrorLogger(url, e)
                    continue  # make sure it doesn't stop at exceptions
                # This error usually happens when some sites are protected by some load balancer
                # example Cloudflare. These domains return a 403 forbidden response in various
                # contexts. For example when making reverse DNS queries.
                except HTTPError as e:
                    if str(e.code) == '403':
                        verbout(R, 'HTTP Authentication Error!')
                        verbout(R, 'Error Code : ' +O+ str(e.code))
                        ErrorLogger(url, e)
                        quit()
        GetLogger()  # The scanning has finished, so now we can log out all the links ;)
        print('\n'+G+""Scan completed!""+'\n')
        Analysis()  # For Post Scan Analysis
    except KeyboardInterrupt as e:  # Incase user wants to exit :') (while crawling)
        verbout(R, 'User Interrupt!')
        time.sleep(1.5)
        Analysis()  # For Post scan Analysis
        print(R+'Aborted!')  # say goodbye
        ErrorLogger('KeyBoard Interrupt', 'Aborted')
        quit()
    except Exception as e:
        verbout(R, e.__str__())
        ErrorLogger(url, e)
/n/n/ncore/options.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

# Importing stuff
import argparse, sys, tld
import urllib.parse, os
from files import config
from core.colors import R, G
from core.updater import updater

# Processing command line arguments
parser = argparse.ArgumentParser('python3 xsrfprobe.py')
parser._action_groups.pop()

# A simple hack to have required argumentsa and optional arguments separately
required = parser.add_argument_group('Required Arguments')
optional = parser.add_argument_group('Optional Arguments')

# Required Options
required.add_argument('-u', '--url', help='Main URL to test', dest='url')

# Optional Arguments (main stuff and necessary)
optional.add_argument('-c', '--cookie', help='Cookie value to be requested with each successive request. If there are multiple cookies, separate them with commas. For example: `-c PHPSESSID=i837c5n83u4, _gid=jdhfbuysf`.', dest='cookie')
optional.add_argument('-o', '--output', help='Output directory where files to be stored. Default is the`files` folder where all files generated will be stored.', dest='output')
optional.add_argument('-d', '--delay', help='Time delay between requests in seconds. Default is zero.', dest='delay', type=float)
optional.add_argument('-q', '--quiet', help='Set the DEBUG mode to quiet. Report only when vulnerabilities are found. Minimal output will be printed on screen. ', dest='quiet', action='store_true')
optional.add_argument('-v', '--verbose', help='Increase the verbosity of the output (e.g., -vv is more than -v). ', dest='verbose', action='store_true')

# Other Options
# optional.add_argument('-h', '--help', help='Show this help message and exit', dest='disp', default=argparse.SUPPRESS, action='store_true')
optional.add_argument('--user-agent', help='Custom user-agent to be used. Only one user-agent can be specified.', dest='user_agent', type=str)
optional.add_argument('--headers', help='Comma separated list of custom headers you\'d want to use. For example: ``--headers ""Accept=text/php, X-Requested-With=Dumb""``.', dest='headers', type=str)
optional.add_argument('--exclude', help='Comma separated list of paths or directories to be excluded which are not in scope. These paths/dirs won\'t be scanned. For example: `--exclude somepage/, sensitive-dir/, pleasedontscan/`', dest='exclude', type=str)
optional.add_argument('--timeout', help='HTTP request timeout value in seconds. The entered value must be in floating point decimal. Example: ``--timeout 10.0``', dest='timeout', type=float)
optional.add_argument('--max-chars', help='Maximum allowed character length for the custom token value to be generated. For example: `--max-chars 5`. Default value is 6.', dest='maxchars', type=int)
optional.add_argument('--crawl', help=""Crawl the whole site and simultaneously test all discovered endpoints for CSRF."", dest='crawl', action='store_true')
optional.add_argument('--skip-analysis', help='Skip the Post-Scan Analysis of Tokens which were gathered during requests', dest='skipal', action='store_true')
optional.add_argument('--skip-poc', help='Skip the PoC Form Generation of POST-Based Cross Site Request Forgeries.', dest='skippoc', action='store_true')
optional.add_argument('--display', help='Print out response headers of requests while making requests.', dest='disphead', action='store_true')
optional.add_argument('--update', help='Update XSRFProbe to latest version on GitHub via git.', dest='update', action='store_true')
optional.add_argument('--random-agent', help='Use random user-agents for making requests.', dest='randagent', action='store_true')
optional.add_argument('--version', help='Display the version of XSRFProbe and exit.', dest='version', action='store_true')
args = parser.parse_args()

if not len(sys.argv) > 1:
    print('''
    \033[1;91mXSRFProbe\033[0m, \033[1;97mA \033[1;93mCross Site Request Forgery \033[1;97mAudit Toolkit\033[0m
''')
    parser.print_help()
    quit('')

# Update XSRFProbe to latest version
if args.update:
    updater()
    quit('')

# Print out XSRFProbe version
if args.version:
    print('\n\033[1;97m [+] \033[1;91mXSRFProbe Version\033[0m : \033[1;97m'+open('files/VersionNum').read())
    quit()

# Now lets update some global config variables
if args.maxchars:
    config.TOKEN_GENERATION_LENGTH = args.maxchars

# Setting custom user-agent
if args.user_agent:
    config.USER_AGENT = args.user_agent

# Option to skip analysis
if args.skipal:
    config.SCAN_ANALYSIS = False

# Option to skip poc generation
if args.skippoc:
    config.POC_GENERATION = False

# Updating main root url
if not args.version and not args.update:
    if args.url: # and not args.help:
        if 'http' in args.url:
            config.SITE_URL = args.url
        else:
            config.SITE_URL = 'http://'+args.url
    else:
        print(R+'You must supply a url/endpoint.')

# Crawl the site if --crawl supplied.
if args.crawl:
    config.CRAWL_SITE = True
    # Turning off the display header feature due to too much log generation.
    config.DISPLAY_HEADERS = False

if args.cookie:
    # Assigning Cookie
    if ',' in args.cookie:
        for cook in args.cookie.split(','):
            config.COOKIE_VALUE.append(cook.strip())
            # This is necessary when a cookie value is supplied
            # Since if the user-agent used to make the request changes
            # from time to time, the remote site might trigger up
            # security mechanisms (or worse, perhaps block your ip?)
            config.USER_AGENT_RANDOM = False

# Set the headers displayer to 1 (actively display headers)
if args.disphead:
    config.DISPLAY_HEADERS = True

# Timeout value
if args.timeout:
    config.TIMEOUT_VALUE = args.timeout

# Custom header values if specified
if args.headers:
    # NOTE: As a default idea, when the user supplies custom headers, we
    # simply add the custom headers to a list of existing headers in
    # files/config.py.
    # Uncomment the following lines to just reinitialise the headers everytime
    # they make a request.
    #
    #config.HEADER_VALUES = {}
    for m in args.headers.split(','):
        config.HEADER_VALUES[m.split('=')[0]] = m.split('=')[1]  # nice hack ;)

if args.exclude:
    exc = args.exclude
    #config.EXCLUDE_URLS = [s for s in exc.split(',').strip()]
    m = exc.split(',').strip()
    for s in m:
        config.EXCLUDE_DIRS.append(urllib.parse.urljoin(config.SITE_URL, s))

if args.randagent:
    # If random-agent argument supplied...
    config.USER_AGENT_RANDOM = True
    # Turn off a single User-Agent mechanism...
    config.USER_AGENT = ''

if config.SITE_URL:
    if args.output:
        # If output directory is mentioned...
        try:
            if not os.path.exists(args.output+tld.get_fld(config.SITE_URL)):
                os.makedirs(args.output+tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = args.output+tld.get_fld(config.SITE_URL) + '/'
    else:
        try:
            os.makedirs(tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = tld.get_fld(config.SITE_URL) + '/'

if args.quiet:
    config.DEBUG = False
/n/n/ncore/utils.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-:-:-:#
#    XSRFProbe     #
#-:-:-:-:-:-:-:-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

from difflib import SequenceMatcher

def sameSequence(str1,str2):
    '''
    This function is intended to find same sequence
                between str1 and str2.
    '''
    # Initialize SequenceMatcher object with
    # Input string
    seqMatch = SequenceMatcher(None, str1, str2)

    # Find match of longest sub-string
    # Output will be like Match(a=0, b=0, size=5)
    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))

    # Print longest substring
    if (match.size!=0):
        return (str1[match.a: match.a + match.size])
    else:
        return ''

def replaceStrIndex(text, index=0, replacement=''):
    '''
    This method returns a tampered string by
                    replacement
    '''
    return '%s%s%s' % (text[:index], replacement, text[index+1:])

def checkDuplicates(iterable):
    '''
    This function works as a byte sequence checker for
            tuples passed onto this function.
    '''
    seen = set()
    for x in iterable:
        if x in seen:
            return True
        seen.add(x)
    return False

def byteString(s, encoding='utf8'):
    """"""
    Return a byte-string version of 's',
            Encoded as utf-8.
    """"""
    try:
        s = s.encode(encoding)
    except (UnicodeEncodeError, UnicodeDecodeError):
        s = str(s)
    return s

def subSequence(str1,str2):
    '''
    Returns whether 'str1' and 'str2' are subsequence
                    of one another.
    '''
    j = 0    # Index of str1
    i = 0    # Index of str2

    # Traverse both str1 and str2
    # Compare current character of str2 with
    # First unmatched character of str1
    # If matched, then move ahead in str1
    m = len(str1)
    n = len(str2)
    while j<m and i<n:
        if str1[j] == str2[i]:
            j = j+1
        i = i + 1

    # If all characters of str1 matched, then j is equal to m
    return j==m
/n/n/n",0
3,3,398ed11584313a371763240392c4dda1cf986deb,"/core/logger.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

import os
from core.colors import *
from files.config import *
from core.verbout import verbout
from files.discovered import INTERNAL_URLS, FILES_EXEC, SCAN_ERRORS
from files.discovered import VULN_LIST, FORMS_TESTED, REQUEST_TOKENS

def logger(filename, content):
    '''
    This module is for logging all the stuff we found
            while crawling and scanning.
    '''
    output_file = OUTPUT_DIR + filename + '.log'
    with open(output_file, 'w+', encoding='utf8') as f:
        if type(content) is tuple or type(content) is list:
            for m in content:  # if it is list or tuple, it is iterable
                f.write(m+'\n')
        else:
            f.write(content)  # else we write out as it is... ;)
        f.write('\n')

def pheaders(tup):
    '''
    This module prints out the headers as received in the
                    requests normally.
    '''
    verbout(GR, 'Receiving headers...\n')
    verbout(color.GREY,'  '+color.UNDERLINE+'HEADERS'+color.END+color.GREY+':'+'\n')
    for key, val in tup.items():
        verbout('  ',color.CYAN+key+': '+color.ORANGE+val)
    verbout('','')

def GetLogger():
    if INTERNAL_URLS:
        logger('internal-links', INTERNAL_URLS)
    if SCAN_ERRORS:
        logger('errored', SCAN_ERRORS)
    if FILES_EXEC:
        logger('files-found', FILES_EXEC)
    if REQUEST_TOKENS:
        logger('anti-csrf-tokens', REQUEST_TOKENS)
    if FORMS_TESTED:
        logger('forms-tested', FORMS_TESTED)
    if VULN_LIST:
        logger('vulnerabilities', VULN_LIST)

def ErrorLogger(url, error):
    con = '(i) '+url+' -> '+error.__str__()
    SCAN_ERRORS.append(con)

def VulnLogger(url, vuln):
    tent = '[!] '+url+' -> '+vuln
    VULN_LIST.append(tent)
/n/n/n/core/options.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

# Importing stuff
import argparse, sys, tld
import urllib.parse, os
from files import config
from core.colors import R, G
from core.updater import updater

# Processing command line arguments
parser = argparse.ArgumentParser('python3 xsrfprobe.py')
parser._action_groups.pop()

# A simple hack to have required argumentsa and optional arguments separately
required = parser.add_argument_group('Required Arguments')
optional = parser.add_argument_group('Optional Arguments')

# Required Options
required.add_argument('-u', '--url', help='Main URL to test', dest='url')

# Optional Arguments (main stuff and necessary)
optional.add_argument('-c', '--cookie', help='Cookie value to be requested with each successive request. If there are multiple cookies, separate them with commas. For example: `-c PHPSESSID=i837c5n83u4, _gid=jdhfbuysf`.', dest='cookie')
optional.add_argument('-o', '--output', help='Output directory where files to be stored. Default is the`files` folder where all files generated will be stored.', dest='output')
optional.add_argument('-d', '--delay', help='Time delay between requests in seconds. Default is zero.', dest='delay', type=float)
optional.add_argument('-q', '--quiet', help='Set the DEBUG mode to quiet. Report only when vulnerabilities are found. Minimal output will be printed on screen. ', dest='quiet', action='store_true')
optional.add_argument('-v', '--verbose', help='Increase the verbosity of the output (e.g., -vv is more than -v). ', dest='verbose', action='store_true')

# Other Options
# optional.add_argument('-h', '--help', help='Show this help message and exit', dest='disp', default=argparse.SUPPRESS, action='store_true')
optional.add_argument('--user-agent', help='Custom user-agent to be used. Only one user-agent can be specified.', dest='user_agent', type=str)
optional.add_argument('--headers', help='Comma separated list of custom headers you\'d want to use. For example: ``--headers ""Accept=text/php, X-Requested-With=Dumb""``.', dest='headers', type=str)
optional.add_argument('--exclude', help='Comma separated list of paths or directories to be excluded which are not in scope. These paths/dirs won\'t be scanned. For example: `--exclude somepage/, sensitive-dir/, pleasedontscan/`', dest='exclude', type=str)
optional.add_argument('--timeout', help='HTTP request timeout value in seconds. The entered value must be in floating point decimal. Example: ``--timeout 10.0``', dest='timeout', type=float)
optional.add_argument('--max-chars', help='Maximum allowed character length for the custom token value to be generated. For example: `--max-chars 5`. Default value is 6.', dest='maxchars', type=int)
optional.add_argument('--crawl', help=""Crawl the whole site and simultaneously test all discovered endpoints for CSRF."", dest='crawl', action='store_true')
optional.add_argument('--skip-analysis', help='Skip the Post-Scan Analysis of Tokens which were gathered during requests', dest='skipal', action='store_true')
optional.add_argument('--skip-poc', help='Skip the PoC Form Generation of POST-Based Cross Site Request Forgeries.', dest='skippoc', action='store_true')
optional.add_argument('--update', help='Update XSRFProbe to latest version on GitHub via git.', dest='update', action='store_true')
optional.add_argument('--random-agent', help='Use random user-agents for making requests.', dest='randagent', action='store_true')
optional.add_argument('--version', help='Display the version of XSRFProbe and exit.', dest='version', action='store_true')
args = parser.parse_args()

if not len(sys.argv) > 1:
    print('''
    \033[1;91mXSRFProbe\033[0m, \033[1;97mA \033[1;93mCross Site Request Forgery \033[1;97mAudit Toolkit\033[0m
''')
    parser.print_help()
    quit('')

# Update XSRFProbe to latest version
if args.update:
    updater()
    quit('')

# Print out XSRFProbe version
if args.version:
    print('\n\033[1;97m [+] \033[1;91mXSRFProbe Version\033[0m : \033[1;97m'+open('files/VersionNum').read())
    quit()

# Now lets update some global config variables
if args.maxchars:
    config.TOKEN_GENERATION_LENGTH = args.maxchars

# Setting custom user-agent
if args.user_agent:
    config.USER_AGENT = args.user_agent

# Option to skip analysis
if args.skipal:
    config.SCAN_ANALYSIS = False

# Option to skip poc generation
if args.skippoc:
    config.POC_GENERATION = False

# Updating main root url
if not args.version and not args.update:
    if args.url: # and not args.help:
        if 'http' in args.url:
            config.SITE_URL = args.url
        else:
            config.SITE_URL = 'http://'+args.url
    else:
        print(R+'You must supply a url/endpoint.')

# Crawl the site if --crawl supplied.
if args.crawl:
    config.CRAWL_SITE = True

if args.cookie:
    # Assigning Cookie
    if ',' in args.cookie:
        for cook in args.cookie.split(','):
            config.COOKIE_VALUE.append(cook.strip())
            # This is necessary when a cookie value is supplied
            # Since if the user-agent used to make the request changes
            # from time to time, the remote site might trigger up
            # security mechanisms (or worse, perhaps block your ip?)
            config.USER_AGENT_RANDOM = False

# Timeout value
if args.timeout:
    config.TIMEOUT_VALUE = args.timeout

# Custom header values if specified
if args.headers:
    # NOTE: As a default idea, when the user supplies custom headers, we
    # simply add the custom headers to a list of existing headers in
    # files/config.py.
    # Uncomment the following lines to just reinitialise the headers everytime
    # they make a request.
    #
    #config.HEADER_VALUES = {}
    for m in args.headers.split(','):
        config.HEADER_VALUES[m.split('=')[0]] = m.split('=')[1]

if args.exclude:
    exc = args.exclude
    #config.EXCLUDE_URLS = [s for s in exc.split(',').strip()]
    m = exc.split(',').strip()
    for s in m:
        config.EXCLUDE_DIRS.append(urllib.parse.urljoin(config.SITE_URL, s))

if args.randagent:
    # If random-agent argument supplied...
    config.USER_AGENT_RANDOM = True
    # Turn off a single User-Agent mechanism...
    config.USER_AGENT = ''

if config.SITE_URL:
    if args.output:
        # If output directory is mentioned...
        try:
            if not os.path.exists(args.output+tld.get_fld(config.SITE_URL)):
                os.makedirs(args.output+tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = args.output+tld.get_fld(config.SITE_URL) + '/'
    else:
        try:
            os.makedirs(tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = tld.get_fld(config.SITE_URL) + '/'

if args.quiet:
    config.DEBUG = False
/n/n/n",1
82,82,c23a5bf6278f55b3f8135e0edab9927599a09236,"appengine/swarming/handlers_bot.py/n/n# Copyright 2015 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Internal bot API handlers.""""""

import base64
import json
import logging
import textwrap

import webob
import webapp2

from google.appengine.api import app_identity
from google.appengine.api import datastore_errors
from google.appengine.datastore import datastore_query
from google.appengine import runtime
from google.appengine.ext import ndb

from components import auth
from components import ereporter2
from components import utils
from server import acl
from server import bot_code
from server import bot_management
from server import stats
from server import task_pack
from server import task_request
from server import task_result
from server import task_scheduler
from server import task_to_run


def has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):
  """"""Returns an error if unexpected keys are present or expected keys are
  missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  actual_keys = frozenset(actual_keys)
  superfluous = actual_keys - expected_keys
  missing = minimum_keys - actual_keys
  if superfluous or missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    msg_superfluous = (
        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')
    return 'Unexpected %s%s%s; did you make a typo?' % (
        name, msg_missing, msg_superfluous)


def has_unexpected_keys(expected_keys, actual_keys, name):
  """"""Return an error if unexpected keys are present or expected keys are
  missing.
  """"""
  return has_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, name)


def log_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  message = has_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, name)
  if message:
    ereporter2.log_request(request, source=source, message=message)
  return message


def log_unexpected_keys(expected_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.
  """"""
  return log_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, request, source, name)


def has_missing_keys(minimum_keys, actual_keys, name):
  """"""Returns an error if expected keys are not present.

  Do not warn about unexpected keys.
  """"""
  actual_keys = frozenset(actual_keys)
  missing = minimum_keys - actual_keys
  if missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)


class BootstrapHandler(auth.AuthenticatingHandler):
  """"""Returns python code to run to bootstrap a swarming bot.""""""

  @auth.require(acl.is_bot)
  def get(self):
    self.response.headers['Content-Type'] = 'text/x-python'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot_bootstrap.py""')
    self.response.out.write(
        bot_code.get_bootstrap(self.request.host_url).content)


class BotCodeHandler(auth.AuthenticatingHandler):
  """"""Returns a zip file with all the files required by a bot.

  Optionally specify the hash version to download. If so, the returned data is
  cacheable.
  """"""

  @auth.require(acl.is_bot)
  def get(self, version=None):
    if version:
      expected = bot_code.get_bot_version(self.request.host_url)
      if version != expected:
        # This can happen when the server is rapidly updated.
        logging.error('Requested Swarming bot %s, have %s', version, expected)
        self.abort(404)
      self.response.headers['Cache-Control'] = 'public, max-age=3600'
    else:
      self.response.headers['Cache-Control'] = 'no-cache, no-store'
    self.response.headers['Content-Type'] = 'application/octet-stream'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot.zip""')
    self.response.out.write(
        bot_code.get_swarming_bot_zip(self.request.host_url))


class _BotBaseHandler(auth.ApiHandler):
  """"""
  Request body is a JSON dict:
    {
      ""dimensions"": <dict of properties>,
      ""state"": <dict of properties>,
      ""version"": <sha-1 of swarming_bot.zip uncompressed content>,
    }
  """"""

  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}
  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  def _process(self):
    """"""Returns True if the bot has invalid parameter and should be automatically
    quarantined.

    Does one DB synchronous GET.

    Returns:
      tuple(request, bot_id, version, state, dimensions, quarantined_msg)
    """"""
    request = self.parse_body()
    version = request.get('version', None)

    dimensions = request.get('dimensions', {})
    state = request.get('state', {})
    bot_id = None
    if dimensions.get('id'):
      dimension_id = dimensions['id']
      if (isinstance(dimension_id, list) and len(dimension_id) == 1
          and isinstance(dimension_id[0], unicode)):
        bot_id = dimensions['id'][0]

    # The bot may decide to ""self-quarantine"" itself. Accept both via
    # dimensions or via state. See bot_management._BotCommon.quarantined for
    # more details.
    if (bool(dimensions.get('quarantined')) or
        bool(state.get('quarantined'))):
      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'

    quarantined_msg = None
    # Use a dummy 'for' to be able to break early from the block.
    for _ in [0]:

      quarantined_msg = has_unexpected_keys(
          self.EXPECTED_KEYS, request, 'keys')
      if quarantined_msg:
        break

      quarantined_msg = has_missing_keys(
          self.REQUIRED_STATE_KEYS, state, 'state')
      if quarantined_msg:
        break

      if not bot_id:
        quarantined_msg = 'Missing bot id'
        break

      if not all(
          isinstance(key, unicode) and
          isinstance(values, list) and
          all(isinstance(value, unicode) for value in values)
          for key, values in dimensions.iteritems()):
        quarantined_msg = (
            'Invalid dimensions type:\n%s' % json.dumps(dimensions,
              sort_keys=True, indent=2, separators=(',', ': ')))
        break

      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)
      if dimensions_count > task_to_run.MAX_DIMENSIONS:
        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count
        break

      if not isinstance(
          state.get('lease_expiration_ts'), (None.__class__, int)):
        quarantined_msg = (
            'lease_expiration_ts (%r) must be int or None' % (
                state['lease_expiration_ts']))
        break

    if quarantined_msg:
      line = 'Quarantined Bot\nhttps://%s/restricted/bot/%s\n%s' % (
          app_identity.get_default_version_hostname(), bot_id,
          quarantined_msg)
      ereporter2.log_request(self.request, source='bot', message=line)
      return request, bot_id, version, state, dimensions, quarantined_msg

    # Look for admin enforced quarantine.
    bot_settings = bot_management.get_settings_key(bot_id).get()
    if bool(bot_settings and bot_settings.quarantined):
      return request, bot_id, version, state, dimensions, 'Quarantined by admin'

    return request, bot_id, version, state, dimensions, None


class BotHandshakeHandler(_BotBaseHandler):
  """"""First request to be called to get initial data like XSRF token.

  The bot is server-controled so the server doesn't have to support multiple API
  version. When running a task, the bot sync the the version specific URL. Once
  abot finished its currently running task, it'll be immediately be upgraded
  after on its next poll.

  This endpoint does not return commands to the bot, for example to upgrade
  itself. It'll be told so when it does its first poll.

  Response body is a JSON dict:
    {
      ""bot_version"": <sha-1 of swarming_bot.zip uncompressed content>,
      ""server_version"": ""138-193f1f3"",
    }
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    bot_management.bot_event(
        event_type='bot_connected', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=dimensions,
        state=state, version=version, quarantined=bool(quarantined_msg),
        task_id='', task_name=None, message=quarantined_msg)

    data = {
      # This access token will be used to validate each subsequent request.
      'bot_version': bot_code.get_bot_version(self.request.host_url),
      # TODO(maruel): Remove this once all the bots have been updated.
      'expiration_sec': auth.handler.XSRFToken.expiration_sec,
      'server_version': utils.get_app_version(),
      # TODO(maruel): Remove this once all the bots have been updated.
      'xsrf_token': self.generate_xsrf_token(),
    }
    self.send_response(data)


class BotPollHandler(_BotBaseHandler):
  """"""The bot polls for a task; returns either a task, update command or sleep.

  In case of exception on the bot, this is enough to get it just far enough to
  eventually self-update to a working version. This is to ensure that coding
  errors in bot code doesn't kill all the fleet at once, they should still be up
  just enough to be able to self-update again even if they don't get task
  assigned anymore.
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    """"""Handles a polling request.

    Be very permissive on missing values. This can happen because of errors
    on the bot, *we don't want to deny them the capacity to update*, so that the
    bot code is eventually fixed and the bot self-update to this working code.

    It makes recovery of the fleet in case of catastrophic failure much easier.
    """"""
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    sleep_streak = state.get('sleep_streak', 0)
    quarantined = bool(quarantined_msg)

    # Note bot existence at two places, one for stats at 1 minute resolution,
    # the other for the list of known bots.
    action = 'bot_inactive' if quarantined else 'bot_active'
    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)

    def bot_event(event_type, task_id=None, task_name=None):
      bot_management.bot_event(
          event_type=event_type, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=dimensions,
          state=state, version=version, quarantined=quarantined,
          task_id=task_id, task_name=task_name, message=quarantined_msg)

    # Bot version is host-specific because the host URL is embedded in
    # swarming_bot.zip
    expected_version = bot_code.get_bot_version(self.request.host_url)
    if version != expected_version:
      bot_event('request_update')
      self._cmd_update(expected_version)
      return
    if quarantined:
      bot_event('request_sleep')
      self._cmd_sleep(sleep_streak, quarantined)
      return

    #
    # At that point, the bot should be in relatively good shape since it's
    # running the right version. It is still possible that invalid code was
    # pushed to the server, so be diligent about it.
    #

    # Bot may need a reboot if it is running for too long. We do not reboot
    # quarantined bots.
    needs_restart, restart_message = bot_management.should_restart_bot(
        bot_id, state)
    if needs_restart:
      bot_event('request_restart')
      self._cmd_restart(restart_message)
      return

    # The bot is in good shape. Try to grab a task.
    try:
      # This is a fairly complex function call, exceptions are expected.
      request, run_result = task_scheduler.bot_reap_task(
          dimensions, bot_id, version, state.get('lease_expiration_ts'))
      if not request:
        # No task found, tell it to sleep a bit.
        bot_event('request_sleep')
        self._cmd_sleep(sleep_streak, quarantined)
        return

      try:
        # This part is tricky since it intentionally runs a transaction after
        # another one.
        if request.properties.is_terminate:
          bot_event('bot_terminate', task_id=run_result.task_id)
          self._cmd_terminate(run_result.task_id)
        else:
          bot_event(
              'request_task', task_id=run_result.task_id,
              task_name=request.name)
          self._cmd_run(request, run_result.key, bot_id)
      except:
        logging.exception('Dang, exception after reaping')
        raise
    except runtime.DeadlineExceededError:
      # If the timeout happened before a task was assigned there is no problems.
      # If the timeout occurred after a task was assigned, that task will
      # timeout (BOT_DIED) since the bot didn't get the details required to
      # run it) and it will automatically get retried (TODO) when the task times
      # out.
      # TODO(maruel): Note the task if possible and hand it out on next poll.
      # https://code.google.com/p/swarming/issues/detail?id=130
      self.abort(500, 'Deadline')

  def _cmd_run(self, request, run_result_key, bot_id):
    cmd = None
    if request.properties.commands:
      cmd = request.properties.commands[0]
    elif request.properties.command:
      cmd = request.properties.command
    out = {
      'cmd': 'run',
      'manifest': {
        'bot_id': bot_id,
        'command': cmd,
        'dimensions': request.properties.dimensions,
        'env': request.properties.env,
        'extra_args': request.properties.extra_args,
        'grace_period': request.properties.grace_period_secs,
        'hard_timeout': request.properties.execution_timeout_secs,
        'host': utils.get_versioned_hosturl(),
        'io_timeout': request.properties.io_timeout_secs,
        'inputs_ref': request.properties.inputs_ref,
        'task_id': task_pack.pack_run_result_key(run_result_key),
      },
    }
    self.send_response(utils.to_json_encodable(out))

  def _cmd_sleep(self, sleep_streak, quarantined):
    out = {
      'cmd': 'sleep',
      'duration': task_scheduler.exponential_backoff(sleep_streak),
      'quarantined': quarantined,
    }
    self.send_response(out)

  def _cmd_terminate(self, task_id):
    out = {
      'cmd': 'terminate',
      'task_id': task_id,
    }
    self.send_response(out)

  def _cmd_update(self, expected_version):
    out = {
      'cmd': 'update',
      'version': expected_version,
    }
    self.send_response(out)

  def _cmd_restart(self, message):
    logging.info('Rebooting bot: %s', message)
    out = {
      'cmd': 'restart',
      'message': message,
    }
    self.send_response(out)


class BotEventHandler(_BotBaseHandler):
  """"""On signal that a bot had an event worth logging.""""""

  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}

  @auth.require(acl.is_bot)
  def post(self):
    (request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    event = request.get('event')
    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):
      self.abort_with_error(400, error='Unsupported event type')
    message = request.get('message')
    bot_management.bot_event(
        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,
        dimensions=dimensions, state=state, version=version,
        quarantined=bool(quarantined_msg), task_id=None, task_name=None,
        message=message)

    if event == 'bot_error':
      line = (
          'Bot: https://%s/restricted/bot/%s\n'
          'Bot error:\n'
          '%s') % (
          app_identity.get_default_version_hostname(), bot_id, message)
      ereporter2.log_request(self.request, source='bot', message=line)
    self.send_response({})


class BotTaskUpdateHandler(auth.ApiHandler):
  """"""Receives updates from a Bot for a task.

  The handler verifies packets are processed in order and will refuse
  out-of-order packets.
  """"""
  ACCEPTED_KEYS = {
    u'bot_overhead', u'cost_usd', u'duration', u'exit_code',
    u'hard_timeout', u'id', u'io_timeout', u'isolated_stats', u'output',
    u'output_chunk_start', u'outputs_ref', u'task_id',
  }
  REQUIRED_KEYS = {u'id', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    # Unlike handshake and poll, we do not accept invalid keys here. This code
    # path is much more strict.
    request = self.parse_body()
    msg = log_unexpected_subset_keys(
        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',
        'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    bot_id = request['id']
    cost_usd = request['cost_usd']
    task_id = request['task_id']

    bot_overhead = request.get('bot_overhead')
    duration = request.get('duration')
    exit_code = request.get('exit_code')
    hard_timeout = request.get('hard_timeout')
    io_timeout = request.get('io_timeout')
    isolated_stats = request.get('isolated_stats')
    output = request.get('output')
    output_chunk_start = request.get('output_chunk_start')
    outputs_ref = request.get('outputs_ref')

    if bool(isolated_stats) != (bot_overhead is not None):
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % task_id)
      self.abort_with_error(
          400,
          error='Both bot_overhead and isolated_stats must be set '
                'simultaneously\nbot_overhead: %s\nisolated_stats: %s' %
                (bot_overhead, isolated_stats))

    run_result_key = task_pack.unpack_run_result_key(task_id)
    performance_stats = None
    if isolated_stats:
      download = isolated_stats['download']
      upload = isolated_stats['upload']
      performance_stats = task_result.PerformanceStats(
          bot_overhead=bot_overhead,
          isolated_download=task_result.IsolatedOperation(
              duration=download['duration'],
              initial_number_items=download['initial_number_items'],
              initial_size=download['initial_size'],
              items_cold=base64.b64decode(download['items_cold']),
              items_hot=base64.b64decode(download['items_hot'])),
          isolated_upload=task_result.IsolatedOperation(
              duration=upload['duration'],
              items_cold=base64.b64decode(upload['items_cold']),
              items_hot=base64.b64decode(upload['items_hot'])))

    if output is not None:
      try:
        output = base64.b64decode(output)
      except UnicodeEncodeError as e:
        logging.error('Failed to decode output\n%s\n%r', e, output)
        output = output.encode('ascii', 'replace')
      except TypeError as e:
        # Save the output as-is instead. The error will be logged in ereporter2
        # and returning a HTTP 500 would only force the bot to stay in a retry
        # loop.
        logging.error('Failed to decode output\n%s\n%r', e, output)
    if outputs_ref:
      outputs_ref = task_request.FilesRef(**outputs_ref)

    try:
      state = task_scheduler.bot_update_task(
          run_result_key=run_result_key,
          bot_id=bot_id,
          output=output,
          output_chunk_start=output_chunk_start,
          exit_code=exit_code,
          duration=duration,
          hard_timeout=hard_timeout,
          io_timeout=io_timeout,
          cost_usd=cost_usd,
          outputs_ref=outputs_ref,
          performance_stats=performance_stats)
      if not state:
        logging.info('Failed to update, please retry')
        self.abort_with_error(500, error='Failed to update, please retry')

      if state in (task_result.State.COMPLETED, task_result.State.TIMED_OUT):
        action = 'task_completed'
      else:
        assert state == task_result.State.RUNNING, state
        action = 'task_update'
      bot_management.bot_event(
          event_type=action, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=None, state=None,
          version=None, quarantined=None, task_id=task_id, task_name=None)
    except ValueError as e:
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % e)
      self.abort_with_error(400, error=str(e))
    except webob.exc.HTTPException:
      raise
    except Exception as e:
      logging.exception('Internal error: %s', e)
      self.abort_with_error(500, error=str(e))

    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot
    # reboots itself to abort the task abruptly. It is useful when a task hangs
    # and the timeout was set too long or the task was superseded by a newer
    # task with more recent executable (e.g. a new Try Server job on a newer
    # patchset on Rietveld).
    self.send_response({'ok': True})


class BotTaskErrorHandler(auth.ApiHandler):
  """"""It is a specialized version of ereporter2's /ereporter2/api/v1/on_error
  that also attaches a task id to it.

  This formally kills the task, marking it as an internal failure. This can be
  used by bot_main.py to kill the task when task_runner misbehaved.
  """"""

  EXPECTED_KEYS = {u'id', u'message', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    request = self.parse_body()
    bot_id = request.get('id')
    task_id = request.get('task_id', '')
    message = request.get('message', 'unknown')

    bot_management.bot_event(
        event_type='task_error', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=None, state=None,
        version=None, quarantined=None, task_id=task_id, task_name=None,
        message=message)
    line = (
        'Bot: https://%s/restricted/bot/%s\n'
        'Task failed: https://%s/user/task/%s\n'
        '%s') % (
        app_identity.get_default_version_hostname(), bot_id,
        app_identity.get_default_version_hostname(), task_id,
        message)
    ereporter2.log_request(self.request, source='bot', message=line)

    msg = log_unexpected_keys(
        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    msg = task_scheduler.bot_kill_task(
        task_pack.unpack_run_result_key(task_id), bot_id)
    if msg:
      logging.error(msg)
      self.abort_with_error(400, error=msg)
    self.send_response({})


class ServerPingHandler(webapp2.RequestHandler):
  """"""Handler to ping when checking if the server is up.

  This handler should be extremely lightweight. It shouldn't do any
  computations, it should just state that the server is up. It's open to
  everyone for simplicity and performance.
  """"""

  def get(self):
    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'
    self.response.out.write('Server up')


def get_routes():
  routes = [
      ('/bootstrap', BootstrapHandler),
      ('/bot_code', BotCodeHandler),
      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),
      ('/swarming/api/v1/bot/event', BotEventHandler),
      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),
      ('/swarming/api/v1/bot/poll', BotPollHandler),
      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),
      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',
          BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),
      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',
          BotTaskErrorHandler),
  ]
  return [webapp2.Route(*i) for i in routes]
/n/n/nappengine/swarming/server/bot_archive.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Generates the swarming_bot.zip archive for the bot.

Unlike the other source files, this file can be run from ../tools/bot_archive.py
stand-alone to generate a swarming_bot.zip for local testing so it doesn't
import anything from the AppEngine SDK.

The hash of the content of the files in the archive is used to define the
current version of the swarming bot code.
""""""

import hashlib
import json
import logging
import os
import StringIO
import zipfile


# List of files needed by the swarming bot.
# TODO(maruel): Make the list automatically generated?
FILES = (
    '__main__.py',
    'api/__init__.py',
    'api/bot.py',
    'api/parallel.py',
    'api/os_utilities.py',
    'api/platforms/__init__.py',
    'api/platforms/android.py',
    'api/platforms/common.py',
    'api/platforms/gce.py',
    'api/platforms/linux.py',
    'api/platforms/osx.py',
    'api/platforms/posix.py',
    'api/platforms/win.py',
    'bot_code/__init__.py',
    'bot_code/bot_main.py',
    'bot_code/common.py',
    'bot_code/singleton.py',
    'bot_code/task_runner.py',
    'client/auth.py',
    'client/isolated_format.py',
    'client/isolateserver.py',
    'client/run_isolated.py',
    'config/__init__.py',
    'third_party/__init__.py',
    'third_party/colorama/__init__.py',
    'third_party/colorama/ansi.py',
    'third_party/colorama/ansitowin32.py',
    'third_party/colorama/initialise.py',
    'third_party/colorama/win32.py',
    'third_party/colorama/winterm.py',
    'third_party/depot_tools/__init__.py',
    'third_party/depot_tools/fix_encoding.py',
    'third_party/depot_tools/subcommand.py',
    'third_party/httplib2/__init__.py',
    'third_party/httplib2/cacerts.txt',
    'third_party/httplib2/iri2uri.py',
    'third_party/httplib2/socks.py',
    'third_party/oauth2client/__init__.py',
    'third_party/oauth2client/_helpers.py',
    'third_party/oauth2client/_openssl_crypt.py',
    'third_party/oauth2client/_pycrypto_crypt.py',
    'third_party/oauth2client/client.py',
    'third_party/oauth2client/clientsecrets.py',
    'third_party/oauth2client/crypt.py',
    'third_party/oauth2client/file.py',
    'third_party/oauth2client/gce.py',
    'third_party/oauth2client/keyring_storage.py',
    'third_party/oauth2client/locked_file.py',
    'third_party/oauth2client/multistore_file.py',
    'third_party/oauth2client/service_account.py',
    'third_party/oauth2client/tools.py',
    'third_party/oauth2client/util.py',
    'third_party/oauth2client/xsrfutil.py',
    'third_party/pyasn1/pyasn1/__init__.py',
    'third_party/pyasn1/pyasn1/codec/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/decoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/encoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/eoo.py',
    'third_party/pyasn1/pyasn1/codec/cer/__init__.py',
    'third_party/pyasn1/pyasn1/codec/cer/decoder.py',
    'third_party/pyasn1/pyasn1/codec/cer/encoder.py',
    'third_party/pyasn1/pyasn1/codec/der/__init__.py',
    'third_party/pyasn1/pyasn1/codec/der/decoder.py',
    'third_party/pyasn1/pyasn1/codec/der/encoder.py',
    'third_party/pyasn1/pyasn1/compat/__init__.py',
    'third_party/pyasn1/pyasn1/compat/binary.py',
    'third_party/pyasn1/pyasn1/compat/octets.py',
    'third_party/pyasn1/pyasn1/debug.py',
    'third_party/pyasn1/pyasn1/error.py',
    'third_party/pyasn1/pyasn1/type/__init__.py',
    'third_party/pyasn1/pyasn1/type/base.py',
    'third_party/pyasn1/pyasn1/type/char.py',
    'third_party/pyasn1/pyasn1/type/constraint.py',
    'third_party/pyasn1/pyasn1/type/error.py',
    'third_party/pyasn1/pyasn1/type/namedtype.py',
    'third_party/pyasn1/pyasn1/type/namedval.py',
    'third_party/pyasn1/pyasn1/type/tag.py',
    'third_party/pyasn1/pyasn1/type/tagmap.py',
    'third_party/pyasn1/pyasn1/type/univ.py',
    'third_party/pyasn1/pyasn1/type/useful.py',
    'third_party/requests/__init__.py',
    'third_party/requests/adapters.py',
    'third_party/requests/api.py',
    'third_party/requests/auth.py',
    'third_party/requests/certs.py',
    'third_party/requests/compat.py',
    'third_party/requests/cookies.py',
    'third_party/requests/exceptions.py',
    'third_party/requests/hooks.py',
    'third_party/requests/models.py',
    'third_party/requests/packages/__init__.py',
    'third_party/requests/packages/urllib3/__init__.py',
    'third_party/requests/packages/urllib3/_collections.py',
    'third_party/requests/packages/urllib3/connection.py',
    'third_party/requests/packages/urllib3/connectionpool.py',
    'third_party/requests/packages/urllib3/contrib/__init__.py',
    'third_party/requests/packages/urllib3/contrib/ntlmpool.py',
    'third_party/requests/packages/urllib3/contrib/pyopenssl.py',
    'third_party/requests/packages/urllib3/exceptions.py',
    'third_party/requests/packages/urllib3/fields.py',
    'third_party/requests/packages/urllib3/filepost.py',
    'third_party/requests/packages/urllib3/packages/__init__.py',
    'third_party/requests/packages/urllib3/packages/ordered_dict.py',
    'third_party/requests/packages/urllib3/packages/six.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '__init__.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '_implementation.py',
    'third_party/requests/packages/urllib3/poolmanager.py',
    'third_party/requests/packages/urllib3/request.py',
    'third_party/requests/packages/urllib3/response.py',
    'third_party/requests/packages/urllib3/util/__init__.py',
    'third_party/requests/packages/urllib3/util/connection.py',
    'third_party/requests/packages/urllib3/util/request.py',
    'third_party/requests/packages/urllib3/util/response.py',
    'third_party/requests/packages/urllib3/util/retry.py',
    'third_party/requests/packages/urllib3/util/ssl_.py',
    'third_party/requests/packages/urllib3/util/timeout.py',
    'third_party/requests/packages/urllib3/util/url.py',
    'third_party/requests/sessions.py',
    'third_party/requests/status_codes.py',
    'third_party/requests/structures.py',
    'third_party/requests/utils.py',
    'third_party/rsa/rsa/__init__.py',
    'third_party/rsa/rsa/_compat.py',
    'third_party/rsa/rsa/_version133.py',
    'third_party/rsa/rsa/_version200.py',
    'third_party/rsa/rsa/asn1.py',
    'third_party/rsa/rsa/bigfile.py',
    'third_party/rsa/rsa/cli.py',
    'third_party/rsa/rsa/common.py',
    'third_party/rsa/rsa/core.py',
    'third_party/rsa/rsa/key.py',
    'third_party/rsa/rsa/parallel.py',
    'third_party/rsa/rsa/pem.py',
    'third_party/rsa/rsa/pkcs1.py',
    'third_party/rsa/rsa/prime.py',
    'third_party/rsa/rsa/randnum.py',
    'third_party/rsa/rsa/transform.py',
    'third_party/rsa/rsa/util.py',
    'third_party/rsa/rsa/varblock.py',
    'third_party/six/__init__.py',
    'utils/__init__.py',
    'utils/cacert.pem',
    'utils/file_path.py',
    'utils/fs.py',
    'utils/large.py',
    'utils/logging_utils.py',
    'utils/lru.py',
    'utils/net.py',
    'utils/oauth.py',
    'utils/on_error.py',
    'utils/subprocess42.py',
    'utils/threading_utils.py',
    'utils/tools.py',
    'utils/zip_package.py',
    'adb/__init__.py',
    'adb/adb_commands.py',
    'adb/adb_protocol.py',
    'adb/common.py',
    'adb/contrib/__init__.py',
    'adb/contrib/adb_commands_safe.py',
    'adb/contrib/high.py',
    'adb/contrib/parallel.py',
    'adb/fastboot.py',
    'adb/filesync_protocol.py',
    'adb/sign_pythonrsa.py',
    'adb/usb_exceptions.py',
    'python_libusb1/__init__.py',
    'python_libusb1/libusb1.py',
    'python_libusb1/usb1.py',
)


def is_windows():
  """"""Returns True if this code is running under Windows.""""""
  return os.__file__[0] != '/'


def resolve_symlink(path):
  """"""Processes path containing symlink on Windows.

  This is needed to make ../swarming_bot/main_test.py pass on Windows because
  git on Windows renders symlinks as normal files.
  """"""
  if not is_windows():
    # Only does this dance on Windows.
    return path
  parts = os.path.normpath(path).split(os.path.sep)
  for i in xrange(2, len(parts)):
    partial = os.path.sep.join(parts[:i])
    if os.path.isfile(partial):
      with open(partial) as f:
        link = f.read()
      assert '\n' not in link and link, link
      parts[i-1] = link
  return os.path.normpath(os.path.sep.join(parts))


def yield_swarming_bot_files(root_dir, host, host_version, additionals):
  """"""Yields all the files to map as tuple(filename, content).

  config.json is injected with json data about the server.

  This function guarantees that the output is sorted by filename.
  """"""
  items = {i: None for i in FILES}
  items.update(additionals)
  config = {
    'server': host.rstrip('/'),
    'server_version': host_version,
  }
  items['config/config.json'] = json.dumps(config)
  for item, content in sorted(items.iteritems()):
    if content is not None:
      yield item, content
    else:
      with open(resolve_symlink(os.path.join(root_dir, item)), 'rb') as f:
        yield item, f.read()


def get_swarming_bot_zip(root_dir, host, host_version, additionals):
  """"""Returns a zipped file of all the files a bot needs to run.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: dict(filepath: content) of additional items to put into the zip
        file, in addition to FILES and MAPPED. In practice, it's going to be a
        custom bot_config.py.
  Returns:
    Tuple(str being the zipped file's content, bot version (SHA-1) it
    represents).
  """"""
  zip_memory_file = StringIO.StringIO()
  h = hashlib.sha1()
  with zipfile.ZipFile(zip_memory_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      zip_file.writestr(name, content)
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)

  data = zip_memory_file.getvalue()
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_zip(%s) is %d bytes; %s',
      additionals.keys(), len(data), bot_version)
  return data, bot_version


def get_swarming_bot_version(root_dir, host, host_version, additionals):
  """"""Returns the SHA1 hash of the bot code, representing the version.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: See get_swarming_bot_zip's doc.

  Returns:
    The SHA1 hash of the bot code.
  """"""
  h = hashlib.sha1()
  try:
    # TODO(maruel): Deduplicate from zip_package.genereate_version().
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)
  except IOError:
    logging.warning('Missing expected file. Hash will be invalid.')
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_version(%s) = %s', sorted(additionals), bot_version)
  return bot_version
/n/n/nappengine/swarming/swarming_bot/__main__.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs either task_runner.py, bot_main.py or bot_config.py.

The imports are done late so if an ImportError occurs, it is localized to this
command only.
""""""

import code
import json
import logging
import os
import optparse
import shutil
import sys
import zipfile

# That's from ../../../client/
from third_party.depot_tools import fix_encoding
from utils import logging_utils
from utils import zip_package

# This file can only be run as a zip.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# libusb1 expects to be directly in sys.path.
sys.path.insert(0, os.path.join(THIS_FILE, 'python_libusb1'))

# Copied from //client/utils/oauth.py.
sys.path.insert(0, os.path.join(THIS_FILE, 'third_party'))
sys.path.insert(0, os.path.join(THIS_FILE, 'third_party', 'pyasn1'))
sys.path.insert(0, os.path.join(THIS_FILE, 'third_party', 'rsa'))

from bot_code import common


# TODO(maruel): Use depot_tools/subcommand.py. The goal here is to have all the
# sub commands packed into the single .zip file as a swiss army knife (think
# busybox but worse).


def CMDattributes(_args):
  """"""Prints out the bot's attributes.""""""
  from bot_code import bot_main
  json.dump(
      bot_main.get_attributes(bot_main.get_bot()), sys.stdout, indent=2,
      sort_keys=True, separators=(',', ': '))
  print('')
  return 0


def CMDconfig(_args):
  """"""Prints the config.json embedded in this zip.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  json.dump(bot_main.get_config(), sys.stdout, indent=2, sort_keys=True)
  print('')
  return 0


def CMDis_fine(_args):
  """"""Just reports that the code doesn't throw.

  That ensures that the bot has minimal viability before transfering control to
  it. For now, it just imports bot_main but later it'll check the config, etc.
  """"""
  # pylint: disable=unused-variable
  from bot_code import bot_main
  from config import bot_config
  # We're #goodenough.
  return 0


def CMDrestart(_args):
  """"""Utility subcommand that hides the difference between each OS to reboot
  the host.""""""
  logging_utils.prepare_logging(None)
  import os_utilities
  # This function doesn't return.
  os_utilities.restart()
  # Should never reach here.
  return 1


def CMDrun_isolated(args):
  """"""Internal command to run an isolated command.""""""
  sys.path.insert(0, os.path.join(THIS_FILE, 'client'))
  # run_isolated setups logging by itself.
  import run_isolated
  return run_isolated.main(args)


def CMDsetup(_args):
  """"""Setup the bot to auto-start but doesn't start the bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))
  from bot_code import bot_main
  bot_main.setup_bot(True)
  return 0


def CMDserver(_args):
  """"""Prints the server url. It's like 'config' but easier to parse.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  print bot_main.get_config()['server']
  return 0


def CMDshell(args):
  """"""Starts a shell with api.* in..""""""
  logging_utils.prepare_logging(None)
  logging_utils.set_console_level(logging.DEBUG)

  from bot_code import bot_main
  from api import os_utilities
  from api import platforms
  local_vars = {
    'bot_main': bot_main,
    'json': json,
    'os_utilities': os_utilities,
    'platforms': platforms,
  }
  # Can't use: from api.platforms import *
  local_vars.update(
      (k, v) for k, v in platforms.__dict__.iteritems()
      if not k.startswith('_'))

  if args:
    for arg in args:
      exec code.compile_command(arg) in local_vars
  else:
    code.interact(
        'Locals:\n  ' + '\n  '.join( sorted(local_vars)), None, local_vars)
  return 0


def CMDstart_bot(args):
  """"""Starts the swarming bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'swarming_bot.log'))
  logging.info(
      'importing bot_main: %s, %s', THIS_FILE, zip_package.generate_version())
  from bot_code import bot_main
  result = bot_main.main(args)
  logging.info('bot_main exit code: %d', result)
  return result


def CMDstart_slave(args):
  """"""Ill named command that actually sets up the bot then start it.""""""
  # TODO(maruel): Rename function.
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))

  parser = optparse.OptionParser()
  parser.add_option(
      '--survive', action='store_true',
      help='Do not reboot the host even if bot_config.setup_bot() asked to')
  options, args = parser.parse_args(args)

  try:
    from bot_code import bot_main
    bot_main.setup_bot(options.survive)
  except Exception:
    logging.exception('bot_main.py failed.')

  logging.info('Starting the bot: %s', THIS_FILE)
  return common.exec_python([THIS_FILE, 'start_bot'])


def CMDtask_runner(args):
  """"""Internal command to run a swarming task.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'task_runner.log'))
  from bot_code import task_runner
  return task_runner.main(args)


def CMDversion(_args):
  """"""Prints the version of this file and the hash of the code.""""""
  logging_utils.prepare_logging(None)
  print zip_package.generate_version()
  return 0


def main():
  if os.getenv('CHROME_REMOTE_DESKTOP_SESSION') == '1':
    # Disable itself when run under Google Chrome Remote Desktop, as it's
    # normally started at the console and starting up via Remote Desktop would
    # cause multiple bots to run concurrently on the host.
    print >> sys.stderr, (
        'Inhibiting Swarming bot under Google Chrome Remote Desktop.')
    return 0

  # Always make the current working directory the directory containing this
  # file. It simplifies assumptions.
  os.chdir(os.path.dirname(THIS_FILE))
  # Always create the logs dir first thing, before printing anything out.
  if not os.path.isdir('logs'):
    os.mkdir('logs')

  # This is necessary so os.path.join() works with unicode path. No kidding.
  # This must be done here as each of the command take wildly different code
  # path and this must be run in every case, as it causes really unexpected
  # issues otherwise, especially in module os.path.
  fix_encoding.fix_encoding()

  if os.path.basename(THIS_FILE) == 'swarming_bot.zip':
    # Self-replicate itself right away as swarming_bot.1.zip and restart as it.
    print >> sys.stderr, 'Self replicating pid:%d.' % os.getpid()
    if os.path.isfile('swarming_bot.1.zip'):
      os.remove('swarming_bot.1.zip')
    shutil.copyfile('swarming_bot.zip', 'swarming_bot.1.zip')
    cmd = ['swarming_bot.1.zip'] + sys.argv[1:]
    print >> sys.stderr, 'cmd: %s' % cmd
    return common.exec_python(cmd)

  # sys.argv[0] is the zip file itself.
  cmd = 'start_slave'
  args = []
  if len(sys.argv) > 1:
    cmd = sys.argv[1]
    args = sys.argv[2:]

  fn = getattr(sys.modules[__name__], 'CMD%s' % cmd, None)
  if fn:
    try:
      return fn(args)
    except ImportError:
      logging.exception('Failed to run %s', cmd)
      with zipfile.ZipFile(THIS_FILE, 'r') as f:
        logging.error('Files in %s:\n%s', THIS_FILE, f.namelist())
      return 1

  print >> sys.stderr, 'Unknown command %s' % cmd
  return 1


if __name__ == '__main__':
  sys.exit(main())
/n/n/nappengine/swarming/swarming_bot/api/bot.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Bot interface used in bot_config.py.""""""

import logging
import os
import threading
import time

import os_utilities
from utils import net
from utils import zip_package

THIS_FILE = os.path.abspath(zip_package.get_main_script_path())

# Method could be a function - pylint: disable=R0201


class Bot(object):
  def __init__(
      self, attributes, server, server_version, base_dir, shutdown_hook):
    # Do not expose attributes  for now, as attributes may be refactored.
    assert server is None or not server.endswith('/'), server
    self._attributes = attributes
    self._base_dir = base_dir
    self._server = server
    self._server_version = server_version
    self._shutdown_hook = shutdown_hook
    self._timers = []
    self._timers_dying = False
    self._timers_lock = threading.Lock()

  @property
  def base_dir(self):
    """"""Returns the working directory.

    It is normally the current workind directory, e.g. os.getcwd() but it is
    preferable to not assume that.
    """"""
    return self._base_dir

  @property
  def dimensions(self):
    """"""The bot's current dimensions.

    Dimensions are relatively static and not expected to change much. They
    should change only when it effectively affects the bot's capacity to execute
    tasks.
    """"""
    return self._attributes.get('dimensions', {}).copy()

  @property
  def id(self):
    """"""Returns the bot's ID.""""""
    return self.dimensions.get('id', ['unknown'])[0]

  @property
  def server(self):
    """"""URL of the swarming server this bot is connected to.

    It includes the https:// prefix but without trailing /, so it looks like
    ""https://foo-bar.appspot.com"".
    """"""
    return self._server

  @property
  def server_version(self):
    """"""Version of the server's implementation.

    The form is nnn-hhhhhhh for pristine version and nnn-hhhhhhh-tainted-uuuu
    for non-upstreamed code base:
      nnn: revision pseudo number
      hhhhhhh: git commit hash
      uuuu: username
    """"""
    return self._server_version

  @property
  def state(self):
    return self._attributes['state']

  @property
  def swarming_bot_zip(self):
    """"""Absolute path to the swarming_bot.zip file.

    The bot itself is run as swarming_bot.1.zip or swarming_bot.2.zip. Always
    return swarming_bot.zip since this is the script that must be used when
    starting up.
    """"""
    return os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')

  def post_event(self, event_type, message):
    """"""Posts an event to the server.""""""
    data = self._attributes.copy()
    data['event'] = event_type
    data['message'] = message
    net.url_read_json(self.server + '/swarming/api/v1/bot/event', data=data)

  def post_error(self, message):
    """"""Posts given string as a failure.

    This is used in case of internal code error. It traps exception.
    """"""
    logging.error('Error: %s\n%s', self._attributes, message)
    try:
      self.post_event('bot_error', message)
    except Exception:
      logging.exception('post_error(%s) failed.', message)

  def restart(self, message):
    """"""Reboots the machine.

    If the reboot is successful, never returns: the process should just be
    killed by OS.

    If reboot fails, logs the error to the server and moves the bot to
    quarantined mode.
    """"""
    self.post_event('bot_rebooting', message)
    self.cancel_all_timers()
    if self._shutdown_hook:
      try:
        self._shutdown_hook(self)
      except Exception as e:
        logging.exception('shutdown hook failed: %s', e)
    # os_utilities.restart should never return, unless restart is not happening.
    # If restart is taking longer than N minutes, it probably not going to
    # finish at all. Report this to the server.
    try:
      os_utilities.restart(message, timeout=15*60)
    except LookupError:
      # This is a special case where OSX is deeply hosed. In that case the disk
      # is likely in read-only mode and there isn't much that can be done. This
      # exception is deep inside pickle.py. So notify the server then hang in
      # there.
      self.post_error('This host partition is bad; please fix the host')
      while True:
        time.sleep(1)
    self.post_error('Bot is stuck restarting for: %s' % message)

  def call_later(self, delay_sec, callback):
    """"""Schedules a function to be called later (if bot is still running).

    All calls are executed in a separate internal thread, be careful with what
    you call from there (Bot object is generally not thread safe).

    Multiple callbacks can be executed concurrently. It is safe to call
    'call_later' from the callback.
    """"""
    timer = None

    def call_wrapper():
      with self._timers_lock:
        # Canceled already?
        if timer not in self._timers:
          return
        self._timers.remove(timer)
      try:
        callback()
      except Exception:
        logging.exception('Timer callback failed')

    with self._timers_lock:
      if not self._timers_dying:
        timer = threading.Timer(delay_sec, call_wrapper)
        self._timers.append(timer)
        timer.daemon = True
        timer.start()

  def cancel_all_timers(self):
    """"""Cancels all pending 'call_later' calls and forbids adding new ones.""""""
    timers = None
    with self._timers_lock:
      self._timers_dying = True
      for t in self._timers:
        t.cancel()
      timers, self._timers = self._timers, []
    for t in timers:
      t.join(timeout=5)
      if t.isAlive():
        logging.error('Timer thread did not terminate fast enough: %s', t)

  def update_dimensions(self, new_dimensions):
    """"""Called internally to update Bot.dimensions.""""""
    self._attributes['dimensions'] = new_dimensions

  def update_state(self, new_state):
    """"""Called internally to update Bot.state.""""""
    self._attributes['state'] = new_state
/n/n/nappengine/swarming/swarming_bot/api/bot_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import os
import sys
import unittest
import threading

THIS_FILE = os.path.abspath(__file__)

import test_env_api
test_env_api.setup_test_env()

import bot


class TestBot(unittest.TestCase):
  def test_bot(self):
    obj = bot.Bot(
        {'dimensions': {'foo': 'bar'}},
        'https://localhost:1',
        '1234-1a2b3c4-tainted-joe',
        'base_dir',
        None)
    self.assertEqual({'foo': 'bar'}, obj.dimensions)
    self.assertEqual(
        os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip'),
        obj.swarming_bot_zip)
    self.assertEqual('1234-1a2b3c4-tainted-joe', obj.server_version)
    self.assertEqual('base_dir', obj.base_dir)

  def test_bot_call_later(self):
    obj = bot.Bot({}, 'https://localhost:1', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.001, ev.set)
    self.assertTrue(ev.wait(1))

  def test_bot_call_later_cancel(self):
    obj = bot.Bot({}, 'https://localhost:1', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.1, ev.set)
    obj.cancel_all_timers()
    self.assertFalse(ev.wait(0.3))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/nappengine/swarming/swarming_bot/bot_code/bot_main.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Swarming bot main process.

This is the program that communicates with the Swarming server, ensures the code
is always up to date and executes a child process to run tasks and upload
results back.

It manages self-update and rebooting the host in case of problems.

Set the environment variable SWARMING_LOAD_TEST=1 to disable the use of
server-provided bot_config.py. This permits safe load testing.
""""""

import contextlib
import json
import logging
import optparse
import os
import shutil
import signal
import sys
import tempfile
import threading
import time
import traceback
import zipfile

import common
import singleton
from api import bot
from api import os_utilities
from utils import file_path
from utils import net
from utils import on_error
from utils import subprocess42
from utils import zip_package


# Used to opportunistically set the error handler to notify the server when the
# process exits due to an exception.
_ERROR_HANDLER_WAS_REGISTERED = False


# Set to the zip's name containing this file. This is set to the absolute path
# to swarming_bot.zip when run as part of swarming_bot.zip. This value is
# overriden in unit tests.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# The singleton, initially unset.
SINGLETON = singleton.Singleton(os.path.dirname(THIS_FILE))


### bot_config handler part.


def _in_load_test_mode():
  """"""Returns True if the default values should be used instead of the server
  provided bot_config.py.

  This also disables server telling the bot to restart.
  """"""
  return os.environ.get('SWARMING_LOAD_TEST') == '1'


def get_dimensions(botobj):
  """"""Returns bot_config.py's get_attributes() dict.""""""
  # Importing this administrator provided script could have side-effects on
  # startup. That is why it is imported late.
  try:
    if _in_load_test_mode():
      # Returns a minimal set of dimensions so it doesn't run tasks by error.
      dimensions = os_utilities.get_dimensions()
      return {
        'id': dimensions['id'],
        'load_test': ['1'],
      }

    from config import bot_config
    out = bot_config.get_dimensions(botobj)
    if not isinstance(out, dict):
      raise ValueError('Unexpected type %s' % out.__class__)
    return out
  except Exception as e:
    logging.exception('get_dimensions() failed')
    try:
      out = os_utilities.get_dimensions()
      out['error'] = [str(e)]
      out['quarantined'] = ['1']
      return out
    except Exception as e:
      try:
        botid = os_utilities.get_hostname_short()
      except Exception as e2:
        botid = 'error_%s' % str(e2)
      return {
          'id': [botid],
          'error': ['%s\n%s' % (e, traceback.format_exc()[-2048:])],
          'quarantined': ['1'],
        }


def get_state(botobj, sleep_streak):
  """"""Returns dict with a state of the bot reported to the server with each poll.
  """"""
  try:
    if _in_load_test_mode():
      state = os_utilities.get_state()
      state['dimensions'] = os_utilities.get_dimensions()
    else:
      from config import bot_config
      state = bot_config.get_state(botobj)
      if not isinstance(state, dict):
        state = {'error': state}
  except Exception as e:
    logging.exception('get_state() failed')
    state = {
      'error': '%s\n%s' % (e, traceback.format_exc()[-2048:]),
      'quarantined': True,
    }

  state['sleep_streak'] = sleep_streak
  return state


def call_hook(botobj, name, *args):
  """"""Calls a hook function in bot_config.py.""""""
  try:
    if _in_load_test_mode():
      return

    logging.info('call_hook(%s)', name)
    from config import bot_config
    hook = getattr(bot_config, name, None)
    if hook:
      return hook(botobj, *args)
  except Exception as e:
    msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
    botobj.post_error('Failed to call hook %s(): %s' % (name, msg))


def setup_bot(skip_reboot):
  """"""Calls bot_config.setup_bot() to have the bot self-configure itself.

  Reboot the host if bot_config.setup_bot() returns False, unless skip_reboot is
  also true.
  """"""
  if _in_load_test_mode():
    return

  botobj = get_bot()
  try:
    from config import bot_config
  except Exception as e:
    msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
    botobj.post_error('bot_config.py is bad: %s' % msg)
    return

  try:
    should_continue = bot_config.setup_bot(botobj)
  except Exception as e:
    msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
    botobj.post_error('bot_config.setup_bot() threw: %s' % msg)
    return

  if not should_continue and not skip_reboot:
    botobj.restart('Starting new swarming bot: %s' % THIS_FILE)


### end of bot_config handler part.


def get_min_free_space():
  """"""Returns free disk space needed.

  Add a ""250 MiB slack space"" for logs, temporary files and whatever other leak.
  """"""
  return int((os_utilities.get_min_free_space(THIS_FILE) + 250.) * 1024 * 1024)


def generate_version():
  """"""Returns the bot's code version.""""""
  try:
    return zip_package.generate_version()
  except Exception as e:
    return 'Error: %s' % e


def get_attributes(botobj):
  """"""Returns the attributes sent to the server.

  Each called function catches all exceptions so the bot doesn't die on startup,
  which is annoying to recover. In that case, we set a special property to catch
  these and help the admin fix the swarming_bot code more quickly.

  Arguments:
  - botobj: bot.Bot instance or None
  """"""
  return {
    'dimensions': get_dimensions(botobj),
    'state': get_state(botobj, 0),
    'version': generate_version(),
  }


def post_error_task(botobj, error, task_id):
  """"""Posts given error as failure cause for the task.

  This is used in case of internal code error, and this causes the task to
  become BOT_DIED.

  Arguments:
    botobj: A bot.Bot instance.
    error: String representing the problem.
    task_id: Task that had an internal error. When the Swarming server sends
        commands to a bot, even though they could be completely wrong, the
        server assumes the job as running. Thus this function acts as the
        exception handler for incoming commands from the Swarming server. If for
        any reason the local test runner script can not be run successfully,
        this function is invoked.
  """"""
  logging.error('Error: %s', error)
  data = {
    'id': botobj.id,
    'message': error,
    'task_id': task_id,
  }
  return net.url_read_json(
      botobj.server + '/swarming/api/v1/bot/task_error/%s' % task_id, data=data)


def on_shutdown_hook(b):
  """"""Called when the bot is restarting.""""""
  call_hook(b, 'on_bot_shutdown')
  # Aggressively set itself up so we ensure the auto-reboot configuration is
  # fine before restarting the host. This is important as some tasks delete the
  # autorestart script (!)
  setup_bot(True)


def get_bot():
  """"""Returns a valid Bot instance.

  Should only be called once in the process lifetime.
  """"""
  # This variable is used to bootstrap the initial bot.Bot object, which then is
  # used to get the dimensions and state.
  attributes = {
    'dimensions': {u'id': ['none']},
    'state': {},
    'version': generate_version(),
  }
  config = get_config()
  assert not config['server'].endswith('/'), config

  # Create a temporary object to call the hooks.
  botobj = bot.Bot(
      attributes,
      config['server'],
      config['server_version'],
      os.path.dirname(THIS_FILE),
      on_shutdown_hook)
  return bot.Bot(
      get_attributes(botobj),
      config['server'],
      config['server_version'],
      os.path.dirname(THIS_FILE),
      on_shutdown_hook)


def clean_isolated_cache(botobj):
  """"""Asks run_isolated to clean its cache.

  This may take a while but it ensures that in the case of a run_isolated run
  failed and it temporarily used more space than min_free_disk, it can cleans up
  the mess properly.

  It will remove unexpected files, remove corrupted files, trim the cache size
  based on the policies and update state.json.
  """"""
  bot_dir = botobj.base_dir
  cmd = [
    sys.executable, THIS_FILE, 'run_isolated',
    '--clean',
    '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),
    '--cache', os.path.join(bot_dir, 'cache'),
    '--min-free-space', str(get_min_free_space()),
  ]
  logging.info('Running: %s', cmd)
  try:
    # Intentionally do not use a timeout, it can take a while to hash 50gb but
    # better be safe than sorry.
    proc = subprocess42.Popen(
        cmd,
        stdin=subprocess42.PIPE,
        stdout=subprocess42.PIPE, stderr=subprocess42.STDOUT,
        cwd=bot_dir,
        detached=True,
        close_fds=sys.platform != 'win32')
    output, _ = proc.communicate(None)
    logging.info('Result:\n%s', output)
    if proc.returncode:
      botobj.post_error(
          'swarming_bot.zip failure during run_isolated --clean:\n%s' % output)
  except OSError:
    botobj.post_error(
        'swarming_bot.zip internal failure during run_isolated --clean')


def run_bot(arg_error):
  """"""Runs the bot until it reboots or self-update or a signal is received.

  When a signal is received, simply exit.
  """"""
  quit_bit = threading.Event()
  def handler(sig, _):
    logging.info('Got signal %s', sig)
    quit_bit.set()

  # TODO(maruel): Set quit_bit when stdin is closed on Windows.

  with subprocess42.set_signal_handler(subprocess42.STOP_SIGNALS, handler):
    config = get_config()
    try:
      # First thing is to get an arbitrary url. This also ensures the network is
      # up and running, which is necessary before trying to get the FQDN below.
      resp = net.url_read(config['server'] + '/swarming/api/v1/bot/server_ping')
      if resp is None:
        logging.error('No response from server_ping')
    except Exception as e:
      # url_read() already traps pretty much every exceptions. This except
      # clause is kept there ""just in case"".
      logging.exception('server_ping threw')

    if quit_bit.is_set():
      logging.info('Early quit 1')
      return 0

    # If this fails, there's hardly anything that can be done, the bot can't
    # even get to the point to be able to self-update.
    botobj = get_bot()
    resp = net.url_read_json(
        botobj.server + '/swarming/api/v1/bot/handshake',
        data=botobj._attributes)
    if not resp:
      logging.error('Failed to contact for handshake')
    else:
      logging.info('Connected to %s', resp.get('server_version'))
      if resp.get('bot_version') != botobj._attributes['version']:
        logging.warning(
            'Found out we\'ll need to update: server said %s; we\'re %s',
            resp.get('bot_version'), botobj._attributes['version'])

    if arg_error:
      botobj.post_error('Bootstrapping error: %s' % arg_error)

    if quit_bit.is_set():
      logging.info('Early quit 2')
      return 0

    clean_isolated_cache(botobj)

    call_hook(botobj, 'on_bot_startup')

    if quit_bit.is_set():
      logging.info('Early quit 3')
      return 0

    # This environment variable is accessible to the tasks executed by this bot.
    os.environ['SWARMING_BOT_ID'] = botobj.id.encode('utf-8')

    # Remove the 'work' directory if present, as not removing it may cause the
    # bot to stay quarantined and not be able to get out of this state.
    work_dir = os.path.join(botobj.base_dir, 'work')
    try:
      if os.path.isdir(work_dir):
        file_path.rmtree(work_dir)
    except Exception as e:
      botobj.post_error('Failed to remove work: %s' % e)

    consecutive_sleeps = 0
    while not quit_bit.is_set():
      try:
        botobj.update_dimensions(get_dimensions(botobj))
        botobj.update_state(get_state(botobj, consecutive_sleeps))
        did_something = poll_server(botobj, quit_bit)
        if did_something:
          consecutive_sleeps = 0
        else:
          consecutive_sleeps += 1
      except Exception as e:
        logging.exception('poll_server failed')
        msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
        botobj.post_error(msg)
        consecutive_sleeps = 0
    logging.info('Quitting')

  # Tell the server we are going away.
  botobj.post_event('bot_shutdown', 'Signal was received')
  botobj.cancel_all_timers()
  return 0


def poll_server(botobj, quit_bit):
  """"""Polls the server to run one loop.

  Returns True if executed some action, False if server asked the bot to sleep.
  """"""
  # Access to a protected member _XXX of a client class - pylint: disable=W0212
  start = time.time()
  resp = net.url_read_json(
     botobj.server + '/swarming/api/v1/bot/poll', data=botobj._attributes)
  if not resp:
    return False
  logging.debug('Server response:\n%s', resp)

  cmd = resp['cmd']
  if cmd == 'sleep':
    quit_bit.wait(resp['duration'])
    return False

  if cmd == 'terminate':
    quit_bit.set()
    # This is similar to post_update() in task_runner.py.
    params = {
      'cost_usd': 0,
      'duration': 0,
      'exit_code': 0,
      'hard_timeout': False,
      'id': botobj.id,
      'io_timeout': False,
      'output': '',
      'output_chunk_start': 0,
      'task_id': resp['task_id'],
    }
    net.url_read_json(
        botobj.server + '/swarming/api/v1/bot/task_update/%s' % resp['task_id'],
        data=params)
    return False

  if cmd == 'run':
    if run_manifest(botobj, resp['manifest'], start):
      # Completed a task successfully so update swarming_bot.zip if necessary.
      update_lkgbc(botobj)
    # TODO(maruel): Handle the case where quit_bit.is_set() happens here. This
    # is concerning as this means a signal (often SIGTERM) was received while
    # running the task. Make sure the host is properly restarting.
  elif cmd == 'update':
    update_bot(botobj, resp['version'])
  elif cmd == 'restart':
    if _in_load_test_mode():
      logging.warning('Would have restarted: %s' % resp['message'])
    else:
      botobj.restart(resp['message'])
  else:
    raise ValueError('Unexpected command: %s\n%s' % (cmd, resp))

  return True


def run_manifest(botobj, manifest, start):
  """"""Defers to task_runner.py.

  Return True if the task succeeded.
  """"""
  # Ensure the manifest is valid. This can throw a json decoding error. Also
  # raise if it is empty.
  if not manifest:
    raise ValueError('Empty manifest')

  # Necessary to signal an internal_failure. This occurs when task_runner fails
  # to execute the command. It is important to note that this data is extracted
  # before any I/O is done, like writting the manifest to disk.
  task_id = manifest['task_id']
  hard_timeout = manifest['hard_timeout'] or None
  # Default the grace period to 30s here, this doesn't affect the grace period
  # for the actual task.
  grace_period = manifest['grace_period'] or 30
  if manifest['hard_timeout']:
    # One for the child process, one for run_isolated, one for task_runner.
    hard_timeout += 3 * manifest['grace_period']
    # For isolated task, download time is not counted for hard timeout so add
    # more time.
    if not manifest['command']:
      hard_timeout += manifest['io_timeout'] or 600

  url = manifest.get('host', botobj.server)
  task_dimensions = manifest['dimensions']
  task_result = {}

  failure = False
  internal_failure = False
  msg = None
  work_dir = os.path.join(botobj.base_dir, 'work')
  try:
    try:
      if os.path.isdir(work_dir):
        file_path.rmtree(work_dir)
    except OSError:
      # If a previous task created an undeleteable file/directory inside 'work',
      # make sure that following tasks are not affected. This is done by working
      # around the undeleteable directory by creating a temporary directory
      # instead. This is not normal behavior. The bot will report a failure on
      # start.
      work_dir = tempfile.mkdtemp(dir=botobj.base_dir, prefix='work')
    else:
      os.makedirs(work_dir)

    env = os.environ.copy()
    # Windows in particular does not tolerate unicode strings in environment
    # variables.
    env['SWARMING_TASK_ID'] = task_id.encode('ascii')

    task_in_file = os.path.join(work_dir, 'task_runner_in.json')
    with open(task_in_file, 'wb') as f:
      f.write(json.dumps(manifest))
    call_hook(botobj, 'on_before_task')
    task_result_file = os.path.join(work_dir, 'task_runner_out.json')
    if os.path.exists(task_result_file):
      os.remove(task_result_file)
    command = [
      sys.executable, THIS_FILE, 'task_runner',
      '--swarming-server', url,
      '--in-file', task_in_file,
      '--out-file', task_result_file,
      '--cost-usd-hour', str(botobj.state.get('cost_usd_hour') or 0.),
      # Include the time taken to poll the task in the cost.
      '--start', str(start),
      '--min-free-space', str(get_min_free_space()),
    ]
    logging.debug('Running command: %s', command)
    # Put the output file into the current working directory, which should be
    # the one containing swarming_bot.zip.
    log_path = os.path.join(botobj.base_dir, 'logs', 'task_runner_stdout.log')
    os_utilities.roll_log(log_path)
    os_utilities.trim_rolled_log(log_path)
    with open(log_path, 'a+b') as f:
      proc = subprocess42.Popen(
          command,
          detached=True,
          cwd=botobj.base_dir,
          env=env,
          stdin=subprocess42.PIPE,
          stdout=f,
          stderr=subprocess42.STDOUT,
          close_fds=sys.platform != 'win32')
      try:
        proc.wait(hard_timeout)
      except subprocess42.TimeoutExpired:
        # That's the last ditch effort; as task_runner should have completed a
        # while ago and had enforced the timeout itself (or run_isolated for
        # hard_timeout for isolated task).
        logging.error('Sending SIGTERM to task_runner')
        proc.terminate()
        internal_failure = True
        msg = 'task_runner hung'
        try:
          proc.wait(grace_period)
        except subprocess42.TimeoutExpired:
          logging.error('Sending SIGKILL to task_runner')
          proc.kill()
        proc.wait()
        return False

    logging.info('task_runner exit: %d', proc.returncode)
    if os.path.exists(task_result_file):
      with open(task_result_file, 'rb') as fd:
        task_result = json.load(fd)

    if proc.returncode:
      msg = 'Execution failed: internal error (%d).' % proc.returncode
      internal_failure = True
    elif not task_result:
      logging.warning('task_runner failed to write metadata')
      msg = 'Execution failed: internal error (no metadata).'
      internal_failure = True
    elif task_result[u'must_signal_internal_failure']:
      msg = (
        'Execution failed: %s' % task_result[u'must_signal_internal_failure'])
      internal_failure = True

    failure = bool(task_result.get('exit_code')) if task_result else False
    return not internal_failure and not failure
  except Exception as e:
    # Failures include IOError when writing if the disk is full, OSError if
    # swarming_bot.zip doesn't exist anymore, etc.
    logging.exception('run_manifest failed')
    msg = 'Internal exception occured: %s\n%s' % (
        e, traceback.format_exc()[-2048:])
    internal_failure = True
  finally:
    if internal_failure:
      post_error_task(botobj, msg, task_id)
    call_hook(
        botobj, 'on_after_task', failure, internal_failure, task_dimensions,
        task_result)
    if os.path.isdir(work_dir):
      try:
        file_path.rmtree(work_dir)
      except Exception as e:
        botobj.post_error(
            'Failed to delete work directory %s: %s' % (work_dir, e))


def update_bot(botobj, version):
  """"""Downloads the new version of the bot code and then runs it.

  Use alternating files; first load swarming_bot.1.zip, then swarming_bot.2.zip,
  never touching swarming_bot.zip which was the originally bootstrapped file.

  LKGBC is handled by update_lkgbc().

  Does not return.
  """"""
  # Alternate between .1.zip and .2.zip.
  new_zip = 'swarming_bot.1.zip'
  if os.path.basename(THIS_FILE) == new_zip:
    new_zip = 'swarming_bot.2.zip'
  new_zip = os.path.join(os.path.dirname(THIS_FILE), new_zip)

  # Download as a new file.
  url = botobj.server + '/swarming/api/v1/bot/bot_code/%s' % version
  if not net.url_retrieve(new_zip, url):
    # It can happen when a server is rapidly updated multiple times in a row.
    botobj.post_error(
        'Unable to download %s from %s; first tried version %s' %
        (new_zip, url, version))
    # Poll again, this may work next time. To prevent busy-loop, sleep a little.
    time.sleep(2)
    return

  s = os.stat(new_zip)
  logging.info('Restarting to %s; %d bytes.', new_zip, s.st_size)
  sys.stdout.flush()
  sys.stderr.flush()

  proc = subprocess42.Popen(
     [sys.executable, new_zip, 'is_fine'],
     stdout=subprocess42.PIPE, stderr=subprocess42.STDOUT)
  output, _ = proc.communicate()
  if proc.returncode:
    botobj.post_error(
        'New bot code is bad: proc exit = %s. stdout:\n%s' %
        (proc.returncode, output))
    # Poll again, the server may have better code next time. To prevent
    # busy-loop, sleep a little.
    time.sleep(2)
    return

  # Don't forget to release the singleton before restarting itself.
  SINGLETON.release()

  # Do not call on_bot_shutdown.
  # On OSX, launchd will be unhappy if we quit so the old code bot process has
  # to outlive the new code child process. Launchd really wants the main process
  # to survive, and it'll restart it if it disappears. os.exec*() replaces the
  # process so this is fine.
  ret = common.exec_python([new_zip, 'start_slave', '--survive'])
  if ret in (1073807364, -1073741510):
    # 1073807364 is returned when the process is killed due to shutdown. No need
    # to alert anyone in that case.
    # -1073741510 is returned when rebooting too. This can happen when the
    # parent code was running the old version and gets confused and decided to
    # poll again.
    # In any case, zap out the error code.
    ret = 0
  elif ret:
    botobj.post_error('Bot failed to respawn after update: %s' % ret)
  sys.exit(ret)


def update_lkgbc(botobj):
  """"""Updates the Last Known Good Bot Code if necessary.""""""
  try:
    if not os.path.isfile(THIS_FILE):
      botobj.post_error('Missing file %s for LKGBC' % THIS_FILE)
      return

    golden = os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')
    if os.path.isfile(golden):
      org = os.stat(golden)
      cur = os.stat(THIS_FILE)
      if org.st_size == org.st_size and org.st_mtime >= cur.st_mtime:
        return

    # Copy the file back.
    shutil.copy(THIS_FILE, golden)
  except Exception as e:
    botobj.post_error('Failed to update LKGBC: %s' % e)


def get_config():
  """"""Returns the data from config.json.""""""
  global _ERROR_HANDLER_WAS_REGISTERED

  with contextlib.closing(zipfile.ZipFile(THIS_FILE, 'r')) as f:
    config = json.load(f.open('config/config.json', 'r'))

  server = config.get('server', '')
  if not _ERROR_HANDLER_WAS_REGISTERED and server:
    on_error.report_on_exception_exit(server)
    _ERROR_HANDLER_WAS_REGISTERED = True
  return config


def main(args):
  # Add SWARMING_HEADLESS into environ so subcommands know that they are running
  # in a headless (non-interactive) mode.
  os.environ['SWARMING_HEADLESS'] = '1'

  # The only reason this is kept is to enable the unit test to use --help to
  # quit the process.
  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)
  _, args = parser.parse_args(args)

  # Enforces that only one process with a bot in this directory can be run on
  # this host at once.
  if not SINGLETON.acquire():
    if sys.platform == 'darwin':
      msg = (
          'Found a previous bot, %d rebooting as a workaround for '
          'https://crbug.com/569610.') % os.getpid()
      print >> sys.stderr, msg
      os_utilities.restart(msg)
    else:
      print >> sys.stderr, 'Found a previous bot, %d exiting.' % os.getpid()
    return 1

  for t in ('out', 'err'):
    log_path = os.path.join(
        os.path.dirname(THIS_FILE), 'logs', 'bot_std%s.log' % t)
    os_utilities.roll_log(log_path)
    os_utilities.trim_rolled_log(log_path)

  error = None
  if len(args) != 0:
    error = 'Unexpected arguments: %s' % args
  try:
    return run_bot(error)
  finally:
    call_hook(bot.Bot(None, None, None, os.path.dirname(THIS_FILE), None),
              'on_bot_shutdown')
    logging.info('main() returning')
/n/n/nappengine/swarming/swarming_bot/bot_code/bot_main_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import json
import logging
import os
import shutil
import sys
import tempfile
import threading
import time
import unittest
import zipfile

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import bot_main
from api import bot
from api import os_utilities
from depot_tools import fix_encoding
from utils import file_path
from utils import logging_utils
from utils import net
from utils import subprocess42
from utils import zip_package


# Access to a protected member XX of a client class - pylint: disable=W0212


class TestBotMain(net_utils.TestCase):
  maxDiff = 2000

  def setUp(self):
    super(TestBotMain, self).setUp()
    os.environ.pop('SWARMING_LOAD_TEST', None)
    self.root_dir = tempfile.mkdtemp(prefix='bot_main')
    self.old_cwd = os.getcwd()
    os.chdir(self.root_dir)
    # __main__ does it for us.
    os.mkdir('logs')
    self.url = 'https://localhost:1'
    self.attributes = {
      'dimensions': {
        'foo': ['bar'],
        'id': ['localhost'],
        'pool': ['default'],
      },
      'state': {
        'cost_usd_hour': 3600.,
        'sleep_streak': 0,
      },
      'version': '123',
    }
    self.mock(zip_package, 'generate_version', lambda: '123')
    self.bot = bot.Bot(
        self.attributes, 'https://localhost:1', 'version1', self.root_dir,
        self.fail)
    self.mock(self.bot, 'post_error', self.fail)
    self.mock(self.bot, 'restart', self.fail)
    self.mock(subprocess42, 'call', self.fail)
    self.mock(time, 'time', lambda: 100.)
    config_path = os.path.join(
        test_env_bot_code.BOT_DIR, 'config', 'config.json')
    with open(config_path, 'rb') as f:
      config = json.load(f)
    self.mock(bot_main, 'get_config', lambda: config)
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(test_env_bot_code.BOT_DIR, 'swarming_bot.zip'))

  def tearDown(self):
    os.environ.pop('SWARMING_BOT_ID', None)
    os.chdir(self.old_cwd)
    file_path.rmtree(self.root_dir)
    super(TestBotMain, self).tearDown()

  def test_get_dimensions(self):
    dimensions = set(bot_main.get_dimensions(None))
    dimensions.discard('hidpi')
    dimensions.discard('zone')  # Only set on GCE bots.
    expected = {'cores', 'cpu', 'gpu', 'id', 'machine_type', 'os', 'pool'}
    self.assertEqual(expected, dimensions)

  def test_get_dimensions_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    self.assertEqual(['id', 'load_test'], sorted(bot_main.get_dimensions(None)))

  def test_generate_version(self):
    self.assertEqual('123', bot_main.generate_version())

  def test_get_state(self):
    self.mock(time, 'time', lambda: 126.0)
    expected = os_utilities.get_state()
    expected['sleep_streak'] = 12
    # During the execution of this test case, the free disk space could have
    # changed.
    for disk in expected['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    actual = bot_main.get_state(None, 12)
    for disk in actual['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    self.assertGreater(actual.pop('nb_files_in_temp'), 0)
    self.assertGreater(expected.pop('nb_files_in_temp'), 0)
    self.assertGreater(actual.pop('uptime'), 0)
    self.assertGreater(expected.pop('uptime'), 0)
    self.assertEqual(sorted(expected.pop('temp', {})),
                     sorted(actual.pop('temp', {})))
    self.assertEqual(expected, actual)

  def test_setup_bot(self):
    setup_bots = []
    def setup_bot(_bot):
      setup_bots.append(1)
      return False
    from config import bot_config
    self.mock(bot_config, 'setup_bot', setup_bot)
    restarts = []
    post_event = []
    self.mock(
        os_utilities, 'restart', lambda *a, **kw: restarts.append((a, kw)))
    self.mock(
        bot.Bot, 'post_event', lambda *a, **kw: post_event.append((a, kw)))
    self.expected_requests([])
    bot_main.setup_bot(False)
    expected = [
      (('Starting new swarming bot: %s' % bot_main.THIS_FILE,),
        {'timeout': 900}),
    ]
    self.assertEqual(expected, restarts)
    # It is called twice, one as part of setup_bot(False), another as part of
    # on_shutdown_hook().
    self.assertEqual([1, 1], setup_bots)
    expected = [
      'Starting new swarming bot: %s' % bot_main.THIS_FILE,
      'Bot is stuck restarting for: Starting new swarming bot: %s' %
        bot_main.THIS_FILE,
    ]
    self.assertEqual(expected, [i[0][2] for i in post_event])

  def test_post_error_task(self):
    self.mock(time, 'time', lambda: 126.0)
    self.mock(logging, 'error', lambda *_, **_kw: None)
    self.mock(
        bot_main, 'get_config',
        lambda: {'server': self.url, 'server_version': '1'})
    expected_attribs = bot_main.get_attributes(None)
    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/task_error/23',
            {
              'data': {
                'id': expected_attribs['dimensions']['id'][0],
                'message': 'error',
                'task_id': 23,
              },
            },
            {'resp': 1},
          ),
        ])
    botobj = bot_main.get_bot()
    self.assertEqual({'resp': 1}, bot_main.post_error_task(botobj, 'error', 23))

  def test_run_bot(self):
    # Test the run_bot() loop. Does not use self.bot.
    self.mock(time, 'time', lambda: 126.0)
    class Foo(Exception):
      pass

    def poll_server(botobj, _):
      sleep_streak = botobj.state['sleep_streak']
      self.assertEqual(self.url, botobj.server)
      if sleep_streak == 5:
        raise Exception('Jumping out of the loop')
      return False
    self.mock(bot_main, 'poll_server', poll_server)

    def post_error(botobj, e):
      self.assertEqual(self.url, botobj.server)
      lines = e.splitlines()
      self.assertEqual('Jumping out of the loop', lines[0])
      self.assertEqual('Traceback (most recent call last):', lines[1])
      raise Foo('Necessary to get out of the loop')
    self.mock(bot.Bot, 'post_error', post_error)

    self.mock(
        bot_main, 'get_config',
        lambda: {'server': self.url, 'server_version': '1'})
    self.mock(
        bot_main, 'get_dimensions', lambda _: self.attributes['dimensions'])
    self.mock(os_utilities, 'get_state', lambda *_: self.attributes['state'])

    # Method should have ""self"" as first argument - pylint: disable=E0213
    # pylint: disable=unused-argument
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        expected = [sys.executable, bot_main.THIS_FILE, 'run_isolated']
        self.assertEqual(expected, cmd[:len(expected)])
        self.assertEqual(True, detached)
        self.assertEqual(subprocess42.PIPE, stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def communicate(self2, i):
        self.assertEqual(None, i)
        self2.returncode = 0
        return '', None
    self.mock(subprocess42, 'Popen', Popen)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/server_ping',
            {}, 'foo', None,
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/handshake',
            {'data': self.attributes},
            {'bot_version': '123', 'server': self.url, 'server_version': 1},
          ),
        ])

    with self.assertRaises(Foo):
      bot_main.run_bot(None)
    self.assertEqual(
        self.attributes['dimensions']['id'][0], os.environ['SWARMING_BOT_ID'])

  def test_poll_server_sleep(self):
    slept = []
    bit = threading.Event()
    self.mock(bit, 'wait', slept.append)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.attributes},
            {
              'cmd': 'sleep',
              'duration': 1.24,
            },
          ),
        ])
    self.assertFalse(bot_main.poll_server(self.bot, bit))
    self.assertEqual([1.24], slept)

  def test_poll_server_run(self):
    manifest = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', lambda *args: manifest.append(args))
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.bot._attributes},
            {
              'cmd': 'run',
              'manifest': {'foo': 'bar'},
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    expected = [(self.bot, {'foo': 'bar'}, time.time())]
    self.assertEqual(expected, manifest)

  def test_poll_server_update(self):
    update = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', lambda *args: update.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.attributes},
            {
              'cmd': 'update',
              'version': '123',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([(self.bot, '123')], update)

  def test_poll_server_restart(self):
    restart = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', lambda *args: restart.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.attributes},
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([('Please die now',)], restart)

  def test_poll_server_restart_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
            },
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))

  def _mock_popen(self, returncode=0, exit_code=0, url='https://localhost:1'):
    result = {
      'exit_code': exit_code,
      'must_signal_internal_failure': None,
      'version': 3,
    }
    # Method should have ""self"" as first argument - pylint: disable=E0213
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, env, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        self2._out_file = os.path.join(
            self.root_dir, 'work', 'task_runner_out.json')
        expected = [
          sys.executable, bot_main.THIS_FILE, 'task_runner',
          '--swarming-server', url,
          '--in-file',
          os.path.join(self.root_dir, 'work', 'task_runner_in.json'),
          '--out-file', self2._out_file,
          '--cost-usd-hour', '3600.0', '--start', '100.0',
          '--min-free-space',
          str(int(
            (os_utilities.get_min_free_space(bot_main.THIS_FILE) + 250.) *
            1024 * 1024)),
        ]
        self.assertEqual(expected, cmd)
        self.assertEqual(True, detached)
        self.assertEqual(self.bot.base_dir, cwd)
        self.assertEqual('24', env['SWARMING_TASK_ID'])
        self.assertTrue(stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def wait(self2, timeout=None): # pylint: disable=unused-argument
        self2.returncode = returncode
        with open(self2._out_file, 'wb') as f:
          json.dump(result, f)
        return 0

    self.mock(subprocess42, 'Popen', Popen)
    return result

  def test_run_manifest(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(self.attributes['dimensions'], botobj.dimensions)
        self.assertEqual(False, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'os': 'Amiga', 'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(url='https://localhost:3')

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'os': 'Amiga', 'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'host': 'https://localhost:3',
      'task_id': '24',
    }
    self.assertEqual(self.root_dir, self.bot.base_dir)
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_task_failure(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(True, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(exit_code=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_internal_failure(self):
    posted = []
    self.mock(bot_main, 'post_error_task', lambda *args: posted.append(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(returncode=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Execution failed: internal error (1).', '24')]
    self.assertEqual(expected, posted)

  def test_run_manifest_exception(self):
    posted = []
    def post_error_task(botobj, msg, task_id):
      posted.append((botobj, msg.splitlines()[0], task_id))
    self.mock(bot_main, 'post_error_task', post_error_task)
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual({}, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    def raiseOSError(*_a, **_k):
      raise OSError('Dang')
    self.mock(subprocess42, 'Popen', raiseOSError)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Internal exception occured: Dang', '24')]
    self.assertEqual(expected, posted)

  def test_update_bot(self):
    # In a real case 'update_bot' never exits and doesn't call 'post_error'.
    # Under the test however forever-blocking calls finish, and post_error is
    # called.
    self.mock(self.bot, 'post_error', lambda *_: None)
    # Mock the file to download in the temporary directory.
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(self.root_dir, 'swarming_bot.1.zip'))
    new_zip = os.path.join(self.root_dir, 'swarming_bot.2.zip')
    # This is necessary otherwise zipfile will crash.
    self.mock(time, 'time', lambda: 1400000000)
    def url_retrieve(f, url):
      self.assertEqual(
          'https://localhost:1/swarming/api/v1/bot/bot_code/123', url)
      self.assertEqual(new_zip, f)
      # Create a valid zip that runs properly.
      with zipfile.ZipFile(f, 'w') as z:
        z.writestr('__main__.py', 'print(""hi"")')
      return True
    self.mock(net, 'url_retrieve', url_retrieve)

    calls = []
    def exec_python(args):
      calls.append(args)
      return 23
    self.mock(bot_main.common, 'exec_python', exec_python)

    with self.assertRaises(SystemExit) as e:
      bot_main.update_bot(self.bot, '123')
    self.assertEqual(23, e.exception.code)

    self.assertEqual([[new_zip, 'start_slave', '--survive']], calls)

  def test_main(self):
    def check(x):
      self.assertEqual(logging.WARNING, x)
    self.mock(logging_utils, 'set_console_level', check)

    def run_bot(error):
      self.assertEqual(None, error)
      return 0
    self.mock(bot_main, 'run_bot', run_bot)

    class Singleton(object):
      # pylint: disable=no-self-argument
      def acquire(self2):
        return True
      def release(self2):
        self.fail()
    self.mock(bot_main, 'SINGLETON', Singleton())

    self.assertEqual(0, bot_main.main([]))


if __name__ == '__main__':
  fix_encoding.fix_encoding()
  if '-v' in sys.argv:
    TestBotMain.maxDiff = None
  logging.basicConfig(
      level=logging.DEBUG if '-v' in sys.argv else logging.CRITICAL)
  unittest.main()
/n/n/nappengine/swarming/swarming_bot/bot_code/task_runner.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs a Swarming task.

Downloads all the necessary files to run the task, executes the command and
streams results back to the Swarming server.

The process exit code is 0 when the task was executed, even if the task itself
failed. If there's any failure in the setup or teardown, like invalid packet
response, failure to contact the server, etc, a non zero exit code is used. It's
up to the calling process (bot_main.py) to signal that there was an internal
failure and to cancel this task run and ask the server to retry it.
""""""

import base64
import json
import logging
import optparse
import os
import signal
import sys
import time

from utils import net
from utils import on_error
from utils import subprocess42
from utils import zip_package


# Path to this file or the zip containing this file.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# Sends a maximum of 100kb of stdout per task_update packet.
MAX_CHUNK_SIZE = 102400


# Maximum wait between task_update packet when there's no output.
MAX_PACKET_INTERVAL = 30


# Minimum wait between task_update packet when there's output.
MIN_PACKET_INTERNAL = 10


# Current task_runner_out version.
OUT_VERSION = 3


# On Windows, SIGTERM is actually sent as SIGBREAK since there's no real
# SIGTERM.  SIGBREAK is not defined on posix since it's a pure Windows concept.
SIG_BREAK_OR_TERM = (
    signal.SIGBREAK if sys.platform == 'win32' else signal.SIGTERM)


# Used to implement monotonic_time for a clock that never goes backward.
_last_now = 0


def monotonic_time():
  """"""Returns monotonically increasing time.""""""
  global _last_now
  now = time.time()
  if now > _last_now:
    # TODO(maruel): If delta is large, probably worth alerting via ereporter2.
    _last_now = now
  return _last_now


def get_run_isolated():
  """"""Returns the path to itself to run run_isolated.

  Mocked in test to point to the real run_isolated.py script.
  """"""
  return [sys.executable, THIS_FILE, 'run_isolated']


def get_isolated_cmd(
    work_dir, task_details, isolated_result, min_free_space):
  """"""Returns the command to call run_isolated. Mocked in tests.""""""
  bot_dir = os.path.dirname(work_dir)
  if os.path.isfile(isolated_result):
    os.remove(isolated_result)
  cmd = get_run_isolated()
  cmd.extend(
      [
        '--isolated', task_details.inputs_ref['isolated'].encode('utf-8'),
        '--namespace', task_details.inputs_ref['namespace'].encode('utf-8'),
        '-I', task_details.inputs_ref['isolatedserver'].encode('utf-8'),
        '--json', isolated_result,
        '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),
        '--cache', os.path.join(bot_dir, 'cache'),
        '--root-dir', os.path.join(work_dir, 'isolated'),
      ])
  if min_free_space:
    cmd.extend(('--min-free-space', str(min_free_space)))

  if task_details.hard_timeout:
    cmd.extend(('--hard-timeout', str(task_details.hard_timeout)))
  if task_details.grace_period:
    cmd.extend(('--grace-period', str(task_details.grace_period)))
  if task_details.extra_args:
    cmd.append('--')
    cmd.extend(task_details.extra_args)
  return cmd


class TaskDetails(object):
  def __init__(self, data):
    """"""Loads the raw data.

    It is expected to have at least:
     - bot_id
     - command as a list of str
     - data as a list of urls
     - env as a dict
     - hard_timeout
     - io_timeout
     - task_id
    """"""
    logging.info('TaskDetails(%s)', data)
    if not isinstance(data, dict):
      raise ValueError('Expected dict, got %r' % data)

    # Get all the data first so it fails early if the task details is invalid.
    self.bot_id = data['bot_id']

    # Raw command. Only self.command or self.inputs_ref can be set.
    self.command = data['command'] or []

    # Isolated command. Is a serialized version of task_request.FilesRef.
    self.inputs_ref = data['inputs_ref']
    self.extra_args = data['extra_args']

    self.env = {
      k.encode('utf-8'): v.encode('utf-8') for k, v in data['env'].iteritems()
    }
    self.grace_period = data['grace_period']
    self.hard_timeout = data['hard_timeout']
    self.io_timeout = data['io_timeout']
    self.task_id = data['task_id']


class MustExit(Exception):
  """"""Raised on signal that the process must exit immediately.""""""
  def __init__(self, sig):
    super(MustExit, self).__init__()
    self.signal = sig


def load_and_run(
    in_file, swarming_server, cost_usd_hour, start, out_file, min_free_space):
  """"""Loads the task's metadata and execute it.

  This may throw all sorts of exceptions in case of failure. It's up to the
  caller to trap them. These shall be considered 'internal_failure' instead of
  'failure' from a TaskRunResult standpoint.
  """"""
  # The work directory is guaranteed to exist since it was created by
  # bot_main.py and contains the manifest. Temporary files will be downloaded
  # there. It's bot_main.py that will delete the directory afterward. Tests are
  # not run from there.
  task_result = None
  def handler(sig, _):
    logging.info('Got signal %s', sig)
    raise MustExit(sig)
  work_dir = os.path.dirname(out_file)
  try:
    with subprocess42.set_signal_handler([SIG_BREAK_OR_TERM], handler):
      if not os.path.isdir(work_dir):
        raise ValueError('%s expected to exist' % work_dir)

      with open(in_file, 'rb') as f:
        task_details = TaskDetails(json.load(f))

      task_result = run_command(
          swarming_server, task_details, work_dir, cost_usd_hour, start,
          min_free_space)
  except MustExit as e:
    # This normally means run_command() didn't get the chance to run, as it
    # itself trap MustExit and will report accordingly. In this case, we want
    # the parent process to send the message instead.
    if not task_result:
      task_result = {
        u'exit_code': None,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure':
            u'task_runner received signal %s' % e.signal,
        u'version': OUT_VERSION,
      }
  finally:
    # We've found tests to delete 'work' when quitting, causing an exception
    # here. Try to recreate the directory if necessary.
    if not os.path.isdir(work_dir):
      os.mkdir(work_dir)
    with open(out_file, 'wb') as f:
      json.dump(task_result, f)


def post_update(swarming_server, params, exit_code, stdout, output_chunk_start):
  """"""Posts task update to task_update.

  Arguments:
    swarming_server: Base URL to Swarming server.
    params: Default JSON parameters for the POST.
    exit_code: Process exit code, only when a command completed.
    stdout: Incremental output since last call, if any.
    output_chunk_start: Total number of stdout previously sent, for coherency
        with the server.
  """"""
  params = params.copy()
  if exit_code is not None:
    params['exit_code'] = exit_code
  if stdout:
    # The output_chunk_start is used by the server to make sure that the stdout
    # chunks are processed and saved in the DB in order.
    params['output'] = base64.b64encode(stdout)
    params['output_chunk_start'] = output_chunk_start
  # TODO(maruel): Support early cancellation.
  # https://code.google.com/p/swarming/issues/detail?id=62
  resp = net.url_read_json(
      swarming_server+'/swarming/api/v1/bot/task_update/%s' % params['task_id'],
      data=params)
  logging.debug('post_update() = %s', resp)
  if resp.get('error'):
    # Abandon it. This will force a process exit.
    raise ValueError(resp.get('error'))


def should_post_update(stdout, now, last_packet):
  """"""Returns True if it's time to send a task_update packet via post_update().

  Sends a packet when one of this condition is met:
  - more than MAX_CHUNK_SIZE of stdout is buffered.
  - last packet was sent more than MIN_PACKET_INTERNAL seconds ago and there was
    stdout.
  - last packet was sent more than MAX_PACKET_INTERVAL seconds ago.
  """"""
  packet_interval = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  return len(stdout) >= MAX_CHUNK_SIZE or (now - last_packet) > packet_interval


def calc_yield_wait(task_details, start, last_io, timed_out, stdout):
  """"""Calculates the maximum number of seconds to wait in yield_any().""""""
  now = monotonic_time()
  if timed_out:
    # Give a |grace_period| seconds delay.
    if task_details.grace_period:
      return max(now - timed_out - task_details.grace_period, 0.)
    return 0.

  out = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  if task_details.hard_timeout:
    out = min(out, start + task_details.hard_timeout - now)
  if task_details.io_timeout:
    out = min(out, last_io + task_details.io_timeout - now)
  out = max(out, 0)
  logging.debug('calc_yield_wait() = %d', out)
  return out


def kill_and_wait(proc, grace_period, reason):
  logging.warning('SIGTERM finally due to %s', reason)
  proc.terminate()
  try:
    proc.wait(grace_period)
  except subprocess42.TimeoutError:
    logging.warning('SIGKILL finally due to %s', reason)
    proc.kill()
  exit_code = proc.wait()
  logging.info('Waiting for proces exit in finally - done')
  return exit_code


def run_command(
    swarming_server, task_details, work_dir, cost_usd_hour, task_start,
    min_free_space):
  """"""Runs a command and sends packets to the server to stream results back.

  Implements both I/O and hard timeouts. Sends the packets numbered, so the
  server can ensure they are processed in order.

  Returns:
    Metadata about the command.
  """"""
  # TODO(maruel): This function is incomprehensible, split and refactor.
  # Signal the command is about to be started.
  last_packet = start = now = monotonic_time()
  params = {
    'cost_usd': cost_usd_hour * (now - task_start) / 60. / 60.,
    'id': task_details.bot_id,
    'task_id': task_details.task_id,
  }
  post_update(swarming_server, params, None, '', 0)

  if task_details.command:
    # Raw command.
    cmd = task_details.command
    isolated_result = None
  else:
    # Isolated task.
    isolated_result = os.path.join(work_dir, 'isolated_result.json')
    cmd = get_isolated_cmd(
        work_dir, task_details, isolated_result, min_free_space)
    # Hard timeout enforcement is deferred to run_isolated. Grace is doubled to
    # give one 'grace_period' slot to the child process and one slot to upload
    # the results back.
    task_details.hard_timeout = 0
    if task_details.grace_period:
      task_details.grace_period *= 2

  try:
    # TODO(maruel): Support both channels independently and display stderr in
    # red.
    env = None
    if task_details.env:
      env = os.environ.copy()
      for key, value in task_details.env.iteritems():
        if not value:
          env.pop(key, None)
        else:
          env[key] = value
    logging.info('cmd=%s', cmd)
    logging.info('env=%s', env)
    try:
      proc = subprocess42.Popen(
          cmd,
          env=env,
          cwd=work_dir,
          detached=True,
          stdout=subprocess42.PIPE,
          stderr=subprocess42.STDOUT,
          stdin=subprocess42.PIPE)
    except OSError as e:
      stdout = 'Command ""%s"" failed to start.\nError: %s' % (' '.join(cmd), e)
      now = monotonic_time()
      params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
      params['duration'] = now - start
      params['io_timeout'] = False
      params['hard_timeout'] = False
      post_update(swarming_server, params, 1, stdout, 0)
      return {
        u'exit_code': -1,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': OUT_VERSION,
      }

    output_chunk_start = 0
    stdout = ''
    exit_code = None
    had_hard_timeout = False
    had_io_timeout = False
    must_signal_internal_failure = None
    kill_sent = False
    timed_out = None
    try:
      calc = lambda: calc_yield_wait(
          task_details, start, last_io, timed_out, stdout)
      maxsize = lambda: MAX_CHUNK_SIZE - len(stdout)
      last_io = monotonic_time()
      for _, new_data in proc.yield_any(maxsize=maxsize, timeout=calc):
        now = monotonic_time()
        if new_data:
          stdout += new_data
          last_io = now

        # Post update if necessary.
        if should_post_update(stdout, now, last_packet):
          last_packet = monotonic_time()
          params['cost_usd'] = (
              cost_usd_hour * (last_packet - task_start) / 60. / 60.)
          post_update(swarming_server, params, None, stdout, output_chunk_start)
          output_chunk_start += len(stdout)
          stdout = ''

        # Send signal on timeout if necessary. Both are failures, not
        # internal_failures.
        # Eventually kill but return 0 so bot_main.py doesn't cancel the task.
        if not timed_out:
          if (task_details.io_timeout and
              now - last_io > task_details.io_timeout):
            had_io_timeout = True
            logging.warning('I/O timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
          elif (task_details.hard_timeout and
              now - start > task_details.hard_timeout):
            had_hard_timeout = True
            logging.warning('Hard timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
        else:
          # During grace period.
          if not kill_sent and now >= timed_out + task_details.grace_period:
            # Now kill for real. The user can distinguish between the following
            # states:
            # - signal but process exited within grace period,
            #   (hard_|io_)_timed_out will be set but the process exit code will
            #   be script provided.
            # - processed exited late, exit code will be -9 on posix.
            logging.warning('Grace exhausted; sending SIGKILL')
            proc.kill()
            kill_sent = True
      logging.info('Waiting for proces exit')
      exit_code = proc.wait()
    except MustExit as e:
      # TODO(maruel): Do the send SIGTERM to child process and give it
      # task_details.grace_period to terminate.
      must_signal_internal_failure = (
          u'task_runner received signal %s' % e.signal)
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'signal %d' % e.signal)
    except (IOError, OSError):
      # Something wrong happened, try to kill the child process.
      had_hard_timeout = True
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'exception %s' % e)

    # This is the very last packet for this command. It if was an isolated task,
    # include the output reference to the archived .isolated file.
    now = monotonic_time()
    params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
    params['duration'] = now - start
    params['io_timeout'] = had_io_timeout
    params['hard_timeout'] = had_hard_timeout
    if isolated_result:
      try:
        if ((had_io_timeout or had_hard_timeout) and
            not os.path.isfile(isolated_result)):
          # It's possible that run_isolated failed to quit quickly enough; it
          # could be because there was too much data to upload back or something
          # else. Do not create an internal error, just send back the (partial)
          # view as task_runner saw it, for example the real exit_code is
          # unknown.
          logging.warning('TIMED_OUT and there\'s no result file')
          exit_code = -1
        else:
          # See run_isolated.py for the format.
          with open(isolated_result, 'rb') as f:
            run_isolated_result = json.load(f)
          logging.debug('run_isolated:\n%s', run_isolated_result)
          # TODO(maruel): Grab statistics (cache hit rate, data downloaded,
          # mapping time, etc) from run_isolated and push them to the server.
          if run_isolated_result['outputs_ref']:
            params['outputs_ref'] = run_isolated_result['outputs_ref']
          had_hard_timeout = (
              had_hard_timeout or run_isolated_result['had_hard_timeout'])
          params['hard_timeout'] = had_hard_timeout
          if not had_io_timeout and not had_hard_timeout:
            if run_isolated_result['internal_failure']:
              must_signal_internal_failure = (
                  run_isolated_result['internal_failure'])
              logging.error('%s', must_signal_internal_failure)
            elif exit_code:
              # TODO(maruel): Grab stdout from run_isolated.
              must_signal_internal_failure = (
                  'run_isolated internal failure %d' % exit_code)
              logging.error('%s', must_signal_internal_failure)
          exit_code = run_isolated_result['exit_code']
          if run_isolated_result.get('duration') is not None:
            # Calculate the real task duration as measured by run_isolated and
            # calculate the remaining overhead.
            params['bot_overhead'] = params['duration']
            params['duration'] = run_isolated_result['duration']
            params['bot_overhead'] -= params['duration']
            params['bot_overhead'] -= run_isolated_result.get(
                'download', {}).get('duration', 0)
            params['bot_overhead'] -= run_isolated_result.get(
                'upload', {}).get('duration', 0)
            if params['bot_overhead'] < 0:
              params['bot_overhead'] = 0
          stats = run_isolated_result.get('stats')
          if stats:
            params['isolated_stats'] = stats
      except (IOError, OSError, ValueError) as e:
        logging.error('Swallowing error: %s', e)
        if not must_signal_internal_failure:
          must_signal_internal_failure = str(e)
    # TODO(maruel): Send the internal failure here instead of sending it through
    # bot_main, this causes a race condition.
    if exit_code is None:
      exit_code = -1
    post_update(swarming_server, params, exit_code, stdout, output_chunk_start)
    return {
      u'exit_code': exit_code,
      u'hard_timeout': had_hard_timeout,
      u'io_timeout': had_io_timeout,
      u'must_signal_internal_failure': must_signal_internal_failure,
      u'version': OUT_VERSION,
    }
  finally:
    if isolated_result:
      try:
        os.remove(isolated_result)
      except OSError:
        pass


def main(args):
  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)
  parser.add_option('--in-file', help='Name of the request file')
  parser.add_option(
      '--out-file', help='Name of the JSON file to write a task summary to')
  parser.add_option(
      '--swarming-server', help='Swarming server to send data back')
  parser.add_option(
      '--cost-usd-hour', type='float', help='Cost of this VM in $/h')
  parser.add_option('--start', type='float', help='Time this task was started')
  parser.add_option(
      '--min-free-space', type='int',
      help='Value to send down to run_isolated')

  options, args = parser.parse_args(args)
  if not options.in_file or not options.out_file or args:
    parser.error('task_runner is meant to be used by swarming_bot.')

  on_error.report_on_exception_exit(options.swarming_server)

  logging.info('starting')
  now = monotonic_time()
  if options.start > now:
    options.start = now

  try:
    load_and_run(
        options.in_file, options.swarming_server, options.cost_usd_hour,
        options.start, options.out_file, options.min_free_space)
    return 0
  finally:
    logging.info('quitting')
/n/n/nappengine/swarming/swarming_bot/bot_code/task_runner_test.py/n/n#!/usr/bin/env python
# coding=utf-8
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import base64
import json
import logging
import os
import signal
import shutil
import sys
import tempfile
import time
import unittest

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

from api import os_utilities
from depot_tools import fix_encoding
from utils import file_path
from utils import large
from utils import logging_utils
from utils import subprocess42
from utils import tools
import fake_swarming
import task_runner

CLIENT_DIR = os.path.normpath(
    os.path.join(test_env_bot_code.BOT_DIR, '..', '..', '..', 'client'))

sys.path.insert(0, os.path.join(CLIENT_DIR, 'tests'))
import isolateserver_mock


def get_manifest(script=None, inputs_ref=None, **kwargs):
  out = {
    'bot_id': 'localhost',
    'command':
        [sys.executable, '-u', '-c', script] if not inputs_ref else None,
    'env': {},
    'extra_args': [],
    'grace_period': 30.,
    'hard_timeout': 10.,
    'inputs_ref': inputs_ref,
    'io_timeout': 10.,
    'task_id': 23,
  }
  out.update(kwargs)
  return out


class TestTaskRunnerBase(net_utils.TestCase):
  def setUp(self):
    super(TestTaskRunnerBase, self).setUp()
    self.root_dir = tempfile.mkdtemp(prefix='task_runner')
    logging.info('Temp: %s', self.root_dir)
    self.work_dir = os.path.join(self.root_dir, 'work')
    os.chdir(self.root_dir)
    os.mkdir(self.work_dir)
    # Create the logs directory so run_isolated.py can put its log there.
    os.mkdir(os.path.join(self.root_dir, 'logs'))

    self.mock(
        task_runner, 'get_run_isolated',
        lambda: [sys.executable, os.path.join(CLIENT_DIR, 'run_isolated.py')])

  def tearDown(self):
    os.chdir(test_env_bot_code.BOT_DIR)
    try:
      file_path.rmtree(self.root_dir)
    except OSError:
      print >> sys.stderr, 'Failed to delete %s' % self.root_dir
    finally:
      super(TestTaskRunnerBase, self).tearDown()

  @classmethod
  def get_task_details(cls, *args, **kwargs):
    return task_runner.TaskDetails(get_manifest(*args, **kwargs))

  def gen_requests(self, cost_usd=0., **kwargs):
    return [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_first(cost_usd),
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_final(**kwargs),
        {},
      ),
    ]

  def requests(self, **kwargs):
    """"""Generates the expected HTTP requests for a task run.""""""
    self.expected_requests(self.gen_requests(**kwargs))

  def get_check_first(self, cost_usd):
    def check_first(kwargs):
      self.assertLessEqual(cost_usd, kwargs['data'].pop('cost_usd'))
      self.assertEqual(
        {
          'data': {
            'id': 'localhost',
            'task_id': 23,
          },
        },
        kwargs)
    return check_first


class TestTaskRunner(TestTaskRunnerBase):
  def setUp(self):
    super(TestTaskRunner, self).setUp()
    self.mock(time, 'time', lambda: 1000000000.)

  def get_check_final(self, exit_code=0, output='hi\n', outputs_ref=None):
    def check_final(kwargs):
      # It makes the diffing easier.
      if 'output' in kwargs['data']:
        kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])
      expected = {
        'data': {
          'cost_usd': 10.,
          'duration': 0.,
          'exit_code': exit_code,
          'hard_timeout': False,
          'id': 'localhost',
          'io_timeout': False,
          'output': output,
          'output_chunk_start': 0,
          'task_id': 23,
        },
      }
      if outputs_ref:
        expected['data']['outputs_ref'] = outputs_ref
      self.assertEqual(expected, kwargs)
    return check_final

  def _run_command(self, task_details):
    start = time.time()
    self.mock(time, 'time', lambda: start + 10)
    server = 'https://localhost:1'
    return task_runner.run_command(
        server, task_details, self.work_dir, 3600., start, 1)

  def test_load_and_run_raw(self):
    server = 'https://localhost:1'

    def run_command(
        swarming_server, task_details, work_dir, cost_usd_hour, start,
        min_free_space):
      self.assertEqual(server, swarming_server)
      # Necessary for OSX.
      self.assertEqual(
          os.path.realpath(self.work_dir), os.path.realpath(work_dir))
      self.assertTrue(isinstance(task_details, task_runner.TaskDetails))
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual(1, min_free_space)
      return {
        u'exit_code': 1,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': task_runner.OUT_VERSION,
      }
    self.mock(task_runner, 'run_command', run_command)

    manifest = os.path.join(self.root_dir, 'manifest')
    with open(manifest, 'wb') as f:
      data = {
        'bot_id': 'localhost',
        'command': ['a'],
        'env': {'d': 'e'},
        'extra_args': [],
        'grace_period': 30.,
        'hard_timeout': 10,
        'inputs_ref': None,
        'io_timeout': 11,
        'task_id': 23,
      }
      json.dump(data, f)

    out_file = os.path.join(self.root_dir, 'work', 'task_runner_out.json')
    task_runner.load_and_run(manifest, server, 3600., time.time(), out_file, 1)
    expected = {
      u'exit_code': 1,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    with open(out_file, 'rb') as f:
      self.assertEqual(expected, json.load(f))

  def test_load_and_run_isolated(self):
    self.expected_requests([])
    server = 'https://localhost:1'

    def run_command(
        swarming_server, task_details, work_dir, cost_usd_hour, start,
        min_free_space):
      self.assertEqual(server, swarming_server)
      # Necessary for OSX.
      self.assertEqual(
          os.path.realpath(self.work_dir), os.path.realpath(work_dir))
      self.assertTrue(isinstance(task_details, task_runner.TaskDetails))
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual(1, min_free_space)
      return {
        u'exit_code': 0,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': task_runner.OUT_VERSION,
      }
    self.mock(task_runner, 'run_command', run_command)

    manifest = os.path.join(self.root_dir, 'manifest')
    with open(manifest, 'wb') as f:
      data = {
        'bot_id': 'localhost',
        'command': None,
        'env': {'d': 'e'},
        'extra_args': ['foo', 'bar'],
        'grace_period': 30.,
        'hard_timeout': 10,
        'io_timeout': 11,
        'inputs_ref': {
          'isolated': '123',
          'isolatedserver': 'http://localhost:1',
          'namespace': 'default-gzip',
        },
        'task_id': 23,
      }
      json.dump(data, f)

    out_file = os.path.join(self.root_dir, 'work', 'task_runner_out.json')
    task_runner.load_and_run(manifest, server, 3600., time.time(), out_file, 1)
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    with open(out_file, 'rb') as f:
      self.assertEqual(expected, json.load(f))

  def test_run_command_raw(self):
    # This runs the command for real.
    self.requests(cost_usd=1, exit_code=0)
    task_details = self.get_task_details('print(\'hi\')')
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_isolated(self):
    # This runs the command for real.
    self.requests(
        cost_usd=1, exit_code=0,
        outputs_ref={
          u'isolated': u'123',
          u'isolatedserver': u'http://localhost:1',
          u'namespace': u'default-gzip',
        })
    task_details = self.get_task_details(inputs_ref={
      'isolated': '123',
      'isolatedserver': 'localhost:1',
      'namespace': 'default-gzip',
    }, extra_args=['foo', 'bar'])
    # Mock running run_isolated with a script.
    SCRIPT_ISOLATED = (
      'import json, sys;\n'
      'if len(sys.argv) != 2:\n'
      '  raise Exception(sys.argv);\n'
      'with open(sys.argv[1], \'wb\') as f:\n'
      '  json.dump({\n'
      '    \'exit_code\': 0,\n'
      '    \'had_hard_timeout\': False,\n'
      '    \'internal_failure\': None,\n'
      '    \'outputs_ref\': {\n'
      '      \'isolated\': \'123\',\n'
      '      \'isolatedserver\': \'http://localhost:1\',\n'
      '       \'namespace\': \'default-gzip\',\n'
      '    },\n'
      '  }, f)\n'
      'sys.stdout.write(\'hi\\n\')')
    self.mock(
        task_runner, 'get_isolated_cmd',
        lambda _work_dir, _details, isolated_result, min_free_space:
          [sys.executable, '-u', '-c', SCRIPT_ISOLATED, isolated_result])
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_fail(self):
    # This runs the command for real.
    self.requests(cost_usd=10., exit_code=1)
    task_details = self.get_task_details(
        'import sys; print(\'hi\'); sys.exit(1)')
    expected = {
      u'exit_code': 1,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_os_error(self):
    # This runs the command for real.
    # OS specific error, fix expectation for other OSes.
    output = (
      'Command ""executable_that_shouldnt_be_on_your_system '
      'thus_raising_OSError"" failed to start.\n'
      'Error: [Error 2] The system cannot find the file specified'
      ) if sys.platform == 'win32' else (
      'Command ""executable_that_shouldnt_be_on_your_system '
      'thus_raising_OSError"" failed to start.\n'
      'Error: [Errno 2] No such file or directory')
    self.requests(cost_usd=10., exit_code=1, output=output)
    task_details = task_runner.TaskDetails(
        {
          'bot_id': 'localhost',
          'command': [
            'executable_that_shouldnt_be_on_your_system',
            'thus_raising_OSError',
          ],
          'env': {},
          'extra_args': [],
          'grace_period': 30.,
          'hard_timeout': 6,
          'inputs_ref': None,
          'io_timeout': 6,
          'task_id': 23,
        })
    expected = {
      u'exit_code': -1,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_large(self):
    # Method should have ""self"" as first argument - pylint: disable=E0213
    class Popen(object):
      """"""Mocks the process so we can control how data is returned.""""""
      def __init__(self2, cmd, cwd, env, stdout, stderr, stdin, detached):
        self.assertEqual(task_details.command, cmd)
        self.assertEqual(self.work_dir, cwd)
        expected_env = os.environ.copy()
        expected_env['foo'] = 'bar'
        self.assertEqual(expected_env, env)
        self.assertEqual(subprocess42.PIPE, stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(True, detached)
        self2._out = [
          'hi!\n',
          'hi!\n',
          'hi!\n' * 100000,
          'hi!\n',
        ]

      def yield_any(self2, maxsize, timeout):
        self.assertLess(0, maxsize)
        self.assertLess(0, timeout)
        for i in self2._out:
          yield 'stdout', i

      @staticmethod
      def wait():
        return 0

      @staticmethod
      def kill():
        self.fail()

    self.mock(subprocess42, 'Popen', Popen)

    def check_final(kwargs):
      self.assertEqual(
          {
            'data': {
              # That's because the cost includes the duration starting at start,
              # not when the process was started.
              'cost_usd': 10.,
              'duration': 0.,
              'exit_code': 0,
              'hard_timeout': False,
              'id': 'localhost',
              'io_timeout': False,
              'output': base64.b64encode('hi!\n'),
              'output_chunk_start': 100002*4,
              'task_id': 23,
            },
          },
          kwargs)

    requests = [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        {
          'data': {
            'cost_usd': 10.,
            'id': 'localhost',
            'task_id': 23,
          },
        },
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        {
          'data': {
            'cost_usd': 10.,
            'id': 'localhost',
            'output': base64.b64encode('hi!\n' * 100002),
            'output_chunk_start': 0,
            'task_id': 23,
          },
        },
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        check_final,
        {},
      ),
    ]
    self.expected_requests(requests)
    task_details = task_runner.TaskDetails(
        {
          'bot_id': 'localhost',
          'command': ['large', 'executable'],
          'env': {'foo': 'bar'},
          'extra_args': [],
          'grace_period': 30.,
          'hard_timeout': 60,
          'inputs_ref': None,
          'io_timeout': 60,
          'task_id': 23,
        })
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_main(self):
    def load_and_run(
        manifest, swarming_server, cost_usd_hour, start, json_file,
        min_free_space):
      self.assertEqual('foo', manifest)
      self.assertEqual('http://localhost', swarming_server)
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual('task_summary.json', json_file)
      self.assertEqual(1, min_free_space)

    self.mock(task_runner, 'load_and_run', load_and_run)
    cmd = [
      '--swarming-server', 'http://localhost',
      '--in-file', 'foo',
      '--out-file', 'task_summary.json',
      '--cost-usd-hour', '3600',
      '--start', str(time.time()),
      '--min-free-space', '1',
    ]
    self.assertEqual(0, task_runner.main(cmd))

  def test_main_reboot(self):
    def load_and_run(
        manifest, swarming_server, cost_usd_hour, start, json_file,
        min_free_space):
      self.assertEqual('foo', manifest)
      self.assertEqual('http://localhost', swarming_server)
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual('task_summary.json', json_file)
      self.assertEqual(1, min_free_space)

    self.mock(task_runner, 'load_and_run', load_and_run)
    cmd = [
      '--swarming-server', 'http://localhost',
      '--in-file', 'foo',
      '--out-file', 'task_summary.json',
      '--cost-usd-hour', '3600',
      '--start', str(time.time()),
      '--min-free-space', '1',
    ]
    self.assertEqual(0, task_runner.main(cmd))


class TestTaskRunnerNoTimeMock(TestTaskRunnerBase):
  # Do not mock time.time() for these tests otherwise it becomes a tricky
  # implementation detail check.
  # These test cases run the command for real.

  # TODO(maruel): Calculate this value automatically through iteration?
  SHORT_TIME_OUT = 0.3

  # Here's a simple script that handles signals properly. Sadly SIGBREAK is not
  # defined on posix.
  SCRIPT_SIGNAL = (
    'import signal, sys, time;\n'
    'l = [];\n'
    'def handler(signum, _):\n'
    '  l.append(signum);\n'
    '  print(\'got signal %%d\' %% signum);\n'
    '  sys.stdout.flush();\n'
    'signal.signal(%s, handler);\n'
    'print(\'hi\');\n'
    'sys.stdout.flush();\n'
    'while not l:\n'
    '  try:\n'
    '    time.sleep(0.01);\n'
    '  except IOError:\n'
    '    pass;\n'
    'print(\'bye\')') % (
        'signal.SIGBREAK' if sys.platform == 'win32' else 'signal.SIGTERM')

  SCRIPT_SIGNAL_HANG = (
    'import signal, sys, time;\n'
    'l = [];\n'
    'def handler(signum, _):\n'
    '  l.append(signum);\n'
    '  print(\'got signal %%d\' %% signum);\n'
    '  sys.stdout.flush();\n'
    'signal.signal(%s, handler);\n'
    'print(\'hi\');\n'
    'sys.stdout.flush();\n'
    'while not l:\n'
    '  try:\n'
    '    time.sleep(0.01);\n'
    '  except IOError:\n'
    '    pass;\n'
    'print(\'bye\');\n'
    'time.sleep(100)') % (
        'signal.SIGBREAK' if sys.platform == 'win32' else 'signal.SIGTERM')

  SCRIPT_HANG = 'import time; print(\'hi\'); time.sleep(100)'

  def get_check_final(
      self, hard_timeout=False, io_timeout=False, exit_code=None,
      output='hi\n'):
    def check_final(kwargs):
      if hard_timeout or io_timeout:
        self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('cost_usd'))
        self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('duration'))
      else:
        self.assertLess(0., kwargs['data'].pop('cost_usd'))
        self.assertLess(0., kwargs['data'].pop('duration'))
      # It makes the diffing easier.
      kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])
      self.assertEqual(
          {
            'data': {
              'exit_code': exit_code,
              'hard_timeout': hard_timeout,
              'id': 'localhost',
              'io_timeout': io_timeout,
              'output': output,
              'output_chunk_start': 0,
              'task_id': 23,
            },
          },
          kwargs)
    return check_final

  def _load_and_run(self, manifest):
    # Dot not mock time since this test class is testing timeouts.
    server = 'https://localhost:1'
    in_file = os.path.join(self.work_dir, 'task_runner_in.json')
    with open(in_file, 'wb') as f:
      json.dump(manifest, f)
    out_file = os.path.join(self.work_dir, 'task_runner_out.json')
    task_runner.load_and_run(in_file, server, 3600., time.time(), out_file, 1)
    with open(out_file, 'rb') as f:
      return json.load(f)

  def _run_command(self, task_details):
    # Dot not mock time since this test class is testing timeouts.
    server = 'https://localhost:1'
    return task_runner.run_command(
        server, task_details, self.work_dir, 3600., time.time(), 1)

  def test_hard(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(hard_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, hard_timeout=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(io_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, io_timeout=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_hard_signal(self):
    self.requests(
        hard_timeout=True,
        exit_code=0,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL, hard_timeout=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': 0,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io_signal(self):
    self.requests(
        io_timeout=True, exit_code=0,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL, io_timeout=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_hard_no_grace(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(hard_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, hard_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io_no_grace(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(io_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, io_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_hard_signal_no_grace(self):
    exit_code = 1 if sys.platform == 'win32' else -signal.SIGKILL
    self.requests(
        hard_timeout=True, exit_code=exit_code,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL_HANG, hard_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': exit_code,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io_signal_no_grace(self):
    exit_code = 1 if sys.platform == 'win32' else -signal.SIGKILL
    self.requests(
        io_timeout=True, exit_code=exit_code,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL_HANG, io_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': exit_code,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_isolated_grand_children(self):
    """"""Runs a normal test involving 3 level deep subprocesses.""""""
    # Uses load_and_run()
    files = {
      'parent.py': (
        'import subprocess, sys\n'
        'sys.exit(subprocess.call([sys.executable,\'-u\',\'children.py\']))\n'),
      'children.py': (
        'import subprocess, sys\n'
        'sys.exit(subprocess.call('
            '[sys.executable, \'-u\', \'grand_children.py\']))\n'),
      'grand_children.py': 'print \'hi\'',
    }

    def check_final(kwargs):
      # Warning: this modifies input arguments.
      self.assertLess(0, kwargs['data'].pop('cost_usd'))
      self.assertLess(0, kwargs['data'].pop('bot_overhead'))
      self.assertLess(0, kwargs['data'].pop('duration'))
      self.assertLess(
          0., kwargs['data']['isolated_stats']['download'].pop('duration'))
      # duration==0 can happen on Windows when the clock is in the default
      # resolution, 15.6ms.
      self.assertLessEqual(
          0., kwargs['data']['isolated_stats']['upload'].pop('duration'))
      # Makes the diffing easier.
      kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])
      for k in ('download', 'upload'):
        for j in ('items_cold', 'items_hot'):
          kwargs['data']['isolated_stats'][k][j] = large.unpack(
              base64.b64decode(kwargs['data']['isolated_stats'][k][j]))
      self.assertEqual(
          {
            'data': {
              'exit_code': 0,
              'hard_timeout': False,
              'id': u'localhost',
              'io_timeout': False,
              'isolated_stats': {
                u'download': {
                  u'initial_number_items': 0,
                  u'initial_size': 0,
                  u'items_cold': [10, 86, 94, 276],
                  u'items_hot': [],
                },
                u'upload': {
                  u'items_cold': [],
                  u'items_hot': [],
                },
              },
              'output': 'hi\n',
              'output_chunk_start': 0,
              'task_id': 23,
            },
          },
          kwargs)
    requests = [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_first(0.),
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        check_final,
        {},
      ),
    ]
    self.expected_requests(requests)

    server = isolateserver_mock.MockIsolateServer()
    try:
      isolated = json.dumps({
        'command': ['python', 'parent.py'],
        'files': {
          name: {
            'h': server.add_content_compressed('default-gzip', content),
            's': len(content),
          } for name, content in files.iteritems()
        },
      })
      isolated_digest = server.add_content_compressed('default-gzip', isolated)
      manifest = get_manifest(
          inputs_ref={
            'isolated': isolated_digest,
            'namespace': 'default-gzip',
            'isolatedserver': server.url,
          })
      expected = {
        u'exit_code': 0,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': task_runner.OUT_VERSION,
      }
      self.assertEqual(expected, self._load_and_run(manifest))
    finally:
      server.close()

  def test_isolated_io_signal_no_grace_grand_children(self):
    """"""Handles grand-children process hanging and signal management.

    In this case, the I/O timeout is implemented by task_runner. An hard timeout
    would be implemented by run_isolated (depending on overhead).
    """"""
    # Uses load_and_run()
    # https://msdn.microsoft.com/library/cc704588.aspx
    # STATUS_CONTROL_C_EXIT=0xC000013A. Python sees it as -1073741510.
    exit_code = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM

    files = {
      'parent.py': (
        'import subprocess, sys\n'
        'print(\'parent\')\n'
        'p = subprocess.Popen([sys.executable, \'-u\', \'children.py\'])\n'
        'print(p.pid)\n'
        'p.wait()\n'
        'sys.exit(p.returncode)\n'),
      'children.py': (
        'import subprocess, sys\n'
        'print(\'children\')\n'
        'p = subprocess.Popen([sys.executable,\'-u\',\'grand_children.py\'])\n'
        'print(p.pid)\n'
        'p.wait()\n'
        'sys.exit(p.returncode)\n'),
      'grand_children.py': self.SCRIPT_SIGNAL_HANG,
    }
    # We need to catch the pid of the grand children to be able to kill it, so
    # create our own check_final() instead of using self._gen_requests().
    to_kill = []
    def check_final(kwargs):
      self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('cost_usd'))
      self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('duration'))
      self.assertLess(0., kwargs['data'].pop('bot_overhead'))
      self.assertLess(
          0., kwargs['data']['isolated_stats']['download'].pop('duration'))
      self.assertLess(
          0., kwargs['data']['isolated_stats']['upload'].pop('duration'))
      # Makes the diffing easier.
      for k in ('download', 'upload'):
        for j in ('items_cold', 'items_hot'):
          kwargs['data']['isolated_stats'][k][j] = large.unpack(
              base64.b64decode(kwargs['data']['isolated_stats'][k][j]))
      # The command print the pid of this child and grand-child processes, each
      # on its line.
      output = base64.b64decode(kwargs['data'].pop('output', ''))
      for line in output.splitlines():
        try:
          to_kill.append(int(line))
        except ValueError:
          pass
      self.assertEqual(
          {
            'data': {
              'exit_code': exit_code,
              'hard_timeout': False,
              'id': u'localhost',
              'io_timeout': True,
              'isolated_stats': {
                u'download': {
                  u'initial_number_items': 0,
                  u'initial_size': 0,
                  u'items_cold': [144, 150, 285, 307],
                  u'items_hot': [],
                },
                u'upload': {
                  u'items_cold': [],
                  u'items_hot': [],
                },
              },
              'output_chunk_start': 0,
              'task_id': 23,
            },
          },
          kwargs)
    requests = [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_first(0.),
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        check_final,
        {},
      ),
    ]
    self.expected_requests(requests)

    server = isolateserver_mock.MockIsolateServer()
    try:
      # TODO(maruel): -u is needed if you don't want python buffering to
      # interfere.
      isolated = json.dumps({
        'command': ['python', '-u', 'parent.py'],
        'files': {
          name: {
            'h': server.add_content_compressed('default-gzip', content),
            's': len(content),
          } for name, content in files.iteritems()
        },
      })
      isolated_digest = server.add_content_compressed('default-gzip', isolated)
      try:
        manifest = get_manifest(
            inputs_ref={
              'isolated': isolated_digest,
              'namespace': 'default-gzip',
              'isolatedserver': server.url,
            },
            # TODO(maruel): A bit cheezy, we'd want the I/O timeout to be just
            # enough to have the time for the PID to be printed but not more.
            io_timeout=1,
            grace_period=self.SHORT_TIME_OUT)
        expected = {
          u'exit_code': exit_code,
          u'hard_timeout': False,
          u'io_timeout': True,
          u'must_signal_internal_failure': None,
          u'version': task_runner.OUT_VERSION,
        }
        self.assertEqual(expected, self._load_and_run(manifest))
        self.assertEqual(2, len(to_kill))
      finally:
        for k in to_kill:
          try:
            if sys.platform == 'win32':
              os.kill(k, signal.SIGTERM)
            else:
              os.kill(k, signal.SIGKILL)
          except OSError:
            pass
    finally:
      server.close()


class TaskRunnerSmoke(unittest.TestCase):
  # Runs a real process and a real Swarming fake server.
  def setUp(self):
    super(TaskRunnerSmoke, self).setUp()
    self.root_dir = tempfile.mkdtemp(prefix='task_runner')
    logging.info('Temp: %s', self.root_dir)
    self._server = fake_swarming.Server(self)

  def tearDown(self):
    try:
      self._server.shutdown()
    finally:
      try:
        file_path.rmtree(self.root_dir)
      except OSError:
        print >> sys.stderr, 'Failed to delete %s' % self.root_dir
      finally:
        super(TaskRunnerSmoke, self).tearDown()

  def test_signal(self):
    # Tests when task_runner gets a SIGTERM.

    # https://msdn.microsoft.com/library/cc704588.aspx
    # STATUS_ENTRYPOINT_NOT_FOUND=0xc0000139. Python sees it as -1073741510.
    exit_code = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM

    os.mkdir(os.path.join(self.root_dir, 'work'))
    signal_file = os.path.join(self.root_dir, 'work', 'signal')
    open(signal_file, 'wb').close()
    manifest = get_manifest(
        script='import os,time;os.remove(%r);time.sleep(60)' % signal_file,
        hard_timeout=60., io_timeout=60.)
    task_in_file = os.path.join(self.root_dir, 'task_runner_in.json')
    task_result_file = os.path.join(self.root_dir, 'task_runner_out.json')
    with open(task_in_file, 'wb') as f:
      json.dump(manifest, f)
    bot = os.path.join(self.root_dir, 'swarming_bot.1.zip')
    code, _ = fake_swarming.gen_zip(self._server.url)
    with open(bot, 'wb') as f:
      f.write(code)
    cmd = [
      sys.executable, bot, 'task_runner',
      '--swarming-server', self._server.url,
      '--in-file', task_in_file,
      '--out-file', task_result_file,
      '--cost-usd-hour', '1',
      # Include the time taken to poll the task in the cost.
      '--start', str(time.time()),
    ]
    logging.info('%s', cmd)
    proc = subprocess42.Popen(cmd, cwd=self.root_dir, detached=True)
    # Wait for the child process to be alive.
    while os.path.isfile(signal_file):
      time.sleep(0.01)
    # Send SIGTERM to task_runner itself. Ensure the right thing happen.
    # Note that on Windows, this is actually sending a SIGBREAK since there's no
    # such thing as SIGTERM.
    proc.send_signal(signal.SIGTERM)
    proc.wait()
    task_runner_log = os.path.join(self.root_dir, 'logs', 'task_runner.log')
    with open(task_runner_log, 'rb') as f:
      logging.info('task_runner.log:\n---\n%s---', f.read())
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual([], self._server.get_events())
    tasks = self._server.get_tasks()
    for task in tasks.itervalues():
      for event in task:
        event.pop('cost_usd')
        event.pop('duration', None)
        event.pop('bot_overhead', None)
    expected = {
      '23': [
        {
          u'id': u'localhost',
          u'task_id': 23,
        },
        {
          u'exit_code': exit_code,
          u'hard_timeout': False,
          u'id': u'localhost',
          u'io_timeout': False,
          u'task_id': 23,
        },
      ],
    }
    self.assertEqual(expected, tasks)
    expected = {
      'swarming_bot.1.zip',
      '4e019f31778ba7191f965469dc673280386bbd60-cacert.pem',
      'work',
      'logs',
      # TODO(maruel): Move inside work.
      'task_runner_in.json',
      'task_runner_out.json',
    }
    self.assertEqual(expected, set(os.listdir(self.root_dir)))
    expected = {
      u'exit_code': exit_code,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure':
          u'task_runner received signal %d' % task_runner.SIG_BREAK_OR_TERM,
      u'version': 3,
    }
    with open(task_result_file, 'rb') as f:
      self.assertEqual(expected, json.load(f))
    self.assertEqual(0, proc.returncode)


if __name__ == '__main__':
  fix_encoding.fix_encoding()
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  logging_utils.prepare_logging(None)
  logging_utils.set_console_level(
      logging.DEBUG if '-v' in sys.argv else logging.CRITICAL+1)
  # Fix litteral text expectation.
  os.environ['LANG'] = 'en_US.UTF-8'
  os.environ['LANGUAGE'] = 'en_US.UTF-8'
  unittest.main()
/n/n/nappengine/swarming/swarming_bot/bot_code/xsrf_client.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Wraps URL requests with an XSRF token using components/auth based service.""""""

import datetime
import logging
import os
import sys

THIS_DIR = os.path.dirname(os.path.abspath(__file__))

sys.path.insert(0, os.path.join(THIS_DIR, 'third_party'))

from utils import net


class Error(Exception):
  pass


def _utcnow():
  """"""So it can be mocked.""""""
  return datetime.datetime.utcnow()


class XsrfRemote(object):
  """"""Transparently adds XSRF token to requests.""""""
  TOKEN_RESOURCE = '/auth/api/v1/accounts/self/xsrf_token'

  def __init__(self, url, token_resource=None):
    self.url = url.rstrip('/')
    self.token = None
    self.token_resource = token_resource or self.TOKEN_RESOURCE
    self.expiration = None
    self.xsrf_request_params = {}

  def url_read(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token for GET.
      return net.url_read(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def url_read_json(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token required for GET.
      return net.url_read_json(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_json_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def refresh_token(self):
    """"""Returns a fresh token. Necessary as the token may expire after an hour.
    """"""
    url = self.url + self.token_resource
    resp = net.url_read_json(
        url,
        headers={'X-XSRF-Token-Request': '1'},
        data=self.xsrf_request_params)
    if resp is None:
      raise Error('Failed to connect to %s' % url)
    self.token = resp['xsrf_token']
    if resp.get('expiration_sec'):
      exp = resp['expiration_sec']
      exp -= min(round(exp * 0.1), 600)
      self.expiration = _utcnow() + datetime.timedelta(seconds=exp)
    return self.token

  def need_refresh(self):
    """"""Returns True if the XSRF token needs to be refreshed.""""""
    return (
        not self.token or (self.expiration and self.expiration <= _utcnow()))

  def _url_read_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read(url, headers=headers, **kwargs)

  def _url_read_json_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read_json(url, headers=headers, **kwargs)
/n/n/nappengine/swarming/swarming_bot/bot_code/xsrf_client_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import datetime
import logging
import os
import sys
import time
import unittest

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import xsrf_client


class UrlHelperTest(net_utils.TestCase):
  def setUp(self):
    super(UrlHelperTest, self).setUp()
    self.mock(logging, 'error', lambda *_: None)
    self.mock(logging, 'exception', lambda *_: None)
    self.mock(logging, 'info', lambda *_: None)
    self.mock(logging, 'warning', lambda *_: None)
    self.mock(time, 'sleep', lambda _: None)

  def testXsrfRemoteGET(self):
    self.expected_requests([('http://localhost/a', {}, 'foo', None)])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a'))

  def testXsrfRemoteSimple(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))

  def testXsrfRemoteRefresh(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'bar',
            None,
          ),
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token2',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token2'}},
            'foo',
            None,
          ),
        ])

    now = xsrf_client._utcnow()
    remote = xsrf_client.XsrfRemote('http://localhost/')
    remote.url_read('/a', data={'foo': 'bar'})
    self.mock(
        xsrf_client, '_utcnow', lambda: now + datetime.timedelta(seconds=91))
    remote.url_read('/a', data={'foo': 'bar'})

  def testXsrfRemoteCustom(self):
    # Use the new swarming bot API as an example of custom XSRF request handler.
    self.expected_requests(
        [
          (
            'http://localhost/swarming/api/v1/bot/handshake',
            {
              'data': {'attributes': 'b'},
              'headers': {'X-XSRF-Token-Request': '1'},
            },
            {
              'expiration_sec': 100,
              'ignored': True,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote(
        'http://localhost/',
        '/swarming/api/v1/bot/handshake')
    remote.xsrf_request_params = {'attributes': 'b'}
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))


if __name__ == '__main__':
  logging.basicConfig(level=logging.ERROR)
  unittest.main()
/n/n/nclient/tests/net_utils.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import logging
import os
import sys
import threading

TEST_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(TEST_DIR)
sys.path.insert(0, ROOT_DIR)
sys.path.insert(0, os.path.join(ROOT_DIR, 'third_party'))

from depot_tools import auto_stub
from utils import net


def make_fake_response(content, url, headers=None):
  """"""Returns HttpResponse with predefined content, useful in tests.""""""
  headers = dict(headers or {})
  headers['Content-Length'] = len(content)
  class _Fake(object):
    def __init__(self):
      self.content = content
    def iter_content(self, chunk_size):
      c = self.content
      while c:
        yield c[:chunk_size]
        c = c[chunk_size:]
    def read(self):
      return self.content
  return net.HttpResponse(_Fake(), url, headers)


class TestCase(auto_stub.TestCase):
  """"""Mocks out url_open() calls.""""""
  def setUp(self):
    super(TestCase, self).setUp()
    self.mock(net, 'url_open', self._url_open)
    self.mock(net, 'url_read_json', self._url_read_json)
    self.mock(net, 'sleep_before_retry', lambda *_: None)
    self._lock = threading.Lock()
    self._requests = []

  def tearDown(self):
    try:
      if not self.has_failed():
        self.assertEqual([], self._requests)
    finally:
      super(TestCase, self).tearDown()

  def expected_requests(self, requests):
    """"""Registers the expected requests along their reponses.

    Arguments:
      request: list of tuple(url, kwargs, response, headers) for normal requests
          and tuple(url, kwargs, response) for json requests. kwargs can be a
          callable. In that case, it's called with the actual kwargs. It's
          useful when the kwargs values are not deterministic.
    """"""
    requests = requests[:]
    for request in requests:
      self.assertEqual(tuple, request.__class__)
      # 3 = json request (url_read_json).
      # 4 = normal request (url_open).
      self.assertIn(len(request), (3, 4))

    with self._lock:
      self.assertEqual([], self._requests)
      self._requests = requests

  def _url_open(self, url, **kwargs):
    logging.warn('url_open(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 4:
            self.fail('Expected normal request, got json data; %s' % url)
          _, expected_kwargs, result, headers = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return make_fake_response(result, url, headers)
          return None
    self.fail('Unknown request %s' % url)

  def _url_read_json(self, url, **kwargs):
    logging.warn('url_read_json(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 3:
            self.fail('Expected json request, got normal data; %s' % url)
          _, expected_kwargs, result = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return result
          return None
    self.fail('Unknown request %s %s' % (url, kwargs))
/n/n/n",0
83,83,c23a5bf6278f55b3f8135e0edab9927599a09236,"/appengine/swarming/handlers_bot.py/n/n# Copyright 2015 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Internal bot API handlers.""""""

import base64
import json
import logging
import textwrap

import webob
import webapp2

from google.appengine.api import app_identity
from google.appengine.api import datastore_errors
from google.appengine.datastore import datastore_query
from google.appengine import runtime
from google.appengine.ext import ndb

from components import auth
from components import ereporter2
from components import utils
from server import acl
from server import bot_code
from server import bot_management
from server import stats
from server import task_pack
from server import task_request
from server import task_result
from server import task_scheduler
from server import task_to_run


def has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):
  """"""Returns an error if unexpected keys are present or expected keys are
  missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  actual_keys = frozenset(actual_keys)
  superfluous = actual_keys - expected_keys
  missing = minimum_keys - actual_keys
  if superfluous or missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    msg_superfluous = (
        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')
    return 'Unexpected %s%s%s; did you make a typo?' % (
        name, msg_missing, msg_superfluous)


def has_unexpected_keys(expected_keys, actual_keys, name):
  """"""Return an error if unexpected keys are present or expected keys are
  missing.
  """"""
  return has_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, name)


def log_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  message = has_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, name)
  if message:
    ereporter2.log_request(request, source=source, message=message)
  return message


def log_unexpected_keys(expected_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.
  """"""
  return log_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, request, source, name)


def has_missing_keys(minimum_keys, actual_keys, name):
  """"""Returns an error if expected keys are not present.

  Do not warn about unexpected keys.
  """"""
  actual_keys = frozenset(actual_keys)
  missing = minimum_keys - actual_keys
  if missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)


class BootstrapHandler(auth.AuthenticatingHandler):
  """"""Returns python code to run to bootstrap a swarming bot.""""""

  @auth.require(acl.is_bot)
  def get(self):
    self.response.headers['Content-Type'] = 'text/x-python'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot_bootstrap.py""')
    self.response.out.write(
        bot_code.get_bootstrap(self.request.host_url).content)


class BotCodeHandler(auth.AuthenticatingHandler):
  """"""Returns a zip file with all the files required by a bot.

  Optionally specify the hash version to download. If so, the returned data is
  cacheable.
  """"""

  @auth.require(acl.is_bot)
  def get(self, version=None):
    if version:
      expected = bot_code.get_bot_version(self.request.host_url)
      if version != expected:
        # This can happen when the server is rapidly updated.
        logging.error('Requested Swarming bot %s, have %s', version, expected)
        self.abort(404)
      self.response.headers['Cache-Control'] = 'public, max-age=3600'
    else:
      self.response.headers['Cache-Control'] = 'no-cache, no-store'
    self.response.headers['Content-Type'] = 'application/octet-stream'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot.zip""')
    self.response.out.write(
        bot_code.get_swarming_bot_zip(self.request.host_url))


class _BotBaseHandler(auth.ApiHandler):
  """"""
  Request body is a JSON dict:
    {
      ""dimensions"": <dict of properties>,
      ""state"": <dict of properties>,
      ""version"": <sha-1 of swarming_bot.zip uncompressed content>,
    }
  """"""

  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}
  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  def _process(self):
    """"""Returns True if the bot has invalid parameter and should be automatically
    quarantined.

    Does one DB synchronous GET.

    Returns:
      tuple(request, bot_id, version, state, dimensions, quarantined_msg)
    """"""
    request = self.parse_body()
    version = request.get('version', None)

    dimensions = request.get('dimensions', {})
    state = request.get('state', {})
    bot_id = None
    if dimensions.get('id'):
      dimension_id = dimensions['id']
      if (isinstance(dimension_id, list) and len(dimension_id) == 1
          and isinstance(dimension_id[0], unicode)):
        bot_id = dimensions['id'][0]

    # The bot may decide to ""self-quarantine"" itself. Accept both via
    # dimensions or via state. See bot_management._BotCommon.quarantined for
    # more details.
    if (bool(dimensions.get('quarantined')) or
        bool(state.get('quarantined'))):
      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'

    quarantined_msg = None
    # Use a dummy 'for' to be able to break early from the block.
    for _ in [0]:

      quarantined_msg = has_unexpected_keys(
          self.EXPECTED_KEYS, request, 'keys')
      if quarantined_msg:
        break

      quarantined_msg = has_missing_keys(
          self.REQUIRED_STATE_KEYS, state, 'state')
      if quarantined_msg:
        break

      if not bot_id:
        quarantined_msg = 'Missing bot id'
        break

      if not all(
          isinstance(key, unicode) and
          isinstance(values, list) and
          all(isinstance(value, unicode) for value in values)
          for key, values in dimensions.iteritems()):
        quarantined_msg = (
            'Invalid dimensions type:\n%s' % json.dumps(dimensions,
              sort_keys=True, indent=2, separators=(',', ': ')))
        break

      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)
      if dimensions_count > task_to_run.MAX_DIMENSIONS:
        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count
        break

      if not isinstance(
          state.get('lease_expiration_ts'), (None.__class__, int)):
        quarantined_msg = (
            'lease_expiration_ts (%r) must be int or None' % (
                state['lease_expiration_ts']))
        break

    if quarantined_msg:
      line = 'Quarantined Bot\nhttps://%s/restricted/bot/%s\n%s' % (
          app_identity.get_default_version_hostname(), bot_id,
          quarantined_msg)
      ereporter2.log_request(self.request, source='bot', message=line)
      return request, bot_id, version, state, dimensions, quarantined_msg

    # Look for admin enforced quarantine.
    bot_settings = bot_management.get_settings_key(bot_id).get()
    if bool(bot_settings and bot_settings.quarantined):
      return request, bot_id, version, state, dimensions, 'Quarantined by admin'

    return request, bot_id, version, state, dimensions, None


class BotHandshakeHandler(_BotBaseHandler):
  """"""First request to be called to get initial data like XSRF token.

  The bot is server-controled so the server doesn't have to support multiple API
  version. When running a task, the bot sync the the version specific URL. Once
  abot finished its currently running task, it'll be immediately be upgraded
  after on its next poll.

  This endpoint does not return commands to the bot, for example to upgrade
  itself. It'll be told so when it does its first poll.

  Response body is a JSON dict:
    {
      ""bot_version"": <sha-1 of swarming_bot.zip uncompressed content>,
      ""server_version"": ""138-193f1f3"",
      ""xsrf_token"": ""......"",
    }
  """"""

  # This handler is called to get XSRF token, there's nothing to enforce yet.
  xsrf_token_enforce_on = ()

  @auth.require_xsrf_token_request
  @auth.require(acl.is_bot)
  def post(self):
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    bot_management.bot_event(
        event_type='bot_connected', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=dimensions,
        state=state, version=version, quarantined=bool(quarantined_msg),
        task_id='', task_name=None, message=quarantined_msg)

    data = {
      # This access token will be used to validate each subsequent request.
      'bot_version': bot_code.get_bot_version(self.request.host_url),
      'expiration_sec': auth.handler.XSRFToken.expiration_sec,
      'server_version': utils.get_app_version(),
      'xsrf_token': self.generate_xsrf_token(),
    }
    self.send_response(data)


class BotPollHandler(_BotBaseHandler):
  """"""The bot polls for a task; returns either a task, update command or sleep.

  In case of exception on the bot, this is enough to get it just far enough to
  eventually self-update to a working version. This is to ensure that coding
  errors in bot code doesn't kill all the fleet at once, they should still be up
  just enough to be able to self-update again even if they don't get task
  assigned anymore.
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    """"""Handles a polling request.

    Be very permissive on missing values. This can happen because of errors
    on the bot, *we don't want to deny them the capacity to update*, so that the
    bot code is eventually fixed and the bot self-update to this working code.

    It makes recovery of the fleet in case of catastrophic failure much easier.
    """"""
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    sleep_streak = state.get('sleep_streak', 0)
    quarantined = bool(quarantined_msg)

    # Note bot existence at two places, one for stats at 1 minute resolution,
    # the other for the list of known bots.
    action = 'bot_inactive' if quarantined else 'bot_active'
    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)

    def bot_event(event_type, task_id=None, task_name=None):
      bot_management.bot_event(
          event_type=event_type, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=dimensions,
          state=state, version=version, quarantined=quarantined,
          task_id=task_id, task_name=task_name, message=quarantined_msg)

    # Bot version is host-specific because the host URL is embedded in
    # swarming_bot.zip
    expected_version = bot_code.get_bot_version(self.request.host_url)
    if version != expected_version:
      bot_event('request_update')
      self._cmd_update(expected_version)
      return
    if quarantined:
      bot_event('request_sleep')
      self._cmd_sleep(sleep_streak, quarantined)
      return

    #
    # At that point, the bot should be in relatively good shape since it's
    # running the right version. It is still possible that invalid code was
    # pushed to the server, so be diligent about it.
    #

    # Bot may need a reboot if it is running for too long. We do not reboot
    # quarantined bots.
    needs_restart, restart_message = bot_management.should_restart_bot(
        bot_id, state)
    if needs_restart:
      bot_event('request_restart')
      self._cmd_restart(restart_message)
      return

    # The bot is in good shape. Try to grab a task.
    try:
      # This is a fairly complex function call, exceptions are expected.
      request, run_result = task_scheduler.bot_reap_task(
          dimensions, bot_id, version, state.get('lease_expiration_ts'))
      if not request:
        # No task found, tell it to sleep a bit.
        bot_event('request_sleep')
        self._cmd_sleep(sleep_streak, quarantined)
        return

      try:
        # This part is tricky since it intentionally runs a transaction after
        # another one.
        if request.properties.is_terminate:
          bot_event('bot_terminate', task_id=run_result.task_id)
          self._cmd_terminate(run_result.task_id)
        else:
          bot_event(
              'request_task', task_id=run_result.task_id,
              task_name=request.name)
          self._cmd_run(request, run_result.key, bot_id)
      except:
        logging.exception('Dang, exception after reaping')
        raise
    except runtime.DeadlineExceededError:
      # If the timeout happened before a task was assigned there is no problems.
      # If the timeout occurred after a task was assigned, that task will
      # timeout (BOT_DIED) since the bot didn't get the details required to
      # run it) and it will automatically get retried (TODO) when the task times
      # out.
      # TODO(maruel): Note the task if possible and hand it out on next poll.
      # https://code.google.com/p/swarming/issues/detail?id=130
      self.abort(500, 'Deadline')

  def _cmd_run(self, request, run_result_key, bot_id):
    cmd = None
    if request.properties.commands:
      cmd = request.properties.commands[0]
    elif request.properties.command:
      cmd = request.properties.command
    out = {
      'cmd': 'run',
      'manifest': {
        'bot_id': bot_id,
        'command': cmd,
        'dimensions': request.properties.dimensions,
        'env': request.properties.env,
        'extra_args': request.properties.extra_args,
        'grace_period': request.properties.grace_period_secs,
        'hard_timeout': request.properties.execution_timeout_secs,
        'host': utils.get_versioned_hosturl(),
        'io_timeout': request.properties.io_timeout_secs,
        'inputs_ref': request.properties.inputs_ref,
        'task_id': task_pack.pack_run_result_key(run_result_key),
      },
    }
    self.send_response(utils.to_json_encodable(out))

  def _cmd_sleep(self, sleep_streak, quarantined):
    out = {
      'cmd': 'sleep',
      'duration': task_scheduler.exponential_backoff(sleep_streak),
      'quarantined': quarantined,
    }
    self.send_response(out)

  def _cmd_terminate(self, task_id):
    out = {
      'cmd': 'terminate',
      'task_id': task_id,
    }
    self.send_response(out)

  def _cmd_update(self, expected_version):
    out = {
      'cmd': 'update',
      'version': expected_version,
    }
    self.send_response(out)

  def _cmd_restart(self, message):
    logging.info('Rebooting bot: %s', message)
    out = {
      'cmd': 'restart',
      'message': message,
    }
    self.send_response(out)


class BotEventHandler(_BotBaseHandler):
  """"""On signal that a bot had an event worth logging.""""""

  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}

  @auth.require(acl.is_bot)
  def post(self):
    (request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    event = request.get('event')
    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):
      self.abort_with_error(400, error='Unsupported event type')
    message = request.get('message')
    bot_management.bot_event(
        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,
        dimensions=dimensions, state=state, version=version,
        quarantined=bool(quarantined_msg), task_id=None, task_name=None,
        message=message)

    if event == 'bot_error':
      line = (
          'Bot: https://%s/restricted/bot/%s\n'
          'Bot error:\n'
          '%s') % (
          app_identity.get_default_version_hostname(), bot_id, message)
      ereporter2.log_request(self.request, source='bot', message=line)
    self.send_response({})


class BotTaskUpdateHandler(auth.ApiHandler):
  """"""Receives updates from a Bot for a task.

  The handler verifies packets are processed in order and will refuse
  out-of-order packets.
  """"""
  ACCEPTED_KEYS = {
    u'bot_overhead', u'cost_usd', u'duration', u'exit_code',
    u'hard_timeout', u'id', u'io_timeout', u'isolated_stats', u'output',
    u'output_chunk_start', u'outputs_ref', u'task_id',
  }
  REQUIRED_KEYS = {u'id', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    # Unlike handshake and poll, we do not accept invalid keys here. This code
    # path is much more strict.
    request = self.parse_body()
    msg = log_unexpected_subset_keys(
        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',
        'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    bot_id = request['id']
    cost_usd = request['cost_usd']
    task_id = request['task_id']

    bot_overhead = request.get('bot_overhead')
    duration = request.get('duration')
    exit_code = request.get('exit_code')
    hard_timeout = request.get('hard_timeout')
    io_timeout = request.get('io_timeout')
    isolated_stats = request.get('isolated_stats')
    output = request.get('output')
    output_chunk_start = request.get('output_chunk_start')
    outputs_ref = request.get('outputs_ref')

    if bool(isolated_stats) != (bot_overhead is not None):
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % task_id)
      self.abort_with_error(
          400,
          error='Both bot_overhead and isolated_stats must be set '
                'simultaneously\nbot_overhead: %s\nisolated_stats: %s' %
                (bot_overhead, isolated_stats))

    run_result_key = task_pack.unpack_run_result_key(task_id)
    performance_stats = None
    if isolated_stats:
      download = isolated_stats['download']
      upload = isolated_stats['upload']
      performance_stats = task_result.PerformanceStats(
          bot_overhead=bot_overhead,
          isolated_download=task_result.IsolatedOperation(
              duration=download['duration'],
              initial_number_items=download['initial_number_items'],
              initial_size=download['initial_size'],
              items_cold=base64.b64decode(download['items_cold']),
              items_hot=base64.b64decode(download['items_hot'])),
          isolated_upload=task_result.IsolatedOperation(
              duration=upload['duration'],
              items_cold=base64.b64decode(upload['items_cold']),
              items_hot=base64.b64decode(upload['items_hot'])))

    if output is not None:
      try:
        output = base64.b64decode(output)
      except UnicodeEncodeError as e:
        logging.error('Failed to decode output\n%s\n%r', e, output)
        output = output.encode('ascii', 'replace')
      except TypeError as e:
        # Save the output as-is instead. The error will be logged in ereporter2
        # and returning a HTTP 500 would only force the bot to stay in a retry
        # loop.
        logging.error('Failed to decode output\n%s\n%r', e, output)
    if outputs_ref:
      outputs_ref = task_request.FilesRef(**outputs_ref)

    try:
      state = task_scheduler.bot_update_task(
          run_result_key=run_result_key,
          bot_id=bot_id,
          output=output,
          output_chunk_start=output_chunk_start,
          exit_code=exit_code,
          duration=duration,
          hard_timeout=hard_timeout,
          io_timeout=io_timeout,
          cost_usd=cost_usd,
          outputs_ref=outputs_ref,
          performance_stats=performance_stats)
      if not state:
        logging.info('Failed to update, please retry')
        self.abort_with_error(500, error='Failed to update, please retry')

      if state in (task_result.State.COMPLETED, task_result.State.TIMED_OUT):
        action = 'task_completed'
      else:
        assert state == task_result.State.RUNNING, state
        action = 'task_update'
      bot_management.bot_event(
          event_type=action, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=None, state=None,
          version=None, quarantined=None, task_id=task_id, task_name=None)
    except ValueError as e:
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % e)
      self.abort_with_error(400, error=str(e))
    except webob.exc.HTTPException:
      raise
    except Exception as e:
      logging.exception('Internal error: %s', e)
      self.abort_with_error(500, error=str(e))

    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot
    # reboots itself to abort the task abruptly. It is useful when a task hangs
    # and the timeout was set too long or the task was superseded by a newer
    # task with more recent executable (e.g. a new Try Server job on a newer
    # patchset on Rietveld).
    self.send_response({'ok': True})


class BotTaskErrorHandler(auth.ApiHandler):
  """"""It is a specialized version of ereporter2's /ereporter2/api/v1/on_error
  that also attaches a task id to it.

  This formally kills the task, marking it as an internal failure. This can be
  used by bot_main.py to kill the task when task_runner misbehaved.
  """"""

  EXPECTED_KEYS = {u'id', u'message', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    request = self.parse_body()
    bot_id = request.get('id')
    task_id = request.get('task_id', '')
    message = request.get('message', 'unknown')

    bot_management.bot_event(
        event_type='task_error', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=None, state=None,
        version=None, quarantined=None, task_id=task_id, task_name=None,
        message=message)
    line = (
        'Bot: https://%s/restricted/bot/%s\n'
        'Task failed: https://%s/user/task/%s\n'
        '%s') % (
        app_identity.get_default_version_hostname(), bot_id,
        app_identity.get_default_version_hostname(), task_id,
        message)
    ereporter2.log_request(self.request, source='bot', message=line)

    msg = log_unexpected_keys(
        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    msg = task_scheduler.bot_kill_task(
        task_pack.unpack_run_result_key(task_id), bot_id)
    if msg:
      logging.error(msg)
      self.abort_with_error(400, error=msg)
    self.send_response({})


class ServerPingHandler(webapp2.RequestHandler):
  """"""Handler to ping when checking if the server is up.

  This handler should be extremely lightweight. It shouldn't do any
  computations, it should just state that the server is up. It's open to
  everyone for simplicity and performance.
  """"""

  def get(self):
    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'
    self.response.out.write('Server up')


def get_routes():
  routes = [
      ('/bootstrap', BootstrapHandler),
      ('/bot_code', BotCodeHandler),
      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),
      ('/swarming/api/v1/bot/event', BotEventHandler),
      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),
      ('/swarming/api/v1/bot/poll', BotPollHandler),
      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),
      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',
          BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),
      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',
          BotTaskErrorHandler),
  ]
  return [webapp2.Route(*i) for i in routes]
/n/n/n/appengine/swarming/server/bot_archive.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Generates the swarming_bot.zip archive for the bot.

Unlike the other source files, this file can be run from ../tools/bot_archive.py
stand-alone to generate a swarming_bot.zip for local testing so it doesn't
import anything from the AppEngine SDK.

The hash of the content of the files in the archive is used to define the
current version of the swarming bot code.
""""""

import hashlib
import json
import logging
import os
import StringIO
import zipfile


# List of files needed by the swarming bot.
# TODO(maruel): Make the list automatically generated?
FILES = (
    '__main__.py',
    'api/__init__.py',
    'api/bot.py',
    'api/parallel.py',
    'api/os_utilities.py',
    'api/platforms/__init__.py',
    'api/platforms/android.py',
    'api/platforms/common.py',
    'api/platforms/gce.py',
    'api/platforms/linux.py',
    'api/platforms/osx.py',
    'api/platforms/posix.py',
    'api/platforms/win.py',
    'bot_code/__init__.py',
    'bot_code/bot_main.py',
    'bot_code/common.py',
    'bot_code/singleton.py',
    'bot_code/task_runner.py',
    'bot_code/xsrf_client.py',
    'client/auth.py',
    'client/isolated_format.py',
    'client/isolateserver.py',
    'client/run_isolated.py',
    'config/__init__.py',
    'third_party/__init__.py',
    'third_party/colorama/__init__.py',
    'third_party/colorama/ansi.py',
    'third_party/colorama/ansitowin32.py',
    'third_party/colorama/initialise.py',
    'third_party/colorama/win32.py',
    'third_party/colorama/winterm.py',
    'third_party/depot_tools/__init__.py',
    'third_party/depot_tools/fix_encoding.py',
    'third_party/depot_tools/subcommand.py',
    'third_party/httplib2/__init__.py',
    'third_party/httplib2/cacerts.txt',
    'third_party/httplib2/iri2uri.py',
    'third_party/httplib2/socks.py',
    'third_party/oauth2client/__init__.py',
    'third_party/oauth2client/_helpers.py',
    'third_party/oauth2client/_openssl_crypt.py',
    'third_party/oauth2client/_pycrypto_crypt.py',
    'third_party/oauth2client/client.py',
    'third_party/oauth2client/clientsecrets.py',
    'third_party/oauth2client/crypt.py',
    'third_party/oauth2client/file.py',
    'third_party/oauth2client/gce.py',
    'third_party/oauth2client/keyring_storage.py',
    'third_party/oauth2client/locked_file.py',
    'third_party/oauth2client/multistore_file.py',
    'third_party/oauth2client/service_account.py',
    'third_party/oauth2client/tools.py',
    'third_party/oauth2client/util.py',
    'third_party/oauth2client/xsrfutil.py',
    'third_party/pyasn1/pyasn1/__init__.py',
    'third_party/pyasn1/pyasn1/codec/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/decoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/encoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/eoo.py',
    'third_party/pyasn1/pyasn1/codec/cer/__init__.py',
    'third_party/pyasn1/pyasn1/codec/cer/decoder.py',
    'third_party/pyasn1/pyasn1/codec/cer/encoder.py',
    'third_party/pyasn1/pyasn1/codec/der/__init__.py',
    'third_party/pyasn1/pyasn1/codec/der/decoder.py',
    'third_party/pyasn1/pyasn1/codec/der/encoder.py',
    'third_party/pyasn1/pyasn1/compat/__init__.py',
    'third_party/pyasn1/pyasn1/compat/binary.py',
    'third_party/pyasn1/pyasn1/compat/octets.py',
    'third_party/pyasn1/pyasn1/debug.py',
    'third_party/pyasn1/pyasn1/error.py',
    'third_party/pyasn1/pyasn1/type/__init__.py',
    'third_party/pyasn1/pyasn1/type/base.py',
    'third_party/pyasn1/pyasn1/type/char.py',
    'third_party/pyasn1/pyasn1/type/constraint.py',
    'third_party/pyasn1/pyasn1/type/error.py',
    'third_party/pyasn1/pyasn1/type/namedtype.py',
    'third_party/pyasn1/pyasn1/type/namedval.py',
    'third_party/pyasn1/pyasn1/type/tag.py',
    'third_party/pyasn1/pyasn1/type/tagmap.py',
    'third_party/pyasn1/pyasn1/type/univ.py',
    'third_party/pyasn1/pyasn1/type/useful.py',
    'third_party/requests/__init__.py',
    'third_party/requests/adapters.py',
    'third_party/requests/api.py',
    'third_party/requests/auth.py',
    'third_party/requests/certs.py',
    'third_party/requests/compat.py',
    'third_party/requests/cookies.py',
    'third_party/requests/exceptions.py',
    'third_party/requests/hooks.py',
    'third_party/requests/models.py',
    'third_party/requests/packages/__init__.py',
    'third_party/requests/packages/urllib3/__init__.py',
    'third_party/requests/packages/urllib3/_collections.py',
    'third_party/requests/packages/urllib3/connection.py',
    'third_party/requests/packages/urllib3/connectionpool.py',
    'third_party/requests/packages/urllib3/contrib/__init__.py',
    'third_party/requests/packages/urllib3/contrib/ntlmpool.py',
    'third_party/requests/packages/urllib3/contrib/pyopenssl.py',
    'third_party/requests/packages/urllib3/exceptions.py',
    'third_party/requests/packages/urllib3/fields.py',
    'third_party/requests/packages/urllib3/filepost.py',
    'third_party/requests/packages/urllib3/packages/__init__.py',
    'third_party/requests/packages/urllib3/packages/ordered_dict.py',
    'third_party/requests/packages/urllib3/packages/six.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '__init__.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '_implementation.py',
    'third_party/requests/packages/urllib3/poolmanager.py',
    'third_party/requests/packages/urllib3/request.py',
    'third_party/requests/packages/urllib3/response.py',
    'third_party/requests/packages/urllib3/util/__init__.py',
    'third_party/requests/packages/urllib3/util/connection.py',
    'third_party/requests/packages/urllib3/util/request.py',
    'third_party/requests/packages/urllib3/util/response.py',
    'third_party/requests/packages/urllib3/util/retry.py',
    'third_party/requests/packages/urllib3/util/ssl_.py',
    'third_party/requests/packages/urllib3/util/timeout.py',
    'third_party/requests/packages/urllib3/util/url.py',
    'third_party/requests/sessions.py',
    'third_party/requests/status_codes.py',
    'third_party/requests/structures.py',
    'third_party/requests/utils.py',
    'third_party/rsa/rsa/__init__.py',
    'third_party/rsa/rsa/_compat.py',
    'third_party/rsa/rsa/_version133.py',
    'third_party/rsa/rsa/_version200.py',
    'third_party/rsa/rsa/asn1.py',
    'third_party/rsa/rsa/bigfile.py',
    'third_party/rsa/rsa/cli.py',
    'third_party/rsa/rsa/common.py',
    'third_party/rsa/rsa/core.py',
    'third_party/rsa/rsa/key.py',
    'third_party/rsa/rsa/parallel.py',
    'third_party/rsa/rsa/pem.py',
    'third_party/rsa/rsa/pkcs1.py',
    'third_party/rsa/rsa/prime.py',
    'third_party/rsa/rsa/randnum.py',
    'third_party/rsa/rsa/transform.py',
    'third_party/rsa/rsa/util.py',
    'third_party/rsa/rsa/varblock.py',
    'third_party/six/__init__.py',
    'utils/__init__.py',
    'utils/cacert.pem',
    'utils/file_path.py',
    'utils/fs.py',
    'utils/large.py',
    'utils/logging_utils.py',
    'utils/lru.py',
    'utils/net.py',
    'utils/oauth.py',
    'utils/on_error.py',
    'utils/subprocess42.py',
    'utils/threading_utils.py',
    'utils/tools.py',
    'utils/zip_package.py',
    'adb/__init__.py',
    'adb/adb_commands.py',
    'adb/adb_protocol.py',
    'adb/common.py',
    'adb/contrib/__init__.py',
    'adb/contrib/adb_commands_safe.py',
    'adb/contrib/high.py',
    'adb/contrib/parallel.py',
    'adb/fastboot.py',
    'adb/filesync_protocol.py',
    'adb/sign_pythonrsa.py',
    'adb/usb_exceptions.py',
    'python_libusb1/__init__.py',
    'python_libusb1/libusb1.py',
    'python_libusb1/usb1.py',
)


def is_windows():
  """"""Returns True if this code is running under Windows.""""""
  return os.__file__[0] != '/'


def resolve_symlink(path):
  """"""Processes path containing symlink on Windows.

  This is needed to make ../swarming_bot/main_test.py pass on Windows because
  git on Windows renders symlinks as normal files.
  """"""
  if not is_windows():
    # Only does this dance on Windows.
    return path
  parts = os.path.normpath(path).split(os.path.sep)
  for i in xrange(2, len(parts)):
    partial = os.path.sep.join(parts[:i])
    if os.path.isfile(partial):
      with open(partial) as f:
        link = f.read()
      assert '\n' not in link and link, link
      parts[i-1] = link
  return os.path.normpath(os.path.sep.join(parts))


def yield_swarming_bot_files(root_dir, host, host_version, additionals):
  """"""Yields all the files to map as tuple(filename, content).

  config.json is injected with json data about the server.

  This function guarantees that the output is sorted by filename.
  """"""
  items = {i: None for i in FILES}
  items.update(additionals)
  config = {
    'server': host.rstrip('/'),
    'server_version': host_version,
  }
  items['config/config.json'] = json.dumps(config)
  for item, content in sorted(items.iteritems()):
    if content is not None:
      yield item, content
    else:
      with open(resolve_symlink(os.path.join(root_dir, item)), 'rb') as f:
        yield item, f.read()


def get_swarming_bot_zip(root_dir, host, host_version, additionals):
  """"""Returns a zipped file of all the files a bot needs to run.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: dict(filepath: content) of additional items to put into the zip
        file, in addition to FILES and MAPPED. In practice, it's going to be a
        custom bot_config.py.
  Returns:
    Tuple(str being the zipped file's content, bot version (SHA-1) it
    represents).
  """"""
  zip_memory_file = StringIO.StringIO()
  h = hashlib.sha1()
  with zipfile.ZipFile(zip_memory_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      zip_file.writestr(name, content)
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)

  data = zip_memory_file.getvalue()
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_zip(%s) is %d bytes; %s',
      additionals.keys(), len(data), bot_version)
  return data, bot_version


def get_swarming_bot_version(root_dir, host, host_version, additionals):
  """"""Returns the SHA1 hash of the bot code, representing the version.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: See get_swarming_bot_zip's doc.

  Returns:
    The SHA1 hash of the bot code.
  """"""
  h = hashlib.sha1()
  try:
    # TODO(maruel): Deduplicate from zip_package.genereate_version().
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)
  except IOError:
    logging.warning('Missing expected file. Hash will be invalid.')
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_version(%s) = %s', sorted(additionals), bot_version)
  return bot_version
/n/n/n/appengine/swarming/swarming_bot/__main__.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs either task_runner.py, bot_main.py or bot_config.py.

The imports are done late so if an ImportError occurs, it is localized to this
command only.
""""""

import code
import json
import logging
import os
import optparse
import shutil
import sys
import zipfile

from bot_code import common

# That's from ../../../client/
from third_party.depot_tools import fix_encoding
from utils import logging_utils
from utils import zip_package

# This file can only be run as a zip.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# libusb1 expects to be directly in sys.path.
sys.path.insert(0, os.path.join(THIS_FILE, 'python_libusb1'))


# TODO(maruel): Use depot_tools/subcommand.py. The goal here is to have all the
# sub commands packed into the single .zip file as a swiss army knife (think
# busybox but worse).


def CMDattributes(_args):
  """"""Prints out the bot's attributes.""""""
  from bot_code import bot_main
  json.dump(
      bot_main.get_attributes(bot_main.get_bot()), sys.stdout, indent=2,
      sort_keys=True, separators=(',', ': '))
  print('')
  return 0


def CMDconfig(_args):
  """"""Prints the config.json embedded in this zip.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  json.dump(bot_main.get_config(), sys.stdout, indent=2, sort_keys=True)
  print('')
  return 0


def CMDis_fine(_args):
  """"""Just reports that the code doesn't throw.

  That ensures that the bot has minimal viability before transfering control to
  it. For now, it just imports bot_main but later it'll check the config, etc.
  """"""
  # pylint: disable=unused-variable
  from bot_code import bot_main
  from config import bot_config
  # We're #goodenough.
  return 0


def CMDrestart(_args):
  """"""Utility subcommand that hides the difference between each OS to reboot
  the host.""""""
  logging_utils.prepare_logging(None)
  import os_utilities
  # This function doesn't return.
  os_utilities.restart()
  # Should never reach here.
  return 1


def CMDrun_isolated(args):
  """"""Internal command to run an isolated command.""""""
  sys.path.insert(0, os.path.join(THIS_FILE, 'client'))
  # run_isolated setups logging by itself.
  import run_isolated
  return run_isolated.main(args)


def CMDsetup(_args):
  """"""Setup the bot to auto-start but doesn't start the bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))
  from bot_code import bot_main
  bot_main.setup_bot(True)
  return 0


def CMDserver(_args):
  """"""Prints the server url. It's like 'config' but easier to parse.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  print bot_main.get_config()['server']
  return 0


def CMDshell(args):
  """"""Starts a shell with api.* in..""""""
  logging_utils.prepare_logging(None)
  logging_utils.set_console_level(logging.DEBUG)

  from bot_code import bot_main
  from api import os_utilities
  from api import platforms
  local_vars = {
    'bot_main': bot_main,
    'json': json,
    'os_utilities': os_utilities,
    'platforms': platforms,
  }
  # Can't use: from api.platforms import *
  local_vars.update(
      (k, v) for k, v in platforms.__dict__.iteritems()
      if not k.startswith('_'))

  if args:
    for arg in args:
      exec code.compile_command(arg) in local_vars
  else:
    code.interact(
        'Locals:\n  ' + '\n  '.join( sorted(local_vars)), None, local_vars)
  return 0


def CMDstart_bot(args):
  """"""Starts the swarming bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'swarming_bot.log'))
  logging.info(
      'importing bot_main: %s, %s', THIS_FILE, zip_package.generate_version())
  from bot_code import bot_main
  result = bot_main.main(args)
  logging.info('bot_main exit code: %d', result)
  return result


def CMDstart_slave(args):
  """"""Ill named command that actually sets up the bot then start it.""""""
  # TODO(maruel): Rename function.
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))

  parser = optparse.OptionParser()
  parser.add_option(
      '--survive', action='store_true',
      help='Do not reboot the host even if bot_config.setup_bot() asked to')
  options, args = parser.parse_args(args)

  try:
    from bot_code import bot_main
    bot_main.setup_bot(options.survive)
  except Exception:
    logging.exception('bot_main.py failed.')

  logging.info('Starting the bot: %s', THIS_FILE)
  return common.exec_python([THIS_FILE, 'start_bot'])


def CMDtask_runner(args):
  """"""Internal command to run a swarming task.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'task_runner.log'))
  from bot_code import task_runner
  return task_runner.main(args)


def CMDversion(_args):
  """"""Prints the version of this file and the hash of the code.""""""
  logging_utils.prepare_logging(None)
  print zip_package.generate_version()
  return 0


def main():
  if os.getenv('CHROME_REMOTE_DESKTOP_SESSION') == '1':
    # Disable itself when run under Google Chrome Remote Desktop, as it's
    # normally started at the console and starting up via Remote Desktop would
    # cause multiple bots to run concurrently on the host.
    print >> sys.stderr, (
        'Inhibiting Swarming bot under Google Chrome Remote Desktop.')
    return 0

  # Always make the current working directory the directory containing this
  # file. It simplifies assumptions.
  os.chdir(os.path.dirname(THIS_FILE))
  # Always create the logs dir first thing, before printing anything out.
  if not os.path.isdir('logs'):
    os.mkdir('logs')

  # This is necessary so os.path.join() works with unicode path. No kidding.
  # This must be done here as each of the command take wildly different code
  # path and this must be run in every case, as it causes really unexpected
  # issues otherwise, especially in module os.path.
  fix_encoding.fix_encoding()

  if os.path.basename(THIS_FILE) == 'swarming_bot.zip':
    # Self-replicate itself right away as swarming_bot.1.zip and restart as it.
    print >> sys.stderr, 'Self replicating pid:%d.' % os.getpid()
    if os.path.isfile('swarming_bot.1.zip'):
      os.remove('swarming_bot.1.zip')
    shutil.copyfile('swarming_bot.zip', 'swarming_bot.1.zip')
    cmd = ['swarming_bot.1.zip'] + sys.argv[1:]
    print >> sys.stderr, 'cmd: %s' % cmd
    return common.exec_python(cmd)

  # sys.argv[0] is the zip file itself.
  cmd = 'start_slave'
  args = []
  if len(sys.argv) > 1:
    cmd = sys.argv[1]
    args = sys.argv[2:]

  fn = getattr(sys.modules[__name__], 'CMD%s' % cmd, None)
  if fn:
    try:
      return fn(args)
    except ImportError:
      logging.exception('Failed to run %s', cmd)
      with zipfile.ZipFile(THIS_FILE, 'r') as f:
        logging.error('Files in %s:\n%s', THIS_FILE, f.namelist())
      return 1

  print >> sys.stderr, 'Unknown command %s' % cmd
  return 1


if __name__ == '__main__':
  sys.exit(main())
/n/n/n/appengine/swarming/swarming_bot/api/bot.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Bot interface used in bot_config.py.""""""

import logging
import os
import threading
import time

import os_utilities
from utils import zip_package

THIS_FILE = os.path.abspath(zip_package.get_main_script_path())

# Method could be a function - pylint: disable=R0201


class Bot(object):
  def __init__(
      self, remote, attributes, server, server_version, base_dir,
      shutdown_hook):
    # Do not expose attributes nor remote for now, as attributes will be
    # refactored soon and remote would have a lot of side effects if used by
    # bot_config.
    self._attributes = attributes
    self._base_dir = base_dir
    self._remote = remote
    self._server = server
    self._server_version = server_version
    self._shutdown_hook = shutdown_hook
    self._timers = []
    self._timers_dying = False
    self._timers_lock = threading.Lock()

  @property
  def base_dir(self):
    """"""Returns the working directory.

    It is normally the current workind directory, e.g. os.getcwd() but it is
    preferable to not assume that.
    """"""
    return self._base_dir

  @property
  def dimensions(self):
    """"""The bot's current dimensions.

    Dimensions are relatively static and not expected to change much. They
    should change only when it effectively affects the bot's capacity to execute
    tasks.
    """"""
    return self._attributes.get('dimensions', {}).copy()

  @property
  def id(self):
    """"""Returns the bot's ID.""""""
    return self.dimensions.get('id', ['unknown'])[0]

  @property
  def remote(self):
    """"""XsrfClient instance to talk to the server.

    Should not be normally used by bot_config.py for now.
    """"""
    return self._remote

  @property
  def server(self):
    """"""URL of the swarming server this bot is connected to.

    It includes the https:// prefix but without trailing /, so it looks like
    ""https://foo-bar.appspot.com"".
    """"""
    return self._server

  @property
  def server_version(self):
    """"""Version of the server's implementation.

    The form is nnn-hhhhhhh for pristine version and nnn-hhhhhhh-tainted-uuuu
    for non-upstreamed code base:
      nnn: revision pseudo number
      hhhhhhh: git commit hash
      uuuu: username
    """"""
    return self._server_version

  @property
  def state(self):
    return self._attributes['state']

  @property
  def swarming_bot_zip(self):
    """"""Absolute path to the swarming_bot.zip file.

    The bot itself is run as swarming_bot.1.zip or swarming_bot.2.zip. Always
    return swarming_bot.zip since this is the script that must be used when
    starting up.
    """"""
    return os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')

  def post_event(self, event_type, message):
    """"""Posts an event to the server.""""""
    data = self._attributes.copy()
    data['event'] = event_type
    data['message'] = message
    self._remote.url_read_json('/swarming/api/v1/bot/event', data=data)

  def post_error(self, message):
    """"""Posts given string as a failure.

    This is used in case of internal code error. It traps exception.
    """"""
    logging.error('Error: %s\n%s', self._attributes, message)
    try:
      self.post_event('bot_error', message)
    except Exception:
      logging.exception('post_error(%s) failed.', message)

  def restart(self, message):
    """"""Reboots the machine.

    If the reboot is successful, never returns: the process should just be
    killed by OS.

    If reboot fails, logs the error to the server and moves the bot to
    quarantined mode.
    """"""
    self.post_event('bot_rebooting', message)
    self.cancel_all_timers()
    if self._shutdown_hook:
      try:
        self._shutdown_hook(self)
      except Exception as e:
        logging.exception('shutdown hook failed: %s', e)
    # os_utilities.restart should never return, unless restart is not happening.
    # If restart is taking longer than N minutes, it probably not going to
    # finish at all. Report this to the server.
    try:
      os_utilities.restart(message, timeout=15*60)
    except LookupError:
      # This is a special case where OSX is deeply hosed. In that case the disk
      # is likely in read-only mode and there isn't much that can be done. This
      # exception is deep inside pickle.py. So notify the server then hang in
      # there.
      self.post_error('This host partition is bad; please fix the host')
      while True:
        time.sleep(1)
    self.post_error('Bot is stuck restarting for: %s' % message)

  def call_later(self, delay_sec, callback):
    """"""Schedules a function to be called later (if bot is still running).

    All calls are executed in a separate internal thread, be careful with what
    you call from there (Bot object is generally not thread safe).

    Multiple callbacks can be executed concurrently. It is safe to call
    'call_later' from the callback.
    """"""
    timer = None

    def call_wrapper():
      with self._timers_lock:
        # Canceled already?
        if timer not in self._timers:
          return
        self._timers.remove(timer)
      try:
        callback()
      except Exception:
        logging.exception('Timer callback failed')

    with self._timers_lock:
      if not self._timers_dying:
        timer = threading.Timer(delay_sec, call_wrapper)
        self._timers.append(timer)
        timer.daemon = True
        timer.start()

  def cancel_all_timers(self):
    """"""Cancels all pending 'call_later' calls and forbids adding new ones.""""""
    timers = None
    with self._timers_lock:
      self._timers_dying = True
      for t in self._timers:
        t.cancel()
      timers, self._timers = self._timers, []
    for t in timers:
      t.join(timeout=5)
      if t.isAlive():
        logging.error('Timer thread did not terminate fast enough: %s', t)

  def update_dimensions(self, new_dimensions):
    """"""Called internally to update Bot.dimensions.""""""
    self._attributes['dimensions'] = new_dimensions

  def update_state(self, new_state):
    """"""Called internally to update Bot.state.""""""
    self._attributes['state'] = new_state
/n/n/n/appengine/swarming/swarming_bot/api/bot_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import os
import sys
import unittest
import threading

THIS_FILE = os.path.abspath(__file__)

import test_env_api
test_env_api.setup_test_env()

import bot


class TestBot(unittest.TestCase):
  def test_bot(self):
    obj = bot.Bot(
        None,
        {'dimensions': {'foo': 'bar'}},
        'https://localhost:1/',
        '1234-1a2b3c4-tainted-joe',
        'base_dir',
        None)
    self.assertEqual({'foo': 'bar'}, obj.dimensions)
    self.assertEqual(
        os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip'),
        obj.swarming_bot_zip)
    self.assertEqual('1234-1a2b3c4-tainted-joe', obj.server_version)
    self.assertEqual('base_dir', obj.base_dir)

  def test_bot_call_later(self):
    obj = bot.Bot(None, {}, 'https://localhost:1/', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.001, ev.set)
    self.assertTrue(ev.wait(1))

  def test_bot_call_later_cancel(self):
    obj = bot.Bot(None, {}, 'https://localhost:1/', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.1, ev.set)
    obj.cancel_all_timers()
    self.assertFalse(ev.wait(0.3))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/n/appengine/swarming/swarming_bot/bot_code/bot_main_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import json
import logging
import os
import shutil
import sys
import tempfile
import threading
import time
import unittest
import zipfile

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import bot_main
import xsrf_client
from api import bot
from api import os_utilities
from depot_tools import fix_encoding
from utils import file_path
from utils import logging_utils
from utils import net
from utils import subprocess42
from utils import zip_package


# Access to a protected member XX of a client class - pylint: disable=W0212


class TestBotMain(net_utils.TestCase):
  maxDiff = 2000

  def setUp(self):
    super(TestBotMain, self).setUp()
    os.environ.pop('SWARMING_LOAD_TEST', None)
    self.root_dir = tempfile.mkdtemp(prefix='bot_main')
    self.old_cwd = os.getcwd()
    os.chdir(self.root_dir)
    # __main__ does it for us.
    os.mkdir('logs')
    self.server = xsrf_client.XsrfRemote('https://localhost:1/')
    self.attributes = {
      'dimensions': {
        'foo': ['bar'],
        'id': ['localhost'],
        'pool': ['default'],
      },
      'state': {
        'cost_usd_hour': 3600.,
      },
      'version': '123',
    }
    self.mock(zip_package, 'generate_version', lambda: '123')
    self.bot = bot.Bot(
        self.server, self.attributes, 'https://localhost:1/', 'version1',
        self.root_dir, self.fail)
    self.mock(self.bot, 'post_error', self.fail)
    self.mock(self.bot, 'restart', self.fail)
    self.mock(subprocess42, 'call', self.fail)
    self.mock(time, 'time', lambda: 100.)
    config_path = os.path.join(
        test_env_bot_code.BOT_DIR, 'config', 'config.json')
    with open(config_path, 'rb') as f:
      config = json.load(f)
    self.mock(bot_main, 'get_config', lambda: config)
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(test_env_bot_code.BOT_DIR, 'swarming_bot.zip'))

  def tearDown(self):
    os.environ.pop('SWARMING_BOT_ID', None)
    os.chdir(self.old_cwd)
    file_path.rmtree(self.root_dir)
    super(TestBotMain, self).tearDown()

  def test_get_dimensions(self):
    dimensions = set(bot_main.get_dimensions(None))
    dimensions.discard('hidpi')
    dimensions.discard('zone')  # Only set on GCE bots.
    expected = {'cores', 'cpu', 'gpu', 'id', 'machine_type', 'os', 'pool'}
    self.assertEqual(expected, dimensions)

  def test_get_dimensions_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    self.assertEqual(['id', 'load_test'], sorted(bot_main.get_dimensions(None)))

  def test_generate_version(self):
    self.assertEqual('123', bot_main.generate_version())

  def test_get_state(self):
    self.mock(time, 'time', lambda: 126.0)
    expected = os_utilities.get_state()
    expected['sleep_streak'] = 12
    # During the execution of this test case, the free disk space could have
    # changed.
    for disk in expected['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    actual = bot_main.get_state(None, 12)
    for disk in actual['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    self.assertGreater(actual.pop('nb_files_in_temp'), 0)
    self.assertGreater(expected.pop('nb_files_in_temp'), 0)
    self.assertGreater(actual.pop('uptime'), 0)
    self.assertGreater(expected.pop('uptime'), 0)
    self.assertEqual(sorted(expected.pop('temp', {})),
                     sorted(actual.pop('temp', {})))
    self.assertEqual(expected, actual)

  def test_setup_bot(self):
    self.mock(bot_main, 'get_remote', lambda: self.server)
    setup_bots = []
    def setup_bot(_bot):
      setup_bots.append(1)
      return False
    from config import bot_config
    self.mock(bot_config, 'setup_bot', setup_bot)
    restarts = []
    post_event = []
    self.mock(
        os_utilities, 'restart', lambda *a, **kw: restarts.append((a, kw)))
    self.mock(
        bot.Bot, 'post_event', lambda *a, **kw: post_event.append((a, kw)))
    self.expected_requests([])
    bot_main.setup_bot(False)
    expected = [
      (('Starting new swarming bot: %s' % bot_main.THIS_FILE,),
        {'timeout': 900}),
    ]
    self.assertEqual(expected, restarts)
    # It is called twice, one as part of setup_bot(False), another as part of
    # on_shutdown_hook().
    self.assertEqual([1, 1], setup_bots)
    expected = [
      'Starting new swarming bot: %s' % bot_main.THIS_FILE,
      'Bot is stuck restarting for: Starting new swarming bot: %s' %
        bot_main.THIS_FILE,
    ]
    self.assertEqual(expected, [i[0][2] for i in post_event])

  def test_post_error_task(self):
    self.mock(time, 'time', lambda: 126.0)
    self.mock(logging, 'error', lambda *_, **_kw: None)
    self.mock(bot_main, 'get_remote', lambda: self.server)
    # get_state() return value changes over time. Hardcode its value for the
    # duration of this test.
    self.mock(os_utilities, 'get_state', lambda : {'foo': 'bar'})
    expected_attribs = bot_main.get_attributes(None)
    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {
              'data': expected_attribs,
              'headers': {'X-XSRF-Token-Request': '1'},
            },
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/task_error/23',
            {
              'data': {
                'id': expected_attribs['dimensions']['id'][0],
                'message': 'error',
                'task_id': 23,
              },
              'headers': {'X-XSRF-Token': 'token'},
            },
            {},
          ),
        ])
    botobj = bot_main.get_bot()
    bot_main.post_error_task(botobj, 'error', 23)

  def test_run_bot(self):
    # Test the run_bot() loop. Does not use self.bot.
    self.mock(time, 'time', lambda: 126.0)
    class Foo(Exception):
      pass

    def poll_server(botobj, _):
      sleep_streak = botobj.state['sleep_streak']
      self.assertEqual(botobj.remote, self.server)
      if sleep_streak == 5:
        raise Exception('Jumping out of the loop')
      return False
    self.mock(bot_main, 'poll_server', poll_server)

    def post_error(botobj, e):
      self.assertEqual(self.server, botobj._remote)
      lines = e.splitlines()
      self.assertEqual('Jumping out of the loop', lines[0])
      self.assertEqual('Traceback (most recent call last):', lines[1])
      raise Foo('Necessary to get out of the loop')
    self.mock(bot.Bot, 'post_error', post_error)

    self.mock(bot_main, 'get_remote', lambda: self.server)

    # Method should have ""self"" as first argument - pylint: disable=E0213
    # pylint: disable=unused-argument
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        expected = [sys.executable, bot_main.THIS_FILE, 'run_isolated']
        self.assertEqual(expected, cmd[:len(expected)])
        self.assertEqual(True, detached)
        self.assertEqual(subprocess42.PIPE, stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def communicate(self2, i):
        self.assertEqual(None, i)
        self2.returncode = 0
        return '', None
    self.mock(subprocess42, 'Popen', Popen)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/server_ping',
            {}, 'foo', None,
          ),
        ])

    with self.assertRaises(Foo):
      bot_main.run_bot(None)
    self.assertEqual(
        os_utilities.get_hostname_short(), os.environ['SWARMING_BOT_ID'])

  def test_poll_server_sleep(self):
    slept = []
    bit = threading.Event()
    self.mock(bit, 'wait', slept.append)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'sleep',
              'duration': 1.24,
            },
          ),
        ])
    self.assertFalse(bot_main.poll_server(self.bot, bit))
    self.assertEqual([1.24], slept)

  def test_poll_server_run(self):
    manifest = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', lambda *args: manifest.append(args))
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.bot._attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'run',
              'manifest': {'foo': 'bar'},
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    expected = [(self.bot, {'foo': 'bar'}, time.time())]
    self.assertEqual(expected, manifest)

  def test_poll_server_update(self):
    update = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', lambda *args: update.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'update',
              'version': '123',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([(self.bot, '123')], update)

  def test_poll_server_restart(self):
    restart = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', lambda *args: restart.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([('Please die now',)], restart)

  def test_poll_server_restart_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))

  def _mock_popen(self, returncode=0, exit_code=0, url='https://localhost:1'):
    result = {
      'exit_code': exit_code,
      'must_signal_internal_failure': None,
      'version': 3,
    }
    # Method should have ""self"" as first argument - pylint: disable=E0213
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, env, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        self2._out_file = os.path.join(
            self.root_dir, 'work', 'task_runner_out.json')
        expected = [
          sys.executable, bot_main.THIS_FILE, 'task_runner',
          '--swarming-server', url,
          '--in-file',
          os.path.join(self.root_dir, 'work', 'task_runner_in.json'),
          '--out-file', self2._out_file,
          '--cost-usd-hour', '3600.0', '--start', '100.0',
          '--min-free-space',
          str(int(
            (os_utilities.get_min_free_space(bot_main.THIS_FILE) + 250.) *
            1024 * 1024)),
        ]
        self.assertEqual(expected, cmd)
        self.assertEqual(True, detached)
        self.assertEqual(self.bot.base_dir, cwd)
        self.assertEqual('24', env['SWARMING_TASK_ID'])
        self.assertTrue(stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def wait(self2, timeout=None): # pylint: disable=unused-argument
        self2.returncode = returncode
        with open(self2._out_file, 'wb') as f:
          json.dump(result, f)
        return 0

    self.mock(subprocess42, 'Popen', Popen)
    return result

  def test_run_manifest(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(self.attributes['dimensions'], botobj.dimensions)
        self.assertEqual(False, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'os': 'Amiga', 'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(url='https://localhost:3')

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'os': 'Amiga', 'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'host': 'https://localhost:3',
      'task_id': '24',
    }
    self.assertEqual(self.root_dir, self.bot.base_dir)
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_task_failure(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(True, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(exit_code=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_internal_failure(self):
    posted = []
    self.mock(bot_main, 'post_error_task', lambda *args: posted.append(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(returncode=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Execution failed: internal error (1).', '24')]
    self.assertEqual(expected, posted)

  def test_run_manifest_exception(self):
    posted = []
    def post_error_task(botobj, msg, task_id):
      posted.append((botobj, msg.splitlines()[0], task_id))
    self.mock(bot_main, 'post_error_task', post_error_task)
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual({}, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    def raiseOSError(*_a, **_k):
      raise OSError('Dang')
    self.mock(subprocess42, 'Popen', raiseOSError)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Internal exception occured: Dang', '24')]
    self.assertEqual(expected, posted)

  def test_update_bot(self):
    # In a real case 'update_bot' never exits and doesn't call 'post_error'.
    # Under the test however forever-blocking calls finish, and post_error is
    # called.
    self.mock(self.bot, 'post_error', lambda *_: None)
    # Mock the file to download in the temporary directory.
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(self.root_dir, 'swarming_bot.1.zip'))
    new_zip = os.path.join(self.root_dir, 'swarming_bot.2.zip')
    # This is necessary otherwise zipfile will crash.
    self.mock(time, 'time', lambda: 1400000000)
    def url_retrieve(f, url):
      self.assertEqual(
          'https://localhost:1/swarming/api/v1/bot/bot_code/123', url)
      self.assertEqual(new_zip, f)
      # Create a valid zip that runs properly.
      with zipfile.ZipFile(f, 'w') as z:
        z.writestr('__main__.py', 'print(""hi"")')
      return True
    self.mock(net, 'url_retrieve', url_retrieve)

    calls = []
    def exec_python(args):
      calls.append(args)
      return 23
    self.mock(bot_main.common, 'exec_python', exec_python)

    with self.assertRaises(SystemExit) as e:
      bot_main.update_bot(self.bot, '123')
    self.assertEqual(23, e.exception.code)

    self.assertEqual([[new_zip, 'start_slave', '--survive']], calls)

  def test_main(self):
    def check(x):
      self.assertEqual(logging.WARNING, x)
    self.mock(logging_utils, 'set_console_level', check)

    def run_bot(error):
      self.assertEqual(None, error)
      return 0
    self.mock(bot_main, 'run_bot', run_bot)

    class Singleton(object):
      # pylint: disable=no-self-argument
      def acquire(self2):
        return True
      def release(self2):
        self.fail()
    self.mock(bot_main, 'SINGLETON', Singleton())

    self.assertEqual(0, bot_main.main([]))


if __name__ == '__main__':
  fix_encoding.fix_encoding()
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  logging.basicConfig(
      level=logging.DEBUG if '-v' in sys.argv else logging.CRITICAL)
  unittest.main()
/n/n/n/appengine/swarming/swarming_bot/bot_code/task_runner.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs a Swarming task.

Downloads all the necessary files to run the task, executes the command and
streams results back to the Swarming server.

The process exit code is 0 when the task was executed, even if the task itself
failed. If there's any failure in the setup or teardown, like invalid packet
response, failure to contact the server, etc, a non zero exit code is used. It's
up to the calling process (bot_main.py) to signal that there was an internal
failure and to cancel this task run and ask the server to retry it.
""""""

import base64
import json
import logging
import optparse
import os
import signal
import sys
import time

import xsrf_client
from utils import net
from utils import on_error
from utils import subprocess42
from utils import zip_package


# Path to this file or the zip containing this file.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# Sends a maximum of 100kb of stdout per task_update packet.
MAX_CHUNK_SIZE = 102400


# Maximum wait between task_update packet when there's no output.
MAX_PACKET_INTERVAL = 30


# Minimum wait between task_update packet when there's output.
MIN_PACKET_INTERNAL = 10


# Current task_runner_out version.
OUT_VERSION = 3


# On Windows, SIGTERM is actually sent as SIGBREAK since there's no real
# SIGTERM.  SIGBREAK is not defined on posix since it's a pure Windows concept.
SIG_BREAK_OR_TERM = (
    signal.SIGBREAK if sys.platform == 'win32' else signal.SIGTERM)


# Used to implement monotonic_time for a clock that never goes backward.
_last_now = 0


def monotonic_time():
  """"""Returns monotonically increasing time.""""""
  global _last_now
  now = time.time()
  if now > _last_now:
    # TODO(maruel): If delta is large, probably worth alerting via ereporter2.
    _last_now = now
  return _last_now


def get_run_isolated():
  """"""Returns the path to itself to run run_isolated.

  Mocked in test to point to the real run_isolated.py script.
  """"""
  return [sys.executable, THIS_FILE, 'run_isolated']


def get_isolated_cmd(
    work_dir, task_details, isolated_result, min_free_space):
  """"""Returns the command to call run_isolated. Mocked in tests.""""""
  bot_dir = os.path.dirname(work_dir)
  if os.path.isfile(isolated_result):
    os.remove(isolated_result)
  cmd = get_run_isolated()
  cmd.extend(
      [
        '--isolated', task_details.inputs_ref['isolated'].encode('utf-8'),
        '--namespace', task_details.inputs_ref['namespace'].encode('utf-8'),
        '-I', task_details.inputs_ref['isolatedserver'].encode('utf-8'),
        '--json', isolated_result,
        '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),
        '--cache', os.path.join(bot_dir, 'cache'),
        '--root-dir', os.path.join(work_dir, 'isolated'),
      ])
  if min_free_space:
    cmd.extend(('--min-free-space', str(min_free_space)))

  if task_details.hard_timeout:
    cmd.extend(('--hard-timeout', str(task_details.hard_timeout)))
  if task_details.grace_period:
    cmd.extend(('--grace-period', str(task_details.grace_period)))
  if task_details.extra_args:
    cmd.append('--')
    cmd.extend(task_details.extra_args)
  return cmd


class TaskDetails(object):
  def __init__(self, data):
    """"""Loads the raw data.

    It is expected to have at least:
     - bot_id
     - command as a list of str
     - data as a list of urls
     - env as a dict
     - hard_timeout
     - io_timeout
     - task_id
    """"""
    logging.info('TaskDetails(%s)', data)
    if not isinstance(data, dict):
      raise ValueError('Expected dict, got %r' % data)

    # Get all the data first so it fails early if the task details is invalid.
    self.bot_id = data['bot_id']

    # Raw command. Only self.command or self.inputs_ref can be set.
    self.command = data['command'] or []

    # Isolated command. Is a serialized version of task_request.FilesRef.
    self.inputs_ref = data['inputs_ref']
    self.extra_args = data['extra_args']

    self.env = {
      k.encode('utf-8'): v.encode('utf-8') for k, v in data['env'].iteritems()
    }
    self.grace_period = data['grace_period']
    self.hard_timeout = data['hard_timeout']
    self.io_timeout = data['io_timeout']
    self.task_id = data['task_id']


class MustExit(Exception):
  """"""Raised on signal that the process must exit immediately.""""""
  def __init__(self, sig):
    super(MustExit, self).__init__()
    self.signal = sig


def load_and_run(
    in_file, swarming_server, cost_usd_hour, start, out_file, min_free_space):
  """"""Loads the task's metadata and execute it.

  This may throw all sorts of exceptions in case of failure. It's up to the
  caller to trap them. These shall be considered 'internal_failure' instead of
  'failure' from a TaskRunResult standpoint.
  """"""
  # The work directory is guaranteed to exist since it was created by
  # bot_main.py and contains the manifest. Temporary files will be downloaded
  # there. It's bot_main.py that will delete the directory afterward. Tests are
  # not run from there.
  task_result = None
  def handler(sig, _):
    logging.info('Got signal %s', sig)
    raise MustExit(sig)
  work_dir = os.path.dirname(out_file)
  try:
    with subprocess42.set_signal_handler([SIG_BREAK_OR_TERM], handler):
      if not os.path.isdir(work_dir):
        raise ValueError('%s expected to exist' % work_dir)

      with open(in_file, 'rb') as f:
        task_details = TaskDetails(json.load(f))

      task_result = run_command(
          swarming_server, task_details, work_dir, cost_usd_hour, start,
          min_free_space)
  except MustExit as e:
    # This normally means run_command() didn't get the chance to run, as it
    # itself trap MustExit and will report accordingly. In this case, we want
    # the parent process to send the message instead.
    if not task_result:
      task_result = {
        u'exit_code': None,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure':
            u'task_runner received signal %s' % e.signal,
        u'version': OUT_VERSION,
      }
  finally:
    # We've found tests to delete 'work' when quitting, causing an exception
    # here. Try to recreate the directory if necessary.
    if not os.path.isdir(work_dir):
      os.mkdir(work_dir)
    with open(out_file, 'wb') as f:
      json.dump(task_result, f)


def post_update(swarming_server, params, exit_code, stdout, output_chunk_start):
  """"""Posts task update to task_update.

  Arguments:
    swarming_server: XsrfRemote instance.
    params: Default JSON parameters for the POST.
    exit_code: Process exit code, only when a command completed.
    stdout: Incremental output since last call, if any.
    output_chunk_start: Total number of stdout previously sent, for coherency
        with the server.
  """"""
  params = params.copy()
  if exit_code is not None:
    params['exit_code'] = exit_code
  if stdout:
    # The output_chunk_start is used by the server to make sure that the stdout
    # chunks are processed and saved in the DB in order.
    params['output'] = base64.b64encode(stdout)
    params['output_chunk_start'] = output_chunk_start
  # TODO(maruel): Support early cancellation.
  # https://code.google.com/p/swarming/issues/detail?id=62
  resp = swarming_server.url_read_json(
      '/swarming/api/v1/bot/task_update/%s' % params['task_id'], data=params)
  logging.debug('post_update() = %s', resp)
  if resp.get('error'):
    # Abandon it. This will force a process exit.
    raise ValueError(resp.get('error'))


def should_post_update(stdout, now, last_packet):
  """"""Returns True if it's time to send a task_update packet via post_update().

  Sends a packet when one of this condition is met:
  - more than MAX_CHUNK_SIZE of stdout is buffered.
  - last packet was sent more than MIN_PACKET_INTERNAL seconds ago and there was
    stdout.
  - last packet was sent more than MAX_PACKET_INTERVAL seconds ago.
  """"""
  packet_interval = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  return len(stdout) >= MAX_CHUNK_SIZE or (now - last_packet) > packet_interval


def calc_yield_wait(task_details, start, last_io, timed_out, stdout):
  """"""Calculates the maximum number of seconds to wait in yield_any().""""""
  now = monotonic_time()
  if timed_out:
    # Give a |grace_period| seconds delay.
    if task_details.grace_period:
      return max(now - timed_out - task_details.grace_period, 0.)
    return 0.

  out = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  if task_details.hard_timeout:
    out = min(out, start + task_details.hard_timeout - now)
  if task_details.io_timeout:
    out = min(out, last_io + task_details.io_timeout - now)
  out = max(out, 0)
  logging.debug('calc_yield_wait() = %d', out)
  return out


def kill_and_wait(proc, grace_period, reason):
  logging.warning('SIGTERM finally due to %s', reason)
  proc.terminate()
  try:
    proc.wait(grace_period)
  except subprocess42.TimeoutError:
    logging.warning('SIGKILL finally due to %s', reason)
    proc.kill()
  exit_code = proc.wait()
  logging.info('Waiting for proces exit in finally - done')
  return exit_code


def run_command(
    swarming_server, task_details, work_dir, cost_usd_hour, task_start,
    min_free_space):
  """"""Runs a command and sends packets to the server to stream results back.

  Implements both I/O and hard timeouts. Sends the packets numbered, so the
  server can ensure they are processed in order.

  Returns:
    Metadata about the command.
  """"""
  # TODO(maruel): This function is incomprehensible, split and refactor.
  # Signal the command is about to be started.
  last_packet = start = now = monotonic_time()
  params = {
    'cost_usd': cost_usd_hour * (now - task_start) / 60. / 60.,
    'id': task_details.bot_id,
    'task_id': task_details.task_id,
  }
  post_update(swarming_server, params, None, '', 0)

  if task_details.command:
    # Raw command.
    cmd = task_details.command
    isolated_result = None
  else:
    # Isolated task.
    isolated_result = os.path.join(work_dir, 'isolated_result.json')
    cmd = get_isolated_cmd(
        work_dir, task_details, isolated_result, min_free_space)
    # Hard timeout enforcement is deferred to run_isolated. Grace is doubled to
    # give one 'grace_period' slot to the child process and one slot to upload
    # the results back.
    task_details.hard_timeout = 0
    if task_details.grace_period:
      task_details.grace_period *= 2

  try:
    # TODO(maruel): Support both channels independently and display stderr in
    # red.
    env = None
    if task_details.env:
      env = os.environ.copy()
      for key, value in task_details.env.iteritems():
        if not value:
          env.pop(key, None)
        else:
          env[key] = value
    logging.info('cmd=%s', cmd)
    logging.info('env=%s', env)
    try:
      proc = subprocess42.Popen(
          cmd,
          env=env,
          cwd=work_dir,
          detached=True,
          stdout=subprocess42.PIPE,
          stderr=subprocess42.STDOUT,
          stdin=subprocess42.PIPE)
    except OSError as e:
      stdout = 'Command ""%s"" failed to start.\nError: %s' % (' '.join(cmd), e)
      now = monotonic_time()
      params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
      params['duration'] = now - start
      params['io_timeout'] = False
      params['hard_timeout'] = False
      post_update(swarming_server, params, 1, stdout, 0)
      return {
        u'exit_code': -1,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': OUT_VERSION,
      }

    output_chunk_start = 0
    stdout = ''
    exit_code = None
    had_hard_timeout = False
    had_io_timeout = False
    must_signal_internal_failure = None
    kill_sent = False
    timed_out = None
    try:
      calc = lambda: calc_yield_wait(
          task_details, start, last_io, timed_out, stdout)
      maxsize = lambda: MAX_CHUNK_SIZE - len(stdout)
      last_io = monotonic_time()
      for _, new_data in proc.yield_any(maxsize=maxsize, timeout=calc):
        now = monotonic_time()
        if new_data:
          stdout += new_data
          last_io = now

        # Post update if necessary.
        if should_post_update(stdout, now, last_packet):
          last_packet = monotonic_time()
          params['cost_usd'] = (
              cost_usd_hour * (last_packet - task_start) / 60. / 60.)
          post_update(swarming_server, params, None, stdout, output_chunk_start)
          output_chunk_start += len(stdout)
          stdout = ''

        # Send signal on timeout if necessary. Both are failures, not
        # internal_failures.
        # Eventually kill but return 0 so bot_main.py doesn't cancel the task.
        if not timed_out:
          if (task_details.io_timeout and
              now - last_io > task_details.io_timeout):
            had_io_timeout = True
            logging.warning('I/O timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
          elif (task_details.hard_timeout and
              now - start > task_details.hard_timeout):
            had_hard_timeout = True
            logging.warning('Hard timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
        else:
          # During grace period.
          if not kill_sent and now >= timed_out + task_details.grace_period:
            # Now kill for real. The user can distinguish between the following
            # states:
            # - signal but process exited within grace period,
            #   (hard_|io_)_timed_out will be set but the process exit code will
            #   be script provided.
            # - processed exited late, exit code will be -9 on posix.
            logging.warning('Grace exhausted; sending SIGKILL')
            proc.kill()
            kill_sent = True
      logging.info('Waiting for proces exit')
      exit_code = proc.wait()
    except MustExit as e:
      # TODO(maruel): Do the send SIGTERM to child process and give it
      # task_details.grace_period to terminate.
      must_signal_internal_failure = (
          u'task_runner received signal %s' % e.signal)
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'signal %d' % e.signal)
    except (IOError, OSError):
      # Something wrong happened, try to kill the child process.
      had_hard_timeout = True
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'exception %s' % e)

    # This is the very last packet for this command. It if was an isolated task,
    # include the output reference to the archived .isolated file.
    now = monotonic_time()
    params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
    params['duration'] = now - start
    params['io_timeout'] = had_io_timeout
    params['hard_timeout'] = had_hard_timeout
    if isolated_result:
      try:
        if ((had_io_timeout or had_hard_timeout) and
            not os.path.isfile(isolated_result)):
          # It's possible that run_isolated failed to quit quickly enough; it
          # could be because there was too much data to upload back or something
          # else. Do not create an internal error, just send back the (partial)
          # view as task_runner saw it, for example the real exit_code is
          # unknown.
          logging.warning('TIMED_OUT and there\'s no result file')
          exit_code = -1
        else:
          # See run_isolated.py for the format.
          with open(isolated_result, 'rb') as f:
            run_isolated_result = json.load(f)
          logging.debug('run_isolated:\n%s', run_isolated_result)
          # TODO(maruel): Grab statistics (cache hit rate, data downloaded,
          # mapping time, etc) from run_isolated and push them to the server.
          if run_isolated_result['outputs_ref']:
            params['outputs_ref'] = run_isolated_result['outputs_ref']
          had_hard_timeout = (
              had_hard_timeout or run_isolated_result['had_hard_timeout'])
          params['hard_timeout'] = had_hard_timeout
          if not had_io_timeout and not had_hard_timeout:
            if run_isolated_result['internal_failure']:
              must_signal_internal_failure = (
                  run_isolated_result['internal_failure'])
              logging.error('%s', must_signal_internal_failure)
            elif exit_code:
              # TODO(maruel): Grab stdout from run_isolated.
              must_signal_internal_failure = (
                  'run_isolated internal failure %d' % exit_code)
              logging.error('%s', must_signal_internal_failure)
          exit_code = run_isolated_result['exit_code']
          if run_isolated_result.get('duration') is not None:
            # Calculate the real task duration as measured by run_isolated and
            # calculate the remaining overhead.
            params['bot_overhead'] = params['duration']
            params['duration'] = run_isolated_result['duration']
            params['bot_overhead'] -= params['duration']
            params['bot_overhead'] -= run_isolated_result.get(
                'download', {}).get('duration', 0)
            params['bot_overhead'] -= run_isolated_result.get(
                'upload', {}).get('duration', 0)
            if params['bot_overhead'] < 0:
              params['bot_overhead'] = 0
          stats = run_isolated_result.get('stats')
          if stats:
            params['isolated_stats'] = stats
      except (IOError, OSError, ValueError) as e:
        logging.error('Swallowing error: %s', e)
        if not must_signal_internal_failure:
          must_signal_internal_failure = str(e)
    # TODO(maruel): Send the internal failure here instead of sending it through
    # bot_main, this causes a race condition.
    if exit_code is None:
      exit_code = -1
    post_update(swarming_server, params, exit_code, stdout, output_chunk_start)
    return {
      u'exit_code': exit_code,
      u'hard_timeout': had_hard_timeout,
      u'io_timeout': had_io_timeout,
      u'must_signal_internal_failure': must_signal_internal_failure,
      u'version': OUT_VERSION,
    }
  finally:
    if isolated_result:
      try:
        os.remove(isolated_result)
      except OSError:
        pass


def main(args):
  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)
  parser.add_option('--in-file', help='Name of the request file')
  parser.add_option(
      '--out-file', help='Name of the JSON file to write a task summary to')
  parser.add_option(
      '--swarming-server', help='Swarming server to send data back')
  parser.add_option(
      '--cost-usd-hour', type='float', help='Cost of this VM in $/h')
  parser.add_option('--start', type='float', help='Time this task was started')
  parser.add_option(
      '--min-free-space', type='int',
      help='Value to send down to run_isolated')

  options, args = parser.parse_args(args)
  if not options.in_file or not options.out_file or args:
    parser.error('task_runner is meant to be used by swarming_bot.')

  on_error.report_on_exception_exit(options.swarming_server)

  logging.info('starting')
  remote = xsrf_client.XsrfRemote(options.swarming_server)

  now = monotonic_time()
  if options.start > now:
    options.start = now

  try:
    load_and_run(
        options.in_file, remote, options.cost_usd_hour, options.start,
        options.out_file, options.min_free_space)
    return 0
  finally:
    logging.info('quitting')
/n/n/n/appengine/swarming/swarming_bot/bot_code/xsrf_client.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Wraps URL requests with an XSRF token using components/auth based service.""""""

import datetime
import logging
import os
import sys

THIS_DIR = os.path.dirname(os.path.abspath(__file__))

sys.path.insert(0, os.path.join(THIS_DIR, 'third_party'))

from utils import net


class Error(Exception):
  pass


def _utcnow():
  """"""So it can be mocked.""""""
  return datetime.datetime.utcnow()


class XsrfRemote(object):
  """"""Transparently adds XSRF token to requests.""""""
  TOKEN_RESOURCE = '/auth/api/v1/accounts/self/xsrf_token'

  def __init__(self, url, token_resource=None):
    self.url = url.rstrip('/')
    self.token = None
    self.token_resource = token_resource or self.TOKEN_RESOURCE
    self.expiration = None
    self.xsrf_request_params = {}

  def url_read(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token for GET.
      return net.url_read(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def url_read_json(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token required for GET.
      return net.url_read_json(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_json_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def refresh_token(self):
    """"""Returns a fresh token. Necessary as the token may expire after an hour.
    """"""
    url = self.url + self.token_resource
    resp = net.url_read_json(
        url,
        headers={'X-XSRF-Token-Request': '1'},
        data=self.xsrf_request_params)
    if resp is None:
      raise Error('Failed to connect to %s' % url)
    self.token = resp['xsrf_token']
    if resp.get('expiration_sec'):
      exp = resp['expiration_sec']
      exp -= min(round(exp * 0.1), 600)
      self.expiration = _utcnow() + datetime.timedelta(seconds=exp)
    return self.token

  def need_refresh(self):
    """"""Returns True if the XSRF token needs to be refreshed.""""""
    return (
        not self.token or (self.expiration and self.expiration <= _utcnow()))

  def _url_read_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read(url, headers=headers, **kwargs)

  def _url_read_json_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read_json(url, headers=headers, **kwargs)
/n/n/n/appengine/swarming/swarming_bot/bot_code/xsrf_client_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import datetime
import logging
import os
import sys
import time
import unittest

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import xsrf_client


class UrlHelperTest(net_utils.TestCase):
  def setUp(self):
    super(UrlHelperTest, self).setUp()
    self.mock(logging, 'error', lambda *_: None)
    self.mock(logging, 'exception', lambda *_: None)
    self.mock(logging, 'info', lambda *_: None)
    self.mock(logging, 'warning', lambda *_: None)
    self.mock(time, 'sleep', lambda _: None)

  def testXsrfRemoteGET(self):
    self.expected_requests([('http://localhost/a', {}, 'foo', None)])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a'))

  def testXsrfRemoteSimple(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))

  def testXsrfRemoteRefresh(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'bar',
            None,
          ),
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token2',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token2'}},
            'foo',
            None,
          ),
        ])

    now = xsrf_client._utcnow()
    remote = xsrf_client.XsrfRemote('http://localhost/')
    remote.url_read('/a', data={'foo': 'bar'})
    self.mock(
        xsrf_client, '_utcnow', lambda: now + datetime.timedelta(seconds=91))
    remote.url_read('/a', data={'foo': 'bar'})

  def testXsrfRemoteCustom(self):
    # Use the new swarming bot API as an example of custom XSRF request handler.
    self.expected_requests(
        [
          (
            'http://localhost/swarming/api/v1/bot/handshake',
            {
              'data': {'attributes': 'b'},
              'headers': {'X-XSRF-Token-Request': '1'},
            },
            {
              'expiration_sec': 100,
              'ignored': True,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote(
        'http://localhost/',
        '/swarming/api/v1/bot/handshake')
    remote.xsrf_request_params = {'attributes': 'b'}
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))


if __name__ == '__main__':
  logging.basicConfig(level=logging.ERROR)
  unittest.main()
/n/n/n/client/tests/net_utils.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import logging
import os
import sys
import threading

TEST_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(TEST_DIR)
sys.path.insert(0, ROOT_DIR)
sys.path.insert(0, os.path.join(ROOT_DIR, 'third_party'))

from depot_tools import auto_stub
from utils import net


def make_fake_response(content, url, headers=None):
  """"""Returns HttpResponse with predefined content, useful in tests.""""""
  headers = dict(headers or {})
  headers['Content-Length'] = len(content)
  class _Fake(object):
    def __init__(self):
      self.content = content
    def iter_content(self, chunk_size):
      c = self.content
      while c:
        yield c[:chunk_size]
        c = c[chunk_size:]
    def read(self):
      return self.content
  return net.HttpResponse(_Fake(), url, headers)


class TestCase(auto_stub.TestCase):
  """"""Mocks out url_open() calls.""""""
  def setUp(self):
    super(TestCase, self).setUp()
    self.mock(net, 'url_open', self._url_open)
    self.mock(net, 'url_read_json', self._url_read_json)
    self.mock(net, 'sleep_before_retry', lambda *_: None)
    self._lock = threading.Lock()
    self._requests = []

  def tearDown(self):
    try:
      if not self.has_failed():
        self.assertEqual([], self._requests)
    finally:
      super(TestCase, self).tearDown()

  def expected_requests(self, requests):
    """"""Registers the expected requests along their reponses.

    Arguments:
      request: list of tuple(url, kwargs, response, headers) for normal requests
          and tuple(url, kwargs, response) for json requests. kwargs can be a
          callable. In that case, it's called with the actual kwargs. It's
          useful when the kwargs values are not deterministic.
    """"""
    requests = requests[:]
    for request in requests:
      self.assertEqual(tuple, request.__class__)
      # 3 = json request (url_read_json).
      # 4 = normal request (url_open).
      self.assertIn(len(request), (3, 4))

    with self._lock:
      self.assertEqual([], self._requests)
      self._requests = requests

  def _url_open(self, url, **kwargs):
    logging.warn('url_open(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 4:
            self.fail('Expected normal request, got json data; %s' % url)
          _, expected_kwargs, result, headers = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return make_fake_response(result, url, headers)
          return None
    self.fail('Unknown request %s' % url)

  def _url_read_json(self, url, **kwargs):
    logging.warn('url_read_json(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 3:
            self.fail('Expected json request, got normal data; %s' % url)
          _, expected_kwargs, result = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return result
          return None
    self.fail('Unknown request %s' % url)
/n/n/n",1
80,80,0ba6a589d77baefc5ae20cde5c3a5dc24a6290f9,"appengine/components/components/auth/handler.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Integration with webapp2.""""""

# Disable 'Method could be a function.'
# pylint: disable=R0201

import functools
import json
import logging
import urllib
import webapp2

from google.appengine.api import urlfetch
from google.appengine.api import users

from components import utils

from . import api
from . import config
from . import delegation
from . import host_token
from . import ipaddr
from . import model
from . import openid
from . import tokens

# Part of public API of 'auth' component, exposed by this module.
__all__ = [
  'ApiHandler',
  'AuthenticatingHandler',
  'gae_cookie_authentication',
  'get_authenticated_routes',
  'oauth_authentication',
  'openid_cookie_authentication',
  'require_xsrf_token_request',
  'service_to_service_authentication',
]


def require_xsrf_token_request(f):
  """"""Use for handshaking APIs.""""""
  @functools.wraps(f)
  def hook(self, *args, **kwargs):
    if not self.request.headers.get('X-XSRF-Token-Request'):
      raise api.AuthorizationError('Missing required XSRF request header')
    return f(self, *args, **kwargs)
  return hook


class XSRFToken(tokens.TokenKind):
  """"""XSRF token parameters.""""""
  expiration_sec = 4 * 3600
  secret_key = api.SecretKey('xsrf_token', scope='local')
  version = 1


class AuthenticatingHandlerMetaclass(type):
  """"""Ensures that 'get', 'post', etc. are marked with @require or @public.""""""

  def __new__(mcs, name, bases, attributes):
    for method in webapp2.WSGIApplication.allowed_methods:
      func = attributes.get(method.lower())
      if func and not api.is_decorated(func):
        raise TypeError(
            'Method \'%s\' of \'%s\' is not protected by @require or @public '
            'decorator' % (method.lower(), name))
    return type.__new__(mcs, name, bases, attributes)


class AuthenticatingHandler(webapp2.RequestHandler):
  """"""Base class for webapp2 request handlers that use Auth system.

  Knows how to extract Identity from request data and how to initialize auth
  request context, so that get_current_identity() and is_group_member() work.

  All request handling methods (like 'get', 'post', etc) should be marked by
  either @require or @public decorators.
  """"""

  # Checks that all 'get', 'post', etc. are marked with @require or @public.
  __metaclass__ = AuthenticatingHandlerMetaclass

  # List of HTTP methods that trigger XSRF token validation.
  xsrf_token_enforce_on = ('DELETE', 'POST', 'PUT')
  # If not None, the header to search for XSRF token.
  xsrf_token_header = 'X-XSRF-Token'
  # If not None, the request parameter (GET or POST) to search for XSRF token.
  xsrf_token_request_param = 'xsrf_token'
  # Embedded data extracted from XSRF token of current request.
  xsrf_token_data = None
  # If not None, sets X_Frame-Options on all replies.
  frame_options = 'DENY'
  # A method used to authenticate this request, see get_auth_methods().
  auth_method = None

  def dispatch(self):
    """"""Extracts and verifies Identity, sets up request auth context.""""""
    # Ensure auth component is configured before executing any code.
    conf = config.ensure_configured()
    auth_context = api.reinitialize_request_cache()

    # http://www.html5rocks.com/en/tutorials/security/content-security-policy/
    # https://www.owasp.org/index.php/Content_Security_Policy
    # TODO(maruel): Remove 'unsafe-inline' once all inline style=""foo:bar"" in
    # all HTML tags were removed. Warning if seeing this post 2016, it could
    # take a while.
    # - https://www.google.com is due to Google Viz library.
    # - https://www.google-analytics.com due to Analytics.
    # - 'unsafe-eval' due to polymer.
    self.response.headers['Content-Security-Policy'] = (
        'default-src https: \'self\' \'unsafe-inline\' https://www.google.com '
        'https://www.google-analytics.com \'unsafe-eval\'')
    # Enforce HTTPS by adding the HSTS header; 365*24*60*60s.
    # https://www.owasp.org/index.php/HTTP_Strict_Transport_Security
    self.response.headers['Strict-Transport-Security'] = (
        'max-age=31536000; includeSubDomains; preload')
    # Disable frame support wholesale.
    # https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet
    if self.frame_options:
      self.response.headers['X-Frame-Options'] = self.frame_options

    identity = None
    for method_func in self.get_auth_methods(conf):
      try:
        identity = method_func(self.request)
        if identity:
          break
      except api.AuthenticationError as err:
        self.authentication_error(err)
        return
      except api.AuthorizationError as err:
        self.authorization_error(err)
        return
    else:
      method_func = None
    self.auth_method = method_func

    # If no authentication method is applicable, default to anonymous identity.
    identity = identity or model.Anonymous

    # XSRF token is required only if using Cookie based or IP whitelist auth.
    # A browser doesn't send Authorization: 'Bearer ...' or any other headers
    # by itself. So XSRF check is not required if header based authentication
    # is used.
    using_headers_auth = method_func in (
        oauth_authentication, service_to_service_authentication)

    # Extract caller host name from host token header, if present and valid.
    host_tok = self.request.headers.get(host_token.HTTP_HEADER)
    if host_tok:
      validated_host = host_token.validate_host_token(host_tok)
      if validated_host:
        auth_context.peer_host = validated_host

    # Verify IP is whitelisted and authenticate requests from bots.
    assert self.request.remote_addr
    ip = ipaddr.ip_from_string(self.request.remote_addr)
    auth_context.peer_ip = ip
    try:
      # 'verify_ip_whitelisted' may change identity for bots, store new one.
      auth_context.peer_identity = api.verify_ip_whitelisted(
          identity, ip, self.request.headers)
    except api.AuthorizationError as err:
      self.authorization_error(err)
      return

    # Parse delegation token, if given, to deduce end-user identity.
    delegation_tok = self.request.headers.get(delegation.HTTP_HEADER)
    if delegation_tok:
      try:
        auth_context.current_identity = delegation.check_delegation_token(
            delegation_tok, auth_context.peer_identity)
      except delegation.BadTokenError as exc:
        self.authorization_error(
            api.AuthorizationError('Bad delegation token: %s' % exc))
      except delegation.TransientError as exc:
        msg = 'Transient error while validating delegation token.\n%s' % exc
        logging.error(msg)
        self.abort(500, detail=msg)
    else:
      auth_context.current_identity = auth_context.peer_identity

    try:
      # Fail if XSRF token is required, but not provided.
      need_xsrf_token = (
          not using_headers_auth and
          self.request.method in self.xsrf_token_enforce_on)
      if need_xsrf_token and self.xsrf_token is None:
        raise api.AuthorizationError('XSRF token is missing')

      # If XSRF token is present, verify it is valid and extract its payload.
      # Do it even if XSRF token is not strictly required, since some handlers
      # use it to store session state (it is similar to a signed cookie).
      self.xsrf_token_data = {}
      if self.xsrf_token is not None:
        # This raises AuthorizationError if token is invalid.
        try:
          self.xsrf_token_data = self.verify_xsrf_token()
        except api.AuthorizationError as exc:
          if not need_xsrf_token:
            logging.warning('XSRF token is broken, ignoring - %s', exc)
          else:
            raise

      # All other ACL checks will be performed by corresponding handlers
      # manually or via '@required' decorator. Failed ACL check raises
      # AuthorizationError.
      super(AuthenticatingHandler, self).dispatch()
    except api.AuthorizationError as err:
      self.authorization_error(err)

  @classmethod
  def get_auth_methods(cls, conf):
    """"""Returns an enumerable of functions to use to authenticate request.

    The handler will try to apply auth methods sequentially one by one by until
    it finds one that works.

    Each auth method is a function that accepts webapp2.Request and can finish
    with 3 outcomes:

    * Return None: authentication method is not applicable to that request
      and next method should be tried (for example cookie-based
      authentication is not applicable when there's no cookies).

    * Returns Identity associated with the request. Means authentication method
      is applicable and request authenticity is confirmed.

    * Raises AuthenticationError: authentication method is applicable, but
      request contains bad credentials or invalid token, etc. For example,
      OAuth2 token is given, but it is revoked.

    A chosen auth method function will be stored in request's auth_method field.

    Args:
      conf: components.auth GAE config, see config.py.
    """"""
    if conf.USE_OPENID:
      cookie_auth = openid_cookie_authentication
    else:
      cookie_auth = gae_cookie_authentication
    return oauth_authentication, cookie_auth, service_to_service_authentication

  def generate_xsrf_token(self, xsrf_token_data=None):
    """"""Returns new XSRF token that embeds |xsrf_token_data|.

    The token is bound to current identity and is valid only when used by same
    identity.
    """"""
    return XSRFToken.generate(
        [api.get_current_identity().to_bytes()], xsrf_token_data)

  @property
  def xsrf_token(self):
    """"""Returns XSRF token passed with the request or None if missing.

    Doesn't do any validation. Use verify_xsrf_token() instead.
    """"""
    token = None
    if self.xsrf_token_header:
      token = self.request.headers.get(self.xsrf_token_header)
    if not token and self.xsrf_token_request_param:
      param = self.request.get_all(self.xsrf_token_request_param)
      token = param[0] if param else None
    return token

  def verify_xsrf_token(self):
    """"""Grabs a token from the request, validates it and extracts embedded data.

    Current identity must be the same as one used to generate the token.

    Returns:
      Whatever was passed as |xsrf_token_data| in 'generate_xsrf_token'
      method call used to generate the token.

    Raises:
      AuthorizationError if token is missing, invalid or expired.
    """"""
    token = self.xsrf_token
    if not token:
      raise api.AuthorizationError('XSRF token is missing')
    # Check that it was generated for the same identity.
    try:
      return XSRFToken.validate(token, [api.get_current_identity().to_bytes()])
    except tokens.InvalidTokenError as err:
      raise api.AuthorizationError(str(err))

  def authentication_error(self, error):
    """"""Called when authentication fails to report the error to requester.

    Authentication error means that some credentials are provided but they are
    invalid. If no credentials are provided at all, no authentication is
    attempted and current identity is just set to 'anonymous:anonymous'.

    Default behavior is to abort the request with HTTP 401 error (and human
    readable HTML body).

    Args:
      error: instance of AuthenticationError subclass.
    """"""
    logging.warning('Authentication error.\n%s', error)
    self.abort(401, detail=str(error))

  def authorization_error(self, error):
    """"""Called when authentication succeeds, but access to a resource is denied.

    Called whenever request handler raises AuthorizationError exception.
    In particular this exception is raised by method decorated with @require if
    current identity doesn't have required permission.

    Default behavior is to abort the request with HTTP 403 error (and human
    readable HTML body).

    Args:
      error: instance of AuthorizationError subclass.
    """"""
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort(403, detail=str(error))

  ### Wrappers around Users API or its OpenID equivalent.

  def get_current_user(self):
    """"""When cookie auth is used returns instance of CurrentUser or None.""""""
    return self._get_users_api().get_current_user(self.request)

  def is_current_user_gae_admin(self):
    """"""When cookie auth is used returns True if current caller is GAE admin.""""""
    return self._get_users_api().is_current_user_gae_admin(self.request)

  def create_login_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to login.""""""
    return self._get_users_api().create_login_url(self.request, dest_url)

  def create_logout_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to logout.""""""
    return self._get_users_api().create_logout_url(self.request, dest_url)

  def _get_users_api(self):
    """"""Returns GAEUsersAPI, OpenIDAPI or raises NotImplementedError.

    Chooses based on what auth_method was used of what methods are available.
    """"""
    method = self.auth_method
    if not method:
      # Anonymous request -> pick first method that supports API.
      for method in self.get_auth_methods(config.ensure_configured()):
        if method in _METHOD_TO_USERS_API:
          break
      else:
        raise NotImplementedError('No methods support UsersAPI')
    elif method not in _METHOD_TO_USERS_API:
      raise NotImplementedError(
          '%s doesn\'t support UsersAPI' % method.__name__)
    return _METHOD_TO_USERS_API[method]


class ApiHandler(AuthenticatingHandler):
  """"""Parses JSON request body to a dict, serializes response to JSON.""""""
  CONTENT_TYPE_BASE = 'application/json'
  CONTENT_TYPE_FULL = 'application/json; charset=utf-8'
  _json_body = None
  # Clickjacking not applicable to APIs.
  frame_options = None

  def authentication_error(self, error):
    logging.warning('Authentication error.\n%s', error)
    self.abort_with_error(401, text=str(error))

  def authorization_error(self, error):
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort_with_error(403, text=str(error))

  def send_response(self, response, http_code=200, headers=None):
    """"""Sends successful reply and continues execution.""""""
    self.response.set_status(http_code)
    self.response.headers.update(headers or {})
    self.response.headers['Content-Type'] = self.CONTENT_TYPE_FULL
    self.response.write(json.dumps(response))

  def abort_with_error(self, http_code, **kwargs):
    """"""Sends error reply and stops execution.""""""
    self.abort(
        http_code,
        json=kwargs,
        headers={'Content-Type': self.CONTENT_TYPE_FULL})

  def parse_body(self):
    """"""Parses JSON body and verifies it's a dict.

    webob.Request doesn't cache the decoded json body, this function does.
    """"""
    if self._json_body is None:
      if (self.CONTENT_TYPE_BASE and
          self.request.content_type != self.CONTENT_TYPE_BASE):
        msg = (
            'Expecting JSON body with content type \'%s\'' %
            self.CONTENT_TYPE_BASE)
        self.abort_with_error(400, text=msg)
      try:
        self._json_body = self.request.json
        if not isinstance(self._json_body, dict):
          raise ValueError()
      except (LookupError, ValueError):
        self.abort_with_error(400, text='Not a valid json dict body')
    return self._json_body.copy()


def get_authenticated_routes(app):
  """"""Given WSGIApplication returns list of routes that use authentication.

  Intended to be used only for testing.
  """"""
  # This code is adapted from router's __repr__ method (that enumerate
  # all routes for pretty-printing).
  routes = list(app.router.match_routes)
  routes.extend(
      v for k, v in app.router.build_routes.iteritems()
      if v not in app.router.match_routes)
  return [r for r in routes if issubclass(r.handler, AuthenticatingHandler)]


################################################################################
## All supported implementations of authentication methods for webapp2 handlers.


def gae_cookie_authentication(_request):
  """"""AppEngine cookie based authentication via users.get_current_user().""""""
  user = users.get_current_user()
  try:
    return model.Identity(model.IDENTITY_USER, user.email()) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email())


def openid_cookie_authentication(request):
  """"""Cookie based authentication that uses OpenID flow for login.""""""
  user = openid.get_current_user(request)
  try:
    return model.Identity(model.IDENTITY_USER, user.email) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email)


def oauth_authentication(request):
  """"""OAuth2 based authentication via oauth.get_current_user().""""""
  if not request.headers.get('Authorization'):
    return None
  if not utils.is_local_dev_server():
    return api.extract_oauth_caller_identity()

  # OAuth2 library is mocked on dev server to return some nonsense. Use (slow,
  # but real) OAuth2 API endpoint instead to validate access_token. It is also
  # what Cloud Endpoints do on a local server. For simplicity ignore client_id
  # on dev server.
  header = request.headers['Authorization'].split(' ', 1)
  if len(header) != 2 or header[0] not in ('OAuth', 'Bearer'):
    raise api.AuthenticationError('Invalid authorization header')

  # Adapted from endpoints/users_id_tokens.py, _set_bearer_user_vars_local.
  base_url = 'https://www.googleapis.com/oauth2/v1/tokeninfo'
  result = urlfetch.fetch(
      url='%s?%s' % (base_url, urllib.urlencode({'access_token': header[1]})),
      follow_redirects=False,
      validate_certificate=True)
  if result.status_code != 200:
    try:
      error = json.loads(result.content)['error_description']
    except (KeyError, ValueError):
      error = repr(result.content)
    raise api.AuthenticationError('Failed to validate the token: %s' % error)

  token_info = json.loads(result.content)
  if 'email' not in token_info:
    raise api.AuthenticationError('Token doesn\'t include an email address')
  if not token_info.get('verified_email'):
    raise api.AuthenticationError('Token email isn\'t verified')

  email = token_info['email']
  try:
    return model.Identity(model.IDENTITY_USER, email)
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % email)


def service_to_service_authentication(request):
  """"""Used for AppEngine <-> AppEngine communication.

  Relies on X-Appengine-Inbound-Appid header set by AppEngine itself. It can't
  be set by external users (with exception of admins).
  """"""
  app_id = request.headers.get('X-Appengine-Inbound-Appid')
  try:
    return model.Identity(model.IDENTITY_SERVICE, app_id) if app_id else None
  except ValueError:
    raise api.AuthenticationError('Unsupported application ID: %s' % app_id)


################################################################################
## API wrapper on top of Users API and OpenID API to make them similar.


class CurrentUser(object):
  """"""Mimics subset of GAE users.User object for ease of transition.

  Also adds .picture().
  """"""

  def __init__(self, user_id, email, picture):
    self._user_id = user_id
    self._email = email
    self._picture = picture

  def nickname(self):
    return self._email

  def email(self):
    return self._email

  def user_id(self):
    return self._user_id

  def picture(self):
    return self._picture

  def __unicode__(self):
    return unicode(self.nickname())

  def __str__(self):
    return str(self.nickname())


class GAEUsersAPI(object):
  @staticmethod
  def get_current_user(request):  # pylint: disable=unused-argument
    user = users.get_current_user()
    return CurrentUser(user.user_id(), user.email(), None) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return users.is_current_user_admin()

  @staticmethod
  def create_login_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_login_url(dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_logout_url(dest_url)


class OpenIDAPI(object):
  @staticmethod
  def get_current_user(request):
    user = openid.get_current_user(request)
    return CurrentUser(user.sub, user.email, user.picture) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return False

  @staticmethod
  def create_login_url(request, dest_url):
    return openid.create_login_url(request, dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):
    return openid.create_logout_url(request, dest_url)


# See AuthenticatingHandler._get_users_api().
_METHOD_TO_USERS_API = {
  gae_cookie_authentication: GAEUsersAPI,
  openid_cookie_authentication: OpenIDAPI,
}
/n/n/nappengine/components/components/auth/handler_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

# Disable 'Unused variable', 'Unused argument' and 'Method could be a function'.
# pylint: disable=W0612,W0613,R0201

import datetime
import json
import os
import sys
import unittest

from test_support import test_env
test_env.setup_test_env()

from google.appengine.api import oauth
from google.appengine.api import users

import webapp2
import webtest

from components import utils
from components.auth import api
from components.auth import delegation
from components.auth import handler
from components.auth import host_token
from components.auth import ipaddr
from components.auth import model
from components.auth.proto import delegation_pb2
from test_support import test_case


class AuthenticatingHandlerMetaclassTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandlerMetaclass.""""""

  def test_good(self):
    # No request handling methods defined at all.
    class TestHandler1(handler.AuthenticatingHandler):
      def some_other_method(self):
        pass

    # @public is used.
    class TestHandler2(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        pass

    # @require is used.
    class TestHandler3(handler.AuthenticatingHandler):
      @api.require(lambda: True)
      def get(self):
        pass

  def test_bad(self):
    # @public or @require is missing.
    with self.assertRaises(TypeError):
      class TestHandler1(handler.AuthenticatingHandler):
        def get(self):
          pass


class AuthenticatingHandlerTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandler class.""""""

  def setUp(self):
    super(AuthenticatingHandlerTest, self).setUp()
    # Reset global config of auth library before each test.
    api.reset_local_state()
    # Capture error and warning log messages.
    self.logged_errors = []
    self.mock(handler.logging, 'error',
        lambda *args, **kwargs: self.logged_errors.append((args, kwargs)))
    self.logged_warnings = []
    self.mock(handler.logging, 'warning',
        lambda *args, **kwargs: self.logged_warnings.append((args, kwargs)))

  def make_test_app(self, path, request_handler):
    """"""Returns webtest.TestApp with single route.""""""
    return webtest.TestApp(
        webapp2.WSGIApplication([(path, request_handler)], debug=True),
        extra_environ={'REMOTE_ADDR': '127.0.0.1'})

  def test_anonymous(self):
    """"""If all auth methods are not applicable, identity is set to Anonymous.""""""
    test = self

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        non_applicable = lambda _request: None
        return [non_applicable, non_applicable]

      @api.public
      def get(self):
        test.assertEqual(model.Anonymous, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

  def test_ip_whitelist_bot(self):
    """"""Requests from client in ""bots"" IP whitelist are authenticated as bot.""""""
    model.bootstrap_ip_whitelist('bots', ['192.168.1.100/32'])

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_current_identity().to_bytes())

    app = self.make_test_app('/request', Handler)
    def call(ip):
      api.reset_local_state()
      return app.get('/request', extra_environ={'REMOTE_ADDR': ip}).body

    self.assertEqual('bot:whitelisted-ip', call('192.168.1.100'))
    self.assertEqual('anonymous:anonymous', call('127.0.0.1'))

  def test_ip_whitelist(self):
    """"""Per-account IP whitelist works.""""""
    ident1 = model.Identity(model.IDENTITY_USER, 'a@example.com')
    ident2 = model.Identity(model.IDENTITY_USER, 'b@example.com')

    model.bootstrap_ip_whitelist('whitelist', ['192.168.1.100/32'])
    model.bootstrap_ip_whitelist_assignment(ident1, 'whitelist')

    mocked_ident = [None]

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _req: mocked_ident[0]]

      @api.public
      def get(self):
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    def call(ident, ip):
      api.reset_local_state()
      mocked_ident[0] = ident
      response = app.get(
          '/request', extra_environ={'REMOTE_ADDR': ip}, expect_errors=True)
      return response.status_int

    # IP is whitelisted.
    self.assertEqual(200, call(ident1, '192.168.1.100'))
    # IP is NOT whitelisted.
    self.assertEqual(403, call(ident1, '127.0.0.1'))
    # Whitelist is not used.
    self.assertEqual(200, call(ident2, '127.0.0.1'))

  def test_auth_method_order(self):
    """"""Registered auth methods are tested in order.""""""
    test = self
    calls = []
    ident = model.Identity(model.IDENTITY_USER, 'joe@example.com')

    def not_applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('not_applicable')
      return None

    def applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('applicable')
      return ident

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [not_applicable, applicable]

      @api.public
      def get(self):
        test.assertEqual(ident, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

    # Both methods should be tried.
    expected_calls = [
      'not_applicable',
      'applicable',
    ]
    self.assertEqual(expected_calls, calls)

  def test_authentication_error(self):
    """"""AuthenticationError in auth method stops request processing.""""""
    test = self
    calls = []

    def failing(request):
      raise api.AuthenticationError('Too bad')

    def skipped(request):
      self.fail('authenticate should not be called')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [failing, skipped]

      @api.public
      def get(self):
        test.fail('Handler code should not be called')

      def authentication_error(self, err):
        test.assertEqual('Too bad', err.message)
        calls.append('authentication_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authentication_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 401.
    self.assertEqual(['authentication_error'], calls)
    self.assertEqual(401, response.status_int)

    # Authentication error is logged.
    self.assertEqual(1, len(self.logged_warnings))

  def test_authorization_error(self):
    """"""AuthorizationError in auth method is handled.""""""
    test = self
    calls = []

    class Handler(handler.AuthenticatingHandler):
      @api.require(lambda: False)
      def get(self):
        test.fail('Handler code should not be called')

      def authorization_error(self, err):
        calls.append('authorization_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authorization_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 403.
    self.assertEqual(['authorization_error'], calls)
    self.assertEqual(403, response.status_int)

  def make_xsrf_handling_app(
      self,
      xsrf_token_enforce_on=None,
      xsrf_token_header=None,
      xsrf_token_request_param=None):
    """"""Returns webtest app with single XSRF-aware handler.

    If generates XSRF tokens on GET and validates them on POST, PUT, DELETE.
    """"""
    calls = []

    def record(request_handler, method):
      is_valid = request_handler.xsrf_token_data == {'some': 'data'}
      calls.append((method, is_valid))

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(self.generate_xsrf_token({'some': 'data'}))
      @api.public
      def post(self):
        record(self, 'POST')
      @api.public
      def put(self):
        record(self, 'PUT')
      @api.public
      def delete(self):
        record(self, 'DELETE')

    if xsrf_token_enforce_on is not None:
      Handler.xsrf_token_enforce_on = xsrf_token_enforce_on
    if xsrf_token_header is not None:
      Handler.xsrf_token_header = xsrf_token_header
    if xsrf_token_request_param is not None:
      Handler.xsrf_token_request_param = xsrf_token_request_param

    app = self.make_test_app('/request', Handler)
    return app, calls

  def mock_get_current_identity(self, ident):
    """"""Mocks api.get_current_identity() to return |ident|.""""""
    self.mock(handler.api, 'get_current_identity', lambda: ident)

  def test_xsrf_token_get_param(self):
    """"""XSRF token works if put in GET parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request?xsrf_token=%s' % token)
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_post_param(self):
    """"""XSRF token works if put in POST parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_header(self):
    """"""XSRF token works if put in the headers.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', headers={'X-XSRF-Token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_missing(self):
    """"""XSRF token is not given but handler requires it.""""""
    app, calls = self.make_xsrf_handling_app()
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_xsrf_token_uses_enforce_on(self):
    """"""Only methods set in |xsrf_token_enforce_on| require token validation.""""""
    # Validate tokens only on PUT (not on POST).
    app, calls = self.make_xsrf_handling_app(xsrf_token_enforce_on=('PUT',))
    token = app.get('/request').body
    # Both POST and PUT work when token provided, verifying it.
    app.post('/request', {'xsrf_token': token})
    app.put('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True), ('PUT', True)], calls)
    # POST works without a token, put PUT doesn't.
    self.assertEqual(200, app.post('/request').status_int)
    self.assertEqual(403, app.put('/request', expect_errors=True).status_int)
    # Only the one that requires the token fails if wrong token is provided.
    bad_token = {'xsrf_token': 'boo'}
    self.assertEqual(200, app.post('/request', bad_token).status_int)
    self.assertEqual(
        403, app.put('/request', bad_token, expect_errors=True).status_int)

  def test_xsrf_token_uses_xsrf_token_header(self):
    """"""Name of the header used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_header='X-Some')
    token = app.get('/request').body
    app.post('/request', headers={'X-Some': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_uses_xsrf_token_request_param(self):
    """"""Name of the request param used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_request_param='tok')
    token = app.get('/request').body
    app.post('/request', {'tok': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_identity_matters(self):
    app, calls = self.make_xsrf_handling_app()
    # Generate token for identity A.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'a@example.com'))
    token = app.get('/request').body
    # Try to use it by identity B.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'b@example.com'))
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_get_authenticated_routes(self):
    class Authenticated(handler.AuthenticatingHandler):
      pass

    class NotAuthenticated(webapp2.RequestHandler):
      pass

    app = webapp2.WSGIApplication([
      webapp2.Route('/authenticated', Authenticated),
      webapp2.Route('/not-authenticated', NotAuthenticated),
    ])
    routes = handler.get_authenticated_routes(app)
    self.assertEqual(1, len(routes))
    self.assertEqual(Authenticated, routes[0].handler)

  def test_get_peer_ip(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(ipaddr.ip_to_string(api.get_peer_ip()))

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', extra_environ={'REMOTE_ADDR': '192.1.2.3'})
    self.assertEqual('192.1.2.3', response.body)

  def test_get_peer_host(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_peer_host() or '<none>')

    app = self.make_test_app('/request', Handler)
    def call(headers):
      api.reset_local_state()
      return app.get('/request', headers=headers).body

    # Good token.
    token = host_token.create_host_token('HOST.domain.com')
    self.assertEqual('host.domain.com', call({'X-Host-Token-V1': token}))

    # Missing or invalid tokens.
    self.assertEqual('<none>', call({}))
    self.assertEqual('<none>', call({'X-Host-Token-V1': 'broken'}))

    # Expired token.
    origin = datetime.datetime(2014, 1, 1, 1, 1, 1)
    self.mock_now(origin)
    token = host_token.create_host_token('HOST.domain.com', expiration_sec=60)
    self.mock_now(origin, 61)
    self.assertEqual('<none>', call({'X-Host-Token-V1': token}))

  def test_delegation_token(self):
    peer_ident = model.Identity.from_bytes('user:peer@a.com')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _request: peer_ident]

      @api.public
      def get(self):
        self.response.write(json.dumps({
          'peer_id': api.get_peer_identity().to_bytes(),
          'cur_id': api.get_current_identity().to_bytes(),
        }))

    app = self.make_test_app('/request', Handler)
    def call(headers=None):
      return json.loads(app.get('/request', headers=headers).body)

    # No delegation.
    self.assertEqual(
        {u'cur_id': u'user:peer@a.com', u'peer_id': u'user:peer@a.com'}, call())

    # TODO(vadimsh): Mint token via some high-level function call.
    subtokens = delegation_pb2.SubtokenList(subtokens=[
        delegation_pb2.Subtoken(
            issuer_id='user:delegated@a.com',
            creation_time=int(utils.time_time()),
            validity_duration=3600),
    ])
    tok = delegation.serialize_token(delegation.seal_token(subtokens))

    # With valid delegation token.
    self.assertEqual(
        {u'cur_id': u'user:delegated@a.com', u'peer_id': u'user:peer@a.com'},
        call({'X-Delegation-Token-V1': tok}))

    # With invalid delegation token.
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok + 'blah'},
        expect_errors=True)
    self.assertEqual(403, r.status_int)

    # Transient error.
    def mocked_check(*_args):
      raise delegation.TransientError('Blah')
    self.mock(delegation, 'check_delegation_token', mocked_check)
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok},
        expect_errors=True)
    self.assertEqual(500, r.status_int)


class GaeCookieAuthenticationTest(test_case.TestCase):
  """"""Tests for gae_cookie_authentication function.""""""

  def test_non_applicable(self):
    self.assertIsNone(handler.gae_cookie_authentication(webapp2.Request({})))

  def test_applicable(self):
    os.environ.update({
      'USER_EMAIL': 'joe@example.com',
      'USER_ID': '123',
      'USER_IS_ADMIN': '0',
    })
    # Actual request is not used by CookieAuthentication.
    self.assertEqual(
        model.Identity(model.IDENTITY_USER, 'joe@example.com'),
        handler.gae_cookie_authentication(webapp2.Request({})))


class ServiceToServiceAuthenticationTest(test_case.TestCase):
  """"""Tests for service_to_service_authentication.""""""

  def test_non_applicable(self):
    request = webapp2.Request({})
    self.assertIsNone(
        handler.service_to_service_authentication(request))

  def test_applicable(self):
    request = webapp2.Request({
      'HTTP_X_APPENGINE_INBOUND_APPID': 'some-app',
    })
    self.assertEqual(
      model.Identity(model.IDENTITY_SERVICE, 'some-app'),
      handler.service_to_service_authentication(request))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/nappengine/components/components/auth/model.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""NDB model classes used to model AuthDB relations.

Overview
--------

Models defined here are used by central authentication service (that stores all
groups and secrets) and by services that implement some concrete functionality
protected with ACLs (like isolate and swarming services).

Applications that use auth component may work in 3 modes:
  1. Standalone. Application is self contained and manages its own groups.
     Useful when developing a new service or for simple installations.
  2. Replica. Application uses a central authentication service. An application
     can be dynamically switched from Standalone to Replica mode.
  3. Primary. Application IS a central authentication service. Only 'auth'
     service is running in this mode. 'configure_as_primary' call during startup
     switches application to that mode.

Central authentication service (Primary) holds authoritative copy of all auth
related information (groups, secrets, etc.) and acts as a single source of truth
for it. All other services (Replicas) hold copies of a relevant subset of
this information (that they use to perform authorization checks).

Primary service is responsible for updating replicas' configuration via
service-to-service push based replication protocol.

AuthDB holds a list of groups. Each group has a unique name and is defined
as union of 3 sets:
  1) Explicit enumeration of particular Identities e.g. 'user:alice@example.com'
  2) Set of glob-like identity patterns e.g. 'user:*@example.com'
  3) Set of nested Groups.

Identity defines an actor making an action (it can be a real person, a bot,
an AppEngine application or special 'anonymous' identity).

In addition to that, AuthDB stores small amount of authentication related
configuration data, such as OAuth2 client_id and client_secret and various
secret keys.

Audit trail
-----------

Each change to AuthDB has an associated revision number (that monotonically
increases with each change). All entities modified by a change are copied to
append-only log under an entity key associated with the revision (see
historical_revision_key below). Removals are marked by special auth_db_deleted
flag in entites in the log. This is enough to recover a snapshot of all groups
at some specific moment in time, or to produce a diff between two revisions.

Note that entities in the historical log are not used by online queries. At any
moment in time most recent version of an AuthDB entity exists in two copies:
  1) Main copy used for online queries. It is mutated in-place with each change.
  2) Most recent record in the historical log. Read only.

To reduce a possibility of misuse of historical copies in online transactions,
history log entity classes are suffixied with 'History' suffix. They also have
all indexes stripped.

This mechanism is enabled only on services in Standalone or Primary mode.
Replicas do not keep track of AuthDB revisions and do not keep any historical
log.
""""""

import collections
import fnmatch
import logging
import os
import re

from google.appengine.api import app_identity
from google.appengine.ext import ndb

from components import datastore_utils
from components import utils

from . import ipaddr

# Part of public API of 'auth' component, exposed by this module.
__all__ = [
  'ADMIN_GROUP',
  'Anonymous',
  'bootstrap_group',
  'bootstrap_ip_whitelist',
  'bootstrap_loopback_ips',
  'BOTS_IP_WHITELIST',
  'configure_as_primary',
  'find_group_dependency_cycle',
  'find_referencing_groups',
  'get_auth_db_revision',
  'get_missing_groups',
  'get_service_self_identity',
  'group_key',
  'Identity',
  'IDENTITY_ANONYMOUS',
  'IDENTITY_BOT',
  'IDENTITY_SERVICE',
  'IDENTITY_USER',
  'IdentityGlob',
  'IdentityProperty',
  'ip_whitelist_key',
  'is_empty_group',
  'is_external_group_name',
  'is_primary',
  'is_replica',
  'is_standalone',
  'is_valid_group_name',
  'is_valid_ip_whitelist_name',
  'replicate_auth_db',
]


# Name of a group whose members have access to Group management UI. It's the
# only group needed to bootstrap everything else.
ADMIN_GROUP = 'administrators'

# Name of AuthIPWhitelist with bots IP ranges. See AuthIPWhitelist.
BOTS_IP_WHITELIST = 'bots'

# No identity information is provided. Identity name is always 'anonymous'.
IDENTITY_ANONYMOUS = 'anonymous'
# Using bot credentials. Identity name is bot's id.
IDENTITY_BOT = 'bot'
# Using App Engine service credentials. Identity name is app name.
IDENTITY_SERVICE = 'service'
# Using user credentials. Identity name is user's email.
IDENTITY_USER = 'user'

# All allowed identity kinds + regexps to validate identity name.
ALLOWED_IDENTITY_KINDS = {
  IDENTITY_ANONYMOUS: re.compile(r'^anonymous$'),
  IDENTITY_BOT: re.compile(r'^[0-9a-zA-Z_\-\.@]+$'),
  IDENTITY_SERVICE: re.compile(r'^[0-9a-zA-Z_\-\:\.]+$'),
  IDENTITY_USER: re.compile(r'^[0-9a-zA-Z_\-\.\+]+@[0-9a-z_\-\.]+$'),
}

# Regular expression that matches group names. ASCII only, no leading or
# trailing spaces allowed (spaces inside are fine).
GROUP_NAME_RE = re.compile(
    r'^([a-z\-]+/)?[0-9a-zA-Z_][0-9a-zA-Z_\-\.\ ]{1,80}[0-9a-zA-Z_\-\.]$')
# Special group name that means 'All possible users' (including anonymous!).
GROUP_ALL = '*'

# Regular expression for IP whitelist name.
IP_WHITELIST_NAME_RE = re.compile(r'^[0-9a-zA-Z_\-\+\.\ ]{2,200}$')


# Configuration of Primary service, set by 'configure_as_primary'.
_replication_callback = None


# Root ndb keys of various models. They can't be defined as a module level
# constants because ndb.Key implicitly includes current APPLICATION_ID. And in
# testing environment it is '_' during module loading time. Trying to use such
# key from within a testbed test case results in the following error:
# BadRequestError: app ""testbed-test"" cannot access app ""_""'s data


def root_key():
  """"""Global root key of auth models entity group.""""""
  return ndb.Key('AuthGlobalConfig', 'root')


def replication_state_key():
  """"""Key of AuthReplicationState entity.""""""
  return ndb.Key('AuthReplicationState', 'self', parent=root_key())


def ip_whitelist_assignments_key():
  """"""Key of AuthIPWhitelistAssignments entity.""""""
  return ndb.Key('AuthIPWhitelistAssignments', 'default', parent=root_key())


def historical_revision_key(auth_db_rev):
  """"""Key for entity subgroup that holds changes done in a concrete revision.""""""
  return ndb.Key('Rev', auth_db_rev, parent=root_key())


################################################################################
## Identity & IdentityGlob.


class Identity(
    datastore_utils.BytesSerializable,
    collections.namedtuple('Identity', 'kind, name')):
  """"""Represents a caller that makes requests. Immutable.

  A tuple of (kind, name) where 'kind' is one of IDENTITY_* constants and
  meaning of 'name' depends on a kind (see comments for IDENTITY_*).
  It generalizes accounts of real people, bot accounts and service-to-service
  accounts.

  It's a pure identity information. Any additional information that may be
  related to an identity (e.g. registration date, last access time, etc.) should
  be stored elsewhere using Identity.to_bytes() as a key.
  """"""

  # Inheriting from tuple requires use of __new__ instead of __init__. __init__
  # is called with object already 'frozen', so it's not possible to modify its
  # attributes in __init__.
  # See http://docs.python.org/2/reference/datamodel.html#object.__new__
  def __new__(cls, kind, name):
    if isinstance(name, unicode):
      try:
        name = name.encode('ascii')
      except UnicodeEncodeError:
        raise ValueError('Identity has invalid format: only ASCII is allowed')
    if (kind not in ALLOWED_IDENTITY_KINDS or
        not ALLOWED_IDENTITY_KINDS[kind].match(name)):
      raise ValueError('Identity has invalid format: %s' % name)
    return super(Identity, cls).__new__(cls, str(kind), name)

  def to_bytes(self):
    """"""Serializes this identity to byte buffer.""""""
    return '%s:%s' % (self.kind, self.name)

  @classmethod
  def from_bytes(cls, byte_buf):
    """"""Given a byte buffer returns corresponding Identity object.""""""
    kind, sep, name = byte_buf.partition(':')
    if not sep:
      raise ValueError('Missing \':\' separator in Identity string')
    return cls(kind, name)

  @property
  def is_anonymous(self):
    """"""True if this object represents anonymous identity.""""""
    return self.kind == IDENTITY_ANONYMOUS

  @property
  def is_bot(self):
    """"""True if this object represents bot account.""""""
    return self.kind == IDENTITY_BOT

  @property
  def is_service(self):
    """"""True if this object represents service account.""""""
    return self.kind == IDENTITY_SERVICE

  @property
  def is_user(self):
    """"""True if this object represents user account.""""""
    return self.kind == IDENTITY_USER


# Predefined Anonymous identity.
Anonymous = Identity(IDENTITY_ANONYMOUS, 'anonymous')


class IdentityProperty(datastore_utils.BytesSerializableProperty):
  """"""NDB model property for Identity values.

  Identities are stored as indexed short blobs internally.
  """"""
  _value_type = Identity
  _indexed = True


class IdentityGlob(
    datastore_utils.BytesSerializable,
    collections.namedtuple('IdentityGlob', 'kind, pattern')):
  """"""Glob-like pattern that matches subset of identities. Immutable.

  Tuple (kind, glob) where 'kind' is is one of IDENTITY_* constants and 'glob'
  defines pattern that identity names' should match. For example, IdentityGlob
  that matches all bots is (IDENTITY_BOT, '*') which is also can be written
  as 'bot:*'.
  """"""

  # See comment for Identity.__new__ regarding use of __new__ here.
  def __new__(cls, kind, pattern):
    if isinstance(pattern, unicode):
      try:
        pattern = pattern.encode('ascii')
      except UnicodeEncodeError:
        raise ValueError('Invalid IdentityGlob pattern: only ASCII is allowed')
    if not pattern:
      raise ValueError('No pattern is given')
    if kind not in ALLOWED_IDENTITY_KINDS:
      raise ValueError('Invalid Identity kind: %s' % kind)
    return super(IdentityGlob, cls).__new__(cls, str(kind), pattern)

  def to_bytes(self):
    """"""Serializes this identity glob to byte buffer.""""""
    return '%s:%s' % (self.kind, self.pattern)

  @classmethod
  def from_bytes(cls, byte_buf):
    """"""Given a byte buffer returns corresponding IdentityGlob object.""""""
    kind, sep, pattern = byte_buf.partition(':')
    if not sep:
      raise ValueError('Missing \':\' separator in IdentityGlob string')
    return cls(kind, pattern)

  def match(self, identity):
    """"""Return True if |identity| matches this pattern.""""""
    if identity.kind != self.kind:
      return False
    return fnmatch.fnmatchcase(identity.name, self.pattern)


class IdentityGlobProperty(datastore_utils.BytesSerializableProperty):
  """"""NDB model property for IdentityGlob values.

  IdentityGlobs are stored as short indexed blobs internally.
  """"""
  _value_type = IdentityGlob
  _indexed = True


################################################################################
## Singleton entities and replication related models.


def configure_as_primary(replication_callback):
  """"""Registers a callback to be called when AuthDB changes.

  Should be called during Primary application startup. The callback will be
  called as 'replication_callback(AuthReplicationState)' from inside transaction
  on root_key() entity group whenever replicate_auth_db() is called (i.e. on
  every change to auth db that should be replication to replicas).
  """"""
  global _replication_callback
  _replication_callback = replication_callback


def is_primary():
  """"""Returns True if current application was configured as Primary.""""""
  return bool(_replication_callback)


def is_replica():
  """"""Returns True if application is in Replica mode.""""""
  return not is_primary() and not is_standalone()


def is_standalone():
  """"""Returns True if application is in Standalone mode.""""""
  ent = get_replication_state()
  return not ent or not ent.primary_id


def get_replication_state():
  """"""Returns AuthReplicationState singleton entity if it exists.""""""
  return replication_state_key().get()


def get_auth_db_revision():
  """"""Returns current revision of AuthDB, it increases with each change.""""""
  state = get_replication_state()
  return state.auth_db_rev if state else 0


def get_service_self_identity():
  """"""Returns Identity that correspond to the current GAE app itself.""""""
  return Identity(IDENTITY_SERVICE, app_identity.get_application_id())


class AuthVersionedEntityMixin(object):
  """"""Mixin class for entities that keep track of when they change.

  Entities that have this mixin are supposed to be updated in get()\put() or
  get()\delete() transactions. Caller must call record_revision(...) sometime
  during the transaction (but before put()). Similarly a call to
  record_deletion(...) is expected sometime before delete().

  replicate_auth_db will store a copy of the entity in the revision log when
  committing a transaction.

  A pair of properties auth_db_rev and auth_db_prev_rev are used to implement
  a linked list of versions of this entity (e.g. one can take most recent entity
  version and go back in time by following auth_db_prev_rev links).
  """"""
  # When the entity was modified last time. Do not use 'auto_now' property since
  # such property overrides any explicitly set value with now() during put. It's
  # undesired when storing a copy of entity received from Primary (Replica
  # should have modified_ts to be same as on Primary).
  modified_ts = ndb.DateTimeProperty()
  # Who modified the entity last time.
  modified_by = IdentityProperty()

  # Revision of Auth DB at which this entity was updated last time.
  auth_db_rev = ndb.IntegerProperty()
  # Revision of Auth DB of previous version of this entity or None.
  auth_db_prev_rev = ndb.IntegerProperty()

  def record_revision(self, modified_by, modified_ts=None, comment=None):
    """"""Updates the entity to record Auth DB revision of the current transaction.

    Stages the entity to be copied to historical log.

    Must be called sometime before 'put' (not necessary right before it). Note
    that NDB hooks are not used because they are buggy. See docstring for
    replicate_auth_db for more info.

    Args:
      modified_by: Identity that made the change.
      modified_ts: datetime when the change was made (or None for current time).
      comment: optional comment to put in the revision log.
    """"""
    _get_pending_auth_db_transaction().record_change(
        entity=self,
        deletion=False,
        modified_by=modified_by,
        modified_ts=modified_ts or utils.utcnow(),
        comment=comment)

  def record_deletion(self, modified_by, modified_ts=None, comment=None):
    """"""Marks entity as being deleted in the current transaction.

    Stages the entity to be copied to historical log (with 'auth_db_deleted'
    flag set). The entity must not be mutated between 'get' and AuthDB commit.

    Must be called sometime before 'delete' (not necessary right before it).
    Note that NDB hooks are not used because they are buggy. See docstring for
    replicate_auth_db for more info.

    Args:
      modified_by: Identity that made the change.
      modified_ts: datetime when the change was made (or None for current time).
      comment: optional comment to put in the revision log.
    """"""
    _get_pending_auth_db_transaction().record_change(
        entity=self,
        deletion=True,
        modified_by=modified_by,
        modified_ts=modified_ts or utils.utcnow(),
        comment=comment)

  ## Internal interface. Do not use directly unless you know what you are doing.

  @classmethod
  def get_historical_copy_class(cls):
    """"""Returns entity class for historical copies of original entity.

    Has all the same properties, but unindexed (not needed), unvalidated
    (original entity is already validated) and not cached.

    The name of the new entity class is ""<original name>History"" (to make sure
    it doesn't show up in indexes for original entity class).
    """"""
    existing = getattr(cls, '_auth_db_historical_copy_cls', None)
    if existing:
      return existing
    props = {}
    for name, prop in cls._properties.iteritems():
      # Whitelist supported property classes. Better to fail loudly when
      # encountering something new, rather than silently produce (possibly)
      # incorrect result. Note that all AuthDB classes are instantiated in
      # unit tests, so there should be no unexpected asserts in production.
      assert prop.__class__ in (
        IdentityGlobProperty,
        IdentityProperty,
        ndb.BlobProperty,
        ndb.BooleanProperty,
        ndb.DateTimeProperty,
        ndb.IntegerProperty,
        ndb.LocalStructuredProperty,
        ndb.StringProperty,
        ndb.TextProperty,
      ), prop.__class__
      kwargs = {
        'name': prop._name,
        'indexed': False,
        'required': False,
        'repeated': prop._repeated,
      }
      if prop.__class__ == ndb.LocalStructuredProperty:
        kwargs['modelclass'] = prop._modelclass
      props[name] = prop.__class__(**kwargs)
    new_cls = type(
        '%sHistory' % cls.__name__, (_AuthDBHistoricalEntity,), props)
    cls._auth_db_historical_copy_cls = new_cls
    return new_cls

  def make_historical_copy(self, deleted, comment):
    """"""Returns an entity to put in the historical log.

    It's a copy of the original entity, but stored under another key and with
    indexes removed. It also has a bunch of additional properties (defined
    in _AuthDBHistoricalEntity). See 'get_historical_copy_class'.

    The key is derived from auth_db_rev and class and ID of the original entity.
    For example, AuthGroup ""admins"" modified at rev 123 will be copied to
    the history as ('AuthGlobalConfig', 'root', 'Rev', 123, 'AuthGroupHistory',
    'admins'), where the key prefix (first two pairs) is obtained with
    historical_revision_key(...).
    """"""
    assert self.key.parent() == root_key() or self.key == root_key(), self.key
    cls = self.get_historical_copy_class()
    entity = cls(
        id=self.key.id(),
        parent=historical_revision_key(self.auth_db_rev))
    for prop in self._properties:
      setattr(entity, prop, getattr(self, prop))
    entity.auth_db_deleted = deleted
    entity.auth_db_change_comment = comment
    entity.auth_db_app_version = utils.get_app_version()
    return entity


class AuthGlobalConfig(ndb.Model, AuthVersionedEntityMixin):
  """"""Acts as a root entity for auth models.

  There should be only one instance of this model in Datastore, with a key set
  to root_key(). A change to an entity group rooted at this key is a signal that
  AuthDB has to be refetched (see 'fetch_auth_db' in api.py).

  Entities that change often or associated with particular bot or user
  MUST NOT be in this entity group.

  Content of this particular entity is replicated from Primary service to all
  Replicas.

  Entities that belong to this entity group are:
   * AuthGroup
   * AuthIPWhitelist
   * AuthIPWhitelistAssignments
   * AuthReplicationState
   * AuthSecret
  """"""
  # OAuth2 client_id to use to mint new OAuth2 tokens.
  oauth_client_id = ndb.StringProperty(indexed=False, default='')
  # OAuth2 client secret. Not so secret really, since it's passed to clients.
  oauth_client_secret = ndb.StringProperty(indexed=False, default='')
  # Additional OAuth2 client_ids allowed to access the services.
  oauth_additional_client_ids = ndb.StringProperty(repeated=True, indexed=False)


class AuthReplicationState(ndb.Model, datastore_utils.SerializableModelMixin):
  """"""Contains state used to control Primary -> Replica replication.

  It's a singleton entity with key replication_state_key() (in same entity
  groups as root_key()). This entity should be small since it is updated
  (auth_db_rev is incremented) whenever AuthDB changes.

  Exists in any AuthDB (on Primary and Replicas). Primary updates it whenever
  changes to AuthDB are made, Replica updates it whenever it receives a push
  from Primary.
  """"""
  # How to convert this entity to or from serializable dict.
  serializable_properties = {
    'primary_id': datastore_utils.READABLE,
    'primary_url': datastore_utils.READABLE,
    'auth_db_rev': datastore_utils.READABLE,
    'modified_ts': datastore_utils.READABLE,
  }

  # For services in Standalone mode it is None.
  # For services in Primary mode: own GAE application ID.
  # For services in Replica mode it is a GAE application ID of Primary.
  primary_id = ndb.StringProperty(indexed=False)

  # For services in Replica mode, root URL of Primary, i.e https://<host>.
  primary_url = ndb.StringProperty(indexed=False)

  # Revision of auth DB. Increased by 1 with every change that should be
  # propagate to replicas. Only services in Standalone or Primary mode
  # update this property by themselves. Replicas receive it from Primary.
  auth_db_rev = ndb.IntegerProperty(default=0, indexed=False)

  # Time when auth_db_rev was created (by Primary clock). For informational
  # purposes only. See comment at AuthGroup.modified_ts for explanation why
  # auto_now is not used.
  modified_ts = ndb.DateTimeProperty(auto_now_add=True, indexed=False)


def replicate_auth_db():
  """"""Increments auth_db_rev, updates historical log, triggers replication.

  Must be called once from inside a transaction (right before exiting it).

  Should only be called for services in Standalone or Primary modes. Will raise
  ValueError if called on Replica. When called for service in Standalone mode,
  will update auth_db_rev but won't kick any replication. For services in
  Primary mode will also initiate replication by calling callback set in
  'configure_as_primary'. The callback usually transactionally enqueues a task
  (to gracefully handle transaction rollbacks).

  WARNING: This function relies on a valid transaction context. NDB hooks and
  asynchronous operations are known to be buggy in this regard: NDB hook for
  an async operation in a transaction may be called with a wrong context
  (main event loop context instead of transaction context). One way to work
  around that is to monkey patch NDB (as done here: https://goo.gl/1yASjL).
  Another is to not use hooks at all. There's no way to differentiate between
  sync and async modes of an NDB operation from inside a hook. And without a
  strict assert it's very easy to forget about ""Do not use put_async"" warning.
  For that reason _post_put_hook is NOT used and replicate_auth_db() should be
  called explicitly whenever relevant part of root_key() entity group is
  updated.

  Returns:
    New AuthDB revision number.
  """"""
  assert ndb.in_transaction()
  txn = _get_pending_auth_db_transaction()
  txn.commit()
  if is_primary():
    _replication_callback(txn.replication_state)
  return txn.replication_state.auth_db_rev


################################################################################
## Auth DB transaction details (used for historical log of changes).


_commit_callbacks = []


def commit_callback(cb):
  """"""Adds a callback that's called before AuthDB transaction is committed.

  Can be used as decorator. Adding a callback second time is noop.

  Args:
    cb: function that takes single auth_db_rev argument as input.
  """"""
  if cb not in _commit_callbacks:
    _commit_callbacks.append(cb)
  return cb


def _get_pending_auth_db_transaction():
  """"""Used internally to keep track of changes done in the transaction.

  Returns:
    Instance of _AuthDBTransaction (stored in the transaction context).
  """"""
  # Use transaction context to store the object. Note that each transaction
  # retry gets its own new transaction context which is what we need,
  # see ndb/context.py, 'transaction' tasklet, around line 982 (for SDK 1.9.6).
  assert ndb.in_transaction()
  ctx = ndb.get_context()
  txn = getattr(ctx, '_auth_db_transaction', None)
  if txn:
    return txn

  # Prepare next AuthReplicationState (auth_db_rev +1).
  state = replication_state_key().get()
  if not state:
    primary_id = app_identity.get_application_id() if is_primary() else None
    state = AuthReplicationState(
        key=replication_state_key(),
        primary_id=primary_id,
        auth_db_rev=0)
  # Assert Primary or Standalone. Replicas can't increment auth db revision.
  if not is_primary() and state.primary_id:
    raise ValueError('Can\'t modify Auth DB on Replica')
  state.auth_db_rev += 1
  state.modified_ts = utils.utcnow()

  # Store the state in the transaction context. Used in replicate_auth_db(...)
  # later.
  txn = _AuthDBTransaction(state)
  ctx._auth_db_transaction = txn
  return txn


class _AuthDBTransaction(object):
  """"""Keeps track of entities updated or removed in current transaction.""""""

  _Change = collections.namedtuple('_Change', 'entity deletion comment')

  def __init__(self, replication_state):
    self.replication_state = replication_state
    self.changes = [] # list of _Change tuples
    self.committed = False

  def record_change(self, entity, deletion, modified_by, modified_ts, comment):
    assert not self.committed
    assert isinstance(entity, AuthVersionedEntityMixin)
    assert all(entity.key != c.entity.key for c in self.changes)

    # Mutate the main entity (the one used to serve online requests).
    entity.modified_by = modified_by
    entity.modified_ts = modified_ts
    entity.auth_db_prev_rev = entity.auth_db_rev # can be None for new entities
    entity.auth_db_rev = self.replication_state.auth_db_rev

    # Keep a historical copy. Delay make_historical_copy call until the commit.
    # Here (in 'record_change') entity may not have all the fields updated yet.
    self.changes.append(self._Change(entity, deletion, comment))

  def commit(self):
    assert not self.committed
    puts = [
      c.entity.make_historical_copy(c.deletion, c.comment)
      for c in self.changes
    ]
    ndb.put_multi(puts + [self.replication_state])
    for cb in _commit_callbacks:
      cb(self.replication_state.auth_db_rev)
    self.committed = True


class _AuthDBHistoricalEntity(ndb.Model):
  """"""Base class for *History magic class in AuthVersionedEntityMixin.

  In addition to properties defined here the child classes (*History) also
  always inherit (for some definition of ""inherit"") properties from
  AuthVersionedEntityMixin.

  See get_historical_copy_class().
  """"""
  # Historical entities are not intended to be read often, and updating the
  # cache will make AuthDB transactions only slower.
  _use_cache = False
  _use_memcache = False

  # True if entity was deleted in the given revision.
  auth_db_deleted = ndb.BooleanProperty(indexed=False)
  # Comment string passed to record_revision or record_deletion.
  auth_db_change_comment = ndb.StringProperty(indexed=False)
  # A GAE module version that committed the change.
  auth_db_app_version = ndb.StringProperty(indexed=False)

  def get_previous_historical_copy_key(self):
    """"""Returns ndb.Key of *History entity matching auth_db_prev_rev revision.""""""
    if self.auth_db_prev_rev is None:
      return None
    return ndb.Key(
        self.__class__, self.key.id(),
        parent=historical_revision_key(self.auth_db_prev_rev))


################################################################################
## Groups.


class AuthGroup(
    ndb.Model,
    AuthVersionedEntityMixin,
    datastore_utils.SerializableModelMixin):
  """"""A group of identities, entity id is a group name.

  Parent is AuthGlobalConfig entity keyed at root_key().

  Primary service holds authoritative list of Groups, that gets replicated to
  all Replicas.
  """"""
  # How to convert this entity to or from serializable dict.
  serializable_properties = {
    'members': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'globs': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'nested': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'description': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'owners': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'created_ts': datastore_utils.READABLE,
    'created_by': datastore_utils.READABLE,
    'modified_ts': datastore_utils.READABLE,
    'modified_by': datastore_utils.READABLE,
  }

  # List of members that are explicitly in this group. Indexed.
  members = IdentityProperty(repeated=True)
  # List of identity-glob expressions (like 'user:*@example.com'). Indexed.
  globs = IdentityGlobProperty(repeated=True)
  # List of nested group names. Indexed.
  nested = ndb.StringProperty(repeated=True)

  # Human readable description.
  description = ndb.TextProperty(default='')
  # A name of the group that can modify or delete this group.
  owners = ndb.StringProperty(default=ADMIN_GROUP)

  # When the group was created.
  created_ts = ndb.DateTimeProperty()
  # Who created the group.
  created_by = IdentityProperty()


def group_key(group):
  """"""Returns ndb.Key for AuthGroup entity.""""""
  return ndb.Key(AuthGroup, group, parent=root_key())


def is_empty_group(group):
  """"""Returns True if group is missing or completely empty.""""""
  group = group_key(group).get()
  return not group or not(group.members or group.globs or group.nested)


def is_valid_group_name(name):
  """"""True if string looks like a valid group name.""""""
  return bool(GROUP_NAME_RE.match(name))


def is_external_group_name(name):
  """"""True if group is imported from outside and is not writable.""""""
  return is_valid_group_name(name) and '/' in name


@ndb.transactional
def bootstrap_group(group, identities, description=''):
  """"""Makes a group (if not yet exists) and adds |identities| to it as members.

  Returns True if modified the group, False if identities are already there.
  """"""
  key = group_key(group)
  entity = key.get()
  if entity and all(i in entity.members for i in identities):
    return False
  now = utils.utcnow()
  if not entity:
    entity = AuthGroup(
        key=key,
        description=description,
        created_ts=now,
        created_by=get_service_self_identity())
  for i in identities:
    if i not in entity.members:
      entity.members.append(i)
  entity.record_revision(
      modified_by=get_service_self_identity(),
      modified_ts=now,
      comment='Bootstrap')
  entity.put()
  replicate_auth_db()
  return True


def find_referencing_groups(group):
  """"""Finds groups that reference the specified group as nested group or owner.

  Used to verify that |group| is safe to delete, i.e. no other group is
  depending on it.

  Returns:
    Set of names of referencing groups.
  """"""
  nesting_groups = AuthGroup.query(
      AuthGroup.nested == group,
      ancestor=root_key()).fetch_async(keys_only=True)
  owned_groups = AuthGroup.query(
      AuthGroup.owners == group,
      ancestor=root_key()).fetch_async(keys_only=True)
  refs = set()
  refs.update(key.id() for key in nesting_groups.get_result())
  refs.update(key.id() for key in owned_groups.get_result())
  return refs


def get_missing_groups(groups):
  """"""Given a list of group names, returns a list of groups that do not exist.""""""
  # We need to iterate over |groups| twice. It won't work if |groups|
  # is a generator. So convert to list first.
  groups = list(groups)
  entities = ndb.get_multi(group_key(name) for name in groups)
  return [name for name, ent in zip(groups, entities) if not ent]


def find_group_dependency_cycle(group):
  """"""Searches for dependency cycle between nested groups.

  Traverses the dependency graph starting from |group|, fetching all necessary
  groups from datastore along the way.

  Args:
    group: instance of AuthGroup to start traversing from. It doesn't have to be
        committed to Datastore itself (but all its nested groups should be
        there already).

  Returns:
    List of names of groups that form a cycle or empty list if no cycles.
  """"""
  # It is a depth-first search on a directed graph with back edge detection.
  # See http://www.cs.nyu.edu/courses/summer04/G22.1170-001/6a-Graphs-More.pdf

  # Cache of already fetched groups.
  groups = {group.key.id(): group}

  # List of groups that are completely explored (all subtree is traversed).
  visited = []
  # Stack of groups that are being explored now. In case cycle is detected
  # it would contain that cycle.
  visiting = []

  def visit(group):
    """"""Recursively explores |group| subtree, returns True if finds a cycle.""""""
    assert group not in visiting
    assert group not in visited

    # Load bodies of nested groups not seen so far into |groups|.
    entities = ndb.get_multi(
        group_key(name) for name in group.nested if name not in groups)
    groups.update({entity.key.id(): entity for entity in entities if entity})

    visiting.append(group)
    for nested in group.nested:
      obj = groups.get(nested)
      # Do not crash if non-existent group is referenced somehow.
      if not obj:
        continue
      # Cross edge. Can happen in diamond-like graph, not a cycle.
      if obj in visited:
        continue
      # Back edge: |group| references its own ancestor -> cycle.
      if obj in visiting:
        return True
      # Explore subtree.
      if visit(obj):
        return True
    visiting.pop()

    visited.append(group)
    return False

  visit(group)
  return [group.key.id() for group in visiting]


################################################################################
## Secrets store.


class AuthSecretScope(ndb.Model):
  """"""Entity to act as parent entity for AuthSecret.

  Parent is AuthGlobalConfig entity keyed at root_key().

  Id of this entity defines scope of secret keys that have this entity as
  a parent. Possible scopes are 'local' and 'global'.

  Secrets in 'local' scope never leave Datastore they are stored in and they
  are different for each service (even for Replicas). Only service that
  generated a local secret knows it.

  Secrets in 'global' scope are known to all services (via Primary -> Replica
  DB replication mechanism). Source of truth for global secrets is in Primary's
  Datastore.
  """"""


def secret_scope_key(scope):
  """"""Key of AuthSecretScope entity for a given scope ('global' or 'local').""""""
  return ndb.Key(AuthSecretScope, scope, parent=root_key())


class AuthSecret(ndb.Model):
  """"""Some service-wide named secret blob.

  Entity can be a child of:
    * Key(AuthSecretScope, 'global', parent=root_key()):
        Global secrets replicated across all services.
    * Key(AuthSecretScope, 'local', parent=root_key()):
        Secrets local to the current service.

  There should be only very limited number of AuthSecret entities around. AuthDB
  fetches them all at once. Do not use this entity for per-user secrets.

  Holds most recent value of a secret as well as several previous values. Most
  recent value is used to generate new tokens, previous values may be used to
  validate existing tokens. That way secret can be rotated without invalidating
  any existing outstanding tokens.
  """"""
  # Last several values of a secret, with current value in front.
  values = ndb.BlobProperty(repeated=True, indexed=False)

  # When secret was modified last time.
  modified_ts = ndb.DateTimeProperty(auto_now_add=True)
  # Who modified the secret last time.
  modified_by = IdentityProperty()

  @classmethod
  def bootstrap(cls, name, scope, length=32):
    """"""Creates a secret if it doesn't exist yet.

    Args:
      name: name of the secret.
      scope: 'local' or 'global', see doc string for AuthSecretScope. 'global'
          scope should only be used on Primary service.
      length: length of the secret to generate if secret doesn't exist yet.

    Returns:
      Instance of AuthSecret (creating it if necessary) with random secret set.
    """"""
    # Note that 'get_or_insert' is a bad fit here. With 'get_or_insert' we'd
    # have to call os.urandom every time we want to get a key. It's a waste of
    # time and entropy.
    if scope not in ('local', 'global'):
      raise ValueError('Invalid secret scope: %s' % scope)
    key = ndb.Key(cls, name, parent=secret_scope_key(scope))
    entity = key.get()
    if entity is not None:
      return entity
    @ndb.transactional
    def create():
      entity = key.get()
      if entity is not None:
        return entity
      logging.info('Creating new secret key %s in %s scope', name, scope)
      # Global keys can only be created on Primary or Standalone service.
      if scope == 'global' and is_replica():
        raise ValueError('Can\'t bootstrap global key on Replica')
      entity = cls(
          key=key,
          values=[os.urandom(length)],
          modified_by=get_service_self_identity())
      entity.put()
      # Only global keys are part of replicated state.
      if scope == 'global':
        replicate_auth_db()
      return entity
    return create()


################################################################################
## IP whitelist.


class AuthIPWhitelistAssignments(ndb.Model, AuthVersionedEntityMixin):
  """"""A singleton entity with ""identity -> AuthIPWhitelist to use"" mapping.

  Entity key is ip_whitelist_assignments_key(). Parent entity is root_key().

  See AuthIPWhitelist for more info about IP whitelists.
  """"""
  class Assignment(ndb.Model):
    # Identity name to limit by IP whitelist. Unique key in 'assignments' list.
    identity = IdentityProperty()
    # Name of IP whitelist to use (see AuthIPWhitelist).
    ip_whitelist = ndb.StringProperty()
    # Why the assignment was created.
    comment = ndb.StringProperty()
    # When the assignment was created.
    created_ts = ndb.DateTimeProperty()
    # Who created the assignment.
    created_by = IdentityProperty()

  # Holds all the assignments.
  assignments = ndb.LocalStructuredProperty(Assignment, repeated=True)


class AuthIPWhitelist(
    ndb.Model,
    AuthVersionedEntityMixin,
    datastore_utils.SerializableModelMixin):
  """"""A named set of whitelisted IPv4 and IPv6 subnets.

  Can be assigned to individual user accounts to forcibly limit them only to
  particular IP addresses, e.g. it can be used to enforce that specific service
  account is used only from some known IP range. The mapping between accounts
  and IP whitelists is stored in AuthIPWhitelistAssignments.

  Entity id is a name of the whitelist. Parent entity is root_key().

  There's a special IP whitelist named 'bots' that can be used to list
  IP addresses of machines the service trusts unconditionally. Requests from
  such machines doesn't have to have any additional credentials attached.
  Requests will be authenticated as coming from identity 'bot:<IP address>'.
  """"""
  # How to convert this entity to or from serializable dict.
  serializable_properties = {
    'subnets': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'description': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'created_ts': datastore_utils.READABLE,
    'created_by': datastore_utils.READABLE,
    'modified_ts': datastore_utils.READABLE,
    'modified_by': datastore_utils.READABLE,
  }

  # The list of subnets. The validator is used only as a last measure. JSON API
  # handler should do validation too.
  subnets = ndb.StringProperty(
      repeated=True, validator=lambda _, val: ipaddr.normalize_subnet(val))

  # Human readable description.
  description = ndb.TextProperty(default='')

  # When the list was created.
  created_ts = ndb.DateTimeProperty()
  # Who created the list.
  created_by = IdentityProperty()

  def is_ip_whitelisted(self, ip):
    """"""Returns True if ipaddr.IP is in the whitelist.""""""
    # TODO(vadimsh): If number of subnets to check grows it makes sense to add
    # an internal cache to 'subnet_from_string' (sort of like in re.compile).
    return any(
        ipaddr.is_in_subnet(ip, ipaddr.subnet_from_string(net))
        for net in self.subnets)


def ip_whitelist_key(name):
  """"""Returns ndb.Key for AuthIPWhitelist entity given its name.""""""
  return ndb.Key(AuthIPWhitelist, name, parent=root_key())


def is_valid_ip_whitelist_name(name):
  """"""True if string looks like a valid IP whitelist name.""""""
  return bool(IP_WHITELIST_NAME_RE.match(name))


@ndb.transactional
def bootstrap_ip_whitelist(name, subnets, description=''):
  """"""Adds subnets to an IP whitelist if not there yet.

  Can be used on local dev appserver to add 127.0.0.1 to IP whitelist during
  startup. Should not be used from request handlers.

  Args:
    name: IP whitelist name to add a subnet to.
    subnets: IP subnet to add (as a list of strings).
    description: description of IP whitelist (if new entity is created).

  Returns:
    True if entry was added, False if it is already there or subnet is invalid.
  """"""
  assert isinstance(subnets, (list, tuple))
  try:
    subnets = [ipaddr.normalize_subnet(s) for s in subnets]
  except ValueError:
    return False
  key = ip_whitelist_key(name)
  entity = key.get()
  if entity and all(s in entity.subnets for s in subnets):
    return False
  now = utils.utcnow()
  if not entity:
    entity = AuthIPWhitelist(
        key=key,
        description=description,
        created_ts=now,
        created_by=get_service_self_identity())
  for s in subnets:
    if s not in entity.subnets:
      entity.subnets.append(s)
  entity.record_revision(
      modified_by=get_service_self_identity(),
      modified_ts=now,
      comment='Bootstrap')
  entity.put()
  replicate_auth_db()
  return True


def bootstrap_loopback_ips():
  """"""Adds 127.0.0.1 and ::1 to 'bots' IP whitelist.

  Useful on local dev server and in tests. Must not be used in production.

  Returns list of corresponding bot Identities.
  """"""
  # See api.py, AuthDB.verify_ip_whitelisted for IP -> Identity conversion.
  assert utils.is_local_dev_server()
  bootstrap_ip_whitelist(BOTS_IP_WHITELIST, ['127.0.0.1', '::1'], 'Local bots')
  return [
    Identity(IDENTITY_BOT, 'whitelisted-ip'),
    Identity(IDENTITY_BOT, '127.0.0.1'),
    Identity(IDENTITY_BOT, '0-0-0-0-0-0-0-1'),
  ]


@ndb.transactional
def bootstrap_ip_whitelist_assignment(identity, ip_whitelist, comment=''):
  """"""Sets a mapping ""identity -> IP whitelist to use"" for some account.

  Replaces existing assignment. Can be used on local dev appserver to configure
  IP whitelist assignments during startup or in tests. Should not be used from
  request handlers.

  Args:
    identity: Identity to modify.
    ip_whitelist: name of AuthIPWhitelist to assign.
    comment: comment to set.

  Returns:
    True if IP whitelist assignment was modified, False if it was already set.
  """"""
  entity = (
      ip_whitelist_assignments_key().get() or
      AuthIPWhitelistAssignments(key=ip_whitelist_assignments_key()))

  found = False
  for assignment in entity.assignments:
    if assignment.identity == identity:
      if assignment.ip_whitelist == ip_whitelist:
        return False
      assignment.ip_whitelist = ip_whitelist
      assignment.comment = comment
      found = True
      break

  now = utils.utcnow()
  if not found:
    entity.assignments.append(
        AuthIPWhitelistAssignments.Assignment(
            identity=identity,
            ip_whitelist=ip_whitelist,
            comment=comment,
            created_ts=now,
            created_by=get_service_self_identity()))

  entity.record_revision(
      modified_by=get_service_self_identity(),
      modified_ts=now,
      comment='Bootstrap')
  entity.put()
  replicate_auth_db()
  return True


def fetch_ip_whitelists():
  """"""Fetches AuthIPWhitelistAssignments and relevant AuthIPWhitelist entities.

  Returns:
    (AuthIPWhitelistAssignments, list of AuthIPWhitelist).
  """"""
  assignments = (
      ip_whitelist_assignments_key().get() or
      AuthIPWhitelistAssignments(key=ip_whitelist_assignments_key()))

  names = set(a.ip_whitelist for a in assignments.assignments)
  names.add(BOTS_IP_WHITELIST)

  whitelists = ndb.get_multi(ip_whitelist_key(n) for n in names)
  whitelists = sorted(filter(None, whitelists), key=lambda x: x.key.id())
  return assignments, whitelists
/n/n/nappengine/swarming/handlers_bot.py/n/n# Copyright 2015 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Internal bot API handlers.""""""

import base64
import json
import logging
import textwrap

import webob
import webapp2

from google.appengine.api import app_identity
from google.appengine.api import datastore_errors
from google.appengine.datastore import datastore_query
from google.appengine import runtime
from google.appengine.ext import ndb

from components import auth
from components import ereporter2
from components import utils
from server import acl
from server import bot_code
from server import bot_management
from server import stats
from server import task_pack
from server import task_scheduler
from server import task_to_run


def has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):
  """"""Returns an error if unexpected keys are present or expected keys are
  missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  actual_keys = frozenset(actual_keys)
  superfluous = actual_keys - expected_keys
  missing = minimum_keys - actual_keys
  if superfluous or missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    msg_superfluous = (
        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')
    return 'Unexpected %s%s%s; did you make a typo?' % (
        name, msg_missing, msg_superfluous)


def has_unexpected_keys(expected_keys, actual_keys, name):
  """"""Return an error if unexpected keys are present or expected keys are
  missing.
  """"""
  return has_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, name)


def log_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  message = has_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, name)
  if message:
    ereporter2.log_request(request, source=source, message=message)
  return message


def log_unexpected_keys(expected_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.
  """"""
  return log_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, request, source, name)


def has_missing_keys(minimum_keys, actual_keys, name):
  """"""Returns an error if expected keys are not present.

  Do not warn about unexpected keys.
  """"""
  actual_keys = frozenset(actual_keys)
  missing = minimum_keys - actual_keys
  if missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)


class BootstrapHandler(auth.AuthenticatingHandler):
  """"""Returns python code to run to bootstrap a swarming bot.""""""

  @auth.require(acl.is_bot)
  def get(self):
    self.response.headers['Content-Type'] = 'text/x-python'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot_bootstrap.py""')
    self.response.out.write(
        bot_code.get_bootstrap(self.request.host_url).content)


class BotCodeHandler(auth.AuthenticatingHandler):
  """"""Returns a zip file with all the files required by a bot.

  Optionally specify the hash version to download. If so, the returned data is
  cacheable.
  """"""

  @auth.require(acl.is_bot)
  def get(self, version=None):
    if version:
      expected = bot_code.get_bot_version(self.request.host_url)
      if version != expected:
        # This can happen when the server is rapidly updated.
        logging.error('Requested Swarming bot %s, have %s', version, expected)
        self.abort(404)
      self.response.headers['Cache-Control'] = 'public, max-age=3600'
    else:
      self.response.headers['Cache-Control'] = 'no-cache, no-store'
    self.response.headers['Content-Type'] = 'application/octet-stream'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot.zip""')
    self.response.out.write(
        bot_code.get_swarming_bot_zip(self.request.host_url))


class _BotBaseHandler(auth.ApiHandler):
  """"""
  Request body is a JSON dict:
    {
      ""dimensions"": <dict of properties>,
      ""state"": <dict of properties>,
      ""version"": <sha-1 of swarming_bot.zip uncompressed content>,
    }
  """"""

  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}
  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  def _process(self):
    """"""Returns True if the bot has invalid parameter and should be automatically
    quarantined.

    Does one DB synchronous GET.

    Returns:
      tuple(request, bot_id, version, state, dimensions, quarantined_msg)
    """"""
    request = self.parse_body()
    version = request.get('version', None)

    dimensions = request.get('dimensions', {})
    state = request.get('state', {})
    bot_id = None
    if dimensions.get('id'):
      dimension_id = dimensions['id']
      if (isinstance(dimension_id, list) and len(dimension_id) == 1
          and isinstance(dimension_id[0], unicode)):
        bot_id = dimensions['id'][0]

    # The bot may decide to ""self-quarantine"" itself. Accept both via
    # dimensions or via state. See bot_management._BotCommon.quarantined for
    # more details.
    if (bool(dimensions.get('quarantined')) or
        bool(state.get('quarantined'))):
      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'

    quarantined_msg = None
    # Use a dummy 'for' to be able to break early from the block.
    for _ in [0]:

      quarantined_msg = has_unexpected_keys(
          self.EXPECTED_KEYS, request, 'keys')
      if quarantined_msg:
        break

      quarantined_msg = has_missing_keys(
          self.REQUIRED_STATE_KEYS, state, 'state')
      if quarantined_msg:
        break

      if not bot_id:
        quarantined_msg = 'Missing bot id'
        break

      if not all(
          isinstance(key, unicode) and
          isinstance(values, list) and
          all(isinstance(value, unicode) for value in values)
          for key, values in dimensions.iteritems()):
        quarantined_msg = (
            'Invalid dimensions type:\n%s' % json.dumps(dimensions,
              sort_keys=True, indent=2, separators=(',', ': ')))
        break

      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)
      if dimensions_count > task_to_run.MAX_DIMENSIONS:
        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count
        break

    if quarantined_msg:
      line = 'Quarantined Bot\nhttps://%s/restricted/bot/%s\n%s' % (
          app_identity.get_default_version_hostname(), bot_id,
          quarantined_msg)
      ereporter2.log_request(self.request, source='bot', message=line)
      return request, bot_id, version, state, dimensions, quarantined_msg

    # Look for admin enforced quarantine.
    bot_settings = bot_management.get_settings_key(bot_id).get()
    if bool(bot_settings and bot_settings.quarantined):
      return request, bot_id, version, state, dimensions, 'Quarantined by admin'

    return request, bot_id, version, state, dimensions, None


class BotHandshakeHandler(_BotBaseHandler):
  """"""First request to be called to get initial data like XSRF token.

  The bot is server-controled so the server doesn't have to support multiple API
  version. When running a task, the bot sync the the version specific URL. Once
  abot finished its currently running task, it'll be immediately be upgraded
  after on its next poll.

  This endpoint does not return commands to the bot, for example to upgrade
  itself. It'll be told so when it does its first poll.

  Response body is a JSON dict:
    {
      ""bot_version"": <sha-1 of swarming_bot.zip uncompressed content>,
      ""server_version"": ""138-193f1f3"",
      ""xsrf_token"": ""......"",
    }
  """"""

  # This handler is called to get XSRF token, there's nothing to enforce yet.
  xsrf_token_enforce_on = ()

  @auth.require_xsrf_token_request
  @auth.require(acl.is_bot)
  def post(self):
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    bot_management.bot_event(
        event_type='bot_connected', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=dimensions,
        state=state, version=version, quarantined=bool(quarantined_msg),
        task_id='', task_name=None, message=quarantined_msg)

    data = {
      # This access token will be used to validate each subsequent request.
      'bot_version': bot_code.get_bot_version(self.request.host_url),
      'expiration_sec': auth.handler.XSRFToken.expiration_sec,
      'server_version': utils.get_app_version(),
      'xsrf_token': self.generate_xsrf_token(),
    }
    self.send_response(data)


class BotPollHandler(_BotBaseHandler):
  """"""The bot polls for a task; returns either a task, update command or sleep.

  In case of exception on the bot, this is enough to get it just far enough to
  eventually self-update to a working version. This is to ensure that coding
  errors in bot code doesn't kill all the fleet at once, they should still be up
  just enough to be able to self-update again even if they don't get task
  assigned anymore.
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    """"""Handles a polling request.

    Be very permissive on missing values. This can happen because of errors
    on the bot, *we don't want to deny them the capacity to update*, so that the
    bot code is eventually fixed and the bot self-update to this working code.

    It makes recovery of the fleet in case of catastrophic failure much easier.
    """"""
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    sleep_streak = state.get('sleep_streak', 0)
    quarantined = bool(quarantined_msg)

    # Note bot existence at two places, one for stats at 1 minute resolution,
    # the other for the list of known bots.
    action = 'bot_inactive' if quarantined else 'bot_active'
    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)

    def bot_event(event_type, task_id=None, task_name=None):
      bot_management.bot_event(
          event_type=event_type, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=dimensions,
          state=state, version=version, quarantined=quarantined,
          task_id=task_id, task_name=task_name, message=quarantined_msg)

    # Bot version is host-specific because the host URL is embedded in
    # swarming_bot.zip
    expected_version = bot_code.get_bot_version(self.request.host_url)
    if version != expected_version:
      bot_event('request_update')
      self._cmd_update(expected_version)
      return
    if quarantined:
      bot_event('request_sleep')
      self._cmd_sleep(sleep_streak, quarantined)
      return

    #
    # At that point, the bot should be in relatively good shape since it's
    # running the right version. It is still possible that invalid code was
    # pushed to the server, so be diligent about it.
    #

    # Bot may need a reboot if it is running for too long. We do not reboot
    # quarantined bots.
    needs_restart, restart_message = bot_management.should_restart_bot(
        bot_id, state)
    if needs_restart:
      bot_event('request_restart')
      self._cmd_restart(restart_message)
      return

    # The bot is in good shape. Try to grab a task.
    try:
      # This is a fairly complex function call, exceptions are expected.
      request, run_result = task_scheduler.bot_reap_task(
          dimensions, bot_id, version)
      if not request:
        # No task found, tell it to sleep a bit.
        bot_event('request_sleep')
        self._cmd_sleep(sleep_streak, quarantined)
        return

      try:
        # This part is tricky since it intentionally runs a transaction after
        # another one.
        if request.properties.is_terminate:
          bot_event('bot_terminate', task_id=run_result.task_id)
          self._cmd_terminate(run_result.task_id)
        else:
          bot_event(
              'request_task', task_id=run_result.task_id,
              task_name=request.name)
          self._cmd_run(request, run_result.key, bot_id)
      except:
        logging.exception('Dang, exception after reaping')
        raise
    except runtime.DeadlineExceededError:
      # If the timeout happened before a task was assigned there is no problems.
      # If the timeout occurred after a task was assigned, that task will
      # timeout (BOT_DIED) since the bot didn't get the details required to
      # run it) and it will automatically get retried (TODO) when the task times
      # out.
      # TODO(maruel): Note the task if possible and hand it out on next poll.
      # https://code.google.com/p/swarming/issues/detail?id=130
      self.abort(500, 'Deadline')

  def _cmd_run(self, request, run_result_key, bot_id):
    # Only one of 'command' or 'inputs_ref' can be set.
    out = {
      'cmd': 'run',
      'manifest': {
        'bot_id': bot_id,
        'command':
            request.properties.commands[0]
            if request.properties.commands else None,
        'data': request.properties.data,
        'dimensions': request.properties.dimensions,
        'env': request.properties.env,
        'extra_args': request.properties.extra_args,
        'grace_period': request.properties.grace_period_secs,
        'hard_timeout': request.properties.execution_timeout_secs,
        'host': utils.get_versioned_hosturl(),
        'io_timeout': request.properties.io_timeout_secs,
        'inputs_ref': request.properties.inputs_ref,
        'task_id': task_pack.pack_run_result_key(run_result_key),
      },
    }
    self.send_response(utils.to_json_encodable(out))

  def _cmd_sleep(self, sleep_streak, quarantined):
    out = {
      'cmd': 'sleep',
      'duration': task_scheduler.exponential_backoff(sleep_streak),
      'quarantined': quarantined,
    }
    self.send_response(out)

  def _cmd_terminate(self, task_id):
    out = {
      'cmd': 'terminate',
      'task_id': task_id,
    }
    self.send_response(out)

  def _cmd_update(self, expected_version):
    out = {
      'cmd': 'update',
      'version': expected_version,
    }
    self.send_response(out)

  def _cmd_restart(self, message):
    logging.info('Rebooting bot: %s', message)
    out = {
      'cmd': 'restart',
      'message': message,
    }
    self.send_response(out)


class BotEventHandler(_BotBaseHandler):
  """"""On signal that a bot had an event worth logging.""""""

  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}

  @auth.require(acl.is_bot)
  def post(self):
    (request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    event = request.get('event')
    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):
      self.abort_with_error(400, error='Unsupported event type')
    message = request.get('message')
    bot_management.bot_event(
        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,
        dimensions=dimensions, state=state, version=version,
        quarantined=bool(quarantined_msg), task_id=None, task_name=None,
        message=message)

    if event == 'bot_error':
      line = (
          'Bot: https://%s/restricted/bot/%s\n'
          'Bot error:\n'
          '%s') % (
          app_identity.get_default_version_hostname(), bot_id, message)
      ereporter2.log_request(self.request, source='bot', message=line)
    self.send_response({})


class BotTaskUpdateHandler(auth.ApiHandler):
  """"""Receives updates from a Bot for a task.

  The handler verifies packets are processed in order and will refuse
  out-of-order packets.
  """"""
  ACCEPTED_KEYS = {
    u'cost_usd', u'duration', u'exit_code', u'hard_timeout',
    u'id', u'io_timeout', u'output', u'output_chunk_start', u'outputs_ref',
    u'task_id',
  }
  REQUIRED_KEYS = {u'id', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    # Unlike handshake and poll, we do not accept invalid keys here. This code
    # path is much more strict.
    request = self.parse_body()
    msg = log_unexpected_subset_keys(
        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',
        'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    bot_id = request['id']
    cost_usd = request['cost_usd']
    task_id = request['task_id']

    duration = request.get('duration')
    exit_code = request.get('exit_code')
    hard_timeout = request.get('hard_timeout')
    io_timeout = request.get('io_timeout')
    output = request.get('output')
    output_chunk_start = request.get('output_chunk_start')
    outputs_ref = request.get('outputs_ref')

    run_result_key = task_pack.unpack_run_result_key(task_id)
    if output is not None:
      try:
        output = base64.b64decode(output)
      except UnicodeEncodeError as e:
        logging.error('Failed to decode output\n%s\n%r', e, output)
        output = output.encode('ascii', 'replace')
      except TypeError as e:
        # Save the output as-is instead. The error will be logged in ereporter2
        # and returning a HTTP 500 would only force the bot to stay in a retry
        # loop.
        logging.error('Failed to decode output\n%s\n%r', e, output)

    try:
      success, completed = task_scheduler.bot_update_task(
          run_result_key, bot_id, output, output_chunk_start,
          exit_code, duration, hard_timeout, io_timeout, cost_usd, outputs_ref)
      if not success:
        logging.info('Failed to update, please retry')
        self.abort_with_error(500, error='Failed to update, please retry')

      action = 'task_completed' if completed else 'task_update'
      bot_management.bot_event(
          event_type=action, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=None, state=None,
          version=None, quarantined=None, task_id=task_id, task_name=None)
    except ValueError as e:
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % e)
      self.abort_with_error(400, error=str(e))
    except webob.exc.HTTPException:
      raise
    except Exception as e:
      logging.exception('Internal error: %s', e)
      self.abort_with_error(500, error=str(e))

    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot
    # reboots itself to abort the task abruptly. It is useful when a task hangs
    # and the timeout was set too long or the task was superseded by a newer
    # task with more recent executable (e.g. a new Try Server job on a newer
    # patchset on Rietveld).
    self.send_response({'ok': True})


class BotTaskErrorHandler(auth.ApiHandler):
  """"""It is a specialized version of ereporter2's /ereporter2/api/v1/on_error
  that also attaches a task id to it.

  This formally kills the task, marking it as an internal failure. This can be
  used by bot_main.py to kill the task when task_runner misbehaved.
  """"""

  EXPECTED_KEYS = {u'id', u'message', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    request = self.parse_body()
    bot_id = request.get('id')
    task_id = request.get('task_id', '')
    message = request.get('message', 'unknown')

    bot_management.bot_event(
        event_type='task_error', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=None, state=None,
        version=None, quarantined=None, task_id=task_id, task_name=None,
        message=message)
    line = (
        'Bot: https://%s/restricted/bot/%s\n'
        'Task failed: https://%s/user/task/%s\n'
        '%s') % (
        app_identity.get_default_version_hostname(), bot_id,
        app_identity.get_default_version_hostname(), task_id,
        message)
    ereporter2.log_request(self.request, source='bot', message=line)

    msg = log_unexpected_keys(
        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    msg = task_scheduler.bot_kill_task(
        task_pack.unpack_run_result_key(task_id), bot_id)
    if msg:
      logging.error(msg)
      self.abort_with_error(400, error=msg)
    self.send_response({})


class ServerPingHandler(webapp2.RequestHandler):
  """"""Handler to ping when checking if the server is up.

  This handler should be extremely lightweight. It shouldn't do any
  computations, it should just state that the server is up. It's open to
  everyone for simplicity and performance.
  """"""

  def get(self):
    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'
    self.response.out.write('Server up')


def get_routes():
  routes = [
      ('/bootstrap', BootstrapHandler),
      ('/bot_code', BotCodeHandler),
      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),
      ('/swarming/api/v1/bot/event', BotEventHandler),
      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),
      ('/swarming/api/v1/bot/poll', BotPollHandler),
      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),
      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',
          BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),
      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',
          BotTaskErrorHandler),
  ]
  return [webapp2.Route(*i) for i in routes]
/n/n/n",0
81,81,0ba6a589d77baefc5ae20cde5c3a5dc24a6290f9,"/appengine/components/components/auth/handler.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Integration with webapp2.""""""

# Disable 'Method could be a function.'
# pylint: disable=R0201

import functools
import json
import logging
import urllib
import webapp2

from google.appengine.api import urlfetch
from google.appengine.api import users

from components import utils

from . import api
from . import config
from . import delegation
from . import host_token
from . import ipaddr
from . import model
from . import openid
from . import tokens

# Part of public API of 'auth' component, exposed by this module.
__all__ = [
  'ApiHandler',
  'AuthenticatingHandler',
  'gae_cookie_authentication',
  'get_authenticated_routes',
  'oauth_authentication',
  'openid_cookie_authentication',
  'require_xsrf_token_request',
  'service_to_service_authentication',
]


def require_xsrf_token_request(f):
  """"""Use for handshaking APIs.""""""
  @functools.wraps(f)
  def hook(self, *args, **kwargs):
    if not self.request.headers.get('X-XSRF-Token-Request'):
      raise api.AuthorizationError('Missing required XSRF request header')
    return f(self, *args, **kwargs)
  return hook


class XSRFToken(tokens.TokenKind):
  """"""XSRF token parameters.""""""
  expiration_sec = 4 * 3600
  secret_key = api.SecretKey('xsrf_token', scope='local')
  version = 1


class AuthenticatingHandlerMetaclass(type):
  """"""Ensures that 'get', 'post', etc. are marked with @require or @public.""""""

  def __new__(mcs, name, bases, attributes):
    for method in webapp2.WSGIApplication.allowed_methods:
      func = attributes.get(method.lower())
      if func and not api.is_decorated(func):
        raise TypeError(
            'Method \'%s\' of \'%s\' is not protected by @require or @public '
            'decorator' % (method.lower(), name))
    return type.__new__(mcs, name, bases, attributes)


class AuthenticatingHandler(webapp2.RequestHandler):
  """"""Base class for webapp2 request handlers that use Auth system.

  Knows how to extract Identity from request data and how to initialize auth
  request context, so that get_current_identity() and is_group_member() work.

  All request handling methods (like 'get', 'post', etc) should be marked by
  either @require or @public decorators.
  """"""

  # Checks that all 'get', 'post', etc. are marked with @require or @public.
  __metaclass__ = AuthenticatingHandlerMetaclass

  # List of HTTP methods that trigger XSRF token validation.
  xsrf_token_enforce_on = ('DELETE', 'POST', 'PUT')
  # If not None, the header to search for XSRF token.
  xsrf_token_header = 'X-XSRF-Token'
  # If not None, the request parameter (GET or POST) to search for XSRF token.
  xsrf_token_request_param = 'xsrf_token'
  # Embedded data extracted from XSRF token of current request.
  xsrf_token_data = None
  # If not None, sets X_Frame-Options on all replies.
  frame_options = 'DENY'
  # A method used to authenticate this request, see get_auth_methods().
  auth_method = None

  def dispatch(self):
    """"""Extracts and verifies Identity, sets up request auth context.""""""
    # Ensure auth component is configured before executing any code.
    conf = config.ensure_configured()
    auth_context = api.reinitialize_request_cache()

    # http://www.html5rocks.com/en/tutorials/security/content-security-policy/
    # https://www.owasp.org/index.php/Content_Security_Policy
    # TODO(maruel): Remove 'unsafe-inline' once all inline style=""foo:bar"" in
    # all HTML tags were removed. Warning if seeing this post 2016, it could
    # take a while.
    # - https://www.google.com is due to Google Viz library.
    # - https://www.google-analytics.com due to Analytics.
    # - 'unsafe-eval' due to polymer.
    self.response.headers['Content-Security-Policy'] = (
        'default-src https: \'self\' \'unsafe-inline\' https://www.google.com '
        'https://www.google-analytics.com \'unsafe-eval\'')
    # Enforce HTTPS by adding the HSTS header; 365*24*60*60s.
    # https://www.owasp.org/index.php/HTTP_Strict_Transport_Security
    self.response.headers['Strict-Transport-Security'] = (
        'max-age=31536000; includeSubDomains; preload')
    # Disable frame support wholesale.
    # https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet
    if self.frame_options:
      self.response.headers['X-Frame-Options'] = self.frame_options

    identity = None
    for method_func in self.get_auth_methods(conf):
      try:
        identity = method_func(self.request)
        if identity:
          break
      except api.AuthenticationError as err:
        self.authentication_error(err)
        return
      except api.AuthorizationError as err:
        self.authorization_error(err)
        return
    else:
      method_func = None
    self.auth_method = method_func

    # If no authentication method is applicable, default to anonymous identity.
    identity = identity or model.Anonymous

    # XSRF token is required only if using Cookie based or IP whitelist auth.
    # A browser doesn't send Authorization: 'Bearer ...' or any other headers
    # by itself. So XSRF check is not required if header based authentication
    # is used.
    using_headers_auth = method_func in (
        oauth_authentication, service_to_service_authentication)

    # Extract caller host name from host token header, if present and valid.
    host_tok = self.request.headers.get(host_token.HTTP_HEADER)
    if host_tok:
      validated_host = host_token.validate_host_token(host_tok)
      if validated_host:
        auth_context.peer_host = validated_host

    # Verify IP is whitelisted and authenticate requests from bots.
    assert self.request.remote_addr
    ip = ipaddr.ip_from_string(self.request.remote_addr)
    auth_context.peer_ip = ip
    try:
      # 'verify_ip_whitelisted' may change identity for bots, store new one.
      auth_context.peer_identity = api.verify_ip_whitelisted(
          identity, ip, self.request.headers)
    except api.AuthorizationError as err:
      self.authorization_error(err)
      return

    # Parse delegation token, if given, to deduce end-user identity.
    delegation_tok = self.request.headers.get(delegation.HTTP_HEADER)
    if delegation_tok:
      try:
        auth_context.current_identity = delegation.check_delegation_token(
            delegation_tok, auth_context.peer_identity)
      except delegation.BadTokenError as exc:
        self.authorization_error(
            api.AuthorizationError('Bad delegation token: %s' % exc))
      except delegation.TransientError as exc:
        msg = 'Transient error while validating delegation token.\n%s' % exc
        logging.error(msg)
        self.abort(500, detail=msg)
    else:
      auth_context.current_identity = auth_context.peer_identity

    try:
      # Fail if XSRF token is required, but not provided.
      need_xsrf_token = (
          not using_headers_auth and
          self.request.method in self.xsrf_token_enforce_on)
      if need_xsrf_token and self.xsrf_token is None:
        raise api.AuthorizationError('XSRF token is missing')

      # If XSRF token is present, verify it is valid and extract its payload.
      # Do it even if XSRF token is not strictly required, since some handlers
      # use it to store session state (it is similar to a signed cookie).
      self.xsrf_token_data = {}
      if self.xsrf_token is not None:
        # This raises AuthorizationError if token is invalid.
        self.xsrf_token_data = self.verify_xsrf_token()

      # All other ACL checks will be performed by corresponding handlers
      # manually or via '@required' decorator. Failed ACL check raises
      # AuthorizationError.
      super(AuthenticatingHandler, self).dispatch()
    except api.AuthorizationError as err:
      self.authorization_error(err)

  @classmethod
  def get_auth_methods(cls, conf):
    """"""Returns an enumerable of functions to use to authenticate request.

    The handler will try to apply auth methods sequentially one by one by until
    it finds one that works.

    Each auth method is a function that accepts webapp2.Request and can finish
    with 3 outcomes:

    * Return None: authentication method is not applicable to that request
      and next method should be tried (for example cookie-based
      authentication is not applicable when there's no cookies).

    * Returns Identity associated with the request. Means authentication method
      is applicable and request authenticity is confirmed.

    * Raises AuthenticationError: authentication method is applicable, but
      request contains bad credentials or invalid token, etc. For example,
      OAuth2 token is given, but it is revoked.

    A chosen auth method function will be stored in request's auth_method field.

    Args:
      conf: components.auth GAE config, see config.py.
    """"""
    if conf.USE_OPENID:
      cookie_auth = openid_cookie_authentication
    else:
      cookie_auth = gae_cookie_authentication
    return oauth_authentication, cookie_auth, service_to_service_authentication

  def generate_xsrf_token(self, xsrf_token_data=None):
    """"""Returns new XSRF token that embeds |xsrf_token_data|.

    The token is bound to current identity and is valid only when used by same
    identity.
    """"""
    return XSRFToken.generate(
        [api.get_current_identity().to_bytes()], xsrf_token_data)

  @property
  def xsrf_token(self):
    """"""Returns XSRF token passed with the request or None if missing.

    Doesn't do any validation. Use verify_xsrf_token() instead.
    """"""
    token = None
    if self.xsrf_token_header:
      token = self.request.headers.get(self.xsrf_token_header)
    if not token and self.xsrf_token_request_param:
      param = self.request.get_all(self.xsrf_token_request_param)
      token = param[0] if param else None
    return token

  def verify_xsrf_token(self):
    """"""Grabs a token from the request, validates it and extracts embedded data.

    Current identity must be the same as one used to generate the token.

    Returns:
      Whatever was passed as |xsrf_token_data| in 'generate_xsrf_token'
      method call used to generate the token.

    Raises:
      AuthorizationError if token is missing, invalid or expired.
    """"""
    token = self.xsrf_token
    if not token:
      raise api.AuthorizationError('XSRF token is missing')
    # Check that it was generated for the same identity.
    try:
      return XSRFToken.validate(token, [api.get_current_identity().to_bytes()])
    except tokens.InvalidTokenError as err:
      raise api.AuthorizationError(str(err))

  def authentication_error(self, error):
    """"""Called when authentication fails to report the error to requester.

    Authentication error means that some credentials are provided but they are
    invalid. If no credentials are provided at all, no authentication is
    attempted and current identity is just set to 'anonymous:anonymous'.

    Default behavior is to abort the request with HTTP 401 error (and human
    readable HTML body).

    Args:
      error: instance of AuthenticationError subclass.
    """"""
    logging.warning('Authentication error.\n%s', error)
    self.abort(401, detail=str(error))

  def authorization_error(self, error):
    """"""Called when authentication succeeds, but access to a resource is denied.

    Called whenever request handler raises AuthorizationError exception.
    In particular this exception is raised by method decorated with @require if
    current identity doesn't have required permission.

    Default behavior is to abort the request with HTTP 403 error (and human
    readable HTML body).

    Args:
      error: instance of AuthorizationError subclass.
    """"""
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort(403, detail=str(error))

  ### Wrappers around Users API or its OpenID equivalent.

  def get_current_user(self):
    """"""When cookie auth is used returns instance of CurrentUser or None.""""""
    return self._get_users_api().get_current_user(self.request)

  def is_current_user_gae_admin(self):
    """"""When cookie auth is used returns True if current caller is GAE admin.""""""
    return self._get_users_api().is_current_user_gae_admin(self.request)

  def create_login_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to login.""""""
    return self._get_users_api().create_login_url(self.request, dest_url)

  def create_logout_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to logout.""""""
    return self._get_users_api().create_logout_url(self.request, dest_url)

  def _get_users_api(self):
    """"""Returns GAEUsersAPI, OpenIDAPI or raises NotImplementedError.

    Chooses based on what auth_method was used of what methods are available.
    """"""
    method = self.auth_method
    if not method:
      # Anonymous request -> pick first method that supports API.
      for method in self.get_auth_methods(config.ensure_configured()):
        if method in _METHOD_TO_USERS_API:
          break
      else:
        raise NotImplementedError('No methods support UsersAPI')
    elif method not in _METHOD_TO_USERS_API:
      raise NotImplementedError(
          '%s doesn\'t support UsersAPI' % method.__name__)
    return _METHOD_TO_USERS_API[method]


class ApiHandler(AuthenticatingHandler):
  """"""Parses JSON request body to a dict, serializes response to JSON.""""""
  CONTENT_TYPE_BASE = 'application/json'
  CONTENT_TYPE_FULL = 'application/json; charset=utf-8'
  _json_body = None
  # Clickjacking not applicable to APIs.
  frame_options = None

  def authentication_error(self, error):
    logging.warning('Authentication error.\n%s', error)
    self.abort_with_error(401, text=str(error))

  def authorization_error(self, error):
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort_with_error(403, text=str(error))

  def send_response(self, response, http_code=200, headers=None):
    """"""Sends successful reply and continues execution.""""""
    self.response.set_status(http_code)
    self.response.headers.update(headers or {})
    self.response.headers['Content-Type'] = self.CONTENT_TYPE_FULL
    self.response.write(json.dumps(response))

  def abort_with_error(self, http_code, **kwargs):
    """"""Sends error reply and stops execution.""""""
    self.abort(
        http_code,
        json=kwargs,
        headers={'Content-Type': self.CONTENT_TYPE_FULL})

  def parse_body(self):
    """"""Parses JSON body and verifies it's a dict.

    webob.Request doesn't cache the decoded json body, this function does.
    """"""
    if self._json_body is None:
      if (self.CONTENT_TYPE_BASE and
          self.request.content_type != self.CONTENT_TYPE_BASE):
        msg = (
            'Expecting JSON body with content type \'%s\'' %
            self.CONTENT_TYPE_BASE)
        self.abort_with_error(400, text=msg)
      try:
        self._json_body = self.request.json
        if not isinstance(self._json_body, dict):
          raise ValueError()
      except (LookupError, ValueError):
        self.abort_with_error(400, text='Not a valid json dict body')
    return self._json_body.copy()


def get_authenticated_routes(app):
  """"""Given WSGIApplication returns list of routes that use authentication.

  Intended to be used only for testing.
  """"""
  # This code is adapted from router's __repr__ method (that enumerate
  # all routes for pretty-printing).
  routes = list(app.router.match_routes)
  routes.extend(
      v for k, v in app.router.build_routes.iteritems()
      if v not in app.router.match_routes)
  return [r for r in routes if issubclass(r.handler, AuthenticatingHandler)]


################################################################################
## All supported implementations of authentication methods for webapp2 handlers.


def gae_cookie_authentication(_request):
  """"""AppEngine cookie based authentication via users.get_current_user().""""""
  user = users.get_current_user()
  try:
    return model.Identity(model.IDENTITY_USER, user.email()) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email())


def openid_cookie_authentication(request):
  """"""Cookie based authentication that uses OpenID flow for login.""""""
  user = openid.get_current_user(request)
  try:
    return model.Identity(model.IDENTITY_USER, user.email) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email)


def oauth_authentication(request):
  """"""OAuth2 based authentication via oauth.get_current_user().""""""
  if not request.headers.get('Authorization'):
    return None
  if not utils.is_local_dev_server():
    return api.extract_oauth_caller_identity()

  # OAuth2 library is mocked on dev server to return some nonsense. Use (slow,
  # but real) OAuth2 API endpoint instead to validate access_token. It is also
  # what Cloud Endpoints do on a local server. For simplicity ignore client_id
  # on dev server.
  header = request.headers['Authorization'].split(' ', 1)
  if len(header) != 2 or header[0] not in ('OAuth', 'Bearer'):
    raise api.AuthenticationError('Invalid authorization header')

  # Adapted from endpoints/users_id_tokens.py, _set_bearer_user_vars_local.
  base_url = 'https://www.googleapis.com/oauth2/v1/tokeninfo'
  result = urlfetch.fetch(
      url='%s?%s' % (base_url, urllib.urlencode({'access_token': header[1]})),
      follow_redirects=False,
      validate_certificate=True)
  if result.status_code != 200:
    try:
      error = json.loads(result.content)['error_description']
    except (KeyError, ValueError):
      error = repr(result.content)
    raise api.AuthenticationError('Failed to validate the token: %s' % error)

  token_info = json.loads(result.content)
  if 'email' not in token_info:
    raise api.AuthenticationError('Token doesn\'t include an email address')
  if not token_info.get('verified_email'):
    raise api.AuthenticationError('Token email isn\'t verified')

  email = token_info['email']
  try:
    return model.Identity(model.IDENTITY_USER, email)
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % email)


def service_to_service_authentication(request):
  """"""Used for AppEngine <-> AppEngine communication.

  Relies on X-Appengine-Inbound-Appid header set by AppEngine itself. It can't
  be set by external users (with exception of admins).
  """"""
  app_id = request.headers.get('X-Appengine-Inbound-Appid')
  try:
    return model.Identity(model.IDENTITY_SERVICE, app_id) if app_id else None
  except ValueError:
    raise api.AuthenticationError('Unsupported application ID: %s' % app_id)


################################################################################
## API wrapper on top of Users API and OpenID API to make them similar.


class CurrentUser(object):
  """"""Mimics subset of GAE users.User object for ease of transition.

  Also adds .picture().
  """"""

  def __init__(self, user_id, email, picture):
    self._user_id = user_id
    self._email = email
    self._picture = picture

  def nickname(self):
    return self._email

  def email(self):
    return self._email

  def user_id(self):
    return self._user_id

  def picture(self):
    return self._picture

  def __unicode__(self):
    return unicode(self.nickname())

  def __str__(self):
    return str(self.nickname())


class GAEUsersAPI(object):
  @staticmethod
  def get_current_user(request):  # pylint: disable=unused-argument
    user = users.get_current_user()
    return CurrentUser(user.user_id(), user.email(), None) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return users.is_current_user_admin()

  @staticmethod
  def create_login_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_login_url(dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_logout_url(dest_url)


class OpenIDAPI(object):
  @staticmethod
  def get_current_user(request):
    user = openid.get_current_user(request)
    return CurrentUser(user.sub, user.email, user.picture) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return False

  @staticmethod
  def create_login_url(request, dest_url):
    return openid.create_login_url(request, dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):
    return openid.create_logout_url(request, dest_url)


# See AuthenticatingHandler._get_users_api().
_METHOD_TO_USERS_API = {
  gae_cookie_authentication: GAEUsersAPI,
  openid_cookie_authentication: OpenIDAPI,
}
/n/n/n/appengine/components/components/auth/handler_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

# Disable 'Unused variable', 'Unused argument' and 'Method could be a function'.
# pylint: disable=W0612,W0613,R0201

import datetime
import json
import os
import sys
import unittest

from test_support import test_env
test_env.setup_test_env()

from google.appengine.api import oauth
from google.appengine.api import users

import webapp2
import webtest

from components import utils
from components.auth import api
from components.auth import delegation
from components.auth import handler
from components.auth import host_token
from components.auth import ipaddr
from components.auth import model
from components.auth.proto import delegation_pb2
from test_support import test_case


class AuthenticatingHandlerMetaclassTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandlerMetaclass.""""""

  def test_good(self):
    # No request handling methods defined at all.
    class TestHandler1(handler.AuthenticatingHandler):
      def some_other_method(self):
        pass

    # @public is used.
    class TestHandler2(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        pass

    # @require is used.
    class TestHandler3(handler.AuthenticatingHandler):
      @api.require(lambda: True)
      def get(self):
        pass

  def test_bad(self):
    # @public or @require is missing.
    with self.assertRaises(TypeError):
      class TestHandler1(handler.AuthenticatingHandler):
        def get(self):
          pass


class AuthenticatingHandlerTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandler class.""""""

  def setUp(self):
    super(AuthenticatingHandlerTest, self).setUp()
    # Reset global config of auth library before each test.
    api.reset_local_state()
    # Capture error and warning log messages.
    self.logged_errors = []
    self.mock(handler.logging, 'error',
        lambda *args, **kwargs: self.logged_errors.append((args, kwargs)))
    self.logged_warnings = []
    self.mock(handler.logging, 'warning',
        lambda *args, **kwargs: self.logged_warnings.append((args, kwargs)))

  def make_test_app(self, path, request_handler):
    """"""Returns webtest.TestApp with single route.""""""
    return webtest.TestApp(
        webapp2.WSGIApplication([(path, request_handler)], debug=True),
        extra_environ={'REMOTE_ADDR': '127.0.0.1'})

  def test_anonymous(self):
    """"""If all auth methods are not applicable, identity is set to Anonymous.""""""
    test = self

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        non_applicable = lambda _request: None
        return [non_applicable, non_applicable]

      @api.public
      def get(self):
        test.assertEqual(model.Anonymous, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

  def test_ip_whitelist_bot(self):
    """"""Requests from client in ""bots"" IP whitelist are authenticated as bot.""""""
    model.bootstrap_ip_whitelist('bots', ['192.168.1.100/32'])

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_current_identity().to_bytes())

    app = self.make_test_app('/request', Handler)
    def call(ip):
      api.reset_local_state()
      return app.get('/request', extra_environ={'REMOTE_ADDR': ip}).body

    self.assertEqual('bot:whitelisted-ip', call('192.168.1.100'))
    self.assertEqual('anonymous:anonymous', call('127.0.0.1'))

  def test_ip_whitelist(self):
    """"""Per-account IP whitelist works.""""""
    ident1 = model.Identity(model.IDENTITY_USER, 'a@example.com')
    ident2 = model.Identity(model.IDENTITY_USER, 'b@example.com')

    model.bootstrap_ip_whitelist('whitelist', ['192.168.1.100/32'])
    model.bootstrap_ip_whitelist_assignment(ident1, 'whitelist')

    mocked_ident = [None]

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _req: mocked_ident[0]]

      @api.public
      def get(self):
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    def call(ident, ip):
      api.reset_local_state()
      mocked_ident[0] = ident
      response = app.get(
          '/request', extra_environ={'REMOTE_ADDR': ip}, expect_errors=True)
      return response.status_int

    # IP is whitelisted.
    self.assertEqual(200, call(ident1, '192.168.1.100'))
    # IP is NOT whitelisted.
    self.assertEqual(403, call(ident1, '127.0.0.1'))
    # Whitelist is not used.
    self.assertEqual(200, call(ident2, '127.0.0.1'))

  def test_auth_method_order(self):
    """"""Registered auth methods are tested in order.""""""
    test = self
    calls = []
    ident = model.Identity(model.IDENTITY_USER, 'joe@example.com')

    def not_applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('not_applicable')
      return None

    def applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('applicable')
      return ident

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [not_applicable, applicable]

      @api.public
      def get(self):
        test.assertEqual(ident, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

    # Both methods should be tried.
    expected_calls = [
      'not_applicable',
      'applicable',
    ]
    self.assertEqual(expected_calls, calls)

  def test_authentication_error(self):
    """"""AuthenticationError in auth method stops request processing.""""""
    test = self
    calls = []

    def failing(request):
      raise api.AuthenticationError('Too bad')

    def skipped(request):
      self.fail('authenticate should not be called')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [failing, skipped]

      @api.public
      def get(self):
        test.fail('Handler code should not be called')

      def authentication_error(self, err):
        test.assertEqual('Too bad', err.message)
        calls.append('authentication_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authentication_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 401.
    self.assertEqual(['authentication_error'], calls)
    self.assertEqual(401, response.status_int)

    # Authentication error is logged.
    self.assertEqual(1, len(self.logged_warnings))

  def test_authorization_error(self):
    """"""AuthorizationError in auth method is handled.""""""
    test = self
    calls = []

    class Handler(handler.AuthenticatingHandler):
      @api.require(lambda: False)
      def get(self):
        test.fail('Handler code should not be called')

      def authorization_error(self, err):
        calls.append('authorization_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authorization_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 403.
    self.assertEqual(['authorization_error'], calls)
    self.assertEqual(403, response.status_int)

  def make_xsrf_handling_app(
      self,
      xsrf_token_enforce_on=None,
      xsrf_token_header=None,
      xsrf_token_request_param=None):
    """"""Returns webtest app with single XSRF-aware handler.

    If generates XSRF tokens on GET and validates them on POST, PUT, DELETE.
    """"""
    calls = []

    def record(request_handler, method):
      is_valid = request_handler.xsrf_token_data == {'some': 'data'}
      calls.append((method, is_valid))

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(self.generate_xsrf_token({'some': 'data'}))
      @api.public
      def post(self):
        record(self, 'POST')
      @api.public
      def put(self):
        record(self, 'PUT')
      @api.public
      def delete(self):
        record(self, 'DELETE')

    if xsrf_token_enforce_on is not None:
      Handler.xsrf_token_enforce_on = xsrf_token_enforce_on
    if xsrf_token_header is not None:
      Handler.xsrf_token_header = xsrf_token_header
    if xsrf_token_request_param is not None:
      Handler.xsrf_token_request_param = xsrf_token_request_param

    app = self.make_test_app('/request', Handler)
    return app, calls

  def mock_get_current_identity(self, ident):
    """"""Mocks api.get_current_identity() to return |ident|.""""""
    self.mock(handler.api, 'get_current_identity', lambda: ident)

  def test_xsrf_token_get_param(self):
    """"""XSRF token works if put in GET parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request?xsrf_token=%s' % token)
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_post_param(self):
    """"""XSRF token works if put in POST parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_header(self):
    """"""XSRF token works if put in the headers.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', headers={'X-XSRF-Token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_missing(self):
    """"""XSRF token is not given but handler requires it.""""""
    app, calls = self.make_xsrf_handling_app()
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_xsrf_token_uses_enforce_on(self):
    """"""Only methods set in |xsrf_token_enforce_on| require token validation.""""""
    # Validate tokens only on PUT (not on POST).
    app, calls = self.make_xsrf_handling_app(xsrf_token_enforce_on=('PUT',))
    token = app.get('/request').body
    # Both POST and PUT work when token provided, verifying it.
    app.post('/request', {'xsrf_token': token})
    app.put('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True), ('PUT', True)], calls)
    # POST works without a token, put PUT doesn't.
    self.assertEqual(200, app.post('/request').status_int)
    self.assertEqual(403, app.put('/request', expect_errors=True).status_int)
    # Both fail if wrong token is provided.
    bad_token = {'xsrf_token': 'boo'}
    self.assertEqual(
        403, app.post('/request', bad_token, expect_errors=True).status_int)
    self.assertEqual(
        403, app.put('/request', bad_token, expect_errors=True).status_int)

  def test_xsrf_token_uses_xsrf_token_header(self):
    """"""Name of the header used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_header='X-Some')
    token = app.get('/request').body
    app.post('/request', headers={'X-Some': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_uses_xsrf_token_request_param(self):
    """"""Name of the request param used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_request_param='tok')
    token = app.get('/request').body
    app.post('/request', {'tok': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_identity_matters(self):
    app, calls = self.make_xsrf_handling_app()
    # Generate token for identity A.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'a@example.com'))
    token = app.get('/request').body
    # Try to use it by identity B.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'b@example.com'))
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_get_authenticated_routes(self):
    class Authenticated(handler.AuthenticatingHandler):
      pass

    class NotAuthenticated(webapp2.RequestHandler):
      pass

    app = webapp2.WSGIApplication([
      webapp2.Route('/authenticated', Authenticated),
      webapp2.Route('/not-authenticated', NotAuthenticated),
    ])
    routes = handler.get_authenticated_routes(app)
    self.assertEqual(1, len(routes))
    self.assertEqual(Authenticated, routes[0].handler)

  def test_get_peer_ip(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(ipaddr.ip_to_string(api.get_peer_ip()))

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', extra_environ={'REMOTE_ADDR': '192.1.2.3'})
    self.assertEqual('192.1.2.3', response.body)

  def test_get_peer_host(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_peer_host() or '<none>')

    app = self.make_test_app('/request', Handler)
    def call(headers):
      api.reset_local_state()
      return app.get('/request', headers=headers).body

    # Good token.
    token = host_token.create_host_token('HOST.domain.com')
    self.assertEqual('host.domain.com', call({'X-Host-Token-V1': token}))

    # Missing or invalid tokens.
    self.assertEqual('<none>', call({}))
    self.assertEqual('<none>', call({'X-Host-Token-V1': 'broken'}))

    # Expired token.
    origin = datetime.datetime(2014, 1, 1, 1, 1, 1)
    self.mock_now(origin)
    token = host_token.create_host_token('HOST.domain.com', expiration_sec=60)
    self.mock_now(origin, 61)
    self.assertEqual('<none>', call({'X-Host-Token-V1': token}))

  def test_delegation_token(self):
    peer_ident = model.Identity.from_bytes('user:peer@a.com')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _request: peer_ident]

      @api.public
      def get(self):
        self.response.write(json.dumps({
          'peer_id': api.get_peer_identity().to_bytes(),
          'cur_id': api.get_current_identity().to_bytes(),
        }))

    app = self.make_test_app('/request', Handler)
    def call(headers=None):
      return json.loads(app.get('/request', headers=headers).body)

    # No delegation.
    self.assertEqual(
        {u'cur_id': u'user:peer@a.com', u'peer_id': u'user:peer@a.com'}, call())

    # TODO(vadimsh): Mint token via some high-level function call.
    subtokens = delegation_pb2.SubtokenList(subtokens=[
        delegation_pb2.Subtoken(
            issuer_id='user:delegated@a.com',
            creation_time=int(utils.time_time()),
            validity_duration=3600),
    ])
    tok = delegation.serialize_token(delegation.seal_token(subtokens))

    # With valid delegation token.
    self.assertEqual(
        {u'cur_id': u'user:delegated@a.com', u'peer_id': u'user:peer@a.com'},
        call({'X-Delegation-Token-V1': tok}))

    # With invalid delegation token.
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok + 'blah'},
        expect_errors=True)
    self.assertEqual(403, r.status_int)

    # Transient error.
    def mocked_check(*_args):
      raise delegation.TransientError('Blah')
    self.mock(delegation, 'check_delegation_token', mocked_check)
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok},
        expect_errors=True)
    self.assertEqual(500, r.status_int)


class GaeCookieAuthenticationTest(test_case.TestCase):
  """"""Tests for gae_cookie_authentication function.""""""

  def test_non_applicable(self):
    self.assertIsNone(handler.gae_cookie_authentication(webapp2.Request({})))

  def test_applicable(self):
    os.environ.update({
      'USER_EMAIL': 'joe@example.com',
      'USER_ID': '123',
      'USER_IS_ADMIN': '0',
    })
    # Actual request is not used by CookieAuthentication.
    self.assertEqual(
        model.Identity(model.IDENTITY_USER, 'joe@example.com'),
        handler.gae_cookie_authentication(webapp2.Request({})))


class ServiceToServiceAuthenticationTest(test_case.TestCase):
  """"""Tests for service_to_service_authentication.""""""

  def test_non_applicable(self):
    request = webapp2.Request({})
    self.assertIsNone(
        handler.service_to_service_authentication(request))

  def test_applicable(self):
    request = webapp2.Request({
      'HTTP_X_APPENGINE_INBOUND_APPID': 'some-app',
    })
    self.assertEqual(
      model.Identity(model.IDENTITY_SERVICE, 'some-app'),
      handler.service_to_service_authentication(request))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/n",1
